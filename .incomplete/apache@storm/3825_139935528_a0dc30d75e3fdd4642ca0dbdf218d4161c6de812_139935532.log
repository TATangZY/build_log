Using worker: worker-linux-docker-d7119792.prod.travis-ci.org:travis-linux-9

travis_fold:start:system_info[0K[33;1mBuild system information[0m
Build language: java
Build group: stable
Build dist: precise
[34m[1mBuild image provisioning date and time[0m
Thu Feb  5 15:09:33 UTC 2015
[34m[1mOperating System Details[0m
Distributor ID:	Ubuntu
Description:	Ubuntu 12.04.5 LTS
Release:	12.04
Codename:	precise
[34m[1mLinux Version[0m
3.13.0-29-generic
[34m[1mCookbooks Version[0m
a68419e https://github.com/travis-ci/travis-cookbooks/tree/a68419e
[34m[1mGCC version[0m
gcc (Ubuntu/Linaro 4.6.3-1ubuntu5) 4.6.3
Copyright (C) 2011 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.

[34m[1mLLVM version[0m
clang version 3.4 (tags/RELEASE_34/final)
Target: x86_64-unknown-linux-gnu
Thread model: posix
[34m[1mPre-installed Ruby versions[0m
ruby-1.9.3-p551
[34m[1mPre-installed Node.js versions[0m
v0.10.36
[34m[1mPre-installed Go versions[0m
1.4.1
[34m[1mRedis version[0m
redis-server 2.8.19
[34m[1mriak version[0m
2.0.2
[34m[1mMongoDB version[0m
MongoDB 2.4.12
[34m[1mCouchDB version[0m
couchdb 1.6.1
[34m[1mNeo4j version[0m
1.9.4
[34m[1mRabbitMQ Version[0m
3.4.3
[34m[1mElasticSearch version[0m
1.4.0
[34m[1mInstalled Sphinx versions[0m
2.0.10
2.1.9
2.2.6
[34m[1mDefault Sphinx version[0m
2.2.6
[34m[1mInstalled Firefox version[0m
firefox 31.0esr
[34m[1mPhantomJS version[0m
1.9.8
[34m[1mant -version[0m
Apache Ant(TM) version 1.8.2 compiled on December 3 2011
[34m[1mmvn -version[0m
Apache Maven 3.2.5 (12a6b3acb947671f09b81f49094c53f426d8cea1; 2014-12-14T17:29:23+00:00)
Maven home: /usr/local/maven
Java version: 1.7.0_76, vendor: Oracle Corporation
Java home: /usr/lib/jvm/java-7-oracle/jre
Default locale: en_US, platform encoding: ANSI_X3.4-1968
OS name: "linux", version: "3.13.0-29-generic", arch: "amd64", family: "unix"
travis_fold:end:system_info[0K
travis_fold:start:fix.CVE-2015-7547[0K$ export DEBIAN_FRONTEND=noninteractive
W: Size of file /var/lib/apt/lists/us.archive.ubuntu.com_ubuntu_dists_precise-backports_multiverse_source_Sources.gz is not what the server reported 5886 5888
W: Size of file /var/lib/apt/lists/ppa.launchpad.net_ubuntugis_ppa_ubuntu_dists_precise_main_binary-amd64_Packages.gz is not what the server reported 36669 36677
W: Size of file /var/lib/apt/lists/ppa.launchpad.net_ubuntugis_ppa_ubuntu_dists_precise_main_binary-i386_Packages.gz is not what the server reported 36729 36733
Reading package lists...
Building dependency tree...
Reading state information...
The following extra packages will be installed:
  libc-bin libc-dev-bin libc6-dev
Suggested packages:
  glibc-doc
The following packages will be upgraded:
  libc-bin libc-dev-bin libc6 libc6-dev
4 upgraded, 0 newly installed, 0 to remove and 247 not upgraded.
Need to get 8,840 kB of archives.
After this operation, 14.3 kB disk space will be freed.
Get:1 http://us.archive.ubuntu.com/ubuntu/ precise-updates/main libc6-dev amd64 2.15-0ubuntu10.15 [2,943 kB]
Get:2 http://us.archive.ubuntu.com/ubuntu/ precise-updates/main libc-dev-bin amd64 2.15-0ubuntu10.15 [84.7 kB]
Get:3 http://us.archive.ubuntu.com/ubuntu/ precise-updates/main libc-bin amd64 2.15-0ubuntu10.15 [1,177 kB]
Get:4 http://us.archive.ubuntu.com/ubuntu/ precise-updates/main libc6 amd64 2.15-0ubuntu10.15 [4,636 kB]
Fetched 8,840 kB in 0s (35.8 MB/s)
Preconfiguring packages ...
(Reading database ... (Reading database ... 5%(Reading database ... 10%(Reading database ... 15%(Reading database ... 20%(Reading database ... 25%(Reading database ... 30%(Reading database ... 35%(Reading database ... 40%(Reading database ... 45%(Reading database ... 50%(Reading database ... 55%(Reading database ... 60%(Reading database ... 65%(Reading database ... 70%(Reading database ... 75%(Reading database ... 80%(Reading database ... 85%(Reading database ... 90%(Reading database ... 95%(Reading database ... 100%(Reading database ... 72019 files and directories currently installed.)
Preparing to replace libc6-dev 2.15-0ubuntu10.10 (using .../libc6-dev_2.15-0ubuntu10.15_amd64.deb) ...
Unpacking replacement libc6-dev ...
Preparing to replace libc-dev-bin 2.15-0ubuntu10.10 (using .../libc-dev-bin_2.15-0ubuntu10.15_amd64.deb) ...
Unpacking replacement libc-dev-bin ...
Preparing to replace libc-bin 2.15-0ubuntu10.10 (using .../libc-bin_2.15-0ubuntu10.15_amd64.deb) ...
Unpacking replacement libc-bin ...
Processing triggers for man-db ...
Setting up libc-bin (2.15-0ubuntu10.15) ...
(Reading database ... (Reading database ... 5%(Reading database ... 10%(Reading database ... 15%(Reading database ... 20%(Reading database ... 25%(Reading database ... 30%(Reading database ... 35%(Reading database ... 40%(Reading database ... 45%(Reading database ... 50%(Reading database ... 55%(Reading database ... 60%(Reading database ... 65%(Reading database ... 70%(Reading database ... 75%(Reading database ... 80%(Reading database ... 85%(Reading database ... 90%(Reading database ... 95%(Reading database ... 100%(Reading database ... 72018 files and directories currently installed.)
Preparing to replace libc6 2.15-0ubuntu10.10 (using .../libc6_2.15-0ubuntu10.15_amd64.deb) ...
Unpacking replacement libc6 ...
Setting up libc6 (2.15-0ubuntu10.15) ...
Setting up libc-dev-bin (2.15-0ubuntu10.15) ...
Setting up libc6-dev (2.15-0ubuntu10.15) ...
Processing triggers for libc-bin ...
ldconfig deferred processing now taking place
travis_fold:end:fix.CVE-2015-7547[0Ktravis_fold:start:git.checkout[0Ktravis_time:start:007aa4e0[0K$ git clone --depth=50 https://github.com/apache/storm.git apache/storm
Cloning into 'apache/storm'...
remote: Counting objects: 17967, done.[K
remote: Compressing objects:   0% (1/7274)   [Kremote: Compressing objects:   1% (73/7274)   [Kremote: Compressing objects:   2% (146/7274)   [Kremote: Compressing objects:   3% (219/7274)   [Kremote: Compressing objects:   4% (291/7274)   [Kremote: Compressing objects:   5% (364/7274)   [Kremote: Compressing objects:   6% (437/7274)   [Kremote: Compressing objects:   7% (510/7274)   [Kremote: Compressing objects:   8% (582/7274)   [Kremote: Compressing objects:   9% (655/7274)   [Kremote: Compressing objects:  10% (728/7274)   [Kremote: Compressing objects:  11% (801/7274)   [Kremote: Compressing objects:  12% (873/7274)   [Kremote: Compressing objects:  13% (946/7274)   [Kremote: Compressing objects:  14% (1019/7274)   [Kremote: Compressing objects:  15% (1092/7274)   [Kremote: Compressing objects:  16% (1164/7274)   [Kremote: Compressing objects:  17% (1237/7274)   [Kremote: Compressing objects:  18% (1310/7274)   [Kremote: Compressing objects:  19% (1383/7274)   [Kremote: Compressing objects:  20% (1455/7274)   [Kremote: Compressing objects:  21% (1528/7274)   [Kremote: Compressing objects:  22% (1601/7274)   [Kremote: Compressing objects:  23% (1674/7274)   [Kremote: Compressing objects:  24% (1746/7274)   [Kremote: Compressing objects:  25% (1819/7274)   [Kremote: Compressing objects:  26% (1892/7274)   [Kremote: Compressing objects:  27% (1964/7274)   [Kremote: Compressing objects:  28% (2037/7274)   [Kremote: Compressing objects:  29% (2110/7274)   [Kremote: Compressing objects:  30% (2183/7274)   [Kremote: Compressing objects:  31% (2255/7274)   [Kremote: Compressing objects:  32% (2328/7274)   [Kremote: Compressing objects:  33% (2401/7274)   [Kremote: Compressing objects:  34% (2474/7274)   [Kremote: Compressing objects:  35% (2546/7274)   [Kremote: Compressing objects:  36% (2619/7274)   [Kremote: Compressing objects:  37% (2692/7274)   [Kremote: Compressing objects:  38% (2765/7274)   [Kremote: Compressing objects:  39% (2837/7274)   [Kremote: Compressing objects:  40% (2910/7274)   [Kremote: Compressing objects:  41% (2983/7274)   [Kremote: Compressing objects:  42% (3056/7274)   [Kremote: Compressing objects:  43% (3128/7274)   [Kremote: Compressing objects:  44% (3201/7274)   [Kremote: Compressing objects:  45% (3274/7274)   [Kremote: Compressing objects:  46% (3347/7274)   [Kremote: Compressing objects:  47% (3419/7274)   [Kremote: Compressing objects:  48% (3492/7274)   [Kremote: Compressing objects:  49% (3565/7274)   [Kremote: Compressing objects:  50% (3637/7274)   [Kremote: Compressing objects:  51% (3710/7274)   [Kremote: Compressing objects:  52% (3783/7274)   [Kremote: Compressing objects:  53% (3856/7274)   [Kremote: Compressing objects:  54% (3928/7274)   [Kremote: Compressing objects:  55% (4001/7274)   [Kremote: Compressing objects:  56% (4074/7274)   [Kremote: Compressing objects:  57% (4147/7274)   [Kremote: Compressing objects:  58% (4219/7274)   [Kremote: Compressing objects:  59% (4292/7274)   [Kremote: Compressing objects:  60% (4365/7274)   [Kremote: Compressing objects:  61% (4438/7274)   [Kremote: Compressing objects:  62% (4510/7274)   [Kremote: Compressing objects:  63% (4583/7274)   [Kremote: Compressing objects:  64% (4656/7274)   [Kremote: Compressing objects:  65% (4729/7274)   [Kremote: Compressing objects:  66% (4801/7274)   [Kremote: Compressing objects:  67% (4874/7274)   [Kremote: Compressing objects:  68% (4947/7274)   [Kremote: Compressing objects:  69% (5020/7274)   [Kremote: Compressing objects:  70% (5092/7274)   [Kremote: Compressing objects:  71% (5165/7274)   [Kremote: Compressing objects:  72% (5238/7274)   [Kremote: Compressing objects:  73% (5311/7274)   [Kremote: Compressing objects:  74% (5383/7274)   [Kremote: Compressing objects:  75% (5456/7274)   [Kremote: Compressing objects:  76% (5529/7274)   [Kremote: Compressing objects:  77% (5601/7274)   [Kremote: Compressing objects:  78% (5674/7274)   [Kremote: Compressing objects:  79% (5747/7274)   [Kremote: Compressing objects:  80% (5820/7274)   [Kremote: Compressing objects:  81% (5892/7274)   [Kremote: Compressing objects:  82% (5965/7274)   [Kremote: Compressing objects:  83% (6038/7274)   [Kremote: Compressing objects:  84% (6111/7274)   [Kremote: Compressing objects:  85% (6183/7274)   [Kremote: Compressing objects:  86% (6256/7274)   [Kremote: Compressing objects:  87% (6329/7274)   [Kremote: Compressing objects:  88% (6402/7274)   [Kremote: Compressing objects:  89% (6474/7274)   [Kremote: Compressing objects:  90% (6547/7274)   [Kremote: Compressing objects:  91% (6620/7274)   [Kremote: Compressing objects:  92% (6693/7274)   [Kremote: Compressing objects:  93% (6765/7274)   [Kremote: Compressing objects:  94% (6838/7274)   [Kremote: Compressing objects:  95% (6911/7274)   [Kremote: Compressing objects:  96% (6984/7274)   [Kremote: Compressing objects:  97% (7056/7274)   [Kremote: Compressing objects:  98% (7129/7274)   [Kremote: Compressing objects:  99% (7202/7274)   [Kremote: Compressing objects: 100% (7274/7274)   [Kremote: Compressing objects: 100% (7274/7274), done.[K
Receiving objects:   0% (1/17967)   Receiving objects:   1% (180/17967)   Receiving objects:   2% (360/17967)   Receiving objects:   3% (540/17967)   Receiving objects:   4% (719/17967)   Receiving objects:   5% (899/17967)   Receiving objects:   6% (1079/17967)   Receiving objects:   7% (1258/17967)   Receiving objects:   8% (1438/17967)   Receiving objects:   9% (1618/17967)   Receiving objects:  10% (1797/17967)   Receiving objects:  11% (1977/17967)   Receiving objects:  12% (2157/17967)   Receiving objects:  13% (2336/17967)   Receiving objects:  14% (2516/17967)   Receiving objects:  15% (2696/17967)   Receiving objects:  16% (2875/17967)   Receiving objects:  17% (3055/17967)   Receiving objects:  18% (3235/17967)   Receiving objects:  19% (3414/17967)   Receiving objects:  20% (3594/17967)   Receiving objects:  21% (3774/17967)   Receiving objects:  22% (3953/17967)   Receiving objects:  23% (4133/17967)   Receiving objects:  24% (4313/17967)   Receiving objects:  25% (4492/17967)   Receiving objects:  26% (4672/17967)   Receiving objects:  27% (4852/17967)   Receiving objects:  28% (5031/17967)   Receiving objects:  29% (5211/17967)   Receiving objects:  30% (5391/17967)   Receiving objects:  31% (5570/17967)   Receiving objects:  32% (5750/17967)   Receiving objects:  33% (5930/17967)   Receiving objects:  34% (6109/17967)   Receiving objects:  35% (6289/17967)   Receiving objects:  36% (6469/17967)   Receiving objects:  37% (6648/17967)   Receiving objects:  38% (6828/17967)   Receiving objects:  39% (7008/17967)   Receiving objects:  40% (7187/17967)   Receiving objects:  41% (7367/17967), 9.77 MiB | 19.49 MiB/s   Receiving objects:  42% (7547/17967), 9.77 MiB | 19.49 MiB/s   Receiving objects:  43% (7726/17967), 9.77 MiB | 19.49 MiB/s   Receiving objects:  44% (7906/17967), 9.77 MiB | 19.49 MiB/s   Receiving objects:  45% (8086/17967), 9.77 MiB | 19.49 MiB/s   Receiving objects:  46% (8265/17967), 9.77 MiB | 19.49 MiB/s   Receiving objects:  47% (8445/17967), 9.77 MiB | 19.49 MiB/s   Receiving objects:  48% (8625/17967), 9.77 MiB | 19.49 MiB/s   Receiving objects:  49% (8804/17967), 9.77 MiB | 19.49 MiB/s   Receiving objects:  50% (8984/17967), 9.77 MiB | 19.49 MiB/s   Receiving objects:  51% (9164/17967), 9.77 MiB | 19.49 MiB/s   Receiving objects:  52% (9343/17967), 9.77 MiB | 19.49 MiB/s   Receiving objects:  53% (9523/17967), 9.77 MiB | 19.49 MiB/s   Receiving objects:  54% (9703/17967), 9.77 MiB | 19.49 MiB/s   Receiving objects:  55% (9882/17967), 9.77 MiB | 19.49 MiB/s   Receiving objects:  56% (10062/17967), 9.77 MiB | 19.49 MiB/s   Receiving objects:  57% (10242/17967), 9.77 MiB | 19.49 MiB/s   Receiving objects:  58% (10421/17967), 9.77 MiB | 19.49 MiB/s   Receiving objects:  59% (10601/17967), 9.77 MiB | 19.49 MiB/s   Receiving objects:  60% (10781/17967), 9.77 MiB | 19.49 MiB/s   Receiving objects:  61% (10960/17967), 9.77 MiB | 19.49 MiB/s   Receiving objects:  62% (11140/17967), 9.77 MiB | 19.49 MiB/s   Receiving objects:  63% (11320/17967), 9.77 MiB | 19.49 MiB/s   Receiving objects:  64% (11499/17967), 9.77 MiB | 19.49 MiB/s   Receiving objects:  65% (11679/17967), 9.77 MiB | 19.49 MiB/s   Receiving objects:  66% (11859/17967), 9.77 MiB | 19.49 MiB/s   Receiving objects:  67% (12038/17967), 9.77 MiB | 19.49 MiB/s   Receiving objects:  68% (12218/17967), 9.77 MiB | 19.49 MiB/s   Receiving objects:  69% (12398/17967), 9.77 MiB | 19.49 MiB/s   Receiving objects:  70% (12577/17967), 9.77 MiB | 19.49 MiB/s   Receiving objects:  71% (12757/17967), 9.77 MiB | 19.49 MiB/s   Receiving objects:  72% (12937/17967), 9.77 MiB | 19.49 MiB/s   Receiving objects:  73% (13116/17967), 9.77 MiB | 19.49 MiB/s   Receiving objects:  74% (13296/17967), 9.77 MiB | 19.49 MiB/s   Receiving objects:  75% (13476/17967), 9.77 MiB | 19.49 MiB/s   Receiving objects:  76% (13655/17967), 9.77 MiB | 19.49 MiB/s   Receiving objects:  77% (13835/17967), 9.77 MiB | 19.49 MiB/s   Receiving objects:  78% (14015/17967), 9.77 MiB | 19.49 MiB/s   Receiving objects:  79% (14194/17967), 9.77 MiB | 19.49 MiB/s   Receiving objects:  80% (14374/17967), 9.77 MiB | 19.49 MiB/s   Receiving objects:  81% (14554/17967), 9.77 MiB | 19.49 MiB/s   Receiving objects:  82% (14733/17967), 9.77 MiB | 19.49 MiB/s   Receiving objects:  83% (14913/17967), 9.77 MiB | 19.49 MiB/s   Receiving objects:  84% (15093/17967), 9.77 MiB | 19.49 MiB/s   Receiving objects:  85% (15272/17967), 9.77 MiB | 19.49 MiB/s   Receiving objects:  86% (15452/17967), 9.77 MiB | 19.49 MiB/s   Receiving objects:  87% (15632/17967), 9.77 MiB | 19.49 MiB/s   Receiving objects:  88% (15811/17967), 9.77 MiB | 19.49 MiB/s   Receiving objects:  89% (15991/17967), 9.77 MiB | 19.49 MiB/s   Receiving objects:  90% (16171/17967), 9.77 MiB | 19.49 MiB/s   Receiving objects:  91% (16350/17967), 9.77 MiB | 19.49 MiB/s   Receiving objects:  92% (16530/17967), 9.77 MiB | 19.49 MiB/s   Receiving objects:  93% (16710/17967), 9.77 MiB | 19.49 MiB/s   Receiving objects:  94% (16889/17967), 9.77 MiB | 19.49 MiB/s   Receiving objects:  95% (17069/17967), 9.77 MiB | 19.49 MiB/s   Receiving objects:  96% (17249/17967), 9.77 MiB | 19.49 MiB/s   Receiving objects:  97% (17428/17967), 9.77 MiB | 19.49 MiB/s   Receiving objects:  98% (17608/17967), 9.77 MiB | 19.49 MiB/s   Receiving objects:  99% (17788/17967), 9.77 MiB | 19.49 MiB/s   remote: Total 17967 (delta 8485), reused 15422 (delta 6583), pack-reused 0[K
Receiving objects: 100% (17967/17967), 9.77 MiB | 19.49 MiB/s   Receiving objects: 100% (17967/17967), 14.39 MiB | 19.49 MiB/s, done.
Resolving deltas:   0% (0/8485)   Resolving deltas:   1% (94/8485)   Resolving deltas:   2% (185/8485)   Resolving deltas:   3% (263/8485)   Resolving deltas:   4% (341/8485)   Resolving deltas:   5% (425/8485)   Resolving deltas:   6% (560/8485)   Resolving deltas:   7% (594/8485)   Resolving deltas:   8% (688/8485)   Resolving deltas:   9% (765/8485)   Resolving deltas:  10% (856/8485)   Resolving deltas:  11% (946/8485)   Resolving deltas:  12% (1019/8485)   Resolving deltas:  13% (1108/8485)   Resolving deltas:  14% (1214/8485)   Resolving deltas:  15% (1273/8485)   Resolving deltas:  16% (1361/8485)   Resolving deltas:  17% (1448/8485)   Resolving deltas:  18% (1534/8485)   Resolving deltas:  19% (1623/8485)   Resolving deltas:  20% (1697/8485)   Resolving deltas:  21% (1782/8485)   Resolving deltas:  22% (1867/8485)   Resolving deltas:  23% (1953/8485)   Resolving deltas:  24% (2044/8485)   Resolving deltas:  25% (2123/8485)   Resolving deltas:  26% (2228/8485)   Resolving deltas:  27% (2291/8485)   Resolving deltas:  28% (2386/8485)   Resolving deltas:  29% (2511/8485)   Resolving deltas:  30% (2553/8485)   Resolving deltas:  32% (2741/8485)   Resolving deltas:  33% (2804/8485)   Resolving deltas:  34% (2890/8485)   Resolving deltas:  35% (2971/8485)   Resolving deltas:  36% (3059/8485)   Resolving deltas:  37% (3149/8485)   Resolving deltas:  38% (3229/8485)   Resolving deltas:  39% (3311/8485)   Resolving deltas:  40% (3394/8485)   Resolving deltas:  41% (3479/8485)   Resolving deltas:  42% (3572/8485)   Resolving deltas:  43% (3649/8485)   Resolving deltas:  44% (3734/8485)   Resolving deltas:  45% (3822/8485)   Resolving deltas:  46% (3904/8485)   Resolving deltas:  47% (3989/8485)   Resolving deltas:  48% (4080/8485)   Resolving deltas:  49% (4159/8485)   Resolving deltas:  50% (4245/8485)   Resolving deltas:  51% (4346/8485)   Resolving deltas:  52% (4419/8485)   Resolving deltas:  53% (4502/8485)   Resolving deltas:  54% (4597/8485)   Resolving deltas:  55% (4667/8485)   Resolving deltas:  56% (4756/8485)   Resolving deltas:  57% (4839/8485)   Resolving deltas:  58% (4922/8485)   Resolving deltas:  59% (5031/8485)   Resolving deltas:  63% (5372/8485)   Resolving deltas:  64% (5433/8485)   Resolving deltas:  66% (5638/8485)   Resolving deltas:  67% (5690/8485)   Resolving deltas:  68% (5772/8485)   Resolving deltas:  71% (6056/8485)   Resolving deltas:  73% (6276/8485)   Resolving deltas:  74% (6348/8485)   Resolving deltas:  77% (6572/8485)   Resolving deltas:  78% (6619/8485)   Resolving deltas:  79% (6705/8485)   Resolving deltas:  80% (6788/8485)   Resolving deltas:  81% (6874/8485)   Resolving deltas:  82% (6983/8485)   Resolving deltas:  83% (7059/8485)   Resolving deltas:  84% (7133/8485)   Resolving deltas:  85% (7216/8485)   Resolving deltas:  86% (7313/8485)   Resolving deltas:  87% (7386/8485)   Resolving deltas:  88% (7472/8485)   Resolving deltas:  89% (7552/8485)   Resolving deltas:  90% (7641/8485)   Resolving deltas:  92% (7861/8485)   Resolving deltas:  93% (7948/8485)   Resolving deltas:  94% (7980/8485)   Resolving deltas:  95% (8061/8485)   Resolving deltas:  96% (8153/8485)   Resolving deltas:  97% (8232/8485)   Resolving deltas:  98% (8318/8485)   Resolving deltas:  99% (8422/8485)   Resolving deltas: 100% (8485/8485)   Resolving deltas: 100% (8485/8485), done.
Checking connectivity... done.

travis_time:end:007aa4e0:start=1466736686598814950,finish=1466736688960083693,duration=2361268743[0K$ cd apache/storm
travis_time:start:07ae7631[0K$ git fetch origin +refs/pull/1515/merge:
remote: Counting objects: 11, done.[K
remote: Compressing objects:  12% (1/8)   [Kremote: Compressing objects:  25% (2/8)   [Kremote: Compressing objects:  37% (3/8)   [Kremote: Compressing objects:  50% (4/8)   [Kremote: Compressing objects:  62% (5/8)   [Kremote: Compressing objects:  75% (6/8)   [Kremote: Compressing objects:  87% (7/8)   [Kremote: Compressing objects: 100% (8/8)   [Kremote: Compressing objects: 100% (8/8), done.[K
remote: Total 11 (delta 6), reused 4 (delta 0), pack-reused 0[K
Unpacking objects:   9% (1/11)   Unpacking objects:  18% (2/11)   Unpacking objects:  27% (3/11)   Unpacking objects:  36% (4/11)   Unpacking objects:  45% (5/11)   Unpacking objects:  54% (6/11)   Unpacking objects:  63% (7/11)   Unpacking objects:  72% (8/11)   Unpacking objects:  81% (9/11)   Unpacking objects:  90% (10/11)   Unpacking objects: 100% (11/11)   Unpacking objects: 100% (11/11), done.
From https://github.com/apache/storm
 * branch            refs/pull/1515/merge -> FETCH_HEAD

travis_time:end:07ae7631:start=1466736688964150791,finish=1466736689255295340,duration=291144549[0K$ git checkout -qf FETCH_HEAD
travis_fold:end:git.checkout[0K
[33;1mThis job is running on container-based infrastructure, which does not allow use of 'sudo', setuid and setguid executables.[0m
[33;1mIf you require sudo, add 'sudo: required' to your .travis.yml[0m
[33;1mSee https://docs.travis-ci.com/user/workers/container-based-infrastructure/ for details.[0m

[33;1mSetting environment variables from .travis.yml[0m
$ export MODULES='!storm-core'

$ jdk_switcher use oraclejdk8
Switching to Oracle JDK8 (java-8-oracle), JAVA_HOME will be set to /usr/lib/jvm/java-8-oracle
travis_fold:start:cache.1[0KSetting up build cache
$ export CASHER_DIR=$HOME/.casher
travis_time:start:2d1f697d[0K$ Installing caching utilities

travis_time:end:2d1f697d:start=1466736691021556078,finish=1466736691081761565,duration=60205487[0Ktravis_time:start:27fb5fa0[0K
travis_time:end:27fb5fa0:start=1466736691086409927,finish=1466736691089584017,duration=3174090[0Ktravis_time:start:07220658[0K[32;1mattempting to download cache archive[0m
[32;1mfetching PR.1515/cache-linux-precise-0b3559b5dae2b3e32156c6d840f23972d650aa066922e3318f7e5db8246681fa--jdk-oraclejdk8.tgz[0m
[32;1mfound cache[0m

travis_time:end:07220658:start=1466736691093331809,finish=1466736700254719503,duration=9161387694[0Ktravis_time:start:22cde4d0[0K
travis_time:end:22cde4d0:start=1466736700258630244,finish=1466736700261972649,duration=3342405[0Ktravis_time:start:2f0fd01e[0K[32;1madding /home/travis/.m2/repository to cache[0m
[32;1madding /home/travis/.rvm to cache[0m
[32;1madding /home/travis/.nvm to cache[0m

travis_time:end:2f0fd01e:start=1466736700265777646,finish=1466736711263977899,duration=10998200253[0Ktravis_fold:end:cache.1[0K$ java -Xmx32m -version
java version "1.8.0_31"
Java(TM) SE Runtime Environment (build 1.8.0_31-b13)
Java HotSpot(TM) 64-Bit Server VM (build 25.31-b07, mixed mode)
$ javac -J-Xmx32m -version
javac 1.8.0_31
travis_fold:start:before_install.1[0Ktravis_time:start:24cfa6ec[0K$ rvm use 2.1.5 --install
[32mUsing /home/travis/.rvm/gems/ruby-2.1.5[0m

travis_time:end:24cfa6ec:start=1466736711666299531,finish=1466736711905392296,duration=239092765[0Ktravis_fold:end:before_install.1[0Ktravis_fold:start:before_install.2[0Ktravis_time:start:08a2f754[0K$ nvm install 0.12.2
v0.12.2 is already installed.
Now using node v0.12.2

travis_time:end:08a2f754:start=1466736711909631740,finish=1466736712168805566,duration=259173826[0Ktravis_fold:end:before_install.2[0Ktravis_fold:start:before_install.3[0Ktravis_time:start:0154d586[0K$ nvm use 0.12.2
Now using node v0.12.2

travis_time:end:0154d586:start=1466736712172981745,finish=1466736712226374229,duration=53392484[0Ktravis_fold:end:before_install.3[0Ktravis_fold:start:install[0Ktravis_time:start:1d846f22[0K$ /bin/bash ./dev-tools/travis/travis-install.sh `pwd`
Python version :   Python 2.7.3
Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=192m; support was removed in 8.0
Maven version  :   Apache Maven 3.2.5 (12a6b3acb947671f09b81f49094c53f426d8cea1; 2014-12-14T17:29:23+00:00) Maven home: /usr/local/maven Java version: 1.8.0_31, vendor: Oracle Corporation Java home: /usr/lib/jvm/java-8-oracle/jre Default locale: en_US, platform encoding: UTF-8 OS name: "linux", version: "3.13.0-40-generic", arch: "amd64", family: "unix"
['mvn', 'clean', 'install', '-DskipTests', '-Pnative', '--batch-mode'] writing to install.txt
1 seconds 2 log lines12 seconds 90 log lines22 seconds 283 log lines54 seconds 324 log lines64 seconds 331 log lines75 seconds 443 log lines94 seconds 653 log lines104 seconds 825 log lines118 seconds 875 log lines128 seconds 915 log lines138 seconds 1053 log lines148 seconds 1341 log lines158 seconds 1640 log lines169 seconds 1781 log lines179 seconds 2188 log lines189 seconds 2259 log lines204 seconds 2632 log lines207 seconds 2685 log lines
['mvn', 'clean', 'install', '-DskipTests', '-Pnative', '--batch-mode'] done 0

travis_time:end:1d846f22:start=1466736712230574749,finish=1466736919843132361,duration=207612557612[0Ktravis_fold:end:install[0Ktravis_time:start:302557db[0K$ /bin/bash ./dev-tools/travis/travis-script.sh `pwd` $MODULES
Python version :   Python 2.7.3
Ruby version   :   ruby 2.1.5p273 (2014-11-13 revision 48405) [x86_64-linux]
NodeJs version :   v0.12.2
Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=192m; support was removed in 8.0
Maven version  :   Apache Maven 3.2.5 (12a6b3acb947671f09b81f49094c53f426d8cea1; 2014-12-14T17:29:23+00:00) Maven home: /usr/local/maven Java version: 1.8.0_31, vendor: Oracle Corporation Java home: /usr/lib/jvm/java-8-oracle/jre Default locale: en_US, platform encoding: UTF-8 OS name: "linux", version: "3.13.0-40-generic", arch: "amd64", family: "unix"
Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=192m; support was removed in 8.0
[INFO] Scanning for projects...
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Build Order:
[INFO] 
[INFO] Storm
[INFO] multilang-javascript
[INFO] multilang-python
[INFO] multilang-ruby
[INFO] maven-shade-clojure-transformer
[INFO] storm-maven-plugins
[INFO] storm-rename-hack
[INFO] storm-kafka
[INFO] storm-hdfs
[INFO] storm-hbase
[INFO] storm-hive
[INFO] storm-jdbc
[INFO] storm-redis
[INFO] storm-eventhubs
[INFO] flux
[INFO] flux-wrappers
[INFO] flux-core
[INFO] flux-examples
[INFO] storm-sql-runtime
[INFO] storm-sql-core
[INFO] storm-sql-kafka
[INFO] sql
[INFO] storm-elasticsearch
[INFO] storm-solr
[INFO] storm-metrics
[INFO] storm-cassandra
[INFO] storm-mqtt-parent
[INFO] storm-mqtt
[INFO] storm-mqtt-examples
[INFO] storm-mongodb
[INFO] storm-clojure
[INFO] storm-starter
[INFO] storm-kafka-client
[INFO] storm-opentsdb
[INFO] storm-kafka-monitor
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Storm 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ storm ---
[INFO] 
[INFO] --- apache-rat-plugin:0.11:check (default) @ storm ---
[INFO] 51 implicit excludes (use -debug for more details).
[INFO] Exclude: **/target/**
[INFO] Exclude: **/*.iml
[INFO] Exclude: **/.idea/**
[INFO] Exclude: **/metastore_db/**
[INFO] Exclude: **/build/**
[INFO] Exclude: **/logs/**
[INFO] Exclude: **/CHANGELOG.md
[INFO] Exclude: **/README.md
[INFO] Exclude: **/README.markdown
[INFO] Exclude: **/DEVELOPER.md
[INFO] Exclude: **/BYLAWS.md
[INFO] Exclude: **/STORM-UI-REST-API.md
[INFO] Exclude: SECURITY.md
[INFO] Exclude: VERSION
[INFO] Exclude: TODO
[INFO] Exclude: **/src/py/**
[INFO] Exclude: **/src/ui/public/js/jquery.dataTables.1.10.4.min.js
[INFO] Exclude: **/src/ui/public/css/jquery.dataTables.1.10.4.min.css
[INFO] Exclude: **/src/ui/public/images/*
[INFO] Exclude: **/src/ui/public/js/bootstrap-3.3.1.min.js
[INFO] Exclude: **/src/ui/public/css/bootstrap-3.3.1.min.css
[INFO] Exclude: **/src/ui/public/js/dataTables.bootstrap.min.js
[INFO] Exclude: **/src/ui/public/css/dataTables.bootstrap.css
[INFO] Exclude: **/src/ui/public/js/jsonFormatter.min.js
[INFO] Exclude: **/src/ui/public/css/jsonFormatter.min.css
[INFO] Exclude: **/src/ui/public/js/jquery-1.11.1.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.cookies.2.2.0.min.js
[INFO] Exclude: **/src/ui/public/js/moment.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.blockUI.min.js
[INFO] Exclude: **/src/ui/public/js/url.min.js
[INFO] Exclude: **/src/ui/public/js/arbor.js
[INFO] Exclude: **/src/ui/public/js/arbor-graphics.js
[INFO] Exclude: **/src/ui/public/js/arbor-tween.js
[INFO] Exclude: **/src/ui/public/js/jquery.mustache.js
[INFO] Exclude: **/src/ui/public/js/typeahead.jquery.min.js
[INFO] Exclude: **/dependency-reduced-pom.xml
[INFO] Exclude: **/docs/**
[INFO] Exclude: **/.git/**
[INFO] Exclude: **/derby.log
[INFO] Exclude: **/src/dev/**
[INFO] Exclude: **/src/codegen/config.fmpp
[INFO] Exclude: **/src/codegen/data/Parser.tdd
[INFO] Exclude: **/src/test/resources/FixedAvroSerializer.config
[INFO] 1850 resources included (use -debug for more details)
Warning:  org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
Compiler warnings:
  WARNING:  'org.apache.xerces.jaxp.SAXParserImpl: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.'
Warning:  org.apache.xerces.parsers.SAXParser: Feature 'http://javax.xml.XMLConstants/feature/secure-processing' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
[INFO] Rat check: Summary of files. Unapproved: 0 unknown: 0 generated: 0 approved: 1837 licence.
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building multilang-javascript 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ multilang-javascript ---
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ multilang-javascript ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 1 resource
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ multilang-javascript ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ multilang-javascript ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/storm-multilang/javascript/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ multilang-javascript ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-surefire-plugin:2.18.1:test (default-test) @ multilang-javascript ---
[INFO] 
[INFO] --- apache-rat-plugin:0.11:check (default) @ multilang-javascript ---
[INFO] 51 implicit excludes (use -debug for more details).
[INFO] Exclude: **/target/**
[INFO] Exclude: **/*.iml
[INFO] Exclude: **/.idea/**
[INFO] Exclude: **/metastore_db/**
[INFO] Exclude: **/build/**
[INFO] Exclude: **/logs/**
[INFO] Exclude: **/CHANGELOG.md
[INFO] Exclude: **/README.md
[INFO] Exclude: **/README.markdown
[INFO] Exclude: **/DEVELOPER.md
[INFO] Exclude: **/BYLAWS.md
[INFO] Exclude: **/STORM-UI-REST-API.md
[INFO] Exclude: SECURITY.md
[INFO] Exclude: VERSION
[INFO] Exclude: TODO
[INFO] Exclude: **/src/py/**
[INFO] Exclude: **/src/ui/public/js/jquery.dataTables.1.10.4.min.js
[INFO] Exclude: **/src/ui/public/css/jquery.dataTables.1.10.4.min.css
[INFO] Exclude: **/src/ui/public/images/*
[INFO] Exclude: **/src/ui/public/js/bootstrap-3.3.1.min.js
[INFO] Exclude: **/src/ui/public/css/bootstrap-3.3.1.min.css
[INFO] Exclude: **/src/ui/public/js/dataTables.bootstrap.min.js
[INFO] Exclude: **/src/ui/public/css/dataTables.bootstrap.css
[INFO] Exclude: **/src/ui/public/js/jsonFormatter.min.js
[INFO] Exclude: **/src/ui/public/css/jsonFormatter.min.css
[INFO] Exclude: **/src/ui/public/js/jquery-1.11.1.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.cookies.2.2.0.min.js
[INFO] Exclude: **/src/ui/public/js/moment.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.blockUI.min.js
[INFO] Exclude: **/src/ui/public/js/url.min.js
[INFO] Exclude: **/src/ui/public/js/arbor.js
[INFO] Exclude: **/src/ui/public/js/arbor-graphics.js
[INFO] Exclude: **/src/ui/public/js/arbor-tween.js
[INFO] Exclude: **/src/ui/public/js/jquery.mustache.js
[INFO] Exclude: **/src/ui/public/js/typeahead.jquery.min.js
[INFO] Exclude: **/dependency-reduced-pom.xml
[INFO] Exclude: **/docs/**
[INFO] Exclude: **/.git/**
[INFO] Exclude: **/derby.log
[INFO] Exclude: **/src/dev/**
[INFO] Exclude: **/src/codegen/config.fmpp
[INFO] Exclude: **/src/codegen/data/Parser.tdd
[INFO] Exclude: **/src/test/resources/FixedAvroSerializer.config
[INFO] 2 resources included (use -debug for more details)
Warning:  org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
Compiler warnings:
  WARNING:  'org.apache.xerces.jaxp.SAXParserImpl: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.'
Warning:  org.apache.xerces.parsers.SAXParser: Feature 'http://javax.xml.XMLConstants/feature/secure-processing' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
[INFO] Rat check: Summary of files. Unapproved: 0 unknown: 0 generated: 0 approved: 2 licence.
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building multilang-python 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ multilang-python ---
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ multilang-python ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 1 resource
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ multilang-python ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ multilang-python ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/storm-multilang/python/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ multilang-python ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-surefire-plugin:2.18.1:test (default-test) @ multilang-python ---
[INFO] 
[INFO] --- apache-rat-plugin:0.11:check (default) @ multilang-python ---
[INFO] 51 implicit excludes (use -debug for more details).
[INFO] Exclude: **/target/**
[INFO] Exclude: **/*.iml
[INFO] Exclude: **/.idea/**
[INFO] Exclude: **/metastore_db/**
[INFO] Exclude: **/build/**
[INFO] Exclude: **/logs/**
[INFO] Exclude: **/CHANGELOG.md
[INFO] Exclude: **/README.md
[INFO] Exclude: **/README.markdown
[INFO] Exclude: **/DEVELOPER.md
[INFO] Exclude: **/BYLAWS.md
[INFO] Exclude: **/STORM-UI-REST-API.md
[INFO] Exclude: SECURITY.md
[INFO] Exclude: VERSION
[INFO] Exclude: TODO
[INFO] Exclude: **/src/py/**
[INFO] Exclude: **/src/ui/public/js/jquery.dataTables.1.10.4.min.js
[INFO] Exclude: **/src/ui/public/css/jquery.dataTables.1.10.4.min.css
[INFO] Exclude: **/src/ui/public/images/*
[INFO] Exclude: **/src/ui/public/js/bootstrap-3.3.1.min.js
[INFO] Exclude: **/src/ui/public/css/bootstrap-3.3.1.min.css
[INFO] Exclude: **/src/ui/public/js/dataTables.bootstrap.min.js
[INFO] Exclude: **/src/ui/public/css/dataTables.bootstrap.css
[INFO] Exclude: **/src/ui/public/js/jsonFormatter.min.js
[INFO] Exclude: **/src/ui/public/css/jsonFormatter.min.css
[INFO] Exclude: **/src/ui/public/js/jquery-1.11.1.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.cookies.2.2.0.min.js
[INFO] Exclude: **/src/ui/public/js/moment.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.blockUI.min.js
[INFO] Exclude: **/src/ui/public/js/url.min.js
[INFO] Exclude: **/src/ui/public/js/arbor.js
[INFO] Exclude: **/src/ui/public/js/arbor-graphics.js
[INFO] Exclude: **/src/ui/public/js/arbor-tween.js
[INFO] Exclude: **/src/ui/public/js/jquery.mustache.js
[INFO] Exclude: **/src/ui/public/js/typeahead.jquery.min.js
[INFO] Exclude: **/dependency-reduced-pom.xml
[INFO] Exclude: **/docs/**
[INFO] Exclude: **/.git/**
[INFO] Exclude: **/derby.log
[INFO] Exclude: **/src/dev/**
[INFO] Exclude: **/src/codegen/config.fmpp
[INFO] Exclude: **/src/codegen/data/Parser.tdd
[INFO] Exclude: **/src/test/resources/FixedAvroSerializer.config
[INFO] 2 resources included (use -debug for more details)
Warning:  org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
Compiler warnings:
  WARNING:  'org.apache.xerces.jaxp.SAXParserImpl: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.'
Warning:  org.apache.xerces.parsers.SAXParser: Feature 'http://javax.xml.XMLConstants/feature/secure-processing' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
[INFO] Rat check: Summary of files. Unapproved: 0 unknown: 0 generated: 0 approved: 2 licence.
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building multilang-ruby 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ multilang-ruby ---
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ multilang-ruby ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 1 resource
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ multilang-ruby ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ multilang-ruby ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/storm-multilang/ruby/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ multilang-ruby ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-surefire-plugin:2.18.1:test (default-test) @ multilang-ruby ---
[INFO] 
[INFO] --- apache-rat-plugin:0.11:check (default) @ multilang-ruby ---
[INFO] 51 implicit excludes (use -debug for more details).
[INFO] Exclude: **/target/**
[INFO] Exclude: **/*.iml
[INFO] Exclude: **/.idea/**
[INFO] Exclude: **/metastore_db/**
[INFO] Exclude: **/build/**
[INFO] Exclude: **/logs/**
[INFO] Exclude: **/CHANGELOG.md
[INFO] Exclude: **/README.md
[INFO] Exclude: **/README.markdown
[INFO] Exclude: **/DEVELOPER.md
[INFO] Exclude: **/BYLAWS.md
[INFO] Exclude: **/STORM-UI-REST-API.md
[INFO] Exclude: SECURITY.md
[INFO] Exclude: VERSION
[INFO] Exclude: TODO
[INFO] Exclude: **/src/py/**
[INFO] Exclude: **/src/ui/public/js/jquery.dataTables.1.10.4.min.js
[INFO] Exclude: **/src/ui/public/css/jquery.dataTables.1.10.4.min.css
[INFO] Exclude: **/src/ui/public/images/*
[INFO] Exclude: **/src/ui/public/js/bootstrap-3.3.1.min.js
[INFO] Exclude: **/src/ui/public/css/bootstrap-3.3.1.min.css
[INFO] Exclude: **/src/ui/public/js/dataTables.bootstrap.min.js
[INFO] Exclude: **/src/ui/public/css/dataTables.bootstrap.css
[INFO] Exclude: **/src/ui/public/js/jsonFormatter.min.js
[INFO] Exclude: **/src/ui/public/css/jsonFormatter.min.css
[INFO] Exclude: **/src/ui/public/js/jquery-1.11.1.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.cookies.2.2.0.min.js
[INFO] Exclude: **/src/ui/public/js/moment.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.blockUI.min.js
[INFO] Exclude: **/src/ui/public/js/url.min.js
[INFO] Exclude: **/src/ui/public/js/arbor.js
[INFO] Exclude: **/src/ui/public/js/arbor-graphics.js
[INFO] Exclude: **/src/ui/public/js/arbor-tween.js
[INFO] Exclude: **/src/ui/public/js/jquery.mustache.js
[INFO] Exclude: **/src/ui/public/js/typeahead.jquery.min.js
[INFO] Exclude: **/dependency-reduced-pom.xml
[INFO] Exclude: **/docs/**
[INFO] Exclude: **/.git/**
[INFO] Exclude: **/derby.log
[INFO] Exclude: **/src/dev/**
[INFO] Exclude: **/src/codegen/config.fmpp
[INFO] Exclude: **/src/codegen/data/Parser.tdd
[INFO] Exclude: **/src/test/resources/FixedAvroSerializer.config
[INFO] 2 resources included (use -debug for more details)
Warning:  org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
Compiler warnings:
  WARNING:  'org.apache.xerces.jaxp.SAXParserImpl: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.'
Warning:  org.apache.xerces.parsers.SAXParser: Feature 'http://javax.xml.XMLConstants/feature/secure-processing' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
[INFO] Rat check: Summary of files. Unapproved: 0 unknown: 0 generated: 0 approved: 2 licence.
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building maven-shade-clojure-transformer 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ maven-shade-clojure-transformer ---
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ maven-shade-clojure-transformer ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/storm-buildtools/maven-shade-clojure-transformer/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ maven-shade-clojure-transformer ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ maven-shade-clojure-transformer ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/storm-buildtools/maven-shade-clojure-transformer/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ maven-shade-clojure-transformer ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-surefire-plugin:2.18.1:test (default-test) @ maven-shade-clojure-transformer ---
[INFO] 
[INFO] --- apache-rat-plugin:0.11:check (default) @ maven-shade-clojure-transformer ---
[INFO] 51 implicit excludes (use -debug for more details).
[INFO] Exclude: **/target/**
[INFO] Exclude: **/*.iml
[INFO] Exclude: **/.idea/**
[INFO] Exclude: **/metastore_db/**
[INFO] Exclude: **/build/**
[INFO] Exclude: **/logs/**
[INFO] Exclude: **/CHANGELOG.md
[INFO] Exclude: **/README.md
[INFO] Exclude: **/README.markdown
[INFO] Exclude: **/DEVELOPER.md
[INFO] Exclude: **/BYLAWS.md
[INFO] Exclude: **/STORM-UI-REST-API.md
[INFO] Exclude: SECURITY.md
[INFO] Exclude: VERSION
[INFO] Exclude: TODO
[INFO] Exclude: **/src/py/**
[INFO] Exclude: **/src/ui/public/js/jquery.dataTables.1.10.4.min.js
[INFO] Exclude: **/src/ui/public/css/jquery.dataTables.1.10.4.min.css
[INFO] Exclude: **/src/ui/public/images/*
[INFO] Exclude: **/src/ui/public/js/bootstrap-3.3.1.min.js
[INFO] Exclude: **/src/ui/public/css/bootstrap-3.3.1.min.css
[INFO] Exclude: **/src/ui/public/js/dataTables.bootstrap.min.js
[INFO] Exclude: **/src/ui/public/css/dataTables.bootstrap.css
[INFO] Exclude: **/src/ui/public/js/jsonFormatter.min.js
[INFO] Exclude: **/src/ui/public/css/jsonFormatter.min.css
[INFO] Exclude: **/src/ui/public/js/jquery-1.11.1.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.cookies.2.2.0.min.js
[INFO] Exclude: **/src/ui/public/js/moment.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.blockUI.min.js
[INFO] Exclude: **/src/ui/public/js/url.min.js
[INFO] Exclude: **/src/ui/public/js/arbor.js
[INFO] Exclude: **/src/ui/public/js/arbor-graphics.js
[INFO] Exclude: **/src/ui/public/js/arbor-tween.js
[INFO] Exclude: **/src/ui/public/js/jquery.mustache.js
[INFO] Exclude: **/src/ui/public/js/typeahead.jquery.min.js
[INFO] Exclude: **/dependency-reduced-pom.xml
[INFO] Exclude: **/docs/**
[INFO] Exclude: **/.git/**
[INFO] Exclude: **/derby.log
[INFO] Exclude: **/src/dev/**
[INFO] Exclude: **/src/codegen/config.fmpp
[INFO] Exclude: **/src/codegen/data/Parser.tdd
[INFO] Exclude: **/src/test/resources/FixedAvroSerializer.config
[INFO] 2 resources included (use -debug for more details)
Warning:  org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
Compiler warnings:
  WARNING:  'org.apache.xerces.jaxp.SAXParserImpl: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.'
Warning:  org.apache.xerces.parsers.SAXParser: Feature 'http://javax.xml.XMLConstants/feature/secure-processing' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
[INFO] Rat check: Summary of files. Unapproved: 0 unknown: 0 generated: 0 approved: 2 licence.
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building storm-maven-plugins 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ storm-maven-plugins ---
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ storm-maven-plugins ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/storm-buildtools/storm-maven-plugins/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ storm-maven-plugins ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-plugin-plugin:3.0:descriptor (default-descriptor) @ storm-maven-plugins ---
[INFO] Using 'UTF-8' encoding to read mojo metadata.
[INFO] Applying mojo extractor for language: java
[INFO] Mojo extractor for language: java found 0 mojo descriptors.
[INFO] Applying mojo extractor for language: bsh
[INFO] Mojo extractor for language: bsh found 0 mojo descriptors.
[INFO] Applying mojo extractor for language: java-annotations
[INFO] Mojo extractor for language: java-annotations found 1 mojo descriptors.
[INFO] 
[INFO] --- maven-plugin-plugin:3.0:descriptor (mojo-descriptor) @ storm-maven-plugins ---
[INFO] Using 'UTF-8' encoding to read mojo metadata.
[INFO] Applying mojo extractor for language: java
[INFO] Mojo extractor for language: java found 0 mojo descriptors.
[INFO] Applying mojo extractor for language: bsh
[INFO] Mojo extractor for language: bsh found 0 mojo descriptors.
[INFO] Applying mojo extractor for language: java-annotations
[INFO] Mojo extractor for language: java-annotations found 1 mojo descriptors.
[INFO] 
[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ storm-maven-plugins ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/storm-buildtools/storm-maven-plugins/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ storm-maven-plugins ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-surefire-plugin:2.18.1:test (default-test) @ storm-maven-plugins ---
[INFO] 
[INFO] --- apache-rat-plugin:0.11:check (default) @ storm-maven-plugins ---
[INFO] 51 implicit excludes (use -debug for more details).
[INFO] Exclude: **/target/**
[INFO] Exclude: **/*.iml
[INFO] Exclude: **/.idea/**
[INFO] Exclude: **/metastore_db/**
[INFO] Exclude: **/build/**
[INFO] Exclude: **/logs/**
[INFO] Exclude: **/CHANGELOG.md
[INFO] Exclude: **/README.md
[INFO] Exclude: **/README.markdown
[INFO] Exclude: **/DEVELOPER.md
[INFO] Exclude: **/BYLAWS.md
[INFO] Exclude: **/STORM-UI-REST-API.md
[INFO] Exclude: SECURITY.md
[INFO] Exclude: VERSION
[INFO] Exclude: TODO
[INFO] Exclude: **/src/py/**
[INFO] Exclude: **/src/ui/public/js/jquery.dataTables.1.10.4.min.js
[INFO] Exclude: **/src/ui/public/css/jquery.dataTables.1.10.4.min.css
[INFO] Exclude: **/src/ui/public/images/*
[INFO] Exclude: **/src/ui/public/js/bootstrap-3.3.1.min.js
[INFO] Exclude: **/src/ui/public/css/bootstrap-3.3.1.min.css
[INFO] Exclude: **/src/ui/public/js/dataTables.bootstrap.min.js
[INFO] Exclude: **/src/ui/public/css/dataTables.bootstrap.css
[INFO] Exclude: **/src/ui/public/js/jsonFormatter.min.js
[INFO] Exclude: **/src/ui/public/css/jsonFormatter.min.css
[INFO] Exclude: **/src/ui/public/js/jquery-1.11.1.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.cookies.2.2.0.min.js
[INFO] Exclude: **/src/ui/public/js/moment.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.blockUI.min.js
[INFO] Exclude: **/src/ui/public/js/url.min.js
[INFO] Exclude: **/src/ui/public/js/arbor.js
[INFO] Exclude: **/src/ui/public/js/arbor-graphics.js
[INFO] Exclude: **/src/ui/public/js/arbor-tween.js
[INFO] Exclude: **/src/ui/public/js/jquery.mustache.js
[INFO] Exclude: **/src/ui/public/js/typeahead.jquery.min.js
[INFO] Exclude: **/dependency-reduced-pom.xml
[INFO] Exclude: **/docs/**
[INFO] Exclude: **/.git/**
[INFO] Exclude: **/derby.log
[INFO] Exclude: **/src/dev/**
[INFO] Exclude: **/src/codegen/config.fmpp
[INFO] Exclude: **/src/codegen/data/Parser.tdd
[INFO] Exclude: **/src/test/resources/FixedAvroSerializer.config
[INFO] 3 resources included (use -debug for more details)
Warning:  org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
Compiler warnings:
  WARNING:  'org.apache.xerces.jaxp.SAXParserImpl: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.'
Warning:  org.apache.xerces.parsers.SAXParser: Feature 'http://javax.xml.XMLConstants/feature/secure-processing' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
[INFO] Rat check: Summary of files. Unapproved: 0 unknown: 0 generated: 0 approved: 3 licence.
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building storm-rename-hack 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] Downloading: https://repository.apache.org/snapshots/org/apache/storm/storm-core/2.0.0-SNAPSHOT/maven-metadata.xml
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ storm-rename-hack ---
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ storm-rename-hack ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/storm-rename-hack/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ storm-rename-hack ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ storm-rename-hack ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/storm-rename-hack/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ storm-rename-hack ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-surefire-plugin:2.18.1:test (default-test) @ storm-rename-hack ---
[INFO] 
[INFO] --- apache-rat-plugin:0.11:check (default) @ storm-rename-hack ---
[INFO] 51 implicit excludes (use -debug for more details).
[INFO] Exclude: **/target/**
[INFO] Exclude: **/*.iml
[INFO] Exclude: **/.idea/**
[INFO] Exclude: **/metastore_db/**
[INFO] Exclude: **/build/**
[INFO] Exclude: **/logs/**
[INFO] Exclude: **/CHANGELOG.md
[INFO] Exclude: **/README.md
[INFO] Exclude: **/README.markdown
[INFO] Exclude: **/DEVELOPER.md
[INFO] Exclude: **/BYLAWS.md
[INFO] Exclude: **/STORM-UI-REST-API.md
[INFO] Exclude: SECURITY.md
[INFO] Exclude: VERSION
[INFO] Exclude: TODO
[INFO] Exclude: **/src/py/**
[INFO] Exclude: **/src/ui/public/js/jquery.dataTables.1.10.4.min.js
[INFO] Exclude: **/src/ui/public/css/jquery.dataTables.1.10.4.min.css
[INFO] Exclude: **/src/ui/public/images/*
[INFO] Exclude: **/src/ui/public/js/bootstrap-3.3.1.min.js
[INFO] Exclude: **/src/ui/public/css/bootstrap-3.3.1.min.css
[INFO] Exclude: **/src/ui/public/js/dataTables.bootstrap.min.js
[INFO] Exclude: **/src/ui/public/css/dataTables.bootstrap.css
[INFO] Exclude: **/src/ui/public/js/jsonFormatter.min.js
[INFO] Exclude: **/src/ui/public/css/jsonFormatter.min.css
[INFO] Exclude: **/src/ui/public/js/jquery-1.11.1.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.cookies.2.2.0.min.js
[INFO] Exclude: **/src/ui/public/js/moment.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.blockUI.min.js
[INFO] Exclude: **/src/ui/public/js/url.min.js
[INFO] Exclude: **/src/ui/public/js/arbor.js
[INFO] Exclude: **/src/ui/public/js/arbor-graphics.js
[INFO] Exclude: **/src/ui/public/js/arbor-tween.js
[INFO] Exclude: **/src/ui/public/js/jquery.mustache.js
[INFO] Exclude: **/src/ui/public/js/typeahead.jquery.min.js
[INFO] Exclude: **/dependency-reduced-pom.xml
[INFO] Exclude: **/docs/**
[INFO] Exclude: **/.git/**
[INFO] Exclude: **/derby.log
[INFO] Exclude: **/src/dev/**
[INFO] Exclude: **/src/codegen/config.fmpp
[INFO] Exclude: **/src/codegen/data/Parser.tdd
[INFO] Exclude: **/src/test/resources/FixedAvroSerializer.config
[INFO] 10 resources included (use -debug for more details)
Warning:  org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
Compiler warnings:
  WARNING:  'org.apache.xerces.jaxp.SAXParserImpl: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.'
Warning:  org.apache.xerces.parsers.SAXParser: Feature 'http://javax.xml.XMLConstants/feature/secure-processing' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
[INFO] Rat check: Summary of files. Unapproved: 0 unknown: 0 generated: 0 approved: 10 licence.
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building storm-kafka 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.2.201409121644:prepare-agent (jacoco-initialize) @ storm-kafka ---
[INFO] argLine set to -javaagent:/home/travis/.m2/repository/org/jacoco/org.jacoco.agent/0.7.2.201409121644/org.jacoco.agent-0.7.2.201409121644-runtime.jar=destfile=/home/travis/build/apache/storm/external/storm-kafka/target/jacoco.exec
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ storm-kafka ---
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ storm-kafka ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/storm-kafka/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ storm-kafka ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ storm-kafka ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/storm-kafka/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ storm-kafka ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:2.18.1:test (default-test) @ storm-kafka ---
[INFO] Surefire report directory: /home/travis/build/apache/storm/external/storm-kafka/target/surefire-reports
[INFO] parallel='none', perCoreThreadCount=true, threadCount=0, useUnlimitedThreads=false, threadCountSuites=0, threadCountClasses=0, threadCountMethods=0, parallelOptimized=true

-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running org.apache.storm.kafka.TestStringScheme
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.839 sec - in org.apache.storm.kafka.TestStringScheme
Running org.apache.storm.kafka.StringKeyValueSchemeTest
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.082 sec - in org.apache.storm.kafka.StringKeyValueSchemeTest
Running org.apache.storm.kafka.DynamicBrokersReaderTest
Tests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 8.384 sec - in org.apache.storm.kafka.DynamicBrokersReaderTest
Running org.apache.storm.kafka.KafkaUtilsTest
Tests run: 18, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 30.489 sec - in org.apache.storm.kafka.KafkaUtilsTest
Running org.apache.storm.kafka.KafkaErrorTest
Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.004 sec - in org.apache.storm.kafka.KafkaErrorTest
Running org.apache.storm.kafka.bolt.KafkaBoltTest
Tests run: 8, Failures: 0, Errors: 7, Skipped: 0, Time elapsed: 11.453 sec <<< FAILURE! - in org.apache.storm.kafka.bolt.KafkaBoltTest
executeWithBrokerDown(org.apache.storm.kafka.bolt.KafkaBoltTest)  Time elapsed: 1.469 sec  <<< ERROR!
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.kafka.bolt.KafkaBoltTest.generateTestTuple(KafkaBoltTest.java:301)
	at org.apache.storm.kafka.bolt.KafkaBoltTest.executeWithBrokerDown(KafkaBoltTest.java:266)

executeWithoutKey(org.apache.storm.kafka.bolt.KafkaBoltTest)  Time elapsed: 1.495 sec  <<< ERROR!
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.kafka.bolt.KafkaBoltTest.generateTestTuple(KafkaBoltTest.java:301)
	at org.apache.storm.kafka.bolt.KafkaBoltTest.executeWithoutKey(KafkaBoltTest.java:255)

executeWithByteArrayKeyAndMessageFire(org.apache.storm.kafka.bolt.KafkaBoltTest)  Time elapsed: 1.464 sec  <<< ERROR!
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.kafka.bolt.KafkaBoltTest.generateTestTuple(KafkaBoltTest.java:290)
	at org.apache.storm.kafka.bolt.KafkaBoltTest.executeWithByteArrayKeyAndMessageFire(KafkaBoltTest.java:176)

executeWithByteArrayKeyAndMessageSync(org.apache.storm.kafka.bolt.KafkaBoltTest)  Time elapsed: 1.424 sec  <<< ERROR!
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.kafka.bolt.KafkaBoltTest.generateTestTuple(KafkaBoltTest.java:290)
	at org.apache.storm.kafka.bolt.KafkaBoltTest.executeWithByteArrayKeyAndMessageSync(KafkaBoltTest.java:131)

executeWithByteArrayKeyAndMessageAsync(org.apache.storm.kafka.bolt.KafkaBoltTest)  Time elapsed: 1.472 sec  <<< ERROR!
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.kafka.bolt.KafkaBoltTest.generateTestTuple(KafkaBoltTest.java:290)
	at org.apache.storm.kafka.bolt.KafkaBoltTest.executeWithByteArrayKeyAndMessageAsync(KafkaBoltTest.java:146)

executeWithKey(org.apache.storm.kafka.bolt.KafkaBoltTest)  Time elapsed: 1.238 sec  <<< ERROR!
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.kafka.bolt.KafkaBoltTest.generateTestTuple(KafkaBoltTest.java:290)
	at org.apache.storm.kafka.bolt.KafkaBoltTest.executeWithKey(KafkaBoltTest.java:115)

executeWithBoltSpecifiedProperties(org.apache.storm.kafka.bolt.KafkaBoltTest)  Time elapsed: 1.429 sec  <<< ERROR!
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.kafka.bolt.KafkaBoltTest.generateTestTuple(KafkaBoltTest.java:290)
	at org.apache.storm.kafka.bolt.KafkaBoltTest.executeWithBoltSpecifiedProperties(KafkaBoltTest.java:199)

Running org.apache.storm.kafka.ExponentialBackoffMsgRetryManagerTest
Tests run: 12, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 2.743 sec - in org.apache.storm.kafka.ExponentialBackoffMsgRetryManagerTest
Running org.apache.storm.kafka.ZkCoordinatorTest
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 5.048 sec - in org.apache.storm.kafka.ZkCoordinatorTest
Running org.apache.storm.kafka.TridentKafkaTest
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.686 sec - in org.apache.storm.kafka.TridentKafkaTest

Results :

Tests in error: 
  KafkaBoltTest.executeWithBoltSpecifiedProperties:199->generateTestTuple:290 » IllegalArgument
  KafkaBoltTest.executeWithBrokerDown:266->generateTestTuple:301 » IllegalArgument
  KafkaBoltTest.executeWithByteArrayKeyAndMessageAsync:146->generateTestTuple:290 » IllegalArgument
  KafkaBoltTest.executeWithByteArrayKeyAndMessageFire:176->generateTestTuple:290 » IllegalArgument
  KafkaBoltTest.executeWithByteArrayKeyAndMessageSync:131->generateTestTuple:290 » IllegalArgument
  KafkaBoltTest.executeWithKey:115->generateTestTuple:290 » IllegalArgument Spou...
  KafkaBoltTest.executeWithoutKey:255->generateTestTuple:301 » IllegalArgument S...

Tests run: 57, Failures: 0, Errors: 7, Skipped: 0

[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building storm-hdfs 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ storm-hdfs ---
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ storm-hdfs ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/storm-hdfs/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:2.5.1:compile (default-compile) @ storm-hdfs ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ storm-hdfs ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 2 resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:2.5.1:testCompile (default-testCompile) @ storm-hdfs ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:2.18.1:test (default-test) @ storm-hdfs ---
[INFO] Surefire report directory: /home/travis/build/apache/storm/external/storm-hdfs/target/surefire-reports
[INFO] parallel='none', perCoreThreadCount=true, threadCount=0, useUnlimitedThreads=false, threadCountSuites=0, threadCountClasses=0, threadCountMethods=0, parallelOptimized=true

-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running org.apache.storm.hdfs.trident.HdfsStateTest
Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.537 sec - in org.apache.storm.hdfs.trident.HdfsStateTest
Running org.apache.storm.hdfs.bolt.TestWritersMap
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0 sec - in org.apache.storm.hdfs.bolt.TestWritersMap
Running org.apache.storm.hdfs.bolt.TestHdfsBolt
Tests run: 6, Failures: 0, Errors: 6, Skipped: 0, Time elapsed: 0.177 sec <<< FAILURE! - in org.apache.storm.hdfs.bolt.TestHdfsBolt
testTwoTuplesTwoFiles(org.apache.storm.hdfs.bolt.TestHdfsBolt)  Time elapsed: 0.005 sec  <<< ERROR!
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hdfs.bolt.TestHdfsBolt.generateTestTuple(TestHdfsBolt.java:243)
	at org.apache.storm.hdfs.bolt.TestHdfsBolt.<init>(TestHdfsBolt.java:69)

testPartitionedOutput(org.apache.storm.hdfs.bolt.TestHdfsBolt)  Time elapsed: 0.002 sec  <<< ERROR!
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hdfs.bolt.TestHdfsBolt.generateTestTuple(TestHdfsBolt.java:243)
	at org.apache.storm.hdfs.bolt.TestHdfsBolt.<init>(TestHdfsBolt.java:69)

testTwoTuplesOneFile(org.apache.storm.hdfs.bolt.TestHdfsBolt)  Time elapsed: 0 sec  <<< ERROR!
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hdfs.bolt.TestHdfsBolt.generateTestTuple(TestHdfsBolt.java:243)
	at org.apache.storm.hdfs.bolt.TestHdfsBolt.<init>(TestHdfsBolt.java:69)

testFailedSync(org.apache.storm.hdfs.bolt.TestHdfsBolt)  Time elapsed: 0 sec  <<< ERROR!
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hdfs.bolt.TestHdfsBolt.generateTestTuple(TestHdfsBolt.java:243)
	at org.apache.storm.hdfs.bolt.TestHdfsBolt.<init>(TestHdfsBolt.java:69)

testTickTuples(org.apache.storm.hdfs.bolt.TestHdfsBolt)  Time elapsed: 0 sec  <<< ERROR!
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hdfs.bolt.TestHdfsBolt.generateTestTuple(TestHdfsBolt.java:243)
	at org.apache.storm.hdfs.bolt.TestHdfsBolt.<init>(TestHdfsBolt.java:69)

testFailureFilecount(org.apache.storm.hdfs.bolt.TestHdfsBolt)  Time elapsed: 0 sec  <<< ERROR!
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hdfs.bolt.TestHdfsBolt.generateTestTuple(TestHdfsBolt.java:243)
	at org.apache.storm.hdfs.bolt.TestHdfsBolt.<init>(TestHdfsBolt.java:69)

Running org.apache.storm.hdfs.bolt.AvroGenericRecordBoltTest
Tests run: 5, Failures: 0, Errors: 5, Skipped: 0, Time elapsed: 0.003 sec <<< FAILURE! - in org.apache.storm.hdfs.bolt.AvroGenericRecordBoltTest
schemaThrashing(org.apache.storm.hdfs.bolt.AvroGenericRecordBoltTest)  Time elapsed: 0.001 sec  <<< ERROR!
java.lang.ExceptionInInitializerError: null
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hdfs.bolt.AvroGenericRecordBoltTest.generateTestTuple(AvroGenericRecordBoltTest.java:220)
	at org.apache.storm.hdfs.bolt.AvroGenericRecordBoltTest.<clinit>(AvroGenericRecordBoltTest.java:95)

forwardSchemaChangeWorks(org.apache.storm.hdfs.bolt.AvroGenericRecordBoltTest)  Time elapsed: 0 sec  <<< ERROR!
java.lang.NoClassDefFoundError: Could not initialize class org.apache.storm.hdfs.bolt.AvroGenericRecordBoltTest
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:408)
	at org.junit.runners.BlockJUnit4ClassRunner.createTest(BlockJUnit4ClassRunner.java:195)
	at org.junit.runners.BlockJUnit4ClassRunner$1.runReflectiveCall(BlockJUnit4ClassRunner.java:244)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.BlockJUnit4ClassRunner.methodBlock(BlockJUnit4ClassRunner.java:241)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:108)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:78)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:54)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:144)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:203)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:155)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:103)

multipleTuplesOneFile(org.apache.storm.hdfs.bolt.AvroGenericRecordBoltTest)  Time elapsed: 0.001 sec  <<< ERROR!
java.lang.NoClassDefFoundError: Could not initialize class org.apache.storm.hdfs.bolt.AvroGenericRecordBoltTest
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:408)
	at org.junit.runners.BlockJUnit4ClassRunner.createTest(BlockJUnit4ClassRunner.java:195)
	at org.junit.runners.BlockJUnit4ClassRunner$1.runReflectiveCall(BlockJUnit4ClassRunner.java:244)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.BlockJUnit4ClassRunner.methodBlock(BlockJUnit4ClassRunner.java:241)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:108)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:78)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:54)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:144)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:203)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:155)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:103)

multipleTuplesMutliplesFiles(org.apache.storm.hdfs.bolt.AvroGenericRecordBoltTest)  Time elapsed: 0 sec  <<< ERROR!
java.lang.NoClassDefFoundError: Could not initialize class org.apache.storm.hdfs.bolt.AvroGenericRecordBoltTest
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:408)
	at org.junit.runners.BlockJUnit4ClassRunner.createTest(BlockJUnit4ClassRunner.java:195)
	at org.junit.runners.BlockJUnit4ClassRunner$1.runReflectiveCall(BlockJUnit4ClassRunner.java:244)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.BlockJUnit4ClassRunner.methodBlock(BlockJUnit4ClassRunner.java:241)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:108)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:78)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:54)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:144)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:203)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:155)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:103)

backwardSchemaChangeWorks(org.apache.storm.hdfs.bolt.AvroGenericRecordBoltTest)  Time elapsed: 0.001 sec  <<< ERROR!
java.lang.NoClassDefFoundError: Could not initialize class org.apache.storm.hdfs.bolt.AvroGenericRecordBoltTest
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:408)
	at org.junit.runners.BlockJUnit4ClassRunner.createTest(BlockJUnit4ClassRunner.java:195)
	at org.junit.runners.BlockJUnit4ClassRunner$1.runReflectiveCall(BlockJUnit4ClassRunner.java:244)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.BlockJUnit4ClassRunner.methodBlock(BlockJUnit4ClassRunner.java:241)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:108)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:78)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:54)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:144)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:203)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:155)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:103)

Running org.apache.storm.hdfs.bolt.TestSequenceFileBolt
Tests run: 3, Failures: 0, Errors: 3, Skipped: 0, Time elapsed: 4.075 sec <<< FAILURE! - in org.apache.storm.hdfs.bolt.TestSequenceFileBolt
testTwoTuplesTwoFiles(org.apache.storm.hdfs.bolt.TestSequenceFileBolt)  Time elapsed: 0 sec  <<< ERROR!
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hdfs.bolt.TestSequenceFileBolt.generateTestTuple(TestSequenceFileBolt.java:161)
	at org.apache.storm.hdfs.bolt.TestSequenceFileBolt.<init>(TestSequenceFileBolt.java:68)

testTwoTuplesOneFile(org.apache.storm.hdfs.bolt.TestSequenceFileBolt)  Time elapsed: 0 sec  <<< ERROR!
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hdfs.bolt.TestSequenceFileBolt.generateTestTuple(TestSequenceFileBolt.java:161)
	at org.apache.storm.hdfs.bolt.TestSequenceFileBolt.<init>(TestSequenceFileBolt.java:68)

testFailedSync(org.apache.storm.hdfs.bolt.TestSequenceFileBolt)  Time elapsed: 0.003 sec  <<< ERROR!
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hdfs.bolt.TestSequenceFileBolt.generateTestTuple(TestSequenceFileBolt.java:161)
	at org.apache.storm.hdfs.bolt.TestSequenceFileBolt.<init>(TestSequenceFileBolt.java:68)

Running org.apache.storm.hdfs.spout.TestDirLock
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 16.213 sec - in org.apache.storm.hdfs.spout.TestDirLock
Running org.apache.storm.hdfs.spout.TestHdfsSemantics
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.689 sec - in org.apache.storm.hdfs.spout.TestHdfsSemantics
Running org.apache.storm.hdfs.spout.TestProgressTracker
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.785 sec - in org.apache.storm.hdfs.spout.TestProgressTracker
Running org.apache.storm.hdfs.spout.TestFileLock
Tests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 25.335 sec - in org.apache.storm.hdfs.spout.TestFileLock
Running org.apache.storm.hdfs.spout.TestHdfsSpout
Tests run: 10, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 9.096 sec - in org.apache.storm.hdfs.spout.TestHdfsSpout
Running org.apache.storm.hdfs.blobstore.HdfsBlobStoreImplTest
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.943 sec - in org.apache.storm.hdfs.blobstore.HdfsBlobStoreImplTest
Running org.apache.storm.hdfs.blobstore.BlobStoreTest
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 10.113 sec - in org.apache.storm.hdfs.blobstore.BlobStoreTest
Running org.apache.storm.hdfs.avro.TestFixedAvroSerializer
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.011 sec - in org.apache.storm.hdfs.avro.TestFixedAvroSerializer
Running org.apache.storm.hdfs.avro.TestGenericAvroSerializer
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.001 sec - in org.apache.storm.hdfs.avro.TestGenericAvroSerializer

Results :

Tests in error: 
  AvroGenericRecordBoltTest.backwardSchemaChangeWorks » NoClassDefFound Could no...
  AvroGenericRecordBoltTest.forwardSchemaChangeWorks » NoClassDefFound Could not...
  AvroGenericRecordBoltTest.multipleTuplesMutliplesFiles » NoClassDefFound Could...
  AvroGenericRecordBoltTest.multipleTuplesOneFile » NoClassDefFound Could not in...
  AvroGenericRecordBoltTest.schemaThrashing » ExceptionInInitializer
  TestHdfsBolt.<init>:69->generateTestTuple:243 » IllegalArgument Spouts is not ...
  TestHdfsBolt.<init>:69->generateTestTuple:243 » IllegalArgument Spouts is not ...
  TestHdfsBolt.<init>:69->generateTestTuple:243 » IllegalArgument Spouts is not ...
  TestHdfsBolt.<init>:69->generateTestTuple:243 » IllegalArgument Spouts is not ...
  TestHdfsBolt.<init>:69->generateTestTuple:243 » IllegalArgument Spouts is not ...
  TestHdfsBolt.<init>:69->generateTestTuple:243 » IllegalArgument Spouts is not ...
  TestSequenceFileBolt.<init>:68->generateTestTuple:161 » IllegalArgument Spouts...
  TestSequenceFileBolt.<init>:68->generateTestTuple:161 » IllegalArgument Spouts...
  TestSequenceFileBolt.<init>:68->generateTestTuple:161 » IllegalArgument Spouts...

Tests run: 54, Failures: 0, Errors: 14, Skipped: 0

[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building storm-hbase 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ storm-hbase ---
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ storm-hbase ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/storm-hbase/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ storm-hbase ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ storm-hbase ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/storm-hbase/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ storm-hbase ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:2.18.1:test (default-test) @ storm-hbase ---
[INFO] 
[INFO] --- apache-rat-plugin:0.11:check (default) @ storm-hbase ---
[INFO] 51 implicit excludes (use -debug for more details).
[INFO] Exclude: **/target/**
[INFO] Exclude: **/*.iml
[INFO] Exclude: **/.idea/**
[INFO] Exclude: **/metastore_db/**
[INFO] Exclude: **/build/**
[INFO] Exclude: **/logs/**
[INFO] Exclude: **/CHANGELOG.md
[INFO] Exclude: **/README.md
[INFO] Exclude: **/README.markdown
[INFO] Exclude: **/DEVELOPER.md
[INFO] Exclude: **/BYLAWS.md
[INFO] Exclude: **/STORM-UI-REST-API.md
[INFO] Exclude: SECURITY.md
[INFO] Exclude: VERSION
[INFO] Exclude: TODO
[INFO] Exclude: **/src/py/**
[INFO] Exclude: **/src/ui/public/js/jquery.dataTables.1.10.4.min.js
[INFO] Exclude: **/src/ui/public/css/jquery.dataTables.1.10.4.min.css
[INFO] Exclude: **/src/ui/public/images/*
[INFO] Exclude: **/src/ui/public/js/bootstrap-3.3.1.min.js
[INFO] Exclude: **/src/ui/public/css/bootstrap-3.3.1.min.css
[INFO] Exclude: **/src/ui/public/js/dataTables.bootstrap.min.js
[INFO] Exclude: **/src/ui/public/css/dataTables.bootstrap.css
[INFO] Exclude: **/src/ui/public/js/jsonFormatter.min.js
[INFO] Exclude: **/src/ui/public/css/jsonFormatter.min.css
[INFO] Exclude: **/src/ui/public/js/jquery-1.11.1.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.cookies.2.2.0.min.js
[INFO] Exclude: **/src/ui/public/js/moment.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.blockUI.min.js
[INFO] Exclude: **/src/ui/public/js/url.min.js
[INFO] Exclude: **/src/ui/public/js/arbor.js
[INFO] Exclude: **/src/ui/public/js/arbor-graphics.js
[INFO] Exclude: **/src/ui/public/js/arbor-tween.js
[INFO] Exclude: **/src/ui/public/js/jquery.mustache.js
[INFO] Exclude: **/src/ui/public/js/typeahead.jquery.min.js
[INFO] Exclude: **/dependency-reduced-pom.xml
[INFO] Exclude: **/docs/**
[INFO] Exclude: **/.git/**
[INFO] Exclude: **/derby.log
[INFO] Exclude: **/src/dev/**
[INFO] Exclude: **/src/codegen/config.fmpp
[INFO] Exclude: **/src/codegen/data/Parser.tdd
[INFO] Exclude: **/src/test/resources/FixedAvroSerializer.config
[INFO] 35 resources included (use -debug for more details)
Warning:  org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
Compiler warnings:
  WARNING:  'org.apache.xerces.jaxp.SAXParserImpl: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.'
Warning:  org.apache.xerces.parsers.SAXParser: Feature 'http://javax.xml.XMLConstants/feature/secure-processing' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
[INFO] Rat check: Summary of files. Unapproved: 0 unknown: 0 generated: 0 approved: 35 licence.
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building storm-hive 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ storm-hive ---
[INFO] Downloading: https://oss.sonatype.org/content/repositories/releases/org/pentaho/pentaho-aggdesigner-algorithm/5.1.3-jhyde/pentaho-aggdesigner-algorithm-5.1.3-jhyde.pom
[INFO] Downloading: https://repository.apache.org/releases/org/pentaho/pentaho-aggdesigner-algorithm/5.1.3-jhyde/pentaho-aggdesigner-algorithm-5.1.3-jhyde.pom
[INFO] Downloading: https://oss.sonatype.org/content/repositories/releases/net/hydromatic/linq4j/0.4/linq4j-0.4.pom
[INFO] Downloading: https://repository.apache.org/releases/net/hydromatic/linq4j/0.4/linq4j-0.4.pom
[INFO] Downloading: https://oss.sonatype.org/content/repositories/releases/net/hydromatic/quidem/0.1.1/quidem-0.1.1.pom
[INFO] Downloading: https://repository.apache.org/releases/net/hydromatic/quidem/0.1.1/quidem-0.1.1.pom
[INFO] Downloading: https://oss.sonatype.org/content/repositories/releases/eigenbase/eigenbase-properties/1.1.4/eigenbase-properties-1.1.4.pom
[INFO] Downloading: https://repository.apache.org/releases/eigenbase/eigenbase-properties/1.1.4/eigenbase-properties-1.1.4.pom
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ storm-hive ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/storm-hive/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ storm-hive ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ storm-hive ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/storm-hive/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ storm-hive ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (cleanup) @ storm-hive ---
[INFO] 
[INFO] --- maven-surefire-plugin:2.18.1:test (default-test) @ storm-hive ---
[INFO] Surefire report directory: /home/travis/build/apache/storm/external/storm-hive/target/surefire-reports
[INFO] parallel='none', perCoreThreadCount=true, threadCount=0, useUnlimitedThreads=false, threadCountSuites=0, threadCountClasses=0, threadCountMethods=0, parallelOptimized=true

-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running org.apache.storm.hive.bolt.TestHiveBolt
Tests run: 11, Failures: 0, Errors: 9, Skipped: 0, Time elapsed: 12.141 sec <<< FAILURE! - in org.apache.storm.hive.bolt.TestHiveBolt
testMultiPartitionTuples(org.apache.storm.hive.bolt.TestHiveBolt)  Time elapsed: 1.378 sec  <<< ERROR!
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hive.bolt.TestHiveBolt.generateTestTuple(TestHiveBolt.java:447)
	at org.apache.storm.hive.bolt.TestHiveBolt.testMultiPartitionTuples(TestHiveBolt.java:411)

testNoAcksUntilFlushed(org.apache.storm.hive.bolt.TestHiveBolt)  Time elapsed: 1.773 sec  <<< ERROR!
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hive.bolt.TestHiveBolt.generateTestTuple(TestHiveBolt.java:447)
	at org.apache.storm.hive.bolt.TestHiveBolt.testNoAcksUntilFlushed(TestHiveBolt.java:297)

testData(org.apache.storm.hive.bolt.TestHiveBolt)  Time elapsed: 1.648 sec  <<< ERROR!
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hive.bolt.TestHiveBolt.generateTestTuple(TestHiveBolt.java:447)
	at org.apache.storm.hive.bolt.TestHiveBolt.testData(TestHiveBolt.java:251)

testWithoutPartitions(org.apache.storm.hive.bolt.TestHiveBolt)  Time elapsed: 1.021 sec  <<< ERROR!
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hive.bolt.TestHiveBolt.generateTestTuple(TestHiveBolt.java:447)
	at org.apache.storm.hive.bolt.TestHiveBolt.testWithoutPartitions(TestHiveBolt.java:194)

testJsonWriter(org.apache.storm.hive.bolt.TestHiveBolt)  Time elapsed: 0.385 sec  <<< ERROR!
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hive.bolt.TestHiveBolt.generateTestTuple(TestHiveBolt.java:447)
	at org.apache.storm.hive.bolt.TestHiveBolt.testJsonWriter(TestHiveBolt.java:274)

testTickTuple(org.apache.storm.hive.bolt.TestHiveBolt)  Time elapsed: 0.325 sec  <<< ERROR!
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hive.bolt.TestHiveBolt.generateTestTuple(TestHiveBolt.java:447)
	at org.apache.storm.hive.bolt.TestHiveBolt.testTickTuple(TestHiveBolt.java:352)

testWithTimeformat(org.apache.storm.hive.bolt.TestHiveBolt)  Time elapsed: 0.562 sec  <<< ERROR!
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hive.bolt.TestHiveBolt.generateTestTuple(TestHiveBolt.java:447)
	at org.apache.storm.hive.bolt.TestHiveBolt.testWithTimeformat(TestHiveBolt.java:230)

testWithByteArrayIdandMessage(org.apache.storm.hive.bolt.TestHiveBolt)  Time elapsed: 0.423 sec  <<< ERROR!
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hive.bolt.TestHiveBolt.generateTestTuple(TestHiveBolt.java:447)
	at org.apache.storm.hive.bolt.TestHiveBolt.testWithByteArrayIdandMessage(TestHiveBolt.java:161)

testNoAcksIfFlushFails(org.apache.storm.hive.bolt.TestHiveBolt)  Time elapsed: 0.439 sec  <<< ERROR!
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hive.bolt.TestHiveBolt.generateTestTuple(TestHiveBolt.java:447)
	at org.apache.storm.hive.bolt.TestHiveBolt.testNoAcksIfFlushFails(TestHiveBolt.java:327)

Running org.apache.storm.hive.common.TestHiveWriter
Tests run: 3, Failures: 0, Errors: 2, Skipped: 0, Time elapsed: 3.142 sec <<< FAILURE! - in org.apache.storm.hive.common.TestHiveWriter
testWriteBasic(org.apache.storm.hive.common.TestHiveWriter)  Time elapsed: 0.668 sec  <<< ERROR!
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hive.common.TestHiveWriter.generateTestTuple(TestHiveWriter.java:164)
	at org.apache.storm.hive.common.TestHiveWriter.writeTuples(TestHiveWriter.java:179)
	at org.apache.storm.hive.common.TestHiveWriter.testWriteBasic(TestHiveWriter.java:127)

testWriteMultiFlush(org.apache.storm.hive.common.TestHiveWriter)  Time elapsed: 0.75 sec  <<< ERROR!
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hive.common.TestHiveWriter.generateTestTuple(TestHiveWriter.java:164)
	at org.apache.storm.hive.common.TestHiveWriter.testWriteMultiFlush(TestHiveWriter.java:142)


Results :

Tests in error: 
  TestHiveBolt.testData:251->generateTestTuple:447 » IllegalArgument Spouts is n...
  TestHiveBolt.testJsonWriter:274->generateTestTuple:447 » IllegalArgument Spout...
  TestHiveBolt.testMultiPartitionTuples:411->generateTestTuple:447 » IllegalArgument
  TestHiveBolt.testNoAcksIfFlushFails:327->generateTestTuple:447 » IllegalArgument
  TestHiveBolt.testNoAcksUntilFlushed:297->generateTestTuple:447 » IllegalArgument
  TestHiveBolt.testTickTuple:352->generateTestTuple:447 » IllegalArgument Spouts...
  TestHiveBolt.testWithByteArrayIdandMessage:161->generateTestTuple:447 » IllegalArgument
  TestHiveBolt.testWithTimeformat:230->generateTestTuple:447 » IllegalArgument S...
  TestHiveBolt.testWithoutPartitions:194->generateTestTuple:447 » IllegalArgument
  TestHiveWriter.testWriteBasic:127->writeTuples:179->generateTestTuple:164 » IllegalArgument
  TestHiveWriter.testWriteMultiFlush:142->generateTestTuple:164 » IllegalArgument

Tests run: 14, Failures: 0, Errors: 11, Skipped: 0

[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building storm-jdbc 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ storm-jdbc ---
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ storm-jdbc ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/storm-jdbc/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ storm-jdbc ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ storm-jdbc ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/storm-jdbc/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- sql-maven-plugin:1.5:execute (create-db) @ storm-jdbc ---
[INFO] Executing file: /tmp/test.1583856302sql
[INFO] 1 of 1 SQL statements executed successfully
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ storm-jdbc ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:2.18.1:test (default-test) @ storm-jdbc ---
[INFO] Surefire report directory: /home/travis/build/apache/storm/external/storm-jdbc/target/surefire-reports
[INFO] parallel='none', perCoreThreadCount=true, threadCount=0, useUnlimitedThreads=false, threadCountSuites=0, threadCountClasses=0, threadCountMethods=0, parallelOptimized=true

-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running org.apache.storm.jdbc.common.UtilTest
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.006 sec - in org.apache.storm.jdbc.common.UtilTest
Running org.apache.storm.jdbc.common.JdbcClientTest
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.832 sec - in org.apache.storm.jdbc.common.JdbcClientTest
Running org.apache.storm.jdbc.bolt.JdbcLookupBoltTest
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.004 sec - in org.apache.storm.jdbc.bolt.JdbcLookupBoltTest
Running org.apache.storm.jdbc.bolt.JdbcInsertBoltTest
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.001 sec - in org.apache.storm.jdbc.bolt.JdbcInsertBoltTest

Results :

Tests run: 5, Failures: 0, Errors: 0, Skipped: 0

[INFO] 
[INFO] --- apache-rat-plugin:0.11:check (default) @ storm-jdbc ---
[INFO] 51 implicit excludes (use -debug for more details).
[INFO] Exclude: **/target/**
[INFO] Exclude: **/*.iml
[INFO] Exclude: **/.idea/**
[INFO] Exclude: **/metastore_db/**
[INFO] Exclude: **/build/**
[INFO] Exclude: **/logs/**
[INFO] Exclude: **/CHANGELOG.md
[INFO] Exclude: **/README.md
[INFO] Exclude: **/README.markdown
[INFO] Exclude: **/DEVELOPER.md
[INFO] Exclude: **/BYLAWS.md
[INFO] Exclude: **/STORM-UI-REST-API.md
[INFO] Exclude: SECURITY.md
[INFO] Exclude: VERSION
[INFO] Exclude: TODO
[INFO] Exclude: **/src/py/**
[INFO] Exclude: **/src/ui/public/js/jquery.dataTables.1.10.4.min.js
[INFO] Exclude: **/src/ui/public/css/jquery.dataTables.1.10.4.min.css
[INFO] Exclude: **/src/ui/public/images/*
[INFO] Exclude: **/src/ui/public/js/bootstrap-3.3.1.min.js
[INFO] Exclude: **/src/ui/public/css/bootstrap-3.3.1.min.css
[INFO] Exclude: **/src/ui/public/js/dataTables.bootstrap.min.js
[INFO] Exclude: **/src/ui/public/css/dataTables.bootstrap.css
[INFO] Exclude: **/src/ui/public/js/jsonFormatter.min.js
[INFO] Exclude: **/src/ui/public/css/jsonFormatter.min.css
[INFO] Exclude: **/src/ui/public/js/jquery-1.11.1.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.cookies.2.2.0.min.js
[INFO] Exclude: **/src/ui/public/js/moment.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.blockUI.min.js
[INFO] Exclude: **/src/ui/public/js/url.min.js
[INFO] Exclude: **/src/ui/public/js/arbor.js
[INFO] Exclude: **/src/ui/public/js/arbor-graphics.js
[INFO] Exclude: **/src/ui/public/js/arbor-tween.js
[INFO] Exclude: **/src/ui/public/js/jquery.mustache.js
[INFO] Exclude: **/src/ui/public/js/typeahead.jquery.min.js
[INFO] Exclude: **/dependency-reduced-pom.xml
[INFO] Exclude: **/docs/**
[INFO] Exclude: **/.git/**
[INFO] Exclude: **/derby.log
[INFO] Exclude: **/src/dev/**
[INFO] Exclude: **/src/codegen/config.fmpp
[INFO] Exclude: **/src/codegen/data/Parser.tdd
[INFO] Exclude: **/src/test/resources/FixedAvroSerializer.config
[INFO] 26 resources included (use -debug for more details)
Warning:  org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
Compiler warnings:
  WARNING:  'org.apache.xerces.jaxp.SAXParserImpl: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.'
Warning:  org.apache.xerces.parsers.SAXParser: Feature 'http://javax.xml.XMLConstants/feature/secure-processing' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
[INFO] Rat check: Summary of files. Unapproved: 0 unknown: 0 generated: 0 approved: 26 licence.
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building storm-redis 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ storm-redis ---
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ storm-redis ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/storm-redis/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ storm-redis ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ storm-redis ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/storm-redis/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ storm-redis ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:2.18.1:test (default-test) @ storm-redis ---
[INFO] Surefire report directory: /home/travis/build/apache/storm/external/storm-redis/target/surefire-reports
[INFO] parallel='none', perCoreThreadCount=true, threadCount=0, useUnlimitedThreads=false, threadCountSuites=0, threadCountClasses=0, threadCountMethods=0, parallelOptimized=true

-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running org.apache.storm.redis.state.DefaultStateSerializerTest
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.099 sec - in org.apache.storm.redis.state.DefaultStateSerializerTest
Running org.apache.storm.redis.state.RedisKeyValueStateTest
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.832 sec - in org.apache.storm.redis.state.RedisKeyValueStateTest
Running org.apache.storm.redis.state.RedisKeyValueStateProviderTest
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.21 sec - in org.apache.storm.redis.state.RedisKeyValueStateProviderTest

Results :

Tests run: 5, Failures: 0, Errors: 0, Skipped: 0

[INFO] 
[INFO] --- apache-rat-plugin:0.11:check (default) @ storm-redis ---
[INFO] 51 implicit excludes (use -debug for more details).
[INFO] Exclude: **/target/**
[INFO] Exclude: **/*.iml
[INFO] Exclude: **/.idea/**
[INFO] Exclude: **/metastore_db/**
[INFO] Exclude: **/build/**
[INFO] Exclude: **/logs/**
[INFO] Exclude: **/CHANGELOG.md
[INFO] Exclude: **/README.md
[INFO] Exclude: **/README.markdown
[INFO] Exclude: **/DEVELOPER.md
[INFO] Exclude: **/BYLAWS.md
[INFO] Exclude: **/STORM-UI-REST-API.md
[INFO] Exclude: SECURITY.md
[INFO] Exclude: VERSION
[INFO] Exclude: TODO
[INFO] Exclude: **/src/py/**
[INFO] Exclude: **/src/ui/public/js/jquery.dataTables.1.10.4.min.js
[INFO] Exclude: **/src/ui/public/css/jquery.dataTables.1.10.4.min.css
[INFO] Exclude: **/src/ui/public/images/*
[INFO] Exclude: **/src/ui/public/js/bootstrap-3.3.1.min.js
[INFO] Exclude: **/src/ui/public/css/bootstrap-3.3.1.min.css
[INFO] Exclude: **/src/ui/public/js/dataTables.bootstrap.min.js
[INFO] Exclude: **/src/ui/public/css/dataTables.bootstrap.css
[INFO] Exclude: **/src/ui/public/js/jsonFormatter.min.js
[INFO] Exclude: **/src/ui/public/css/jsonFormatter.min.css
[INFO] Exclude: **/src/ui/public/js/jquery-1.11.1.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.cookies.2.2.0.min.js
[INFO] Exclude: **/src/ui/public/js/moment.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.blockUI.min.js
[INFO] Exclude: **/src/ui/public/js/url.min.js
[INFO] Exclude: **/src/ui/public/js/arbor.js
[INFO] Exclude: **/src/ui/public/js/arbor-graphics.js
[INFO] Exclude: **/src/ui/public/js/arbor-tween.js
[INFO] Exclude: **/src/ui/public/js/jquery.mustache.js
[INFO] Exclude: **/src/ui/public/js/typeahead.jquery.min.js
[INFO] Exclude: **/dependency-reduced-pom.xml
[INFO] Exclude: **/docs/**
[INFO] Exclude: **/.git/**
[INFO] Exclude: **/derby.log
[INFO] Exclude: **/src/dev/**
[INFO] Exclude: **/src/codegen/config.fmpp
[INFO] Exclude: **/src/codegen/data/Parser.tdd
[INFO] Exclude: **/src/test/resources/FixedAvroSerializer.config
[INFO] 44 resources included (use -debug for more details)
Warning:  org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
Compiler warnings:
  WARNING:  'org.apache.xerces.jaxp.SAXParserImpl: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.'
Warning:  org.apache.xerces.parsers.SAXParser: Feature 'http://javax.xml.XMLConstants/feature/secure-processing' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
[INFO] Rat check: Summary of files. Unapproved: 0 unknown: 0 generated: 0 approved: 44 licence.
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building storm-eventhubs 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ storm-eventhubs ---
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ storm-eventhubs ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 1 resource
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ storm-eventhubs ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ storm-eventhubs ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/storm-eventhubs/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ storm-eventhubs ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:2.18.1:test (default-test) @ storm-eventhubs ---
[INFO] Surefire report directory: /home/travis/build/apache/storm/external/storm-eventhubs/target/surefire-reports
[INFO] parallel='none', perCoreThreadCount=true, threadCount=0, useUnlimitedThreads=false, threadCountSuites=0, threadCountClasses=0, threadCountMethods=0, parallelOptimized=true

-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running org.apache.storm.eventhubs.trident.TestTransactionalTridentEmitter
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.451 sec - in org.apache.storm.eventhubs.trident.TestTransactionalTridentEmitter
Running org.apache.storm.eventhubs.spout.TestPartitionManager
Tests run: 8, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.037 sec - in org.apache.storm.eventhubs.spout.TestPartitionManager
Running org.apache.storm.eventhubs.spout.TestEventData
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0 sec - in org.apache.storm.eventhubs.spout.TestEventData
Running org.apache.storm.eventhubs.spout.TestEventHubSpout
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 2.023 sec - in org.apache.storm.eventhubs.spout.TestEventHubSpout

Results :

Tests run: 14, Failures: 0, Errors: 0, Skipped: 0

[INFO] 
[INFO] --- apache-rat-plugin:0.11:check (default) @ storm-eventhubs ---
[INFO] 51 implicit excludes (use -debug for more details).
[INFO] Exclude: **/target/**
[INFO] Exclude: **/*.iml
[INFO] Exclude: **/.idea/**
[INFO] Exclude: **/metastore_db/**
[INFO] Exclude: **/build/**
[INFO] Exclude: **/logs/**
[INFO] Exclude: **/CHANGELOG.md
[INFO] Exclude: **/README.md
[INFO] Exclude: **/README.markdown
[INFO] Exclude: **/DEVELOPER.md
[INFO] Exclude: **/BYLAWS.md
[INFO] Exclude: **/STORM-UI-REST-API.md
[INFO] Exclude: SECURITY.md
[INFO] Exclude: VERSION
[INFO] Exclude: TODO
[INFO] Exclude: **/src/py/**
[INFO] Exclude: **/src/ui/public/js/jquery.dataTables.1.10.4.min.js
[INFO] Exclude: **/src/ui/public/css/jquery.dataTables.1.10.4.min.css
[INFO] Exclude: **/src/ui/public/images/*
[INFO] Exclude: **/src/ui/public/js/bootstrap-3.3.1.min.js
[INFO] Exclude: **/src/ui/public/css/bootstrap-3.3.1.min.css
[INFO] Exclude: **/src/ui/public/js/dataTables.bootstrap.min.js
[INFO] Exclude: **/src/ui/public/css/dataTables.bootstrap.css
[INFO] Exclude: **/src/ui/public/js/jsonFormatter.min.js
[INFO] Exclude: **/src/ui/public/css/jsonFormatter.min.css
[INFO] Exclude: **/src/ui/public/js/jquery-1.11.1.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.cookies.2.2.0.min.js
[INFO] Exclude: **/src/ui/public/js/moment.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.blockUI.min.js
[INFO] Exclude: **/src/ui/public/js/url.min.js
[INFO] Exclude: **/src/ui/public/js/arbor.js
[INFO] Exclude: **/src/ui/public/js/arbor-graphics.js
[INFO] Exclude: **/src/ui/public/js/arbor-tween.js
[INFO] Exclude: **/src/ui/public/js/jquery.mustache.js
[INFO] Exclude: **/src/ui/public/js/typeahead.jquery.min.js
[INFO] Exclude: **/dependency-reduced-pom.xml
[INFO] Exclude: **/docs/**
[INFO] Exclude: **/.git/**
[INFO] Exclude: **/derby.log
[INFO] Exclude: **/src/dev/**
[INFO] Exclude: **/src/codegen/config.fmpp
[INFO] Exclude: **/src/codegen/data/Parser.tdd
[INFO] Exclude: **/src/test/resources/FixedAvroSerializer.config
[INFO] 52 resources included (use -debug for more details)
Warning:  org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
Compiler warnings:
  WARNING:  'org.apache.xerces.jaxp.SAXParserImpl: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.'
Warning:  org.apache.xerces.parsers.SAXParser: Feature 'http://javax.xml.XMLConstants/feature/secure-processing' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
[INFO] Rat check: Summary of files. Unapproved: 0 unknown: 0 generated: 0 approved: 52 licence.
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building flux 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ flux ---
[INFO] 
[INFO] --- apache-rat-plugin:0.11:check (default) @ flux ---
[INFO] 51 implicit excludes (use -debug for more details).
[INFO] Exclude: **/target/**
[INFO] Exclude: **/*.iml
[INFO] Exclude: **/.idea/**
[INFO] Exclude: **/metastore_db/**
[INFO] Exclude: **/build/**
[INFO] Exclude: **/logs/**
[INFO] Exclude: **/CHANGELOG.md
[INFO] Exclude: **/README.md
[INFO] Exclude: **/README.markdown
[INFO] Exclude: **/DEVELOPER.md
[INFO] Exclude: **/BYLAWS.md
[INFO] Exclude: **/STORM-UI-REST-API.md
[INFO] Exclude: SECURITY.md
[INFO] Exclude: VERSION
[INFO] Exclude: TODO
[INFO] Exclude: **/src/py/**
[INFO] Exclude: **/src/ui/public/js/jquery.dataTables.1.10.4.min.js
[INFO] Exclude: **/src/ui/public/css/jquery.dataTables.1.10.4.min.css
[INFO] Exclude: **/src/ui/public/images/*
[INFO] Exclude: **/src/ui/public/js/bootstrap-3.3.1.min.js
[INFO] Exclude: **/src/ui/public/css/bootstrap-3.3.1.min.css
[INFO] Exclude: **/src/ui/public/js/dataTables.bootstrap.min.js
[INFO] Exclude: **/src/ui/public/css/dataTables.bootstrap.css
[INFO] Exclude: **/src/ui/public/js/jsonFormatter.min.js
[INFO] Exclude: **/src/ui/public/css/jsonFormatter.min.css
[INFO] Exclude: **/src/ui/public/js/jquery-1.11.1.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.cookies.2.2.0.min.js
[INFO] Exclude: **/src/ui/public/js/moment.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.blockUI.min.js
[INFO] Exclude: **/src/ui/public/js/url.min.js
[INFO] Exclude: **/src/ui/public/js/arbor.js
[INFO] Exclude: **/src/ui/public/js/arbor-graphics.js
[INFO] Exclude: **/src/ui/public/js/arbor-tween.js
[INFO] Exclude: **/src/ui/public/js/jquery.mustache.js
[INFO] Exclude: **/src/ui/public/js/typeahead.jquery.min.js
[INFO] Exclude: **/dependency-reduced-pom.xml
[INFO] Exclude: **/docs/**
[INFO] Exclude: **/.git/**
[INFO] Exclude: **/derby.log
[INFO] Exclude: **/src/dev/**
[INFO] Exclude: **/src/codegen/config.fmpp
[INFO] Exclude: **/src/codegen/data/Parser.tdd
[INFO] Exclude: **/src/test/resources/FixedAvroSerializer.config
[INFO] 69 resources included (use -debug for more details)
Warning:  org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
Compiler warnings:
  WARNING:  'org.apache.xerces.jaxp.SAXParserImpl: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.'
Warning:  org.apache.xerces.parsers.SAXParser: Feature 'http://javax.xml.XMLConstants/feature/secure-processing' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
[INFO] Rat check: Summary of files. Unapproved: 0 unknown: 0 generated: 0 approved: 68 licence.
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building flux-wrappers 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ flux-wrappers ---
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ flux-wrappers ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 2 resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.3:compile (default-compile) @ flux-wrappers ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ flux-wrappers ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/flux/flux-wrappers/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.3:testCompile (default-testCompile) @ flux-wrappers ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-surefire-plugin:2.18.1:test (default-test) @ flux-wrappers ---
[INFO] 
[INFO] --- apache-rat-plugin:0.11:check (default) @ flux-wrappers ---
[INFO] 51 implicit excludes (use -debug for more details).
[INFO] Exclude: **/target/**
[INFO] Exclude: **/*.iml
[INFO] Exclude: **/.idea/**
[INFO] Exclude: **/metastore_db/**
[INFO] Exclude: **/build/**
[INFO] Exclude: **/logs/**
[INFO] Exclude: **/CHANGELOG.md
[INFO] Exclude: **/README.md
[INFO] Exclude: **/README.markdown
[INFO] Exclude: **/DEVELOPER.md
[INFO] Exclude: **/BYLAWS.md
[INFO] Exclude: **/STORM-UI-REST-API.md
[INFO] Exclude: SECURITY.md
[INFO] Exclude: VERSION
[INFO] Exclude: TODO
[INFO] Exclude: **/src/py/**
[INFO] Exclude: **/src/ui/public/js/jquery.dataTables.1.10.4.min.js
[INFO] Exclude: **/src/ui/public/css/jquery.dataTables.1.10.4.min.css
[INFO] Exclude: **/src/ui/public/images/*
[INFO] Exclude: **/src/ui/public/js/bootstrap-3.3.1.min.js
[INFO] Exclude: **/src/ui/public/css/bootstrap-3.3.1.min.css
[INFO] Exclude: **/src/ui/public/js/dataTables.bootstrap.min.js
[INFO] Exclude: **/src/ui/public/css/dataTables.bootstrap.css
[INFO] Exclude: **/src/ui/public/js/jsonFormatter.min.js
[INFO] Exclude: **/src/ui/public/css/jsonFormatter.min.css
[INFO] Exclude: **/src/ui/public/js/jquery-1.11.1.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.cookies.2.2.0.min.js
[INFO] Exclude: **/src/ui/public/js/moment.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.blockUI.min.js
[INFO] Exclude: **/src/ui/public/js/url.min.js
[INFO] Exclude: **/src/ui/public/js/arbor.js
[INFO] Exclude: **/src/ui/public/js/arbor-graphics.js
[INFO] Exclude: **/src/ui/public/js/arbor-tween.js
[INFO] Exclude: **/src/ui/public/js/jquery.mustache.js
[INFO] Exclude: **/src/ui/public/js/typeahead.jquery.min.js
[INFO] Exclude: **/dependency-reduced-pom.xml
[INFO] Exclude: **/docs/**
[INFO] Exclude: **/.git/**
[INFO] Exclude: **/derby.log
[INFO] Exclude: **/src/dev/**
[INFO] Exclude: **/src/codegen/config.fmpp
[INFO] Exclude: **/src/codegen/data/Parser.tdd
[INFO] Exclude: **/src/test/resources/FixedAvroSerializer.config
[INFO] 6 resources included (use -debug for more details)
Warning:  org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
Compiler warnings:
  WARNING:  'org.apache.xerces.jaxp.SAXParserImpl: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.'
Warning:  org.apache.xerces.parsers.SAXParser: Feature 'http://javax.xml.XMLConstants/feature/secure-processing' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
[INFO] Rat check: Summary of files. Unapproved: 0 unknown: 0 generated: 0 approved: 6 licence.
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Skipping flux-core
[INFO] This project has been banned from the build due to previous failures.
[INFO] ------------------------------------------------------------------------
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Skipping flux-examples
[INFO] This project has been banned from the build due to previous failures.
[INFO] ------------------------------------------------------------------------
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building storm-sql-runtime 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ storm-sql-runtime ---
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ storm-sql-runtime ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/sql/storm-sql-runtime/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ storm-sql-runtime ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ storm-sql-runtime ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/sql/storm-sql-runtime/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ storm-sql-runtime ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:2.18.1:test (default-test) @ storm-sql-runtime ---
[INFO] Surefire report directory: /home/travis/build/apache/storm/external/sql/storm-sql-runtime/target/surefire-reports
[INFO] parallel='none', perCoreThreadCount=true, threadCount=0, useUnlimitedThreads=false, threadCountSuites=0, threadCountClasses=0, threadCountMethods=0, parallelOptimized=true

-------------------------------------------------------
 T E S T S
-------------------------------------------------------

Results :

Tests run: 0, Failures: 0, Errors: 0, Skipped: 0

[INFO] 
[INFO] --- apache-rat-plugin:0.11:check (default) @ storm-sql-runtime ---
[INFO] 51 implicit excludes (use -debug for more details).
[INFO] Exclude: **/target/**
[INFO] Exclude: **/*.iml
[INFO] Exclude: **/.idea/**
[INFO] Exclude: **/metastore_db/**
[INFO] Exclude: **/build/**
[INFO] Exclude: **/logs/**
[INFO] Exclude: **/CHANGELOG.md
[INFO] Exclude: **/README.md
[INFO] Exclude: **/README.markdown
[INFO] Exclude: **/DEVELOPER.md
[INFO] Exclude: **/BYLAWS.md
[INFO] Exclude: **/STORM-UI-REST-API.md
[INFO] Exclude: SECURITY.md
[INFO] Exclude: VERSION
[INFO] Exclude: TODO
[INFO] Exclude: **/src/py/**
[INFO] Exclude: **/src/ui/public/js/jquery.dataTables.1.10.4.min.js
[INFO] Exclude: **/src/ui/public/css/jquery.dataTables.1.10.4.min.css
[INFO] Exclude: **/src/ui/public/images/*
[INFO] Exclude: **/src/ui/public/js/bootstrap-3.3.1.min.js
[INFO] Exclude: **/src/ui/public/css/bootstrap-3.3.1.min.css
[INFO] Exclude: **/src/ui/public/js/dataTables.bootstrap.min.js
[INFO] Exclude: **/src/ui/public/css/dataTables.bootstrap.css
[INFO] Exclude: **/src/ui/public/js/jsonFormatter.min.js
[INFO] Exclude: **/src/ui/public/css/jsonFormatter.min.css
[INFO] Exclude: **/src/ui/public/js/jquery-1.11.1.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.cookies.2.2.0.min.js
[INFO] Exclude: **/src/ui/public/js/moment.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.blockUI.min.js
[INFO] Exclude: **/src/ui/public/js/url.min.js
[INFO] Exclude: **/src/ui/public/js/arbor.js
[INFO] Exclude: **/src/ui/public/js/arbor-graphics.js
[INFO] Exclude: **/src/ui/public/js/arbor-tween.js
[INFO] Exclude: **/src/ui/public/js/jquery.mustache.js
[INFO] Exclude: **/src/ui/public/js/typeahead.jquery.min.js
[INFO] Exclude: **/dependency-reduced-pom.xml
[INFO] Exclude: **/docs/**
[INFO] Exclude: **/.git/**
[INFO] Exclude: **/derby.log
[INFO] Exclude: **/src/dev/**
[INFO] Exclude: **/src/codegen/config.fmpp
[INFO] Exclude: **/src/codegen/data/Parser.tdd
[INFO] Exclude: **/src/test/resources/FixedAvroSerializer.config
[INFO] 15 resources included (use -debug for more details)
Warning:  org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
Compiler warnings:
  WARNING:  'org.apache.xerces.jaxp.SAXParserImpl: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.'
Warning:  org.apache.xerces.parsers.SAXParser: Feature 'http://javax.xml.XMLConstants/feature/secure-processing' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
[INFO] Rat check: Summary of files. Unapproved: 0 unknown: 0 generated: 0 approved: 15 licence.
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building storm-sql-core 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-resources-plugin:2.5:copy-resources (copy-fmpp-resources) @ storm-sql-core ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 4 resources
[INFO] 
[INFO] --- maven-dependency-plugin:2.8:unpack (unpack-parser-template) @ storm-sql-core ---
[INFO] Configured Artifact: org.apache.calcite:calcite-core:?:jar
[INFO] Unpacking /home/travis/.m2/repository/org/apache/calcite/calcite-core/1.4.0-incubating/calcite-core-1.4.0-incubating.jar to /home/travis/build/apache/storm/external/sql/storm-sql-core/target with includes "**/Parser.jj" and excludes ""
[INFO] 
[INFO] --- fmpp-maven-plugin:1.0:generate (generate-fmpp-sources) @ storm-sql-core ---
- Executing: Parser.jj
log4j:WARN No appenders could be found for logger (freemarker.cache).
log4j:WARN Please initialize the log4j system properly.
[INFO] Done
[INFO] 
[INFO] --- javacc-maven-plugin:2.4:javacc (javacc) @ storm-sql-core ---
Java Compiler Compiler Version 4.0 (Parser Generator)
(type "javacc" with no arguments for help)
Reading from file /home/travis/build/apache/storm/external/sql/storm-sql-core/target/generated-sources/javacc/Parser.jj . . .
Note: UNICODE_INPUT option is specified. Please make sure you create the parser/lexer using a Reader with the correct character encoding.
Warning: Lookahead adequacy checking not being performed since option LOOKAHEAD is more than 1.  Set option FORCE_LA_CHECK to true to force checking.
Parser generated with 0 errors and 1 warnings.
[INFO] Processed 1 grammar
[INFO] 
[INFO] --- maven-resources-plugin:2.5:copy-resources (copy-java-sources) @ storm-sql-core ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 19 resources
[INFO] Copying 8 resources
[INFO] Copying 8 resources
[INFO] 
[INFO] --- build-helper-maven-plugin:1.5:add-source (add-generated-sources) @ storm-sql-core ---
[INFO] Source directory: /home/travis/build/apache/storm/external/sql/storm-sql-core/target/generated-sources added.
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ storm-sql-core ---
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ storm-sql-core ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/sql/storm-sql-core/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ storm-sql-core ---
[INFO] Changes detected - recompiling the module!
[INFO] Compiling 26 source files to /home/travis/build/apache/storm/external/sql/storm-sql-core/target/classes
[WARNING] /home/travis/build/apache/storm/external/sql/storm-sql-core/target/generated-sources/org/apache/storm/sql/parser/impl/StormParserImpl.java: Some input files use or override a deprecated API.
[WARNING] /home/travis/build/apache/storm/external/sql/storm-sql-core/target/generated-sources/org/apache/storm/sql/parser/impl/StormParserImpl.java: Recompile with -Xlint:deprecation for details.
[WARNING] /home/travis/build/apache/storm/external/sql/storm-sql-core/target/generated-sources/org/apache/storm/sql/parser/impl/StormParserImpl.java: /home/travis/build/apache/storm/external/sql/storm-sql-core/target/generated-sources/org/apache/storm/sql/parser/impl/StormParserImpl.java uses unchecked or unsafe operations.
[WARNING] /home/travis/build/apache/storm/external/sql/storm-sql-core/target/generated-sources/org/apache/storm/sql/parser/impl/StormParserImpl.java: Recompile with -Xlint:unchecked for details.
[INFO] 
[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ storm-sql-core ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/sql/storm-sql-core/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ storm-sql-core ---
[INFO] Changes detected - recompiling the module!
[INFO] Compiling 15 source files to /home/travis/build/apache/storm/external/sql/storm-sql-core/target/test-classes
[WARNING] /home/travis/build/apache/storm/external/sql/storm-sql-core/target/generated-sources/org/apache/storm/sql/parser/impl/StormParserImpl.java: /home/travis/build/apache/storm/external/sql/storm-sql-core/target/generated-sources/org/apache/storm/sql/parser/impl/StormParserImpl.java uses or overrides a deprecated API.
[WARNING] /home/travis/build/apache/storm/external/sql/storm-sql-core/target/generated-sources/org/apache/storm/sql/parser/impl/StormParserImpl.java: Recompile with -Xlint:deprecation for details.
[WARNING] /home/travis/build/apache/storm/external/sql/storm-sql-core/target/generated-sources/org/apache/storm/sql/parser/impl/StormParserImpl.java: /home/travis/build/apache/storm/external/sql/storm-sql-core/target/generated-sources/org/apache/storm/sql/parser/impl/StormParserImpl.java uses unchecked or unsafe operations.
[WARNING] /home/travis/build/apache/storm/external/sql/storm-sql-core/target/generated-sources/org/apache/storm/sql/parser/impl/StormParserImpl.java: Recompile with -Xlint:unchecked for details.
[INFO] 
[INFO] --- maven-surefire-plugin:2.18.1:test (default-test) @ storm-sql-core ---
[INFO] Surefire report directory: /home/travis/build/apache/storm/external/sql/storm-sql-core/target/surefire-reports
[INFO] parallel='none', perCoreThreadCount=true, threadCount=0, useUnlimitedThreads=false, threadCountSuites=0, threadCountClasses=0, threadCountMethods=0, parallelOptimized=true

-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running org.apache.storm.sql.parser.TestSqlParser
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.213 sec - in org.apache.storm.sql.parser.TestSqlParser
Running org.apache.storm.sql.compiler.backends.standalone.TestRelNodeCompiler
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.519 sec - in org.apache.storm.sql.compiler.backends.standalone.TestRelNodeCompiler
Running org.apache.storm.sql.compiler.backends.standalone.TestPlanCompiler
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 2.204 sec - in org.apache.storm.sql.compiler.backends.standalone.TestPlanCompiler
Running org.apache.storm.sql.compiler.backends.trident.TestPlanCompiler
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 12.497 sec - in org.apache.storm.sql.compiler.backends.trident.TestPlanCompiler
Running org.apache.storm.sql.compiler.TestExprCompiler
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.016 sec - in org.apache.storm.sql.compiler.TestExprCompiler
Running org.apache.storm.sql.compiler.TestExprSemantic
Tests run: 10, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.275 sec - in org.apache.storm.sql.compiler.TestExprSemantic
Running org.apache.storm.sql.TestStormSql
Tests run: 15, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.967 sec - in org.apache.storm.sql.TestStormSql

Results :

Tests run: 39, Failures: 0, Errors: 0, Skipped: 0

[INFO] 
[INFO] --- apache-rat-plugin:0.11:check (default) @ storm-sql-core ---
[INFO] 51 implicit excludes (use -debug for more details).
[INFO] Exclude: **/target/**
[INFO] Exclude: **/*.iml
[INFO] Exclude: **/.idea/**
[INFO] Exclude: **/metastore_db/**
[INFO] Exclude: **/build/**
[INFO] Exclude: **/logs/**
[INFO] Exclude: **/CHANGELOG.md
[INFO] Exclude: **/README.md
[INFO] Exclude: **/README.markdown
[INFO] Exclude: **/DEVELOPER.md
[INFO] Exclude: **/BYLAWS.md
[INFO] Exclude: **/STORM-UI-REST-API.md
[INFO] Exclude: SECURITY.md
[INFO] Exclude: VERSION
[INFO] Exclude: TODO
[INFO] Exclude: **/src/py/**
[INFO] Exclude: **/src/ui/public/js/jquery.dataTables.1.10.4.min.js
[INFO] Exclude: **/src/ui/public/css/jquery.dataTables.1.10.4.min.css
[INFO] Exclude: **/src/ui/public/images/*
[INFO] Exclude: **/src/ui/public/js/bootstrap-3.3.1.min.js
[INFO] Exclude: **/src/ui/public/css/bootstrap-3.3.1.min.css
[INFO] Exclude: **/src/ui/public/js/dataTables.bootstrap.min.js
[INFO] Exclude: **/src/ui/public/css/dataTables.bootstrap.css
[INFO] Exclude: **/src/ui/public/js/jsonFormatter.min.js
[INFO] Exclude: **/src/ui/public/css/jsonFormatter.min.css
[INFO] Exclude: **/src/ui/public/js/jquery-1.11.1.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.cookies.2.2.0.min.js
[INFO] Exclude: **/src/ui/public/js/moment.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.blockUI.min.js
[INFO] Exclude: **/src/ui/public/js/url.min.js
[INFO] Exclude: **/src/ui/public/js/arbor.js
[INFO] Exclude: **/src/ui/public/js/arbor-graphics.js
[INFO] Exclude: **/src/ui/public/js/arbor-tween.js
[INFO] Exclude: **/src/ui/public/js/jquery.mustache.js
[INFO] Exclude: **/src/ui/public/js/typeahead.jquery.min.js
[INFO] Exclude: **/dependency-reduced-pom.xml
[INFO] Exclude: **/docs/**
[INFO] Exclude: **/.git/**
[INFO] Exclude: **/derby.log
[INFO] Exclude: **/src/dev/**
[INFO] Exclude: **/src/codegen/config.fmpp
[INFO] Exclude: **/src/codegen/data/Parser.tdd
[INFO] Exclude: **/src/test/resources/FixedAvroSerializer.config
[INFO] 30 resources included (use -debug for more details)
Warning:  org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
Compiler warnings:
  WARNING:  'org.apache.xerces.jaxp.SAXParserImpl: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.'
Warning:  org.apache.xerces.parsers.SAXParser: Feature 'http://javax.xml.XMLConstants/feature/secure-processing' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
[INFO] Rat check: Summary of files. Unapproved: 0 unknown: 0 generated: 0 approved: 30 licence.
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Skipping storm-sql-kafka
[INFO] This project has been banned from the build due to previous failures.
[INFO] ------------------------------------------------------------------------
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building sql 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ sql ---
[INFO] 
[INFO] --- apache-rat-plugin:0.11:check (default) @ sql ---
[INFO] 51 implicit excludes (use -debug for more details).
[INFO] Exclude: **/target/**
[INFO] Exclude: **/*.iml
[INFO] Exclude: **/.idea/**
[INFO] Exclude: **/metastore_db/**
[INFO] Exclude: **/build/**
[INFO] Exclude: **/logs/**
[INFO] Exclude: **/CHANGELOG.md
[INFO] Exclude: **/README.md
[INFO] Exclude: **/README.markdown
[INFO] Exclude: **/DEVELOPER.md
[INFO] Exclude: **/BYLAWS.md
[INFO] Exclude: **/STORM-UI-REST-API.md
[INFO] Exclude: SECURITY.md
[INFO] Exclude: VERSION
[INFO] Exclude: TODO
[INFO] Exclude: **/src/py/**
[INFO] Exclude: **/src/ui/public/js/jquery.dataTables.1.10.4.min.js
[INFO] Exclude: **/src/ui/public/css/jquery.dataTables.1.10.4.min.css
[INFO] Exclude: **/src/ui/public/images/*
[INFO] Exclude: **/src/ui/public/js/bootstrap-3.3.1.min.js
[INFO] Exclude: **/src/ui/public/css/bootstrap-3.3.1.min.css
[INFO] Exclude: **/src/ui/public/js/dataTables.bootstrap.min.js
[INFO] Exclude: **/src/ui/public/css/dataTables.bootstrap.css
[INFO] Exclude: **/src/ui/public/js/jsonFormatter.min.js
[INFO] Exclude: **/src/ui/public/css/jsonFormatter.min.css
[INFO] Exclude: **/src/ui/public/js/jquery-1.11.1.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.cookies.2.2.0.min.js
[INFO] Exclude: **/src/ui/public/js/moment.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.blockUI.min.js
[INFO] Exclude: **/src/ui/public/js/url.min.js
[INFO] Exclude: **/src/ui/public/js/arbor.js
[INFO] Exclude: **/src/ui/public/js/arbor-graphics.js
[INFO] Exclude: **/src/ui/public/js/arbor-tween.js
[INFO] Exclude: **/src/ui/public/js/jquery.mustache.js
[INFO] Exclude: **/src/ui/public/js/typeahead.jquery.min.js
[INFO] Exclude: **/dependency-reduced-pom.xml
[INFO] Exclude: **/docs/**
[INFO] Exclude: **/.git/**
[INFO] Exclude: **/derby.log
[INFO] Exclude: **/src/dev/**
[INFO] Exclude: **/src/codegen/config.fmpp
[INFO] Exclude: **/src/codegen/data/Parser.tdd
[INFO] Exclude: **/src/test/resources/FixedAvroSerializer.config
[INFO] 53 resources included (use -debug for more details)
Warning:  org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
Compiler warnings:
  WARNING:  'org.apache.xerces.jaxp.SAXParserImpl: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.'
Warning:  org.apache.xerces.parsers.SAXParser: Feature 'http://javax.xml.XMLConstants/feature/secure-processing' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
[INFO] Rat check: Summary of files. Unapproved: 0 unknown: 0 generated: 0 approved: 53 licence.
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building storm-elasticsearch 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ storm-elasticsearch ---
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ storm-elasticsearch ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/storm-elasticsearch/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ storm-elasticsearch ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ storm-elasticsearch ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/storm-elasticsearch/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ storm-elasticsearch ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (cleanup) @ storm-elasticsearch ---
[INFO] 
[INFO] --- maven-surefire-plugin:2.18.1:test (default-test) @ storm-elasticsearch ---
[INFO] Surefire report directory: /home/travis/build/apache/storm/external/storm-elasticsearch/target/surefire-reports
[INFO] parallel='none', perCoreThreadCount=true, threadCount=0, useUnlimitedThreads=false, threadCountSuites=0, threadCountClasses=0, threadCountMethods=0, parallelOptimized=true

-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running org.apache.storm.elasticsearch.bolt.EsLookupBoltTest
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.505 sec - in org.apache.storm.elasticsearch.bolt.EsLookupBoltTest
Running org.apache.storm.elasticsearch.trident.EsStateFactoryTest
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.003 sec - in org.apache.storm.elasticsearch.trident.EsStateFactoryTest
Running org.apache.storm.elasticsearch.common.EsConfigTest
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.037 sec - in org.apache.storm.elasticsearch.common.EsConfigTest
Running org.apache.storm.elasticsearch.common.TransportAddressesTest
Tests run: 9, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.02 sec - in org.apache.storm.elasticsearch.common.TransportAddressesTest

Results :

Tests run: 18, Failures: 0, Errors: 0, Skipped: 0

[INFO] 
[INFO] --- apache-rat-plugin:0.11:check (default) @ storm-elasticsearch ---
[INFO] 51 implicit excludes (use -debug for more details).
[INFO] Exclude: **/target/**
[INFO] Exclude: **/*.iml
[INFO] Exclude: **/.idea/**
[INFO] Exclude: **/metastore_db/**
[INFO] Exclude: **/build/**
[INFO] Exclude: **/logs/**
[INFO] Exclude: **/CHANGELOG.md
[INFO] Exclude: **/README.md
[INFO] Exclude: **/README.markdown
[INFO] Exclude: **/DEVELOPER.md
[INFO] Exclude: **/BYLAWS.md
[INFO] Exclude: **/STORM-UI-REST-API.md
[INFO] Exclude: SECURITY.md
[INFO] Exclude: VERSION
[INFO] Exclude: TODO
[INFO] Exclude: **/src/py/**
[INFO] Exclude: **/src/ui/public/js/jquery.dataTables.1.10.4.min.js
[INFO] Exclude: **/src/ui/public/css/jquery.dataTables.1.10.4.min.css
[INFO] Exclude: **/src/ui/public/images/*
[INFO] Exclude: **/src/ui/public/js/bootstrap-3.3.1.min.js
[INFO] Exclude: **/src/ui/public/css/bootstrap-3.3.1.min.css
[INFO] Exclude: **/src/ui/public/js/dataTables.bootstrap.min.js
[INFO] Exclude: **/src/ui/public/css/dataTables.bootstrap.css
[INFO] Exclude: **/src/ui/public/js/jsonFormatter.min.js
[INFO] Exclude: **/src/ui/public/css/jsonFormatter.min.css
[INFO] Exclude: **/src/ui/public/js/jquery-1.11.1.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.cookies.2.2.0.min.js
[INFO] Exclude: **/src/ui/public/js/moment.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.blockUI.min.js
[INFO] Exclude: **/src/ui/public/js/url.min.js
[INFO] Exclude: **/src/ui/public/js/arbor.js
[INFO] Exclude: **/src/ui/public/js/arbor-graphics.js
[INFO] Exclude: **/src/ui/public/js/arbor-tween.js
[INFO] Exclude: **/src/ui/public/js/jquery.mustache.js
[INFO] Exclude: **/src/ui/public/js/typeahead.jquery.min.js
[INFO] Exclude: **/dependency-reduced-pom.xml
[INFO] Exclude: **/docs/**
[INFO] Exclude: **/.git/**
[INFO] Exclude: **/derby.log
[INFO] Exclude: **/src/dev/**
[INFO] Exclude: **/src/codegen/config.fmpp
[INFO] Exclude: **/src/codegen/data/Parser.tdd
[INFO] Exclude: **/src/test/resources/FixedAvroSerializer.config
[INFO] 28 resources included (use -debug for more details)
Warning:  org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
Compiler warnings:
  WARNING:  'org.apache.xerces.jaxp.SAXParserImpl: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.'
Warning:  org.apache.xerces.parsers.SAXParser: Feature 'http://javax.xml.XMLConstants/feature/secure-processing' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
[INFO] Rat check: Summary of files. Unapproved: 0 unknown: 0 generated: 0 approved: 28 licence.
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building storm-solr 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ storm-solr ---
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ storm-solr ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/storm-solr/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ storm-solr ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ storm-solr ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/storm-solr/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ storm-solr ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:2.18.1:test (default-test) @ storm-solr ---
[INFO] Surefire report directory: /home/travis/build/apache/storm/external/storm-solr/target/surefire-reports
[INFO] parallel='none', perCoreThreadCount=true, threadCount=0, useUnlimitedThreads=false, threadCountSuites=0, threadCountClasses=0, threadCountMethods=0, parallelOptimized=true

-------------------------------------------------------
 T E S T S
-------------------------------------------------------

Results :

Tests run: 0, Failures: 0, Errors: 0, Skipped: 0

[INFO] 
[INFO] --- apache-rat-plugin:0.11:check (default) @ storm-solr ---
[INFO] 51 implicit excludes (use -debug for more details).
[INFO] Exclude: **/target/**
[INFO] Exclude: **/*.iml
[INFO] Exclude: **/.idea/**
[INFO] Exclude: **/metastore_db/**
[INFO] Exclude: **/build/**
[INFO] Exclude: **/logs/**
[INFO] Exclude: **/CHANGELOG.md
[INFO] Exclude: **/README.md
[INFO] Exclude: **/README.markdown
[INFO] Exclude: **/DEVELOPER.md
[INFO] Exclude: **/BYLAWS.md
[INFO] Exclude: **/STORM-UI-REST-API.md
[INFO] Exclude: SECURITY.md
[INFO] Exclude: VERSION
[INFO] Exclude: TODO
[INFO] Exclude: **/src/py/**
[INFO] Exclude: **/src/ui/public/js/jquery.dataTables.1.10.4.min.js
[INFO] Exclude: **/src/ui/public/css/jquery.dataTables.1.10.4.min.css
[INFO] Exclude: **/src/ui/public/images/*
[INFO] Exclude: **/src/ui/public/js/bootstrap-3.3.1.min.js
[INFO] Exclude: **/src/ui/public/css/bootstrap-3.3.1.min.css
[INFO] Exclude: **/src/ui/public/js/dataTables.bootstrap.min.js
[INFO] Exclude: **/src/ui/public/css/dataTables.bootstrap.css
[INFO] Exclude: **/src/ui/public/js/jsonFormatter.min.js
[INFO] Exclude: **/src/ui/public/css/jsonFormatter.min.css
[INFO] Exclude: **/src/ui/public/js/jquery-1.11.1.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.cookies.2.2.0.min.js
[INFO] Exclude: **/src/ui/public/js/moment.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.blockUI.min.js
[INFO] Exclude: **/src/ui/public/js/url.min.js
[INFO] Exclude: **/src/ui/public/js/arbor.js
[INFO] Exclude: **/src/ui/public/js/arbor-graphics.js
[INFO] Exclude: **/src/ui/public/js/arbor-tween.js
[INFO] Exclude: **/src/ui/public/js/jquery.mustache.js
[INFO] Exclude: **/src/ui/public/js/typeahead.jquery.min.js
[INFO] Exclude: **/dependency-reduced-pom.xml
[INFO] Exclude: **/docs/**
[INFO] Exclude: **/.git/**
[INFO] Exclude: **/derby.log
[INFO] Exclude: **/src/dev/**
[INFO] Exclude: **/src/codegen/config.fmpp
[INFO] Exclude: **/src/codegen/data/Parser.tdd
[INFO] Exclude: **/src/test/resources/FixedAvroSerializer.config
[INFO] 27 resources included (use -debug for more details)
Warning:  org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
Compiler warnings:
  WARNING:  'org.apache.xerces.jaxp.SAXParserImpl: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.'
Warning:  org.apache.xerces.parsers.SAXParser: Feature 'http://javax.xml.XMLConstants/feature/secure-processing' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
[INFO] Rat check: Summary of files. Unapproved: 0 unknown: 0 generated: 0 approved: 27 licence.
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building storm-metrics 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-antrun-plugin:1.6:run (prepare) @ storm-metrics ---
[WARNING] Parameter tasks is deprecated, use target instead
[INFO] Executing tasks

main:
     [echo] Downloading sigar native binaries...
      [get] Destination already exists (skipping): /home/travis/.m2/repository/org/fusesource/sigar/1.6.4/hyperic-sigar-1.6.4.zip
    [unzip] Expanding: /home/travis/.m2/repository/org/fusesource/sigar/1.6.4/hyperic-sigar-1.6.4.zip into /home/travis/build/apache/storm/external/storm-metrics/target/classes/resources
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ storm-metrics ---
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ storm-metrics ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/storm-metrics/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ storm-metrics ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ storm-metrics ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/storm-metrics/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ storm-metrics ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-surefire-plugin:2.18.1:test (default-test) @ storm-metrics ---
[INFO] 
[INFO] --- apache-rat-plugin:0.11:check (default) @ storm-metrics ---
[INFO] 51 implicit excludes (use -debug for more details).
[INFO] Exclude: **/target/**
[INFO] Exclude: **/*.iml
[INFO] Exclude: **/.idea/**
[INFO] Exclude: **/metastore_db/**
[INFO] Exclude: **/build/**
[INFO] Exclude: **/logs/**
[INFO] Exclude: **/CHANGELOG.md
[INFO] Exclude: **/README.md
[INFO] Exclude: **/README.markdown
[INFO] Exclude: **/DEVELOPER.md
[INFO] Exclude: **/BYLAWS.md
[INFO] Exclude: **/STORM-UI-REST-API.md
[INFO] Exclude: SECURITY.md
[INFO] Exclude: VERSION
[INFO] Exclude: TODO
[INFO] Exclude: **/src/py/**
[INFO] Exclude: **/src/ui/public/js/jquery.dataTables.1.10.4.min.js
[INFO] Exclude: **/src/ui/public/css/jquery.dataTables.1.10.4.min.css
[INFO] Exclude: **/src/ui/public/images/*
[INFO] Exclude: **/src/ui/public/js/bootstrap-3.3.1.min.js
[INFO] Exclude: **/src/ui/public/css/bootstrap-3.3.1.min.css
[INFO] Exclude: **/src/ui/public/js/dataTables.bootstrap.min.js
[INFO] Exclude: **/src/ui/public/css/dataTables.bootstrap.css
[INFO] Exclude: **/src/ui/public/js/jsonFormatter.min.js
[INFO] Exclude: **/src/ui/public/css/jsonFormatter.min.css
[INFO] Exclude: **/src/ui/public/js/jquery-1.11.1.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.cookies.2.2.0.min.js
[INFO] Exclude: **/src/ui/public/js/moment.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.blockUI.min.js
[INFO] Exclude: **/src/ui/public/js/url.min.js
[INFO] Exclude: **/src/ui/public/js/arbor.js
[INFO] Exclude: **/src/ui/public/js/arbor-graphics.js
[INFO] Exclude: **/src/ui/public/js/arbor-tween.js
[INFO] Exclude: **/src/ui/public/js/jquery.mustache.js
[INFO] Exclude: **/src/ui/public/js/typeahead.jquery.min.js
[INFO] Exclude: **/dependency-reduced-pom.xml
[INFO] Exclude: **/docs/**
[INFO] Exclude: **/.git/**
[INFO] Exclude: **/derby.log
[INFO] Exclude: **/src/dev/**
[INFO] Exclude: **/src/codegen/config.fmpp
[INFO] Exclude: **/src/codegen/data/Parser.tdd
[INFO] Exclude: **/src/test/resources/FixedAvroSerializer.config
[INFO] 3 resources included (use -debug for more details)
Warning:  org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
Compiler warnings:
  WARNING:  'org.apache.xerces.jaxp.SAXParserImpl: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.'
Warning:  org.apache.xerces.parsers.SAXParser: Feature 'http://javax.xml.XMLConstants/feature/secure-processing' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
[INFO] Rat check: Summary of files. Unapproved: 0 unknown: 0 generated: 0 approved: 3 licence.
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building storm-cassandra 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ storm-cassandra ---
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ storm-cassandra ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/storm-cassandra/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ storm-cassandra ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ storm-cassandra ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 1 resource
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ storm-cassandra ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:2.18.1:test (default-test) @ storm-cassandra ---
[INFO] 
[INFO] --- apache-rat-plugin:0.11:check (default) @ storm-cassandra ---
[INFO] 51 implicit excludes (use -debug for more details).
[INFO] Exclude: **/target/**
[INFO] Exclude: **/*.iml
[INFO] Exclude: **/.idea/**
[INFO] Exclude: **/metastore_db/**
[INFO] Exclude: **/build/**
[INFO] Exclude: **/logs/**
[INFO] Exclude: **/CHANGELOG.md
[INFO] Exclude: **/README.md
[INFO] Exclude: **/README.markdown
[INFO] Exclude: **/DEVELOPER.md
[INFO] Exclude: **/BYLAWS.md
[INFO] Exclude: **/STORM-UI-REST-API.md
[INFO] Exclude: SECURITY.md
[INFO] Exclude: VERSION
[INFO] Exclude: TODO
[INFO] Exclude: **/src/py/**
[INFO] Exclude: **/src/ui/public/js/jquery.dataTables.1.10.4.min.js
[INFO] Exclude: **/src/ui/public/css/jquery.dataTables.1.10.4.min.css
[INFO] Exclude: **/src/ui/public/images/*
[INFO] Exclude: **/src/ui/public/js/bootstrap-3.3.1.min.js
[INFO] Exclude: **/src/ui/public/css/bootstrap-3.3.1.min.css
[INFO] Exclude: **/src/ui/public/js/dataTables.bootstrap.min.js
[INFO] Exclude: **/src/ui/public/css/dataTables.bootstrap.css
[INFO] Exclude: **/src/ui/public/js/jsonFormatter.min.js
[INFO] Exclude: **/src/ui/public/css/jsonFormatter.min.css
[INFO] Exclude: **/src/ui/public/js/jquery-1.11.1.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.cookies.2.2.0.min.js
[INFO] Exclude: **/src/ui/public/js/moment.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.blockUI.min.js
[INFO] Exclude: **/src/ui/public/js/url.min.js
[INFO] Exclude: **/src/ui/public/js/arbor.js
[INFO] Exclude: **/src/ui/public/js/arbor-graphics.js
[INFO] Exclude: **/src/ui/public/js/arbor-tween.js
[INFO] Exclude: **/src/ui/public/js/jquery.mustache.js
[INFO] Exclude: **/src/ui/public/js/typeahead.jquery.min.js
[INFO] Exclude: **/dependency-reduced-pom.xml
[INFO] Exclude: **/docs/**
[INFO] Exclude: **/.git/**
[INFO] Exclude: **/derby.log
[INFO] Exclude: **/src/dev/**
[INFO] Exclude: **/src/codegen/config.fmpp
[INFO] Exclude: **/src/codegen/data/Parser.tdd
[INFO] Exclude: **/src/test/resources/FixedAvroSerializer.config
[INFO] 50 resources included (use -debug for more details)
Warning:  org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
Compiler warnings:
  WARNING:  'org.apache.xerces.jaxp.SAXParserImpl: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.'
Warning:  org.apache.xerces.parsers.SAXParser: Feature 'http://javax.xml.XMLConstants/feature/secure-processing' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
[INFO] Rat check: Summary of files. Unapproved: 0 unknown: 0 generated: 0 approved: 50 licence.
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building storm-mqtt-parent 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ storm-mqtt-parent ---
[INFO] 
[INFO] --- apache-rat-plugin:0.11:check (default) @ storm-mqtt-parent ---
[INFO] 51 implicit excludes (use -debug for more details).
[INFO] Exclude: **/target/**
[INFO] Exclude: **/*.iml
[INFO] Exclude: **/.idea/**
[INFO] Exclude: **/metastore_db/**
[INFO] Exclude: **/build/**
[INFO] Exclude: **/logs/**
[INFO] Exclude: **/CHANGELOG.md
[INFO] Exclude: **/README.md
[INFO] Exclude: **/README.markdown
[INFO] Exclude: **/DEVELOPER.md
[INFO] Exclude: **/BYLAWS.md
[INFO] Exclude: **/STORM-UI-REST-API.md
[INFO] Exclude: SECURITY.md
[INFO] Exclude: VERSION
[INFO] Exclude: TODO
[INFO] Exclude: **/src/py/**
[INFO] Exclude: **/src/ui/public/js/jquery.dataTables.1.10.4.min.js
[INFO] Exclude: **/src/ui/public/css/jquery.dataTables.1.10.4.min.css
[INFO] Exclude: **/src/ui/public/images/*
[INFO] Exclude: **/src/ui/public/js/bootstrap-3.3.1.min.js
[INFO] Exclude: **/src/ui/public/css/bootstrap-3.3.1.min.css
[INFO] Exclude: **/src/ui/public/js/dataTables.bootstrap.min.js
[INFO] Exclude: **/src/ui/public/css/dataTables.bootstrap.css
[INFO] Exclude: **/src/ui/public/js/jsonFormatter.min.js
[INFO] Exclude: **/src/ui/public/css/jsonFormatter.min.css
[INFO] Exclude: **/src/ui/public/js/jquery-1.11.1.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.cookies.2.2.0.min.js
[INFO] Exclude: **/src/ui/public/js/moment.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.blockUI.min.js
[INFO] Exclude: **/src/ui/public/js/url.min.js
[INFO] Exclude: **/src/ui/public/js/arbor.js
[INFO] Exclude: **/src/ui/public/js/arbor-graphics.js
[INFO] Exclude: **/src/ui/public/js/arbor-tween.js
[INFO] Exclude: **/src/ui/public/js/jquery.mustache.js
[INFO] Exclude: **/src/ui/public/js/typeahead.jquery.min.js
[INFO] Exclude: **/dependency-reduced-pom.xml
[INFO] Exclude: **/docs/**
[INFO] Exclude: **/.git/**
[INFO] Exclude: **/derby.log
[INFO] Exclude: **/src/dev/**
[INFO] Exclude: **/src/codegen/config.fmpp
[INFO] Exclude: **/src/codegen/data/Parser.tdd
[INFO] Exclude: **/src/test/resources/FixedAvroSerializer.config
[INFO] 25 resources included (use -debug for more details)
Warning:  org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
Compiler warnings:
  WARNING:  'org.apache.xerces.jaxp.SAXParserImpl: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.'
Warning:  org.apache.xerces.parsers.SAXParser: Feature 'http://javax.xml.XMLConstants/feature/secure-processing' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
[INFO] Rat check: Summary of files. Unapproved: 0 unknown: 0 generated: 0 approved: 25 licence.
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building storm-mqtt 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ storm-mqtt ---
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ storm-mqtt ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/storm-mqtt/core/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ storm-mqtt ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ storm-mqtt ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/storm-mqtt/core/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ storm-mqtt ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:2.18.1:test (default-test) @ storm-mqtt ---
[WARNING] The parameter forkMode is deprecated since version 2.14. Use forkCount and reuseForks instead.
[INFO] Surefire report directory: /home/travis/build/apache/storm/external/storm-mqtt/core/target/surefire-reports
[INFO] parallel='none', perCoreThreadCount=true, threadCount=0, useUnlimitedThreads=false, threadCountSuites=0, threadCountClasses=0, threadCountMethods=0, parallelOptimized=true

-------------------------------------------------------
 T E S T S
-------------------------------------------------------

Results :

Tests run: 0, Failures: 0, Errors: 0, Skipped: 0

[INFO] 
[INFO] --- apache-rat-plugin:0.11:check (default) @ storm-mqtt ---
[INFO] 51 implicit excludes (use -debug for more details).
[INFO] Exclude: **/target/**
[INFO] Exclude: **/*.iml
[INFO] Exclude: **/.idea/**
[INFO] Exclude: **/metastore_db/**
[INFO] Exclude: **/build/**
[INFO] Exclude: **/logs/**
[INFO] Exclude: **/CHANGELOG.md
[INFO] Exclude: **/README.md
[INFO] Exclude: **/README.markdown
[INFO] Exclude: **/DEVELOPER.md
[INFO] Exclude: **/BYLAWS.md
[INFO] Exclude: **/STORM-UI-REST-API.md
[INFO] Exclude: SECURITY.md
[INFO] Exclude: VERSION
[INFO] Exclude: TODO
[INFO] Exclude: **/src/py/**
[INFO] Exclude: **/src/ui/public/js/jquery.dataTables.1.10.4.min.js
[INFO] Exclude: **/src/ui/public/css/jquery.dataTables.1.10.4.min.css
[INFO] Exclude: **/src/ui/public/images/*
[INFO] Exclude: **/src/ui/public/js/bootstrap-3.3.1.min.js
[INFO] Exclude: **/src/ui/public/css/bootstrap-3.3.1.min.css
[INFO] Exclude: **/src/ui/public/js/dataTables.bootstrap.min.js
[INFO] Exclude: **/src/ui/public/css/dataTables.bootstrap.css
[INFO] Exclude: **/src/ui/public/js/jsonFormatter.min.js
[INFO] Exclude: **/src/ui/public/css/jsonFormatter.min.css
[INFO] Exclude: **/src/ui/public/js/jquery-1.11.1.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.cookies.2.2.0.min.js
[INFO] Exclude: **/src/ui/public/js/moment.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.blockUI.min.js
[INFO] Exclude: **/src/ui/public/js/url.min.js
[INFO] Exclude: **/src/ui/public/js/arbor.js
[INFO] Exclude: **/src/ui/public/js/arbor-graphics.js
[INFO] Exclude: **/src/ui/public/js/arbor-tween.js
[INFO] Exclude: **/src/ui/public/js/jquery.mustache.js
[INFO] Exclude: **/src/ui/public/js/typeahead.jquery.min.js
[INFO] Exclude: **/dependency-reduced-pom.xml
[INFO] Exclude: **/docs/**
[INFO] Exclude: **/.git/**
[INFO] Exclude: **/derby.log
[INFO] Exclude: **/src/dev/**
[INFO] Exclude: **/src/codegen/config.fmpp
[INFO] Exclude: **/src/codegen/data/Parser.tdd
[INFO] Exclude: **/src/test/resources/FixedAvroSerializer.config
[INFO] 18 resources included (use -debug for more details)
Warning:  org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
Compiler warnings:
  WARNING:  'org.apache.xerces.jaxp.SAXParserImpl: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.'
Warning:  org.apache.xerces.parsers.SAXParser: Feature 'http://javax.xml.XMLConstants/feature/secure-processing' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
[INFO] Rat check: Summary of files. Unapproved: 0 unknown: 0 generated: 0 approved: 18 licence.
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Skipping storm-mqtt-examples
[INFO] This project has been banned from the build due to previous failures.
[INFO] ------------------------------------------------------------------------
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building storm-mongodb 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ storm-mongodb ---
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ storm-mongodb ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/storm-mongodb/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ storm-mongodb ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ storm-mongodb ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/storm-mongodb/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ storm-mongodb ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:2.18.1:test (default-test) @ storm-mongodb ---
[INFO] 
[INFO] --- apache-rat-plugin:0.11:check (default) @ storm-mongodb ---
[INFO] 51 implicit excludes (use -debug for more details).
[INFO] Exclude: **/target/**
[INFO] Exclude: **/*.iml
[INFO] Exclude: **/.idea/**
[INFO] Exclude: **/metastore_db/**
[INFO] Exclude: **/build/**
[INFO] Exclude: **/logs/**
[INFO] Exclude: **/CHANGELOG.md
[INFO] Exclude: **/README.md
[INFO] Exclude: **/README.markdown
[INFO] Exclude: **/DEVELOPER.md
[INFO] Exclude: **/BYLAWS.md
[INFO] Exclude: **/STORM-UI-REST-API.md
[INFO] Exclude: SECURITY.md
[INFO] Exclude: VERSION
[INFO] Exclude: TODO
[INFO] Exclude: **/src/py/**
[INFO] Exclude: **/src/ui/public/js/jquery.dataTables.1.10.4.min.js
[INFO] Exclude: **/src/ui/public/css/jquery.dataTables.1.10.4.min.css
[INFO] Exclude: **/src/ui/public/images/*
[INFO] Exclude: **/src/ui/public/js/bootstrap-3.3.1.min.js
[INFO] Exclude: **/src/ui/public/css/bootstrap-3.3.1.min.css
[INFO] Exclude: **/src/ui/public/js/dataTables.bootstrap.min.js
[INFO] Exclude: **/src/ui/public/css/dataTables.bootstrap.css
[INFO] Exclude: **/src/ui/public/js/jsonFormatter.min.js
[INFO] Exclude: **/src/ui/public/css/jsonFormatter.min.css
[INFO] Exclude: **/src/ui/public/js/jquery-1.11.1.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.cookies.2.2.0.min.js
[INFO] Exclude: **/src/ui/public/js/moment.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.blockUI.min.js
[INFO] Exclude: **/src/ui/public/js/url.min.js
[INFO] Exclude: **/src/ui/public/js/arbor.js
[INFO] Exclude: **/src/ui/public/js/arbor-graphics.js
[INFO] Exclude: **/src/ui/public/js/arbor-tween.js
[INFO] Exclude: **/src/ui/public/js/jquery.mustache.js
[INFO] Exclude: **/src/ui/public/js/typeahead.jquery.min.js
[INFO] Exclude: **/dependency-reduced-pom.xml
[INFO] Exclude: **/docs/**
[INFO] Exclude: **/.git/**
[INFO] Exclude: **/derby.log
[INFO] Exclude: **/src/dev/**
[INFO] Exclude: **/src/codegen/config.fmpp
[INFO] Exclude: **/src/codegen/data/Parser.tdd
[INFO] Exclude: **/src/test/resources/FixedAvroSerializer.config
[INFO] 18 resources included (use -debug for more details)
Warning:  org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
Compiler warnings:
  WARNING:  'org.apache.xerces.jaxp.SAXParserImpl: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.'
Warning:  org.apache.xerces.parsers.SAXParser: Feature 'http://javax.xml.XMLConstants/feature/secure-processing' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
[INFO] Rat check: Summary of files. Unapproved: 0 unknown: 0 generated: 0 approved: 18 licence.
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building storm-clojure 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ storm-clojure ---
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ storm-clojure ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/storm-clojure/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ storm-clojure ---
[INFO] No sources to compile
[INFO] 
[INFO] --- clojure-maven-plugin:1.7.1:compile (compile) @ storm-clojure ---
[INFO] 
[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ storm-clojure ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/storm-clojure/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ storm-clojure ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-surefire-plugin:2.18.1:test (default-test) @ storm-clojure ---
[INFO] 
[INFO] --- apache-rat-plugin:0.11:check (default) @ storm-clojure ---
[INFO] 51 implicit excludes (use -debug for more details).
[INFO] Exclude: **/target/**
[INFO] Exclude: **/*.iml
[INFO] Exclude: **/.idea/**
[INFO] Exclude: **/metastore_db/**
[INFO] Exclude: **/build/**
[INFO] Exclude: **/logs/**
[INFO] Exclude: **/CHANGELOG.md
[INFO] Exclude: **/README.md
[INFO] Exclude: **/README.markdown
[INFO] Exclude: **/DEVELOPER.md
[INFO] Exclude: **/BYLAWS.md
[INFO] Exclude: **/STORM-UI-REST-API.md
[INFO] Exclude: SECURITY.md
[INFO] Exclude: VERSION
[INFO] Exclude: TODO
[INFO] Exclude: **/src/py/**
[INFO] Exclude: **/src/ui/public/js/jquery.dataTables.1.10.4.min.js
[INFO] Exclude: **/src/ui/public/css/jquery.dataTables.1.10.4.min.css
[INFO] Exclude: **/src/ui/public/images/*
[INFO] Exclude: **/src/ui/public/js/bootstrap-3.3.1.min.js
[INFO] Exclude: **/src/ui/public/css/bootstrap-3.3.1.min.css
[INFO] Exclude: **/src/ui/public/js/dataTables.bootstrap.min.js
[INFO] Exclude: **/src/ui/public/css/dataTables.bootstrap.css
[INFO] Exclude: **/src/ui/public/js/jsonFormatter.min.js
[INFO] Exclude: **/src/ui/public/css/jsonFormatter.min.css
[INFO] Exclude: **/src/ui/public/js/jquery-1.11.1.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.cookies.2.2.0.min.js
[INFO] Exclude: **/src/ui/public/js/moment.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.blockUI.min.js
[INFO] Exclude: **/src/ui/public/js/url.min.js
[INFO] Exclude: **/src/ui/public/js/arbor.js
[INFO] Exclude: **/src/ui/public/js/arbor-graphics.js
[INFO] Exclude: **/src/ui/public/js/arbor-tween.js
[INFO] Exclude: **/src/ui/public/js/jquery.mustache.js
[INFO] Exclude: **/src/ui/public/js/typeahead.jquery.min.js
[INFO] Exclude: **/dependency-reduced-pom.xml
[INFO] Exclude: **/docs/**
[INFO] Exclude: **/.git/**
[INFO] Exclude: **/derby.log
[INFO] Exclude: **/src/dev/**
[INFO] Exclude: **/src/codegen/config.fmpp
[INFO] Exclude: **/src/codegen/data/Parser.tdd
[INFO] Exclude: **/src/test/resources/FixedAvroSerializer.config
[INFO] 4 resources included (use -debug for more details)
Warning:  org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
Compiler warnings:
  WARNING:  'org.apache.xerces.jaxp.SAXParserImpl: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.'
Warning:  org.apache.xerces.parsers.SAXParser: Feature 'http://javax.xml.XMLConstants/feature/secure-processing' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
[INFO] Rat check: Summary of files. Unapproved: 0 unknown: 0 generated: 0 approved: 4 licence.
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Skipping storm-starter
[INFO] This project has been banned from the build due to previous failures.
[INFO] ------------------------------------------------------------------------
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building storm-kafka-client 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ storm-kafka-client ---
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ storm-kafka-client ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/storm-kafka-client/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ storm-kafka-client ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ storm-kafka-client ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/storm-kafka-client/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ storm-kafka-client ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:2.18.1:test (default-test) @ storm-kafka-client ---
[INFO] 
[INFO] --- apache-rat-plugin:0.11:check (default) @ storm-kafka-client ---
[INFO] 51 implicit excludes (use -debug for more details).
[INFO] Exclude: **/target/**
[INFO] Exclude: **/*.iml
[INFO] Exclude: **/.idea/**
[INFO] Exclude: **/metastore_db/**
[INFO] Exclude: **/build/**
[INFO] Exclude: **/logs/**
[INFO] Exclude: **/CHANGELOG.md
[INFO] Exclude: **/README.md
[INFO] Exclude: **/README.markdown
[INFO] Exclude: **/DEVELOPER.md
[INFO] Exclude: **/BYLAWS.md
[INFO] Exclude: **/STORM-UI-REST-API.md
[INFO] Exclude: SECURITY.md
[INFO] Exclude: VERSION
[INFO] Exclude: TODO
[INFO] Exclude: **/src/py/**
[INFO] Exclude: **/src/ui/public/js/jquery.dataTables.1.10.4.min.js
[INFO] Exclude: **/src/ui/public/css/jquery.dataTables.1.10.4.min.css
[INFO] Exclude: **/src/ui/public/images/*
[INFO] Exclude: **/src/ui/public/js/bootstrap-3.3.1.min.js
[INFO] Exclude: **/src/ui/public/css/bootstrap-3.3.1.min.css
[INFO] Exclude: **/src/ui/public/js/dataTables.bootstrap.min.js
[INFO] Exclude: **/src/ui/public/css/dataTables.bootstrap.css
[INFO] Exclude: **/src/ui/public/js/jsonFormatter.min.js
[INFO] Exclude: **/src/ui/public/css/jsonFormatter.min.css
[INFO] Exclude: **/src/ui/public/js/jquery-1.11.1.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.cookies.2.2.0.min.js
[INFO] Exclude: **/src/ui/public/js/moment.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.blockUI.min.js
[INFO] Exclude: **/src/ui/public/js/url.min.js
[INFO] Exclude: **/src/ui/public/js/arbor.js
[INFO] Exclude: **/src/ui/public/js/arbor-graphics.js
[INFO] Exclude: **/src/ui/public/js/arbor-tween.js
[INFO] Exclude: **/src/ui/public/js/jquery.mustache.js
[INFO] Exclude: **/src/ui/public/js/typeahead.jquery.min.js
[INFO] Exclude: **/dependency-reduced-pom.xml
[INFO] Exclude: **/docs/**
[INFO] Exclude: **/.git/**
[INFO] Exclude: **/derby.log
[INFO] Exclude: **/src/dev/**
[INFO] Exclude: **/src/codegen/config.fmpp
[INFO] Exclude: **/src/codegen/data/Parser.tdd
[INFO] Exclude: **/src/test/resources/FixedAvroSerializer.config
[INFO] 14 resources included (use -debug for more details)
Warning:  org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
Compiler warnings:
  WARNING:  'org.apache.xerces.jaxp.SAXParserImpl: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.'
Warning:  org.apache.xerces.parsers.SAXParser: Feature 'http://javax.xml.XMLConstants/feature/secure-processing' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
[INFO] Rat check: Summary of files. Unapproved: 0 unknown: 0 generated: 0 approved: 14 licence.
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building storm-opentsdb 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ storm-opentsdb ---
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ storm-opentsdb ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/storm-opentsdb/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ storm-opentsdb ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ storm-opentsdb ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/storm-opentsdb/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ storm-opentsdb ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:2.18.1:test (default-test) @ storm-opentsdb ---
[INFO] 
[INFO] --- apache-rat-plugin:0.11:check (default) @ storm-opentsdb ---
[INFO] 51 implicit excludes (use -debug for more details).
[INFO] Exclude: **/target/**
[INFO] Exclude: **/*.iml
[INFO] Exclude: **/.idea/**
[INFO] Exclude: **/metastore_db/**
[INFO] Exclude: **/build/**
[INFO] Exclude: **/logs/**
[INFO] Exclude: **/CHANGELOG.md
[INFO] Exclude: **/README.md
[INFO] Exclude: **/README.markdown
[INFO] Exclude: **/DEVELOPER.md
[INFO] Exclude: **/BYLAWS.md
[INFO] Exclude: **/STORM-UI-REST-API.md
[INFO] Exclude: SECURITY.md
[INFO] Exclude: VERSION
[INFO] Exclude: TODO
[INFO] Exclude: **/src/py/**
[INFO] Exclude: **/src/ui/public/js/jquery.dataTables.1.10.4.min.js
[INFO] Exclude: **/src/ui/public/css/jquery.dataTables.1.10.4.min.css
[INFO] Exclude: **/src/ui/public/images/*
[INFO] Exclude: **/src/ui/public/js/bootstrap-3.3.1.min.js
[INFO] Exclude: **/src/ui/public/css/bootstrap-3.3.1.min.css
[INFO] Exclude: **/src/ui/public/js/dataTables.bootstrap.min.js
[INFO] Exclude: **/src/ui/public/css/dataTables.bootstrap.css
[INFO] Exclude: **/src/ui/public/js/jsonFormatter.min.js
[INFO] Exclude: **/src/ui/public/css/jsonFormatter.min.css
[INFO] Exclude: **/src/ui/public/js/jquery-1.11.1.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.cookies.2.2.0.min.js
[INFO] Exclude: **/src/ui/public/js/moment.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.blockUI.min.js
[INFO] Exclude: **/src/ui/public/js/url.min.js
[INFO] Exclude: **/src/ui/public/js/arbor.js
[INFO] Exclude: **/src/ui/public/js/arbor-graphics.js
[INFO] Exclude: **/src/ui/public/js/arbor-tween.js
[INFO] Exclude: **/src/ui/public/js/jquery.mustache.js
[INFO] Exclude: **/src/ui/public/js/typeahead.jquery.min.js
[INFO] Exclude: **/dependency-reduced-pom.xml
[INFO] Exclude: **/docs/**
[INFO] Exclude: **/.git/**
[INFO] Exclude: **/derby.log
[INFO] Exclude: **/src/dev/**
[INFO] Exclude: **/src/codegen/config.fmpp
[INFO] Exclude: **/src/codegen/data/Parser.tdd
[INFO] Exclude: **/src/test/resources/FixedAvroSerializer.config
[INFO] 14 resources included (use -debug for more details)
Warning:  org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
Compiler warnings:
  WARNING:  'org.apache.xerces.jaxp.SAXParserImpl: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.'
Warning:  org.apache.xerces.parsers.SAXParser: Feature 'http://javax.xml.XMLConstants/feature/secure-processing' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
[INFO] Rat check: Summary of files. Unapproved: 0 unknown: 0 generated: 0 approved: 14 licence.
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building storm-kafka-monitor 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ storm-kafka-monitor ---
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ storm-kafka-monitor ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/storm-kafka-monitor/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ storm-kafka-monitor ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ storm-kafka-monitor ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/storm-kafka-monitor/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ storm-kafka-monitor ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-surefire-plugin:2.18.1:test (default-test) @ storm-kafka-monitor ---
[INFO] 
[INFO] --- apache-rat-plugin:0.11:check (default) @ storm-kafka-monitor ---
[INFO] 51 implicit excludes (use -debug for more details).
[INFO] Exclude: **/target/**
[INFO] Exclude: **/*.iml
[INFO] Exclude: **/.idea/**
[INFO] Exclude: **/metastore_db/**
[INFO] Exclude: **/build/**
[INFO] Exclude: **/logs/**
[INFO] Exclude: **/CHANGELOG.md
[INFO] Exclude: **/README.md
[INFO] Exclude: **/README.markdown
[INFO] Exclude: **/DEVELOPER.md
[INFO] Exclude: **/BYLAWS.md
[INFO] Exclude: **/STORM-UI-REST-API.md
[INFO] Exclude: SECURITY.md
[INFO] Exclude: VERSION
[INFO] Exclude: TODO
[INFO] Exclude: **/src/py/**
[INFO] Exclude: **/src/ui/public/js/jquery.dataTables.1.10.4.min.js
[INFO] Exclude: **/src/ui/public/css/jquery.dataTables.1.10.4.min.css
[INFO] Exclude: **/src/ui/public/images/*
[INFO] Exclude: **/src/ui/public/js/bootstrap-3.3.1.min.js
[INFO] Exclude: **/src/ui/public/css/bootstrap-3.3.1.min.css
[INFO] Exclude: **/src/ui/public/js/dataTables.bootstrap.min.js
[INFO] Exclude: **/src/ui/public/css/dataTables.bootstrap.css
[INFO] Exclude: **/src/ui/public/js/jsonFormatter.min.js
[INFO] Exclude: **/src/ui/public/css/jsonFormatter.min.css
[INFO] Exclude: **/src/ui/public/js/jquery-1.11.1.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.cookies.2.2.0.min.js
[INFO] Exclude: **/src/ui/public/js/moment.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.blockUI.min.js
[INFO] Exclude: **/src/ui/public/js/url.min.js
[INFO] Exclude: **/src/ui/public/js/arbor.js
[INFO] Exclude: **/src/ui/public/js/arbor-graphics.js
[INFO] Exclude: **/src/ui/public/js/arbor-tween.js
[INFO] Exclude: **/src/ui/public/js/jquery.mustache.js
[INFO] Exclude: **/src/ui/public/js/typeahead.jquery.min.js
[INFO] Exclude: **/dependency-reduced-pom.xml
[INFO] Exclude: **/docs/**
[INFO] Exclude: **/.git/**
[INFO] Exclude: **/derby.log
[INFO] Exclude: **/src/dev/**
[INFO] Exclude: **/src/codegen/config.fmpp
[INFO] Exclude: **/src/codegen/data/Parser.tdd
[INFO] Exclude: **/src/test/resources/FixedAvroSerializer.config
[INFO] 5 resources included (use -debug for more details)
Warning:  org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
Compiler warnings:
  WARNING:  'org.apache.xerces.jaxp.SAXParserImpl: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.'
Warning:  org.apache.xerces.parsers.SAXParser: Feature 'http://javax.xml.XMLConstants/feature/secure-processing' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
[INFO] Rat check: Summary of files. Unapproved: 0 unknown: 0 generated: 0 approved: 5 licence.
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO] 
[INFO] Storm .............................................. SUCCESS [  6.372 s]
[INFO] multilang-javascript ............................... SUCCESS [  1.211 s]
[INFO] multilang-python ................................... SUCCESS [  0.222 s]
[INFO] multilang-ruby ..................................... SUCCESS [  0.208 s]
[INFO] maven-shade-clojure-transformer .................... SUCCESS [  2.077 s]
[INFO] storm-maven-plugins ................................ SUCCESS [  4.515 s]
[INFO] storm-rename-hack .................................. SUCCESS [  2.691 s]
[INFO] storm-kafka ........................................ FAILURE [01:04 min]
[INFO] storm-hdfs ......................................... FAILURE [01:17 min]
[INFO] storm-hbase ........................................ SUCCESS [  6.597 s]
[INFO] storm-hive ......................................... FAILURE [ 36.625 s]
[INFO] storm-jdbc ......................................... SUCCESS [  1.809 s]
[INFO] storm-redis ........................................ SUCCESS [  1.848 s]
[INFO] storm-eventhubs .................................... SUCCESS [  3.333 s]
[INFO] flux ............................................... SUCCESS [  0.108 s]
[INFO] flux-wrappers ...................................... SUCCESS [  0.180 s]
[INFO] flux-core .......................................... SKIPPED
[INFO] flux-examples ...................................... SKIPPED
[INFO] storm-sql-runtime .................................. SUCCESS [  0.266 s]
[INFO] storm-sql-core ..................................... SUCCESS [ 37.250 s]
[INFO] storm-sql-kafka .................................... SKIPPED
[INFO] sql ................................................ SUCCESS [  0.135 s]
[INFO] storm-elasticsearch ................................ SUCCESS [  3.008 s]
[INFO] storm-solr ......................................... SUCCESS [  2.299 s]
[INFO] storm-metrics ...................................... SUCCESS [  0.504 s]
[INFO] storm-cassandra .................................... SUCCESS [  0.666 s]
[INFO] storm-mqtt-parent .................................. SUCCESS [  0.080 s]
[INFO] storm-mqtt ......................................... SUCCESS [  1.087 s]
[INFO] storm-mqtt-examples ................................ SKIPPED
[INFO] storm-mongodb ...................................... SUCCESS [  0.057 s]
[INFO] storm-clojure ...................................... SUCCESS [  1.306 s]
[INFO] storm-starter ...................................... SKIPPED
[INFO] storm-kafka-client ................................. SUCCESS [  0.138 s]
[INFO] storm-opentsdb ..................................... SUCCESS [  1.210 s]
[INFO] storm-kafka-monitor ................................ SUCCESS [  0.430 s]
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 04:20 min
[INFO] Finished at: 2016-06-24T02:59:42+00:00
[INFO] Final Memory: 83M/485M
[INFO] ------------------------------------------------------------------------
[WARNING] The requested profile "native" could not be activated because it does not exist.
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:2.18.1:test (default-test) on project storm-kafka: There are test failures.
[ERROR] 
[ERROR] Please refer to /home/travis/build/apache/storm/external/storm-kafka/target/surefire-reports for the individual test results.
[ERROR] -> [Help 1]
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:2.18.1:test (default-test) on project storm-hdfs: There are test failures.
[ERROR] 
[ERROR] Please refer to /home/travis/build/apache/storm/external/storm-hdfs/target/surefire-reports for the individual test results.
[ERROR] -> [Help 1]
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:2.18.1:test (default-test) on project storm-hive: There are test failures.
[ERROR] 
[ERROR] Please refer to /home/travis/build/apache/storm/external/storm-hive/target/surefire-reports for the individual test results.
[ERROR] -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException
[ERROR] 
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn <goals> -rf :storm-kafka
Looking for errors in ./external/sql/storm-sql-core/target/surefire-reports
Checking ./external/sql/storm-sql-core/target/surefire-reports/TEST-org.apache.storm.sql.parser.TestSqlParser.xml
Checking ./external/sql/storm-sql-core/target/surefire-reports/TEST-org.apache.storm.sql.compiler.backends.standalone.TestRelNodeCompiler.xml
Checking ./external/sql/storm-sql-core/target/surefire-reports/TEST-org.apache.storm.sql.compiler.backends.standalone.TestPlanCompiler.xml
Checking ./external/sql/storm-sql-core/target/surefire-reports/TEST-org.apache.storm.sql.compiler.backends.trident.TestPlanCompiler.xml
Checking ./external/sql/storm-sql-core/target/surefire-reports/TEST-org.apache.storm.sql.compiler.TestExprCompiler.xml
Checking ./external/sql/storm-sql-core/target/surefire-reports/TEST-org.apache.storm.sql.compiler.TestExprSemantic.xml
Checking ./external/sql/storm-sql-core/target/surefire-reports/TEST-org.apache.storm.sql.TestStormSql.xml
Looking for errors in ./external/storm-elasticsearch/target/surefire-reports
Checking ./external/storm-elasticsearch/target/surefire-reports/TEST-org.apache.storm.elasticsearch.bolt.EsLookupBoltTest.xml
Checking ./external/storm-elasticsearch/target/surefire-reports/TEST-org.apache.storm.elasticsearch.trident.EsStateFactoryTest.xml
Checking ./external/storm-elasticsearch/target/surefire-reports/TEST-org.apache.storm.elasticsearch.common.EsConfigTest.xml
Checking ./external/storm-elasticsearch/target/surefire-reports/TEST-org.apache.storm.elasticsearch.common.TransportAddressesTest.xml
Looking for errors in ./external/storm-eventhubs/target/surefire-reports
Checking ./external/storm-eventhubs/target/surefire-reports/TEST-org.apache.storm.eventhubs.trident.TestTransactionalTridentEmitter.xml
Checking ./external/storm-eventhubs/target/surefire-reports/TEST-org.apache.storm.eventhubs.spout.TestPartitionManager.xml
Checking ./external/storm-eventhubs/target/surefire-reports/TEST-org.apache.storm.eventhubs.spout.TestEventData.xml
Checking ./external/storm-eventhubs/target/surefire-reports/TEST-org.apache.storm.eventhubs.spout.TestEventHubSpout.xml
Looking for errors in ./external/storm-hdfs/target/surefire-reports
Checking ./external/storm-hdfs/target/surefire-reports/TEST-org.apache.storm.hdfs.trident.HdfsStateTest.xml
Checking ./external/storm-hdfs/target/surefire-reports/TEST-org.apache.storm.hdfs.bolt.TestWritersMap.xml
Checking ./external/storm-hdfs/target/surefire-reports/TEST-org.apache.storm.hdfs.bolt.TestHdfsBolt.xml
--------------------------------------------------
classname: org.apache.storm.hdfs.bolt.TestHdfsBolt / testname: testTwoTuplesTwoFiles
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hdfs.bolt.TestHdfsBolt.generateTestTuple(TestHdfsBolt.java:243)
	at org.apache.storm.hdfs.bolt.TestHdfsBolt.<init>(TestHdfsBolt.java:69)

--------------------------------------------------
--------------------------------------------------
classname: org.apache.storm.hdfs.bolt.TestHdfsBolt / testname: testPartitionedOutput
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hdfs.bolt.TestHdfsBolt.generateTestTuple(TestHdfsBolt.java:243)
	at org.apache.storm.hdfs.bolt.TestHdfsBolt.<init>(TestHdfsBolt.java:69)

--------------------------------------------------
--------------------------------------------------
classname: org.apache.storm.hdfs.bolt.TestHdfsBolt / testname: testTwoTuplesOneFile
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hdfs.bolt.TestHdfsBolt.generateTestTuple(TestHdfsBolt.java:243)
	at org.apache.storm.hdfs.bolt.TestHdfsBolt.<init>(TestHdfsBolt.java:69)

--------------------------------------------------
--------------------------------------------------
classname: org.apache.storm.hdfs.bolt.TestHdfsBolt / testname: testFailedSync
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hdfs.bolt.TestHdfsBolt.generateTestTuple(TestHdfsBolt.java:243)
	at org.apache.storm.hdfs.bolt.TestHdfsBolt.<init>(TestHdfsBolt.java:69)

--------------------------------------------------
--------------------------------------------------
classname: org.apache.storm.hdfs.bolt.TestHdfsBolt / testname: testTickTuples
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hdfs.bolt.TestHdfsBolt.generateTestTuple(TestHdfsBolt.java:243)
	at org.apache.storm.hdfs.bolt.TestHdfsBolt.<init>(TestHdfsBolt.java:69)

--------------------------------------------------
--------------------------------------------------
classname: org.apache.storm.hdfs.bolt.TestHdfsBolt / testname: testFailureFilecount
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hdfs.bolt.TestHdfsBolt.generateTestTuple(TestHdfsBolt.java:243)
	at org.apache.storm.hdfs.bolt.TestHdfsBolt.<init>(TestHdfsBolt.java:69)

--------------------------------------------------
Checking ./external/storm-hdfs/target/surefire-reports/TEST-org.apache.storm.hdfs.bolt.AvroGenericRecordBoltTest.xml
--------------------------------------------------
classname: org.apache.storm.hdfs.bolt.AvroGenericRecordBoltTest / testname: schemaThrashing
java.lang.ExceptionInInitializerError: null
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hdfs.bolt.AvroGenericRecordBoltTest.generateTestTuple(AvroGenericRecordBoltTest.java:220)
	at org.apache.storm.hdfs.bolt.AvroGenericRecordBoltTest.<clinit>(AvroGenericRecordBoltTest.java:95)

--------------------------------------------------
--------------------------------------------------
classname: org.apache.storm.hdfs.bolt.AvroGenericRecordBoltTest / testname: forwardSchemaChangeWorks
java.lang.NoClassDefFoundError: Could not initialize class org.apache.storm.hdfs.bolt.AvroGenericRecordBoltTest
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:408)
	at org.junit.runners.BlockJUnit4ClassRunner.createTest(BlockJUnit4ClassRunner.java:195)
	at org.junit.runners.BlockJUnit4ClassRunner$1.runReflectiveCall(BlockJUnit4ClassRunner.java:244)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.BlockJUnit4ClassRunner.methodBlock(BlockJUnit4ClassRunner.java:241)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:108)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:78)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:54)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:144)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:203)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:155)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:103)

--------------------------------------------------
--------------------------------------------------
classname: org.apache.storm.hdfs.bolt.AvroGenericRecordBoltTest / testname: multipleTuplesOneFile
java.lang.NoClassDefFoundError: Could not initialize class org.apache.storm.hdfs.bolt.AvroGenericRecordBoltTest
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:408)
	at org.junit.runners.BlockJUnit4ClassRunner.createTest(BlockJUnit4ClassRunner.java:195)
	at org.junit.runners.BlockJUnit4ClassRunner$1.runReflectiveCall(BlockJUnit4ClassRunner.java:244)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.BlockJUnit4ClassRunner.methodBlock(BlockJUnit4ClassRunner.java:241)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:108)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:78)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:54)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:144)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:203)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:155)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:103)

--------------------------------------------------
--------------------------------------------------
classname: org.apache.storm.hdfs.bolt.AvroGenericRecordBoltTest / testname: multipleTuplesMutliplesFiles
java.lang.NoClassDefFoundError: Could not initialize class org.apache.storm.hdfs.bolt.AvroGenericRecordBoltTest
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:408)
	at org.junit.runners.BlockJUnit4ClassRunner.createTest(BlockJUnit4ClassRunner.java:195)
	at org.junit.runners.BlockJUnit4ClassRunner$1.runReflectiveCall(BlockJUnit4ClassRunner.java:244)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.BlockJUnit4ClassRunner.methodBlock(BlockJUnit4ClassRunner.java:241)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:108)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:78)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:54)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:144)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:203)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:155)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:103)

--------------------------------------------------
--------------------------------------------------
classname: org.apache.storm.hdfs.bolt.AvroGenericRecordBoltTest / testname: backwardSchemaChangeWorks
java.lang.NoClassDefFoundError: Could not initialize class org.apache.storm.hdfs.bolt.AvroGenericRecordBoltTest
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:408)
	at org.junit.runners.BlockJUnit4ClassRunner.createTest(BlockJUnit4ClassRunner.java:195)
	at org.junit.runners.BlockJUnit4ClassRunner$1.runReflectiveCall(BlockJUnit4ClassRunner.java:244)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.BlockJUnit4ClassRunner.methodBlock(BlockJUnit4ClassRunner.java:241)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:108)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:78)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:54)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:144)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:203)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:155)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:103)

--------------------------------------------------
Checking ./external/storm-hdfs/target/surefire-reports/TEST-org.apache.storm.hdfs.bolt.TestSequenceFileBolt.xml
--------------------------------------------------
classname: org.apache.storm.hdfs.bolt.TestSequenceFileBolt / testname: testTwoTuplesTwoFiles
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hdfs.bolt.TestSequenceFileBolt.generateTestTuple(TestSequenceFileBolt.java:161)
	at org.apache.storm.hdfs.bolt.TestSequenceFileBolt.<init>(TestSequenceFileBolt.java:68)

--------------------------------------------------
--------------------------------------------------
classname: org.apache.storm.hdfs.bolt.TestSequenceFileBolt / testname: testTwoTuplesOneFile
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hdfs.bolt.TestSequenceFileBolt.generateTestTuple(TestSequenceFileBolt.java:161)
	at org.apache.storm.hdfs.bolt.TestSequenceFileBolt.<init>(TestSequenceFileBolt.java:68)

--------------------------------------------------
--------------------------------------------------
classname: org.apache.storm.hdfs.bolt.TestSequenceFileBolt / testname: testFailedSync
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hdfs.bolt.TestSequenceFileBolt.generateTestTuple(TestSequenceFileBolt.java:161)
	at org.apache.storm.hdfs.bolt.TestSequenceFileBolt.<init>(TestSequenceFileBolt.java:68)

--------------------------------------------------
Checking ./external/storm-hdfs/target/surefire-reports/TEST-org.apache.storm.hdfs.spout.TestDirLock.xml
Checking ./external/storm-hdfs/target/surefire-reports/TEST-org.apache.storm.hdfs.spout.TestHdfsSemantics.xml
Checking ./external/storm-hdfs/target/surefire-reports/TEST-org.apache.storm.hdfs.spout.TestProgressTracker.xml
Checking ./external/storm-hdfs/target/surefire-reports/TEST-org.apache.storm.hdfs.spout.TestFileLock.xml
Checking ./external/storm-hdfs/target/surefire-reports/TEST-org.apache.storm.hdfs.spout.TestHdfsSpout.xml
Checking ./external/storm-hdfs/target/surefire-reports/TEST-org.apache.storm.hdfs.blobstore.HdfsBlobStoreImplTest.xml
Checking ./external/storm-hdfs/target/surefire-reports/TEST-org.apache.storm.hdfs.blobstore.BlobStoreTest.xml
Checking ./external/storm-hdfs/target/surefire-reports/TEST-org.apache.storm.hdfs.avro.TestFixedAvroSerializer.xml
Checking ./external/storm-hdfs/target/surefire-reports/TEST-org.apache.storm.hdfs.avro.TestGenericAvroSerializer.xml
Looking for errors in ./external/storm-hive/target/surefire-reports
Checking ./external/storm-hive/target/surefire-reports/TEST-org.apache.storm.hive.bolt.TestHiveBolt.xml
--------------------------------------------------
classname: org.apache.storm.hive.bolt.TestHiveBolt / testname: testMultiPartitionTuples
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hive.bolt.TestHiveBolt.generateTestTuple(TestHiveBolt.java:447)
	at org.apache.storm.hive.bolt.TestHiveBolt.testMultiPartitionTuples(TestHiveBolt.java:411)

-------------------- system-out --------------------
3407 [main] WARN  o.a.h.u.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
3483 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
3503 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
3672 [main] INFO  D.Persistence - Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
3672 [main] INFO  D.Persistence - Property datanucleus.cache.level2 unknown - will be ignored
4589 [main] INFO  o.a.h.h.m.ObjectStore - Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
4666 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
5572 [main] INFO  D.Datastore - The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
5572 [main] INFO  D.Datastore - The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
7071 [main] INFO  D.Datastore - The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
7071 [main] INFO  D.Datastore - The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
7412 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
7490 [main] WARN  o.a.h.h.m.ObjectStore - Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 0.14.0
7648 [main] WARN  o.a.h.h.m.ObjectStore - Failed to get database default, returning NoSuchObjectException
7848 [main] INFO  o.a.h.h.m.HiveMetaStore - Added admin role in metastore
7850 [main] INFO  o.a.h.h.m.HiveMetaStore - Added public role in metastore
7920 [main] INFO  o.a.h.h.m.HiveMetaStore - No user is added in admin role, since config is empty
8080 [main] INFO  o.a.h.h.q.s.SessionState - Created HDFS directory: /tmp/hive/travis
8090 [main] INFO  o.a.h.h.q.s.SessionState - Created local directory: /tmp/travis
8092 [main] INFO  o.a.h.h.q.s.SessionState - Created local directory: /tmp/e9e10ab5-3d08-4d94-9a14-ff6fb0cc36bc_resources
8094 [main] INFO  o.a.h.h.q.s.SessionState - Created HDFS directory: /tmp/hive/travis/e9e10ab5-3d08-4d94-9a14-ff6fb0cc36bc
8106 [main] INFO  o.a.h.h.q.s.SessionState - Created local directory: /tmp/travis/e9e10ab5-3d08-4d94-9a14-ff6fb0cc36bc
8116 [main] INFO  o.a.h.h.q.s.SessionState - Created HDFS directory: /tmp/hive/travis/e9e10ab5-3d08-4d94-9a14-ff6fb0cc36bc/_tmp_space.db
8117 [main] INFO  o.a.h.h.q.s.SessionState - No Tez session required at this point. hive.execution.engine=mr.
8223 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
8224 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
8229 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
8230 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
8230 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_table_names_by_filter: db = testdb, filter = 
8230 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table_names_by_filter: db = testdb, filter = 	
8268 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_database: testdb
8268 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_database: testdb	
8269 [main] WARN  o.a.h.h.m.ObjectStore - Failed to get database testdb, returning NoSuchObjectException
8271 [main] ERROR o.a.h.h.m.RetryingHMSHandler - NoSuchObjectException(message:testdb)
	at org.apache.hadoop.hive.metastore.ObjectStore.getDatabase(ObjectStore.java:539)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:483)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:98)
	at com.sun.proxy.$Proxy24.getDatabase(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_database_core(HiveMetaStore.java:914)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_database(HiveMetaStore.java:888)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:483)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:102)
	at com.sun.proxy.$Proxy26.get_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getDatabase(HiveMetaStoreClient.java:1043)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:657)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:644)
	at org.apache.storm.hive.bolt.HiveSetupUtil.dropDB(HiveSetupUtil.java:166)
	at org.apache.storm.hive.bolt.TestHiveBolt.setup(TestHiveBolt.java:119)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:483)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:108)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:78)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:54)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:144)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:203)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:155)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:103)

8271 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Shutting down the object store...
8272 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=Shutting down the object store...	
8272 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Metastore shutdown complete.
8272 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=Metastore shutdown complete.	
8272 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: create_database: Database(name:testdb, description:null, locationUri:raw:///tmp/junit4086320569023999407/testdb.db, parameters:null)
8273 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=create_database: Database(name:testdb, description:null, locationUri:raw:///tmp/junit4086320569023999407/testdb.db, parameters:null)	
8273 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
8274 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
8274 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
8276 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
8276 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
8278 [main] WARN  o.a.h.h.m.ObjectStore - Failed to get database testdb, returning NoSuchObjectException
8292 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: create_table: Table(tableName:test_table, dbName:testdb, owner:null, createTime:0, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:), FieldSchema(name:msg, type:string, comment:)], location:raw:///tmp/junit4086320569023999407/testdb.db/test_table, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:1, serdeInfo:SerDeInfo(name:test_table, serializationLib:org.apache.hadoop.hive.ql.io.orc.OrcSerde, parameters:{serialization.format=1}), bucketCols:[], sortCols:null, parameters:null), partitionKeys:[FieldSchema(name:city, type:string, comment:), FieldSchema(name:state, type:string, comment:)], parameters:{}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)
8292 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=create_table: Table(tableName:test_table, dbName:testdb, owner:null, createTime:0, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:), FieldSchema(name:msg, type:string, comment:)], location:raw:///tmp/junit4086320569023999407/testdb.db/test_table, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:1, serdeInfo:SerDeInfo(name:test_table, serializationLib:org.apache.hadoop.hive.ql.io.orc.OrcSerde, parameters:{serialization.format=1}), bucketCols:[], sortCols:null, parameters:null), partitionKeys:[FieldSchema(name:city, type:string, comment:), FieldSchema(name:state, type:string, comment:)], parameters:{}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
8311 [main] WARN  o.a.h.h.m.HiveMetaStore - Location: raw:///tmp/junit4086320569023999407/testdb.db/test_table specified for non-external table:test_table
8314 [main] INFO  o.a.h.h.c.FileUtils - Creating directory if it doesn't exist: raw:/tmp/junit4086320569023999407/testdb.db/test_table
8503 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: add_partition : db=testdb tbl=test_table
8503 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=add_partition : db=testdb tbl=test_table	
8613 [main] INFO  o.a.h.h.c.FileUtils - Creating directory if it doesn't exist: raw:/tmp/junit4086320569023999407/testdb.db/test_table/city=sunnyvale/state=ca
8704 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Shutting down the object store...
8710 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=Shutting down the object store...	
8710 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Metastore shutdown complete.
8710 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=Metastore shutdown complete.	
done
8716 [main] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=compile from=org.apache.hadoop.hive.ql.Driver>
8754 [main] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=parse from=org.apache.hadoop.hive.ql.Driver>
8756 [main] INFO  h.q.p.ParseDriver - Parsing command: select * from testdb.test_table
9060 [main] INFO  h.q.p.ParseDriver - Parse Completed
9061 [main] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=parse start=1466737111693 end=1466737112000 duration=307 from=org.apache.hadoop.hive.ql.Driver>
9098 [main] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=semanticAnalyze from=org.apache.hadoop.hive.ql.Driver>
9139 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Starting Semantic Analysis
9140 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Completed phase 1 of Semantic Analysis
9140 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Get metadata for source tables
9140 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_table : db=testdb tbl=test_table
9141 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table : db=testdb tbl=test_table	
9141 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
9142 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
9143 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
9146 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
9146 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
9167 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Get metadata for subqueries
9175 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Get metadata for destination tables
9179 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Completed getting MetaData in Semantic Analysis
9281 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Set stats collection dir : file:/tmp/travis/e9e10ab5-3d08-4d94-9a14-ff6fb0cc36bc/hive_2016-06-24_02-58-31_692_1269361870725661470-1/-ext-10002
9360 [main] INFO  o.a.h.h.q.p.OpProcFactory - Processing for FS(2)
9361 [main] INFO  o.a.h.h.q.p.OpProcFactory - Processing for SEL(1)
9361 [main] INFO  o.a.h.h.q.p.OpProcFactory - Processing for TS(0)
9382 [main] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=partition-retrieving from=org.apache.hadoop.hive.ql.optimizer.ppr.PartitionPruner>
9382 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_partitions : db=testdb tbl=test_table
9382 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_partitions : db=testdb tbl=test_table	
9433 [main] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=partition-retrieving start=1466737112321 end=1466737112372 duration=51 from=org.apache.hadoop.hive.ql.optimizer.ppr.PartitionPruner>
9457 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Completed plan generation
9462 [main] INFO  o.a.h.h.q.Driver - Semantic Analysis Completed
9462 [main] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=semanticAnalyze start=1466737112037 end=1466737112401 duration=364 from=org.apache.hadoop.hive.ql.Driver>
9475 [main] INFO  o.a.h.h.q.e.TableScanOperator - Initializing Self TS[0]
9475 [main] INFO  o.a.h.h.q.e.TableScanOperator - Operator 0 TS initialized
9475 [main] INFO  o.a.h.h.q.e.TableScanOperator - Initializing children of 0 TS
9475 [main] INFO  o.a.h.h.q.e.SelectOperator - Initializing child 1 SEL
9476 [main] INFO  o.a.h.h.q.e.SelectOperator - Initializing Self SEL[1]
9478 [main] INFO  o.a.h.h.q.e.SelectOperator - SELECT struct<id:int,msg:string,city:string,state:string>
9478 [main] INFO  o.a.h.h.q.e.SelectOperator - Operator 1 SEL initialized
9478 [main] INFO  o.a.h.h.q.e.SelectOperator - Initializing children of 1 SEL
9478 [main] INFO  o.a.h.h.q.e.ListSinkOperator - Initializing child 3 OP
9478 [main] INFO  o.a.h.h.q.e.ListSinkOperator - Initializing Self OP[3]
9479 [main] INFO  o.a.h.h.q.e.ListSinkOperator - Operator 3 OP initialized
9480 [main] INFO  o.a.h.h.q.e.ListSinkOperator - Initialization Done 3 OP
9480 [main] INFO  o.a.h.h.q.e.SelectOperator - Initialization Done 1 SEL
9480 [main] INFO  o.a.h.h.q.e.TableScanOperator - Initialization Done 0 TS
9482 [main] INFO  o.a.h.h.q.Driver - Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:test_table.id, type:int, comment:null), FieldSchema(name:test_table.msg, type:string, comment:null), FieldSchema(name:test_table.city, type:string, comment:null), FieldSchema(name:test_table.state, type:string, comment:null)], properties:null)
9482 [main] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=compile start=1466737111655 end=1466737112421 duration=766 from=org.apache.hadoop.hive.ql.Driver>

-------------------- system-err --------------------
Unable to drop index HL_TXNID_INDEX Index 'HL_TXNID_INDEX' does not exist.
Unable to drop table TXN_COMPONENTS: 'DROP TABLE' cannot be performed on 'TXN_COMPONENTS' because it does not exist.
Unable to drop table COMPLETED_TXN_COMPONENTS: 'DROP TABLE' cannot be performed on 'COMPLETED_TXN_COMPONENTS' because it does not exist.
Unable to drop table TXNS: 'DROP TABLE' cannot be performed on 'TXNS' because it does not exist.
Unable to drop table NEXT_TXN_ID: 'DROP TABLE' cannot be performed on 'NEXT_TXN_ID' because it does not exist.
Unable to drop table HIVE_LOCKS: 'DROP TABLE' cannot be performed on 'HIVE_LOCKS' because it does not exist.
Unable to drop table NEXT_LOCK_ID: 'DROP TABLE' cannot be performed on 'NEXT_LOCK_ID' because it does not exist.
Unable to drop table COMPACTION_QUEUE: 'DROP TABLE' cannot be performed on 'COMPACTION_QUEUE' because it does not exist.
Unable to drop table NEXT_COMPACTION_QUEUE_ID: 'DROP TABLE' cannot be performed on 'NEXT_COMPACTION_QUEUE_ID' because it does not exist.

--------------------------------------------------
--------------------------------------------------
classname: org.apache.storm.hive.bolt.TestHiveBolt / testname: testNoAcksUntilFlushed
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hive.bolt.TestHiveBolt.generateTestTuple(TestHiveBolt.java:447)
	at org.apache.storm.hive.bolt.TestHiveBolt.testNoAcksUntilFlushed(TestHiveBolt.java:297)

-------------------- system-out --------------------
10046 [main] INFO  o.a.h.h.q.s.SessionState - Created local directory: /tmp/6e0b4e2a-a281-47ee-893e-b69dbcc75532_resources
10063 [main] INFO  o.a.h.h.q.s.SessionState - Created HDFS directory: /tmp/hive/travis/6e0b4e2a-a281-47ee-893e-b69dbcc75532
10075 [main] INFO  o.a.h.h.q.s.SessionState - Created local directory: /tmp/travis/6e0b4e2a-a281-47ee-893e-b69dbcc75532
10079 [main] INFO  o.a.h.h.q.s.SessionState - Created HDFS directory: /tmp/hive/travis/6e0b4e2a-a281-47ee-893e-b69dbcc75532/_tmp_space.db
10079 [main] INFO  o.a.h.h.q.s.SessionState - No Tez session required at this point. hive.execution.engine=mr.
10081 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
10082 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
10083 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
10084 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
10084 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_table_names_by_filter: db = testdb, filter = 
10084 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table_names_by_filter: db = testdb, filter = 	
10091 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_table : db=testdb tbl=test_table
10091 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table : db=testdb tbl=test_table	
10118 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: drop_table : db=testdb tbl=test_table
10119 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=drop_table : db=testdb tbl=test_table	
10133 [main] INFO  D.Datastore - The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
10133 [main] INFO  D.Datastore - The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
10332 [main] INFO  D.Datastore - The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
10332 [main] INFO  D.Datastore - The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
10458 [main] INFO  D.Datastore - The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
10458 [main] INFO  D.Datastore - The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
10713 [main] INFO  D.Datastore - The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
10713 [main] INFO  D.Datastore - The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
10827 [main] INFO  D.Datastore - The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
10839 [main] INFO  D.Datastore - The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
11202 [main] INFO  h.m.hivemetastoressimpl - deleting  raw:/tmp/junit4086320569023999407/testdb.db/test_table
11203 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_database: testdb
11203 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_database: testdb	
11222 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: drop_database: testdb
11223 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=drop_database: testdb	
11224 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_all_tables: db=testdb
11224 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_all_tables: db=testdb	
11233 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_functions: db=testdb pat=*
11233 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_functions: db=testdb pat=*	
11234 [main] INFO  D.Datastore - The class "org.apache.hadoop.hive.metastore.model.MResourceUri" is tagged as "embedded-only" so does not have its own datastore table.
11331 [main] INFO  o.a.h.h.m.ObjectStore - Dropping database testdb along with all tables
11416 [main] INFO  h.m.hivemetastoressimpl - deleting  raw:/tmp/junit4086320569023999407/testdb.db
11466 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
11467 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
11473 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
11473 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
11473 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: create_database: Database(name:testdb, description:null, locationUri:raw:///tmp/junit564205636662151506/testdb.db, parameters:null)
11474 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=create_database: Database(name:testdb, description:null, locationUri:raw:///tmp/junit564205636662151506/testdb.db, parameters:null)	
11476 [main] WARN  o.a.h.h.m.ObjectStore - Failed to get database testdb, returning NoSuchObjectException
11503 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: create_table: Table(tableName:test_table, dbName:testdb, owner:null, createTime:0, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:), FieldSchema(name:msg, type:string, comment:)], location:raw:///tmp/junit564205636662151506/testdb.db/test_table, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:1, serdeInfo:SerDeInfo(name:test_table, serializationLib:org.apache.hadoop.hive.ql.io.orc.OrcSerde, parameters:{serialization.format=1}), bucketCols:[], sortCols:null, parameters:null), partitionKeys:[FieldSchema(name:city, type:string, comment:), FieldSchema(name:state, type:string, comment:)], parameters:{}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)
11503 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=create_table: Table(tableName:test_table, dbName:testdb, owner:null, createTime:0, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:), FieldSchema(name:msg, type:string, comment:)], location:raw:///tmp/junit564205636662151506/testdb.db/test_table, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:1, serdeInfo:SerDeInfo(name:test_table, serializationLib:org.apache.hadoop.hive.ql.io.orc.OrcSerde, parameters:{serialization.format=1}), bucketCols:[], sortCols:null, parameters:null), partitionKeys:[FieldSchema(name:city, type:string, comment:), FieldSchema(name:state, type:string, comment:)], parameters:{}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
11508 [main] WARN  o.a.h.h.m.HiveMetaStore - Location: raw:///tmp/junit564205636662151506/testdb.db/test_table specified for non-external table:test_table
11508 [main] INFO  o.a.h.h.c.FileUtils - Creating directory if it doesn't exist: raw:/tmp/junit564205636662151506/testdb.db/test_table
11629 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: add_partition : db=testdb tbl=test_table
11629 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=add_partition : db=testdb tbl=test_table	
11763 [main] INFO  o.a.h.h.c.FileUtils - Creating directory if it doesn't exist: raw:/tmp/junit564205636662151506/testdb.db/test_table/city=sunnyvale/state=ca
11822 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Shutting down the object store...
11827 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=Shutting down the object store...	
11827 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Metastore shutdown complete.
11827 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=Metastore shutdown complete.	
done

--------------------------------------------------
--------------------------------------------------
classname: org.apache.storm.hive.bolt.TestHiveBolt / testname: testData
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hive.bolt.TestHiveBolt.generateTestTuple(TestHiveBolt.java:447)
	at org.apache.storm.hive.bolt.TestHiveBolt.testData(TestHiveBolt.java:251)

-------------------- system-out --------------------
12296 [main] INFO  o.a.h.h.q.s.SessionState - Created local directory: /tmp/5b772949-4be0-4a8f-adfe-250c9bf549e2_resources
12317 [main] INFO  o.a.h.h.q.s.SessionState - Created HDFS directory: /tmp/hive/travis/5b772949-4be0-4a8f-adfe-250c9bf549e2
12337 [main] INFO  o.a.h.h.q.s.SessionState - Created local directory: /tmp/travis/5b772949-4be0-4a8f-adfe-250c9bf549e2
12357 [main] INFO  o.a.h.h.q.s.SessionState - Created HDFS directory: /tmp/hive/travis/5b772949-4be0-4a8f-adfe-250c9bf549e2/_tmp_space.db
12358 [main] INFO  o.a.h.h.q.s.SessionState - No Tez session required at this point. hive.execution.engine=mr.
12359 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_table_names_by_filter: db = testdb, filter = 
12359 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table_names_by_filter: db = testdb, filter = 	
12359 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
12360 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
12361 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
12416 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
12417 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
12423 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_table : db=testdb tbl=test_table
12423 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table : db=testdb tbl=test_table	
12451 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: drop_table : db=testdb tbl=test_table
12451 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=drop_table : db=testdb tbl=test_table	
13068 [main] INFO  h.m.hivemetastoressimpl - deleting  raw:/tmp/junit564205636662151506/testdb.db/test_table
13068 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_database: testdb
13068 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_database: testdb	
13086 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: drop_database: testdb
13086 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=drop_database: testdb	
13087 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_all_tables: db=testdb
13087 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_all_tables: db=testdb	
13767 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_functions: db=testdb pat=*
13768 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_functions: db=testdb pat=*	
13786 [main] INFO  o.a.h.h.m.ObjectStore - Dropping database testdb along with all tables
13805 [main] INFO  h.m.hivemetastoressimpl - deleting  raw:/tmp/junit564205636662151506/testdb.db
13807 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
13807 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
13809 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
13809 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
13809 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: create_database: Database(name:testdb, description:null, locationUri:raw:///tmp/junit2690151829900245655/testdb.db, parameters:null)
13809 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=create_database: Database(name:testdb, description:null, locationUri:raw:///tmp/junit2690151829900245655/testdb.db, parameters:null)	
13810 [main] WARN  o.a.h.h.m.ObjectStore - Failed to get database testdb, returning NoSuchObjectException
13814 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: create_table: Table(tableName:test_table, dbName:testdb, owner:null, createTime:0, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:), FieldSchema(name:msg, type:string, comment:)], location:raw:///tmp/junit2690151829900245655/testdb.db/test_table, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:1, serdeInfo:SerDeInfo(name:test_table, serializationLib:org.apache.hadoop.hive.ql.io.orc.OrcSerde, parameters:{serialization.format=1}), bucketCols:[], sortCols:null, parameters:null), partitionKeys:[FieldSchema(name:city, type:string, comment:), FieldSchema(name:state, type:string, comment:)], parameters:{}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)
13814 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=create_table: Table(tableName:test_table, dbName:testdb, owner:null, createTime:0, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:), FieldSchema(name:msg, type:string, comment:)], location:raw:///tmp/junit2690151829900245655/testdb.db/test_table, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:1, serdeInfo:SerDeInfo(name:test_table, serializationLib:org.apache.hadoop.hive.ql.io.orc.OrcSerde, parameters:{serialization.format=1}), bucketCols:[], sortCols:null, parameters:null), partitionKeys:[FieldSchema(name:city, type:string, comment:), FieldSchema(name:state, type:string, comment:)], parameters:{}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
13815 [main] WARN  o.a.h.h.m.HiveMetaStore - Location: raw:///tmp/junit2690151829900245655/testdb.db/test_table specified for non-external table:test_table
13815 [main] INFO  o.a.h.h.c.FileUtils - Creating directory if it doesn't exist: raw:/tmp/junit2690151829900245655/testdb.db/test_table
13854 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: add_partition : db=testdb tbl=test_table
13854 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=add_partition : db=testdb tbl=test_table	
13891 [main] INFO  o.a.h.h.c.FileUtils - Creating directory if it doesn't exist: raw:/tmp/junit2690151829900245655/testdb.db/test_table/city=sunnyvale/state=ca
13973 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Shutting down the object store...
13973 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=Shutting down the object store...	
13973 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Metastore shutdown complete.
13973 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=Metastore shutdown complete.	
done

--------------------------------------------------
--------------------------------------------------
classname: org.apache.storm.hive.bolt.TestHiveBolt / testname: testWithoutPartitions
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hive.bolt.TestHiveBolt.generateTestTuple(TestHiveBolt.java:447)
	at org.apache.storm.hive.bolt.TestHiveBolt.testWithoutPartitions(TestHiveBolt.java:194)

-------------------- system-out --------------------
14343 [main] INFO  o.a.h.h.q.s.SessionState - Created local directory: /tmp/44841fd0-a084-44ab-9e37-5908055322e0_resources
14358 [main] INFO  o.a.h.h.q.s.SessionState - Created HDFS directory: /tmp/hive/travis/44841fd0-a084-44ab-9e37-5908055322e0
14363 [main] INFO  o.a.h.h.q.s.SessionState - Created local directory: /tmp/travis/44841fd0-a084-44ab-9e37-5908055322e0
14364 [main] INFO  o.a.h.h.q.s.SessionState - Created HDFS directory: /tmp/hive/travis/44841fd0-a084-44ab-9e37-5908055322e0/_tmp_space.db
14365 [main] INFO  o.a.h.h.q.s.SessionState - No Tez session required at this point. hive.execution.engine=mr.
14366 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_table_names_by_filter: db = testdb, filter = 
14366 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table_names_by_filter: db = testdb, filter = 	
14366 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
14367 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
14368 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
14382 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
14382 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
14414 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_table : db=testdb tbl=test_table
14414 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table : db=testdb tbl=test_table	
14437 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: drop_table : db=testdb tbl=test_table
14437 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=drop_table : db=testdb tbl=test_table	
14975 [main] INFO  h.m.hivemetastoressimpl - deleting  raw:/tmp/junit2690151829900245655/testdb.db/test_table
14975 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_database: testdb
14975 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_database: testdb	
14983 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: drop_database: testdb
14984 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=drop_database: testdb	
14984 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_all_tables: db=testdb
14984 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_all_tables: db=testdb	
14988 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_functions: db=testdb pat=*
14988 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_functions: db=testdb pat=*	
14992 [main] INFO  o.a.h.h.m.ObjectStore - Dropping database testdb along with all tables
15008 [main] INFO  h.m.hivemetastoressimpl - deleting  raw:/tmp/junit2690151829900245655/testdb.db
15009 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
15010 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
15011 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
15011 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
15011 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: create_database: Database(name:testdb, description:null, locationUri:raw:///tmp/junit9128036246447358169/testdb.db, parameters:null)
15012 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=create_database: Database(name:testdb, description:null, locationUri:raw:///tmp/junit9128036246447358169/testdb.db, parameters:null)	
15012 [main] WARN  o.a.h.h.m.ObjectStore - Failed to get database testdb, returning NoSuchObjectException
15021 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: create_table: Table(tableName:test_table, dbName:testdb, owner:null, createTime:0, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:), FieldSchema(name:msg, type:string, comment:)], location:raw:///tmp/junit9128036246447358169/testdb.db/test_table, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:1, serdeInfo:SerDeInfo(name:test_table, serializationLib:org.apache.hadoop.hive.ql.io.orc.OrcSerde, parameters:{serialization.format=1}), bucketCols:[], sortCols:null, parameters:null), partitionKeys:[FieldSchema(name:city, type:string, comment:), FieldSchema(name:state, type:string, comment:)], parameters:{}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)
15021 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=create_table: Table(tableName:test_table, dbName:testdb, owner:null, createTime:0, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:), FieldSchema(name:msg, type:string, comment:)], location:raw:///tmp/junit9128036246447358169/testdb.db/test_table, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:1, serdeInfo:SerDeInfo(name:test_table, serializationLib:org.apache.hadoop.hive.ql.io.orc.OrcSerde, parameters:{serialization.format=1}), bucketCols:[], sortCols:null, parameters:null), partitionKeys:[FieldSchema(name:city, type:string, comment:), FieldSchema(name:state, type:string, comment:)], parameters:{}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
15022 [main] WARN  o.a.h.h.m.HiveMetaStore - Location: raw:///tmp/junit9128036246447358169/testdb.db/test_table specified for non-external table:test_table
15023 [main] INFO  o.a.h.h.c.FileUtils - Creating directory if it doesn't exist: raw:/tmp/junit9128036246447358169/testdb.db/test_table
15097 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: add_partition : db=testdb tbl=test_table
15106 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=add_partition : db=testdb tbl=test_table	
15197 [main] INFO  o.a.h.h.c.FileUtils - Creating directory if it doesn't exist: raw:/tmp/junit9128036246447358169/testdb.db/test_table/city=sunnyvale/state=ca
15238 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Shutting down the object store...
15239 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=Shutting down the object store...	
15239 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Metastore shutdown complete.
15239 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=Metastore shutdown complete.	
done
15239 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_table_names_by_filter: db = testdb1, filter = 
15239 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table_names_by_filter: db = testdb1, filter = 	
15239 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
15240 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
15241 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
15242 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
15243 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
15244 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_database: testdb1
15251 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_database: testdb1	
15252 [main] WARN  o.a.h.h.m.ObjectStore - Failed to get database testdb1, returning NoSuchObjectException
15253 [main] ERROR o.a.h.h.m.RetryingHMSHandler - NoSuchObjectException(message:testdb1)
	at org.apache.hadoop.hive.metastore.ObjectStore.getDatabase(ObjectStore.java:539)
	at sun.reflect.GeneratedMethodAccessor10.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:483)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:98)
	at com.sun.proxy.$Proxy24.getDatabase(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_database_core(HiveMetaStore.java:914)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_database(HiveMetaStore.java:888)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:483)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:102)
	at com.sun.proxy.$Proxy26.get_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getDatabase(HiveMetaStoreClient.java:1043)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:657)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:644)
	at org.apache.storm.hive.bolt.HiveSetupUtil.dropDB(HiveSetupUtil.java:166)
	at org.apache.storm.hive.bolt.TestHiveBolt.testWithoutPartitions(TestHiveBolt.java:175)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:483)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:108)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:78)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:54)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:144)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:203)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:155)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:103)

15253 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Shutting down the object store...
15253 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=Shutting down the object store...	
15253 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Metastore shutdown complete.
15253 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=Metastore shutdown complete.	
15254 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: create_database: Database(name:testdb1, description:null, locationUri:raw:///tmp/junit9128036246447358169/testdb.db, parameters:null)
15254 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=create_database: Database(name:testdb1, description:null, locationUri:raw:///tmp/junit9128036246447358169/testdb.db, parameters:null)	
15254 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
15255 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
15256 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
15258 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
15259 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
15260 [main] WARN  o.a.h.h.m.ObjectStore - Failed to get database testdb1, returning NoSuchObjectException
15285 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: create_table: Table(tableName:test_table1, dbName:testdb1, owner:null, createTime:0, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:), FieldSchema(name:msg, type:string, comment:)], location:raw:///tmp/junit9128036246447358169/testdb.db/test_table1, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:1, serdeInfo:SerDeInfo(name:test_table1, serializationLib:org.apache.hadoop.hive.ql.io.orc.OrcSerde, parameters:{serialization.format=1}), bucketCols:[], sortCols:null, parameters:null), partitionKeys:null, parameters:{}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)
15285 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=create_table: Table(tableName:test_table1, dbName:testdb1, owner:null, createTime:0, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:), FieldSchema(name:msg, type:string, comment:)], location:raw:///tmp/junit9128036246447358169/testdb.db/test_table1, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:1, serdeInfo:SerDeInfo(name:test_table1, serializationLib:org.apache.hadoop.hive.ql.io.orc.OrcSerde, parameters:{serialization.format=1}), bucketCols:[], sortCols:null, parameters:null), partitionKeys:null, parameters:{}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
15287 [main] WARN  o.a.h.h.m.HiveMetaStore - Location: raw:///tmp/junit9128036246447358169/testdb.db/test_table1 specified for non-external table:test_table1
15287 [main] INFO  o.a.h.h.c.FileUtils - Creating directory if it doesn't exist: raw:/tmp/junit9128036246447358169/testdb.db/test_table1
15339 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Shutting down the object store...
15339 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=Shutting down the object store...	
15339 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Metastore shutdown complete.
15339 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=Metastore shutdown complete.	
15343 [main] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=compile from=org.apache.hadoop.hive.ql.Driver>
15344 [main] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=parse from=org.apache.hadoop.hive.ql.Driver>
15344 [main] INFO  h.q.p.ParseDriver - Parsing command: select * from testdb1.test_table1
15344 [main] INFO  h.q.p.ParseDriver - Parse Completed
15344 [main] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=parse start=1466737118283 end=1466737118283 duration=0 from=org.apache.hadoop.hive.ql.Driver>
15352 [main] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=semanticAnalyze from=org.apache.hadoop.hive.ql.Driver>
15352 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Starting Semantic Analysis
15352 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Completed phase 1 of Semantic Analysis
15352 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Get metadata for source tables
15352 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_table : db=testdb1 tbl=test_table1
15352 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table : db=testdb1 tbl=test_table1	
15352 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
15353 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
15354 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
15358 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
15358 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
15366 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Get metadata for subqueries
15366 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Get metadata for destination tables
15384 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Completed getting MetaData in Semantic Analysis
15385 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Set stats collection dir : file:/tmp/travis/44841fd0-a084-44ab-9e37-5908055322e0/hive_2016-06-24_02-58-38_283_1535900988296703713-1/-ext-10002
15386 [main] INFO  o.a.h.h.q.p.OpProcFactory - Processing for FS(6)
15386 [main] INFO  o.a.h.h.q.p.OpProcFactory - Processing for SEL(5)
15386 [main] INFO  o.a.h.h.q.p.OpProcFactory - Processing for TS(4)
15388 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Completed plan generation
15388 [main] INFO  o.a.h.h.q.Driver - Semantic Analysis Completed
15388 [main] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=semanticAnalyze start=1466737118291 end=1466737118327 duration=36 from=org.apache.hadoop.hive.ql.Driver>
15388 [main] INFO  o.a.h.h.q.e.TableScanOperator - Initializing Self TS[4]
15388 [main] INFO  o.a.h.h.q.e.TableScanOperator - Operator 4 TS initialized
15389 [main] INFO  o.a.h.h.q.e.TableScanOperator - Initializing children of 4 TS
15389 [main] INFO  o.a.h.h.q.e.SelectOperator - Initializing child 5 SEL
15389 [main] INFO  o.a.h.h.q.e.SelectOperator - Initializing Self SEL[5]
15389 [main] INFO  o.a.h.h.q.e.SelectOperator - SELECT struct<id:int,msg:string>
15389 [main] INFO  o.a.h.h.q.e.SelectOperator - Operator 5 SEL initialized
15389 [main] INFO  o.a.h.h.q.e.SelectOperator - Initializing children of 5 SEL
15389 [main] INFO  o.a.h.h.q.e.ListSinkOperator - Initializing child 7 OP
15389 [main] INFO  o.a.h.h.q.e.ListSinkOperator - Initializing Self OP[7]
15389 [main] INFO  o.a.h.h.q.e.ListSinkOperator - Operator 7 OP initialized
15389 [main] INFO  o.a.h.h.q.e.ListSinkOperator - Initialization Done 7 OP
15389 [main] INFO  o.a.h.h.q.e.SelectOperator - Initialization Done 5 SEL
15389 [main] INFO  o.a.h.h.q.e.TableScanOperator - Initialization Done 4 TS
15389 [main] INFO  o.a.h.h.q.Driver - Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:test_table1.id, type:int, comment:null), FieldSchema(name:test_table1.msg, type:string, comment:null)], properties:null)
15389 [main] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=compile start=1466737118282 end=1466737118328 duration=46 from=org.apache.hadoop.hive.ql.Driver>

--------------------------------------------------
--------------------------------------------------
classname: org.apache.storm.hive.bolt.TestHiveBolt / testname: testJsonWriter
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hive.bolt.TestHiveBolt.generateTestTuple(TestHiveBolt.java:447)
	at org.apache.storm.hive.bolt.TestHiveBolt.testJsonWriter(TestHiveBolt.java:274)

-------------------- system-out --------------------
16321 [main] INFO  o.a.h.h.q.s.SessionState - Created local directory: /tmp/ac3eee48-a900-412f-aecf-56b9265d1559_resources
16323 [main] INFO  o.a.h.h.q.s.SessionState - Created HDFS directory: /tmp/hive/travis/ac3eee48-a900-412f-aecf-56b9265d1559
16324 [main] INFO  o.a.h.h.q.s.SessionState - Created local directory: /tmp/travis/ac3eee48-a900-412f-aecf-56b9265d1559
16326 [main] INFO  o.a.h.h.q.s.SessionState - Created HDFS directory: /tmp/hive/travis/ac3eee48-a900-412f-aecf-56b9265d1559/_tmp_space.db
16326 [main] INFO  o.a.h.h.q.s.SessionState - No Tez session required at this point. hive.execution.engine=mr.
16327 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_table_names_by_filter: db = testdb, filter = 
16327 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table_names_by_filter: db = testdb, filter = 	
16327 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
16328 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
16329 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
16330 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
16331 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
16336 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_table : db=testdb tbl=test_table
16336 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table : db=testdb tbl=test_table	
16366 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: drop_table : db=testdb tbl=test_table
16366 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=drop_table : db=testdb tbl=test_table	
16544 [main] INFO  h.m.hivemetastoressimpl - deleting  raw:/tmp/junit3953092904366313005/testdb.db/test_table
16544 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_database: testdb
16545 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_database: testdb	
16556 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: drop_database: testdb
16556 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=drop_database: testdb	
16557 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_all_tables: db=testdb
16557 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_all_tables: db=testdb	
16575 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_functions: db=testdb pat=*
16575 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_functions: db=testdb pat=*	
16580 [main] INFO  o.a.h.h.m.ObjectStore - Dropping database testdb along with all tables
16591 [main] INFO  h.m.hivemetastoressimpl - deleting  raw:/tmp/junit3953092904366313005/testdb.db
16592 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
16592 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
16594 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
16594 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
16594 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: create_database: Database(name:testdb, description:null, locationUri:raw:///tmp/junit3798332333384956769/testdb.db, parameters:null)
16594 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=create_database: Database(name:testdb, description:null, locationUri:raw:///tmp/junit3798332333384956769/testdb.db, parameters:null)	
16595 [main] WARN  o.a.h.h.m.ObjectStore - Failed to get database testdb, returning NoSuchObjectException
16598 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: create_table: Table(tableName:test_table, dbName:testdb, owner:null, createTime:0, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:), FieldSchema(name:msg, type:string, comment:)], location:raw:///tmp/junit3798332333384956769/testdb.db/test_table, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:1, serdeInfo:SerDeInfo(name:test_table, serializationLib:org.apache.hadoop.hive.ql.io.orc.OrcSerde, parameters:{serialization.format=1}), bucketCols:[], sortCols:null, parameters:null), partitionKeys:[FieldSchema(name:city, type:string, comment:), FieldSchema(name:state, type:string, comment:)], parameters:{}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)
16598 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=create_table: Table(tableName:test_table, dbName:testdb, owner:null, createTime:0, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:), FieldSchema(name:msg, type:string, comment:)], location:raw:///tmp/junit3798332333384956769/testdb.db/test_table, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:1, serdeInfo:SerDeInfo(name:test_table, serializationLib:org.apache.hadoop.hive.ql.io.orc.OrcSerde, parameters:{serialization.format=1}), bucketCols:[], sortCols:null, parameters:null), partitionKeys:[FieldSchema(name:city, type:string, comment:), FieldSchema(name:state, type:string, comment:)], parameters:{}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
16599 [main] WARN  o.a.h.h.m.HiveMetaStore - Location: raw:///tmp/junit3798332333384956769/testdb.db/test_table specified for non-external table:test_table
16599 [main] INFO  o.a.h.h.c.FileUtils - Creating directory if it doesn't exist: raw:/tmp/junit3798332333384956769/testdb.db/test_table
16633 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: add_partition : db=testdb tbl=test_table
16635 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=add_partition : db=testdb tbl=test_table	
16685 [main] INFO  o.a.h.h.c.FileUtils - Creating directory if it doesn't exist: raw:/tmp/junit3798332333384956769/testdb.db/test_table/city=sunnyvale/state=ca
16707 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Shutting down the object store...
16707 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=Shutting down the object store...	
16707 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Metastore shutdown complete.
16707 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=Metastore shutdown complete.	
done

--------------------------------------------------
--------------------------------------------------
classname: org.apache.storm.hive.bolt.TestHiveBolt / testname: testTickTuple
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hive.bolt.TestHiveBolt.generateTestTuple(TestHiveBolt.java:447)
	at org.apache.storm.hive.bolt.TestHiveBolt.testTickTuple(TestHiveBolt.java:352)

-------------------- system-out --------------------
17678 [main] INFO  o.a.h.h.q.s.SessionState - Created local directory: /tmp/bfa81e12-55b9-4162-86b6-b1385674e727_resources
17687 [main] INFO  o.a.h.h.q.s.SessionState - Created HDFS directory: /tmp/hive/travis/bfa81e12-55b9-4162-86b6-b1385674e727
17689 [main] INFO  o.a.h.h.q.s.SessionState - Created local directory: /tmp/travis/bfa81e12-55b9-4162-86b6-b1385674e727
17690 [main] INFO  o.a.h.h.q.s.SessionState - Created HDFS directory: /tmp/hive/travis/bfa81e12-55b9-4162-86b6-b1385674e727/_tmp_space.db
17690 [main] INFO  o.a.h.h.q.s.SessionState - No Tez session required at this point. hive.execution.engine=mr.
17691 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_table_names_by_filter: db = testdb, filter = 
17691 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table_names_by_filter: db = testdb, filter = 	
17691 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
17692 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
17692 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
17694 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
17694 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
17701 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_table : db=testdb tbl=test_table
17701 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table : db=testdb tbl=test_table	
17709 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: drop_table : db=testdb tbl=test_table
17709 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=drop_table : db=testdb tbl=test_table	
17891 [main] INFO  h.m.hivemetastoressimpl - deleting  raw:/tmp/junit317350862890316436/testdb.db/test_table
17891 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_database: testdb
17891 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_database: testdb	
17895 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: drop_database: testdb
17895 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=drop_database: testdb	
17896 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_all_tables: db=testdb
17896 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_all_tables: db=testdb	
17899 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_functions: db=testdb pat=*
17899 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_functions: db=testdb pat=*	
17902 [main] INFO  o.a.h.h.m.ObjectStore - Dropping database testdb along with all tables
17917 [main] INFO  h.m.hivemetastoressimpl - deleting  raw:/tmp/junit317350862890316436/testdb.db
17918 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
17919 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
17920 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
17920 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
17920 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: create_database: Database(name:testdb, description:null, locationUri:raw:///tmp/junit2827493602031080775/testdb.db, parameters:null)
17920 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=create_database: Database(name:testdb, description:null, locationUri:raw:///tmp/junit2827493602031080775/testdb.db, parameters:null)	
17921 [main] WARN  o.a.h.h.m.ObjectStore - Failed to get database testdb, returning NoSuchObjectException
17924 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: create_table: Table(tableName:test_table, dbName:testdb, owner:null, createTime:0, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:), FieldSchema(name:msg, type:string, comment:)], location:raw:///tmp/junit2827493602031080775/testdb.db/test_table, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:1, serdeInfo:SerDeInfo(name:test_table, serializationLib:org.apache.hadoop.hive.ql.io.orc.OrcSerde, parameters:{serialization.format=1}), bucketCols:[], sortCols:null, parameters:null), partitionKeys:[FieldSchema(name:city, type:string, comment:), FieldSchema(name:state, type:string, comment:)], parameters:{}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)
17924 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=create_table: Table(tableName:test_table, dbName:testdb, owner:null, createTime:0, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:), FieldSchema(name:msg, type:string, comment:)], location:raw:///tmp/junit2827493602031080775/testdb.db/test_table, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:1, serdeInfo:SerDeInfo(name:test_table, serializationLib:org.apache.hadoop.hive.ql.io.orc.OrcSerde, parameters:{serialization.format=1}), bucketCols:[], sortCols:null, parameters:null), partitionKeys:[FieldSchema(name:city, type:string, comment:), FieldSchema(name:state, type:string, comment:)], parameters:{}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
17925 [main] WARN  o.a.h.h.m.HiveMetaStore - Location: raw:///tmp/junit2827493602031080775/testdb.db/test_table specified for non-external table:test_table
17926 [main] INFO  o.a.h.h.c.FileUtils - Creating directory if it doesn't exist: raw:/tmp/junit2827493602031080775/testdb.db/test_table
17958 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: add_partition : db=testdb tbl=test_table
17958 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=add_partition : db=testdb tbl=test_table	
17984 [main] INFO  o.a.h.h.c.FileUtils - Creating directory if it doesn't exist: raw:/tmp/junit2827493602031080775/testdb.db/test_table/city=sunnyvale/state=ca
17999 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Shutting down the object store...
17999 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=Shutting down the object store...	
17999 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Metastore shutdown complete.
17999 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=Metastore shutdown complete.	
done

--------------------------------------------------
--------------------------------------------------
classname: org.apache.storm.hive.bolt.TestHiveBolt / testname: testWithTimeformat
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hive.bolt.TestHiveBolt.generateTestTuple(TestHiveBolt.java:447)
	at org.apache.storm.hive.bolt.TestHiveBolt.testWithTimeformat(TestHiveBolt.java:230)

-------------------- system-out --------------------
18207 [main] INFO  o.a.h.h.q.s.SessionState - Created local directory: /tmp/551c7463-8c28-499c-82e7-1642877df147_resources
18208 [main] INFO  o.a.h.h.q.s.SessionState - Created HDFS directory: /tmp/hive/travis/551c7463-8c28-499c-82e7-1642877df147
18210 [main] INFO  o.a.h.h.q.s.SessionState - Created local directory: /tmp/travis/551c7463-8c28-499c-82e7-1642877df147
18216 [main] INFO  o.a.h.h.q.s.SessionState - Created HDFS directory: /tmp/hive/travis/551c7463-8c28-499c-82e7-1642877df147/_tmp_space.db
18216 [main] INFO  o.a.h.h.q.s.SessionState - No Tez session required at this point. hive.execution.engine=mr.
18217 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_table_names_by_filter: db = testdb, filter = 
18217 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table_names_by_filter: db = testdb, filter = 	
18217 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
18218 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
18218 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
18220 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
18220 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
18225 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_table : db=testdb tbl=test_table
18233 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table : db=testdb tbl=test_table	
18253 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: drop_table : db=testdb tbl=test_table
18253 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=drop_table : db=testdb tbl=test_table	
18474 [main] INFO  h.m.hivemetastoressimpl - deleting  raw:/tmp/junit2827493602031080775/testdb.db/test_table
18475 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_database: testdb
18475 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_database: testdb	
18479 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: drop_database: testdb
18479 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=drop_database: testdb	
18479 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_all_tables: db=testdb
18479 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_all_tables: db=testdb	
18482 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_functions: db=testdb pat=*
18482 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_functions: db=testdb pat=*	
18485 [main] INFO  o.a.h.h.m.ObjectStore - Dropping database testdb along with all tables
18495 [main] INFO  h.m.hivemetastoressimpl - deleting  raw:/tmp/junit2827493602031080775/testdb.db
18496 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
18496 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
18503 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
18503 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
18503 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: create_database: Database(name:testdb, description:null, locationUri:raw:///tmp/junit5821905732039988369/testdb.db, parameters:null)
18503 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=create_database: Database(name:testdb, description:null, locationUri:raw:///tmp/junit5821905732039988369/testdb.db, parameters:null)	
18504 [main] WARN  o.a.h.h.m.ObjectStore - Failed to get database testdb, returning NoSuchObjectException
18511 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: create_table: Table(tableName:test_table, dbName:testdb, owner:null, createTime:0, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:), FieldSchema(name:msg, type:string, comment:)], location:raw:///tmp/junit5821905732039988369/testdb.db/test_table, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:1, serdeInfo:SerDeInfo(name:test_table, serializationLib:org.apache.hadoop.hive.ql.io.orc.OrcSerde, parameters:{serialization.format=1}), bucketCols:[], sortCols:null, parameters:null), partitionKeys:[FieldSchema(name:city, type:string, comment:), FieldSchema(name:state, type:string, comment:)], parameters:{}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)
18511 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=create_table: Table(tableName:test_table, dbName:testdb, owner:null, createTime:0, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:), FieldSchema(name:msg, type:string, comment:)], location:raw:///tmp/junit5821905732039988369/testdb.db/test_table, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:1, serdeInfo:SerDeInfo(name:test_table, serializationLib:org.apache.hadoop.hive.ql.io.orc.OrcSerde, parameters:{serialization.format=1}), bucketCols:[], sortCols:null, parameters:null), partitionKeys:[FieldSchema(name:city, type:string, comment:), FieldSchema(name:state, type:string, comment:)], parameters:{}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
18512 [main] WARN  o.a.h.h.m.HiveMetaStore - Location: raw:///tmp/junit5821905732039988369/testdb.db/test_table specified for non-external table:test_table
18512 [main] INFO  o.a.h.h.c.FileUtils - Creating directory if it doesn't exist: raw:/tmp/junit5821905732039988369/testdb.db/test_table
18570 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: add_partition : db=testdb tbl=test_table
18570 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=add_partition : db=testdb tbl=test_table	
18605 [main] INFO  o.a.h.h.c.FileUtils - Creating directory if it doesn't exist: raw:/tmp/junit5821905732039988369/testdb.db/test_table/city=sunnyvale/state=ca
18618 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Shutting down the object store...
18618 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=Shutting down the object store...	
18618 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Metastore shutdown complete.
18618 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=Metastore shutdown complete.	
done
18619 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_table_names_by_filter: db = testdb1, filter = 
18619 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table_names_by_filter: db = testdb1, filter = 	
18619 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
18620 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
18620 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
18621 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
18621 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
18626 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_table : db=testdb1 tbl=test_table1
18626 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table : db=testdb1 tbl=test_table1	
18632 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: drop_table : db=testdb1 tbl=test_table1
18632 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=drop_table : db=testdb1 tbl=test_table1	
18703 [main] INFO  h.m.hivemetastoressimpl - deleting  raw:/tmp/junit9128036246447358169/testdb.db/test_table1
18703 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_database: testdb1
18703 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_database: testdb1	
18704 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: drop_database: testdb1
18704 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=drop_database: testdb1	
18704 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_all_tables: db=testdb1
18704 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_all_tables: db=testdb1	
18705 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_functions: db=testdb1 pat=*
18705 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_functions: db=testdb1 pat=*	
18705 [main] INFO  o.a.h.h.m.ObjectStore - Dropping database testdb1 along with all tables
18715 [main] INFO  h.m.hivemetastoressimpl - deleting  raw:/tmp/junit9128036246447358169/testdb.db
18715 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
18716 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
18717 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
18717 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
18717 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: create_database: Database(name:testdb1, description:null, locationUri:raw:///tmp/junit5821905732039988369/testdb.db, parameters:null)
18717 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=create_database: Database(name:testdb1, description:null, locationUri:raw:///tmp/junit5821905732039988369/testdb.db, parameters:null)	
18718 [main] WARN  o.a.h.h.m.ObjectStore - Failed to get database testdb1, returning NoSuchObjectException
18722 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: create_table: Table(tableName:test_table1, dbName:testdb1, owner:null, createTime:0, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:), FieldSchema(name:msg, type:string, comment:)], location:raw:///tmp/junit5821905732039988369/testdb.db/test_table1, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:1, serdeInfo:SerDeInfo(name:test_table1, serializationLib:org.apache.hadoop.hive.ql.io.orc.OrcSerde, parameters:{serialization.format=1}), bucketCols:[], sortCols:null, parameters:null), partitionKeys:[FieldSchema(name:dt, type:string, comment:)], parameters:{}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)
18722 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=create_table: Table(tableName:test_table1, dbName:testdb1, owner:null, createTime:0, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:), FieldSchema(name:msg, type:string, comment:)], location:raw:///tmp/junit5821905732039988369/testdb.db/test_table1, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:1, serdeInfo:SerDeInfo(name:test_table1, serializationLib:org.apache.hadoop.hive.ql.io.orc.OrcSerde, parameters:{serialization.format=1}), bucketCols:[], sortCols:null, parameters:null), partitionKeys:[FieldSchema(name:dt, type:string, comment:)], parameters:{}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
18723 [main] WARN  o.a.h.h.m.HiveMetaStore - Location: raw:///tmp/junit5821905732039988369/testdb.db/test_table1 specified for non-external table:test_table1
18723 [main] INFO  o.a.h.h.c.FileUtils - Creating directory if it doesn't exist: raw:/tmp/junit5821905732039988369/testdb.db/test_table1
18751 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Shutting down the object store...
18752 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=Shutting down the object store...	
18752 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Metastore shutdown complete.
18752 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=Metastore shutdown complete.	
18759 [main] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=compile from=org.apache.hadoop.hive.ql.Driver>
18759 [main] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=parse from=org.apache.hadoop.hive.ql.Driver>
18759 [main] INFO  h.q.p.ParseDriver - Parsing command: select * from testdb1.test_table1
18759 [main] INFO  h.q.p.ParseDriver - Parse Completed
18759 [main] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=parse start=1466737121698 end=1466737121698 duration=0 from=org.apache.hadoop.hive.ql.Driver>
18765 [main] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=semanticAnalyze from=org.apache.hadoop.hive.ql.Driver>
18766 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Starting Semantic Analysis
18766 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Completed phase 1 of Semantic Analysis
18766 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Get metadata for source tables
18767 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_table : db=testdb1 tbl=test_table1
18767 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table : db=testdb1 tbl=test_table1	
18767 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
18767 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
18768 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
18769 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
18769 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
18776 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Get metadata for subqueries
18776 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Get metadata for destination tables
18778 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Completed getting MetaData in Semantic Analysis
18779 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Set stats collection dir : file:/tmp/travis/551c7463-8c28-499c-82e7-1642877df147/hive_2016-06-24_02-58-41_698_8892567846681679722-1/-ext-10002
18780 [main] INFO  o.a.h.h.q.p.OpProcFactory - Processing for FS(10)
18780 [main] INFO  o.a.h.h.q.p.OpProcFactory - Processing for SEL(9)
18780 [main] INFO  o.a.h.h.q.p.OpProcFactory - Processing for TS(8)
18781 [main] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=partition-retrieving from=org.apache.hadoop.hive.ql.optimizer.ppr.PartitionPruner>
18781 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_partitions : db=testdb1 tbl=test_table1
18781 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_partitions : db=testdb1 tbl=test_table1	
18784 [main] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=partition-retrieving start=1466737121720 end=1466737121723 duration=3 from=org.apache.hadoop.hive.ql.optimizer.ppr.PartitionPruner>
18785 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Completed plan generation
18785 [main] INFO  o.a.h.h.q.Driver - Semantic Analysis Completed
18785 [main] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=semanticAnalyze start=1466737121704 end=1466737121724 duration=20 from=org.apache.hadoop.hive.ql.Driver>
18785 [main] INFO  o.a.h.h.q.e.TableScanOperator - Initializing Self TS[8]
18785 [main] INFO  o.a.h.h.q.e.TableScanOperator - Operator 8 TS initialized
18785 [main] INFO  o.a.h.h.q.e.TableScanOperator - Initializing children of 8 TS
18785 [main] INFO  o.a.h.h.q.e.SelectOperator - Initializing child 9 SEL
18785 [main] INFO  o.a.h.h.q.e.SelectOperator - Initializing Self SEL[9]
18785 [main] INFO  o.a.h.h.q.e.SelectOperator - SELECT struct<id:int,msg:string,dt:string>
18785 [main] INFO  o.a.h.h.q.e.SelectOperator - Operator 9 SEL initialized
18787 [main] INFO  o.a.h.h.q.e.SelectOperator - Initializing children of 9 SEL
18787 [main] INFO  o.a.h.h.q.e.ListSinkOperator - Initializing child 11 OP
18788 [main] INFO  o.a.h.h.q.e.ListSinkOperator - Initializing Self OP[11]
18788 [main] INFO  o.a.h.h.q.e.ListSinkOperator - Operator 11 OP initialized
18788 [main] INFO  o.a.h.h.q.e.ListSinkOperator - Initialization Done 11 OP
18788 [main] INFO  o.a.h.h.q.e.SelectOperator - Initialization Done 9 SEL
18788 [main] INFO  o.a.h.h.q.e.TableScanOperator - Initialization Done 8 TS
18788 [main] INFO  o.a.h.h.q.Driver - Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:test_table1.id, type:int, comment:null), FieldSchema(name:test_table1.msg, type:string, comment:null), FieldSchema(name:test_table1.dt, type:string, comment:null)], properties:null)
18788 [main] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=compile start=1466737121698 end=1466737121727 duration=29 from=org.apache.hadoop.hive.ql.Driver>

--------------------------------------------------
--------------------------------------------------
classname: org.apache.storm.hive.bolt.TestHiveBolt / testname: testWithByteArrayIdandMessage
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hive.bolt.TestHiveBolt.generateTestTuple(TestHiveBolt.java:447)
	at org.apache.storm.hive.bolt.TestHiveBolt.testWithByteArrayIdandMessage(TestHiveBolt.java:161)

-------------------- system-out --------------------
18950 [main] INFO  o.a.h.h.q.s.SessionState - Created local directory: /tmp/4fc41f81-9f3c-4534-b389-f062419fca10_resources
18952 [main] INFO  o.a.h.h.q.s.SessionState - Created HDFS directory: /tmp/hive/travis/4fc41f81-9f3c-4534-b389-f062419fca10
18965 [main] INFO  o.a.h.h.q.s.SessionState - Created local directory: /tmp/travis/4fc41f81-9f3c-4534-b389-f062419fca10
18985 [main] INFO  o.a.h.h.q.s.SessionState - Created HDFS directory: /tmp/hive/travis/4fc41f81-9f3c-4534-b389-f062419fca10/_tmp_space.db
18986 [main] INFO  o.a.h.h.q.s.SessionState - No Tez session required at this point. hive.execution.engine=mr.
18987 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
18988 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
18989 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
18989 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
18989 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_table_names_by_filter: db = testdb, filter = 
18989 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table_names_by_filter: db = testdb, filter = 	
18990 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_table : db=testdb tbl=test_table
18990 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table : db=testdb tbl=test_table	
19047 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: drop_table : db=testdb tbl=test_table
19047 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=drop_table : db=testdb tbl=test_table	
19217 [main] INFO  h.m.hivemetastoressimpl - deleting  raw:/tmp/junit5821905732039988369/testdb.db/test_table
19218 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_database: testdb
19218 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_database: testdb	
19221 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: drop_database: testdb
19222 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=drop_database: testdb	
19222 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_all_tables: db=testdb
19222 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_all_tables: db=testdb	
19225 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_functions: db=testdb pat=*
19225 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_functions: db=testdb pat=*	
19234 [main] INFO  o.a.h.h.m.ObjectStore - Dropping database testdb along with all tables
19248 [main] INFO  h.m.hivemetastoressimpl - deleting  raw:/tmp/junit5821905732039988369/testdb.db
19248 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
19249 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
19250 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
19250 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
19250 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: create_database: Database(name:testdb, description:null, locationUri:raw:///tmp/junit7984166411665858017/testdb.db, parameters:null)
19250 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=create_database: Database(name:testdb, description:null, locationUri:raw:///tmp/junit7984166411665858017/testdb.db, parameters:null)	
19251 [main] WARN  o.a.h.h.m.ObjectStore - Failed to get database testdb, returning NoSuchObjectException
19265 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: create_table: Table(tableName:test_table, dbName:testdb, owner:null, createTime:0, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:), FieldSchema(name:msg, type:string, comment:)], location:raw:///tmp/junit7984166411665858017/testdb.db/test_table, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:1, serdeInfo:SerDeInfo(name:test_table, serializationLib:org.apache.hadoop.hive.ql.io.orc.OrcSerde, parameters:{serialization.format=1}), bucketCols:[], sortCols:null, parameters:null), partitionKeys:[FieldSchema(name:city, type:string, comment:), FieldSchema(name:state, type:string, comment:)], parameters:{}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)
19265 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=create_table: Table(tableName:test_table, dbName:testdb, owner:null, createTime:0, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:), FieldSchema(name:msg, type:string, comment:)], location:raw:///tmp/junit7984166411665858017/testdb.db/test_table, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:1, serdeInfo:SerDeInfo(name:test_table, serializationLib:org.apache.hadoop.hive.ql.io.orc.OrcSerde, parameters:{serialization.format=1}), bucketCols:[], sortCols:null, parameters:null), partitionKeys:[FieldSchema(name:city, type:string, comment:), FieldSchema(name:state, type:string, comment:)], parameters:{}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
19266 [main] WARN  o.a.h.h.m.HiveMetaStore - Location: raw:///tmp/junit7984166411665858017/testdb.db/test_table specified for non-external table:test_table
19266 [main] INFO  o.a.h.h.c.FileUtils - Creating directory if it doesn't exist: raw:/tmp/junit7984166411665858017/testdb.db/test_table
19308 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: add_partition : db=testdb tbl=test_table
19308 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=add_partition : db=testdb tbl=test_table	
19352 [main] INFO  o.a.h.h.c.FileUtils - Creating directory if it doesn't exist: raw:/tmp/junit7984166411665858017/testdb.db/test_table/city=sunnyvale/state=ca
19365 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Shutting down the object store...
19365 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=Shutting down the object store...	
19365 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Metastore shutdown complete.
19365 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=Metastore shutdown complete.	
done
19366 [main] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=compile from=org.apache.hadoop.hive.ql.Driver>
19366 [main] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=parse from=org.apache.hadoop.hive.ql.Driver>
19366 [main] INFO  h.q.p.ParseDriver - Parsing command: select * from testdb.test_table
19366 [main] INFO  h.q.p.ParseDriver - Parse Completed
19367 [main] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=parse start=1466737122305 end=1466737122306 duration=1 from=org.apache.hadoop.hive.ql.Driver>
19372 [main] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=semanticAnalyze from=org.apache.hadoop.hive.ql.Driver>
19372 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Starting Semantic Analysis
19372 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Completed phase 1 of Semantic Analysis
19372 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Get metadata for source tables
19372 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_table : db=testdb tbl=test_table
19372 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table : db=testdb tbl=test_table	
19372 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
19373 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
19373 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
19374 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
19374 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
19382 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Get metadata for subqueries
19382 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Get metadata for destination tables
19384 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Completed getting MetaData in Semantic Analysis
19385 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Set stats collection dir : file:/tmp/travis/4fc41f81-9f3c-4534-b389-f062419fca10/hive_2016-06-24_02-58-42_305_3657884180760006060-1/-ext-10002
19393 [main] INFO  o.a.h.h.q.p.OpProcFactory - Processing for FS(14)
19393 [main] INFO  o.a.h.h.q.p.OpProcFactory - Processing for SEL(13)
19393 [main] INFO  o.a.h.h.q.p.OpProcFactory - Processing for TS(12)
19394 [main] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=partition-retrieving from=org.apache.hadoop.hive.ql.optimizer.ppr.PartitionPruner>
19394 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_partitions : db=testdb tbl=test_table
19394 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_partitions : db=testdb tbl=test_table	
19414 [main] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=partition-retrieving start=1466737122333 end=1466737122353 duration=20 from=org.apache.hadoop.hive.ql.optimizer.ppr.PartitionPruner>
19415 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Completed plan generation
19415 [main] INFO  o.a.h.h.q.Driver - Semantic Analysis Completed
19415 [main] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=semanticAnalyze start=1466737122311 end=1466737122354 duration=43 from=org.apache.hadoop.hive.ql.Driver>
19415 [main] INFO  o.a.h.h.q.e.TableScanOperator - Initializing Self TS[12]
19416 [main] INFO  o.a.h.h.q.e.TableScanOperator - Operator 12 TS initialized
19416 [main] INFO  o.a.h.h.q.e.TableScanOperator - Initializing children of 12 TS
19416 [main] INFO  o.a.h.h.q.e.SelectOperator - Initializing child 13 SEL
19416 [main] INFO  o.a.h.h.q.e.SelectOperator - Initializing Self SEL[13]
19416 [main] INFO  o.a.h.h.q.e.SelectOperator - SELECT struct<id:int,msg:string,city:string,state:string>
19416 [main] INFO  o.a.h.h.q.e.SelectOperator - Operator 13 SEL initialized
19416 [main] INFO  o.a.h.h.q.e.SelectOperator - Initializing children of 13 SEL
19416 [main] INFO  o.a.h.h.q.e.ListSinkOperator - Initializing child 15 OP
19416 [main] INFO  o.a.h.h.q.e.ListSinkOperator - Initializing Self OP[15]
19416 [main] INFO  o.a.h.h.q.e.ListSinkOperator - Operator 15 OP initialized
19416 [main] INFO  o.a.h.h.q.e.ListSinkOperator - Initialization Done 15 OP
19416 [main] INFO  o.a.h.h.q.e.SelectOperator - Initialization Done 13 SEL
19416 [main] INFO  o.a.h.h.q.e.TableScanOperator - Initialization Done 12 TS
19416 [main] INFO  o.a.h.h.q.Driver - Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:test_table.id, type:int, comment:null), FieldSchema(name:test_table.msg, type:string, comment:null), FieldSchema(name:test_table.city, type:string, comment:null), FieldSchema(name:test_table.state, type:string, comment:null)], properties:null)
19416 [main] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=compile start=1466737122305 end=1466737122355 duration=50 from=org.apache.hadoop.hive.ql.Driver>

--------------------------------------------------
--------------------------------------------------
classname: org.apache.storm.hive.bolt.TestHiveBolt / testname: testNoAcksIfFlushFails
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hive.bolt.TestHiveBolt.generateTestTuple(TestHiveBolt.java:447)
	at org.apache.storm.hive.bolt.TestHiveBolt.testNoAcksIfFlushFails(TestHiveBolt.java:327)

-------------------- system-out --------------------
19589 [main] INFO  o.a.h.h.q.s.SessionState - Created local directory: /tmp/aa246f8f-068e-48ef-aa64-10a75dbebea9_resources
19590 [main] INFO  o.a.h.h.q.s.SessionState - Created HDFS directory: /tmp/hive/travis/aa246f8f-068e-48ef-aa64-10a75dbebea9
19592 [main] INFO  o.a.h.h.q.s.SessionState - Created local directory: /tmp/travis/aa246f8f-068e-48ef-aa64-10a75dbebea9
19594 [main] INFO  o.a.h.h.q.s.SessionState - Created HDFS directory: /tmp/hive/travis/aa246f8f-068e-48ef-aa64-10a75dbebea9/_tmp_space.db
19594 [main] INFO  o.a.h.h.q.s.SessionState - No Tez session required at this point. hive.execution.engine=mr.
19595 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
19596 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
19597 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
19597 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
19597 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_table_names_by_filter: db = testdb, filter = 
19597 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table_names_by_filter: db = testdb, filter = 	
19600 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_table : db=testdb tbl=test_table
19600 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table : db=testdb tbl=test_table	
19607 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: drop_table : db=testdb tbl=test_table
19607 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=drop_table : db=testdb tbl=test_table	
19810 [main] INFO  h.m.hivemetastoressimpl - deleting  raw:/tmp/junit7984166411665858017/testdb.db/test_table
19811 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_database: testdb
19811 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_database: testdb	
19830 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: drop_database: testdb
19830 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=drop_database: testdb	
19830 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_all_tables: db=testdb
19831 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_all_tables: db=testdb	
19833 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_functions: db=testdb pat=*
19834 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_functions: db=testdb pat=*	
19836 [main] INFO  o.a.h.h.m.ObjectStore - Dropping database testdb along with all tables
19847 [main] INFO  h.m.hivemetastoressimpl - deleting  raw:/tmp/junit7984166411665858017/testdb.db
19848 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
19849 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
19850 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
19850 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
19850 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: create_database: Database(name:testdb, description:null, locationUri:raw:///tmp/junit8765441595077161124/testdb.db, parameters:null)
19850 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=create_database: Database(name:testdb, description:null, locationUri:raw:///tmp/junit8765441595077161124/testdb.db, parameters:null)	
19851 [main] WARN  o.a.h.h.m.ObjectStore - Failed to get database testdb, returning NoSuchObjectException
19857 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: create_table: Table(tableName:test_table, dbName:testdb, owner:null, createTime:0, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:), FieldSchema(name:msg, type:string, comment:)], location:raw:///tmp/junit8765441595077161124/testdb.db/test_table, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:1, serdeInfo:SerDeInfo(name:test_table, serializationLib:org.apache.hadoop.hive.ql.io.orc.OrcSerde, parameters:{serialization.format=1}), bucketCols:[], sortCols:null, parameters:null), partitionKeys:[FieldSchema(name:city, type:string, comment:), FieldSchema(name:state, type:string, comment:)], parameters:{}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)
19857 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=create_table: Table(tableName:test_table, dbName:testdb, owner:null, createTime:0, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:), FieldSchema(name:msg, type:string, comment:)], location:raw:///tmp/junit8765441595077161124/testdb.db/test_table, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:1, serdeInfo:SerDeInfo(name:test_table, serializationLib:org.apache.hadoop.hive.ql.io.orc.OrcSerde, parameters:{serialization.format=1}), bucketCols:[], sortCols:null, parameters:null), partitionKeys:[FieldSchema(name:city, type:string, comment:), FieldSchema(name:state, type:string, comment:)], parameters:{}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
19858 [main] WARN  o.a.h.h.m.HiveMetaStore - Location: raw:///tmp/junit8765441595077161124/testdb.db/test_table specified for non-external table:test_table
19858 [main] INFO  o.a.h.h.c.FileUtils - Creating directory if it doesn't exist: raw:/tmp/junit8765441595077161124/testdb.db/test_table
19889 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: add_partition : db=testdb tbl=test_table
19889 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=add_partition : db=testdb tbl=test_table	
19911 [main] INFO  o.a.h.h.c.FileUtils - Creating directory if it doesn't exist: raw:/tmp/junit8765441595077161124/testdb.db/test_table/city=sunnyvale/state=ca
19927 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Shutting down the object store...
19927 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=Shutting down the object store...	
19927 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Metastore shutdown complete.
19928 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=Metastore shutdown complete.	
done

--------------------------------------------------
Checking ./external/storm-hive/target/surefire-reports/TEST-org.apache.storm.hive.common.TestHiveWriter.xml
--------------------------------------------------
classname: org.apache.storm.hive.common.TestHiveWriter / testname: testWriteBasic
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hive.common.TestHiveWriter.generateTestTuple(TestHiveWriter.java:164)
	at org.apache.storm.hive.common.TestHiveWriter.writeTuples(TestHiveWriter.java:179)
	at org.apache.storm.hive.common.TestHiveWriter.testWriteBasic(TestHiveWriter.java:127)

-------------------- system-out --------------------
20256 [main] INFO  o.a.h.h.q.s.SessionState - Created local directory: /tmp/b5b560ee-e2c5-43c3-92ea-affff281d20c_resources
20258 [main] INFO  o.a.h.h.q.s.SessionState - Created HDFS directory: /tmp/hive/travis/b5b560ee-e2c5-43c3-92ea-affff281d20c
20259 [main] INFO  o.a.h.h.q.s.SessionState - Created local directory: /tmp/travis/b5b560ee-e2c5-43c3-92ea-affff281d20c
20261 [main] INFO  o.a.h.h.q.s.SessionState - Created HDFS directory: /tmp/hive/travis/b5b560ee-e2c5-43c3-92ea-affff281d20c/_tmp_space.db
20261 [main] INFO  o.a.h.h.q.s.SessionState - No Tez session required at this point. hive.execution.engine=mr.
20261 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_table_names_by_filter: db = testdb, filter = 
20261 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table_names_by_filter: db = testdb, filter = 	
20261 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
20262 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
20263 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
20264 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
20264 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
20287 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_table : db=testdb tbl=test_table
20287 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table : db=testdb tbl=test_table	
20312 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: drop_table : db=testdb tbl=test_table
20312 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=drop_table : db=testdb tbl=test_table	
20514 [main] INFO  h.m.hivemetastoressimpl - deleting  raw:/tmp/junit8765441595077161124/testdb.db/test_table
20514 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_database: testdb
20514 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_database: testdb	
20518 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: drop_database: testdb
20518 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=drop_database: testdb	
20519 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_all_tables: db=testdb
20519 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_all_tables: db=testdb	
20522 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_functions: db=testdb pat=*
20522 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_functions: db=testdb pat=*	
20527 [main] INFO  o.a.h.h.m.ObjectStore - Dropping database testdb along with all tables
20553 [main] INFO  h.m.hivemetastoressimpl - deleting  raw:/tmp/junit8765441595077161124/testdb.db
20554 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
20555 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
20557 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
20557 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
20557 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: create_database: Database(name:testdb, description:null, locationUri:/tmp/junit6408392379210321956/testdb.db, parameters:null)
20557 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=create_database: Database(name:testdb, description:null, locationUri:/tmp/junit6408392379210321956/testdb.db, parameters:null)	
20558 [main] WARN  o.a.h.h.m.ObjectStore - Failed to get database testdb, returning NoSuchObjectException
20558 [main] INFO  o.a.h.h.c.FileUtils - Creating directory if it doesn't exist: file:/tmp/junit6408392379210321956/testdb.db
20561 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: create_table: Table(tableName:test_table2, dbName:testdb, owner:null, createTime:0, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:), FieldSchema(name:msg, type:string, comment:)], location:/tmp/junit6408392379210321956/testdb.db/test_table2, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:1, serdeInfo:SerDeInfo(name:test_table2, serializationLib:org.apache.hadoop.hive.ql.io.orc.OrcSerde, parameters:{serialization.format=1}), bucketCols:[], sortCols:null, parameters:null), partitionKeys:[FieldSchema(name:city, type:string, comment:), FieldSchema(name:state, type:string, comment:)], parameters:{}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)
20561 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=create_table: Table(tableName:test_table2, dbName:testdb, owner:null, createTime:0, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:), FieldSchema(name:msg, type:string, comment:)], location:/tmp/junit6408392379210321956/testdb.db/test_table2, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:1, serdeInfo:SerDeInfo(name:test_table2, serializationLib:org.apache.hadoop.hive.ql.io.orc.OrcSerde, parameters:{serialization.format=1}), bucketCols:[], sortCols:null, parameters:null), partitionKeys:[FieldSchema(name:city, type:string, comment:), FieldSchema(name:state, type:string, comment:)], parameters:{}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
20562 [main] WARN  o.a.h.h.m.HiveMetaStore - Location: /tmp/junit6408392379210321956/testdb.db/test_table2 specified for non-external table:test_table2
20562 [main] INFO  o.a.h.h.c.FileUtils - Creating directory if it doesn't exist: file:/tmp/junit6408392379210321956/testdb.db/test_table2
20580 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: add_partition : db=testdb tbl=test_table2
20580 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=add_partition : db=testdb tbl=test_table2	
20607 [main] INFO  o.a.h.h.c.FileUtils - Creating directory if it doesn't exist: file:/tmp/junit6408392379210321956/testdb.db/test_table2/city=sunnyvale/state=ca
20623 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Shutting down the object store...
20624 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=Shutting down the object store...	
20624 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Metastore shutdown complete.
20624 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=Metastore shutdown complete.	
20711 [hiveWriterTest] INFO  o.a.h.h.q.s.SessionState - Created local directory: /tmp/6e217024-31b5-44fd-a398-b70231b2f5e9_resources
20712 [hiveWriterTest] INFO  o.a.h.h.q.s.SessionState - Created HDFS directory: /tmp/hive/travis/6e217024-31b5-44fd-a398-b70231b2f5e9
20714 [hiveWriterTest] INFO  o.a.h.h.q.s.SessionState - Created local directory: /tmp/travis/6e217024-31b5-44fd-a398-b70231b2f5e9
20715 [hiveWriterTest] INFO  o.a.h.h.q.s.SessionState - Created HDFS directory: /tmp/hive/travis/6e217024-31b5-44fd-a398-b70231b2f5e9/_tmp_space.db
20715 [hiveWriterTest] INFO  o.a.h.h.q.s.SessionState - No Tez session required at this point. hive.execution.engine=mr.
20715 [hiveWriterTest] INFO  o.a.h.h.m.HiveMetaStore - 1: get_table : db=testdb tbl=test_table2
20716 [hiveWriterTest] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table : db=testdb tbl=test_table2	
20716 [hiveWriterTest] INFO  o.a.h.h.m.HiveMetaStore - 1: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
20716 [hiveWriterTest] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
20717 [hiveWriterTest] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
20727 [hiveWriterTest] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
20727 [hiveWriterTest] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
20760 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=Driver.run from=org.apache.hadoop.hive.ql.Driver>
20760 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=TimeToSubmit from=org.apache.hadoop.hive.ql.Driver>
20760 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=compile from=org.apache.hadoop.hive.ql.Driver>
20761 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=parse from=org.apache.hadoop.hive.ql.Driver>
20761 [hiveWriterTest] INFO  h.q.p.ParseDriver - Parsing command: use testdb
20761 [hiveWriterTest] INFO  h.q.p.ParseDriver - Parse Completed
20761 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=parse start=1466737123700 end=1466737123700 duration=0 from=org.apache.hadoop.hive.ql.Driver>
20767 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=semanticAnalyze from=org.apache.hadoop.hive.ql.Driver>
20773 [hiveWriterTest] INFO  o.a.h.h.m.HiveMetaStore - 1: get_database: testdb
20773 [hiveWriterTest] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_database: testdb	
20788 [hiveWriterTest] INFO  o.a.h.h.q.Driver - Semantic Analysis Completed
20788 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=semanticAnalyze start=1466737123706 end=1466737123727 duration=21 from=org.apache.hadoop.hive.ql.Driver>
20788 [hiveWriterTest] INFO  o.a.h.h.q.Driver - Returning Hive schema: Schema(fieldSchemas:null, properties:null)
20788 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=compile start=1466737123699 end=1466737123727 duration=28 from=org.apache.hadoop.hive.ql.Driver>
20788 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=acquireReadWriteLocks from=org.apache.hadoop.hive.ql.Driver>
20790 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=acquireReadWriteLocks start=1466737123727 end=1466737123729 duration=2 from=org.apache.hadoop.hive.ql.Driver>
20790 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=Driver.execute from=org.apache.hadoop.hive.ql.Driver>
20790 [hiveWriterTest] INFO  o.a.h.h.q.Driver - Starting command: use testdb
20793 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=TimeToSubmit start=1466737123699 end=1466737123732 duration=33 from=org.apache.hadoop.hive.ql.Driver>
20793 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=runTasks from=org.apache.hadoop.hive.ql.Driver>
20793 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=task.DDL.Stage-0 from=org.apache.hadoop.hive.ql.Driver>
20796 [hiveWriterTest] INFO  o.a.h.h.q.Driver - Starting task [Stage-0:DDL] in serial mode
20796 [hiveWriterTest] INFO  o.a.h.h.m.HiveMetaStore - 1: get_database: testdb
20797 [hiveWriterTest] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_database: testdb	
20802 [hiveWriterTest] INFO  o.a.h.h.m.HiveMetaStore - 1: get_database: testdb
20802 [hiveWriterTest] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_database: testdb	
20802 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=runTasks start=1466737123732 end=1466737123741 duration=9 from=org.apache.hadoop.hive.ql.Driver>
20803 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=Driver.execute start=1466737123729 end=1466737123742 duration=13 from=org.apache.hadoop.hive.ql.Driver>
20803 [hiveWriterTest] INFO  o.a.h.h.q.Driver - OK
20803 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=releaseLocks from=org.apache.hadoop.hive.ql.Driver>
20803 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=releaseLocks start=1466737123742 end=1466737123742 duration=0 from=org.apache.hadoop.hive.ql.Driver>
20803 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=Driver.run start=1466737123699 end=1466737123742 duration=43 from=org.apache.hadoop.hive.ql.Driver>
20803 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=Driver.run from=org.apache.hadoop.hive.ql.Driver>
20803 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=TimeToSubmit from=org.apache.hadoop.hive.ql.Driver>
20804 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=compile from=org.apache.hadoop.hive.ql.Driver>
20804 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=parse from=org.apache.hadoop.hive.ql.Driver>
20804 [hiveWriterTest] INFO  h.q.p.ParseDriver - Parsing command: alter table test_table2 add if not exists partition  ( city='sunnyvale',state='ca' )
20807 [hiveWriterTest] INFO  h.q.p.ParseDriver - Parse Completed
20807 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=parse start=1466737123743 end=1466737123746 duration=3 from=org.apache.hadoop.hive.ql.Driver>
20812 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=semanticAnalyze from=org.apache.hadoop.hive.ql.Driver>
20813 [hiveWriterTest] INFO  o.a.h.h.m.HiveMetaStore - 1: get_table : db=testdb tbl=test_table2
20813 [hiveWriterTest] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table : db=testdb tbl=test_table2	
20829 [hiveWriterTest] INFO  o.a.h.h.q.Driver - Semantic Analysis Completed
20829 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=semanticAnalyze start=1466737123751 end=1466737123768 duration=17 from=org.apache.hadoop.hive.ql.Driver>
20830 [hiveWriterTest] INFO  o.a.h.h.q.Driver - Returning Hive schema: Schema(fieldSchemas:null, properties:null)
20830 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=compile start=1466737123743 end=1466737123769 duration=26 from=org.apache.hadoop.hive.ql.Driver>
20830 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=acquireReadWriteLocks from=org.apache.hadoop.hive.ql.Driver>
20858 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=acquireReadWriteLocks start=1466737123769 end=1466737123797 duration=28 from=org.apache.hadoop.hive.ql.Driver>
20858 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=Driver.execute from=org.apache.hadoop.hive.ql.Driver>
20859 [hiveWriterTest] INFO  o.a.h.h.q.Driver - Starting command: alter table test_table2 add if not exists partition  ( city='sunnyvale',state='ca' )
20859 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=TimeToSubmit start=1466737123742 end=1466737123798 duration=56 from=org.apache.hadoop.hive.ql.Driver>
20859 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=runTasks from=org.apache.hadoop.hive.ql.Driver>
20859 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=task.DDL.Stage-0 from=org.apache.hadoop.hive.ql.Driver>
20860 [hiveWriterTest] INFO  o.a.h.h.q.Driver - Starting task [Stage-0:DDL] in serial mode
20860 [hiveWriterTest] INFO  o.a.h.h.m.HiveMetaStore - 1: get_table : db=testdb tbl=test_table2
20860 [hiveWriterTest] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table : db=testdb tbl=test_table2	
20871 [hiveWriterTest] INFO  o.a.h.h.m.HiveMetaStore - 1: add_partitions
20872 [hiveWriterTest] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=add_partitions	
20877 [hiveWriterTest] INFO  o.a.h.h.m.HiveMetaStore - Not adding partition Partition(values:[sunnyvale, ca], dbName:testdb, tableName:test_table2, createTime:0, lastAccessTime:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:), FieldSchema(name:msg, type:string, comment:)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:1, serdeInfo:SerDeInfo(name:test_table2, serializationLib:org.apache.hadoop.hive.ql.io.orc.OrcSerde, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), parameters:null) as it already exists
20879 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=runTasks start=1466737123798 end=1466737123818 duration=20 from=org.apache.hadoop.hive.ql.Driver>
20879 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=Driver.execute start=1466737123797 end=1466737123818 duration=21 from=org.apache.hadoop.hive.ql.Driver>
20879 [hiveWriterTest] INFO  o.a.h.h.q.Driver - OK
20879 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=releaseLocks from=org.apache.hadoop.hive.ql.Driver>
20886 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=releaseLocks start=1466737123818 end=1466737123825 duration=7 from=org.apache.hadoop.hive.ql.Driver>
20886 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=Driver.run start=1466737123742 end=1466737123825 duration=83 from=org.apache.hadoop.hive.ql.Driver>
20938 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_table : db=testdb tbl=test_table2
20938 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table : db=testdb tbl=test_table2	
20938 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
20939 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
20939 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
20940 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
20940 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
20948 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_partition : db=testdb tbl=test_table2[sunnyvale,ca]
20948 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_partition : db=testdb tbl=test_table2[sunnyvale,ca]	
20965 [hiveWriterTest] INFO  o.a.h.h.m.HiveMetaStore - 1: get_table : db=testdb tbl=test_table2
20965 [hiveWriterTest] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table : db=testdb tbl=test_table2	

-------------------- system-err --------------------
OK
OK

--------------------------------------------------
--------------------------------------------------
classname: org.apache.storm.hive.common.TestHiveWriter / testname: testWriteMultiFlush
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hive.common.TestHiveWriter.generateTestTuple(TestHiveWriter.java:164)
	at org.apache.storm.hive.common.TestHiveWriter.testWriteMultiFlush(TestHiveWriter.java:142)

-------------------- system-out --------------------
21256 [main] INFO  o.a.h.h.q.s.SessionState - Created local directory: /tmp/cf3f87e8-ab1e-4043-87c2-0034bc373ab2_resources
21257 [main] INFO  o.a.h.h.q.s.SessionState - Created HDFS directory: /tmp/hive/travis/cf3f87e8-ab1e-4043-87c2-0034bc373ab2
21258 [main] INFO  o.a.h.h.q.s.SessionState - Created local directory: /tmp/travis/cf3f87e8-ab1e-4043-87c2-0034bc373ab2
21259 [main] INFO  o.a.h.h.q.s.SessionState - Created HDFS directory: /tmp/hive/travis/cf3f87e8-ab1e-4043-87c2-0034bc373ab2/_tmp_space.db
21260 [main] INFO  o.a.h.h.q.s.SessionState - No Tez session required at this point. hive.execution.engine=mr.
21260 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
21261 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
21262 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
21262 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
21262 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_table_names_by_filter: db = testdb, filter = 
21262 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table_names_by_filter: db = testdb, filter = 	
21265 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_table : db=testdb tbl=test_table2
21265 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table : db=testdb tbl=test_table2	
21278 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: drop_table : db=testdb tbl=test_table2
21278 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=drop_table : db=testdb tbl=test_table2	
21418 [main] INFO  h.m.hivemetastoressimpl - deleting  file:/tmp/junit6408392379210321956/testdb.db/test_table2
21419 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_database: testdb
21419 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_database: testdb	
21423 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: drop_database: testdb
21423 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=drop_database: testdb	
21424 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_all_tables: db=testdb
21424 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_all_tables: db=testdb	
21426 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_functions: db=testdb pat=*
21427 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_functions: db=testdb pat=*	
21430 [main] INFO  o.a.h.h.m.ObjectStore - Dropping database testdb along with all tables
21440 [main] INFO  h.m.hivemetastoressimpl - deleting  file:/tmp/junit6408392379210321956/testdb.db
21441 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
21442 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
21448 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
21448 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
21448 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: create_database: Database(name:testdb, description:null, locationUri:/tmp/junit4858986149037536153/testdb.db, parameters:null)
21448 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=create_database: Database(name:testdb, description:null, locationUri:/tmp/junit4858986149037536153/testdb.db, parameters:null)	
21449 [main] WARN  o.a.h.h.m.ObjectStore - Failed to get database testdb, returning NoSuchObjectException
21449 [main] INFO  o.a.h.h.c.FileUtils - Creating directory if it doesn't exist: file:/tmp/junit4858986149037536153/testdb.db
21456 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: create_table: Table(tableName:test_table2, dbName:testdb, owner:null, createTime:0, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:), FieldSchema(name:msg, type:string, comment:)], location:/tmp/junit4858986149037536153/testdb.db/test_table2, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:1, serdeInfo:SerDeInfo(name:test_table2, serializationLib:org.apache.hadoop.hive.ql.io.orc.OrcSerde, parameters:{serialization.format=1}), bucketCols:[], sortCols:null, parameters:null), partitionKeys:[FieldSchema(name:city, type:string, comment:), FieldSchema(name:state, type:string, comment:)], parameters:{}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)
21456 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=create_table: Table(tableName:test_table2, dbName:testdb, owner:null, createTime:0, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:), FieldSchema(name:msg, type:string, comment:)], location:/tmp/junit4858986149037536153/testdb.db/test_table2, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:1, serdeInfo:SerDeInfo(name:test_table2, serializationLib:org.apache.hadoop.hive.ql.io.orc.OrcSerde, parameters:{serialization.format=1}), bucketCols:[], sortCols:null, parameters:null), partitionKeys:[FieldSchema(name:city, type:string, comment:), FieldSchema(name:state, type:string, comment:)], parameters:{}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
21457 [main] WARN  o.a.h.h.m.HiveMetaStore - Location: /tmp/junit4858986149037536153/testdb.db/test_table2 specified for non-external table:test_table2
21457 [main] INFO  o.a.h.h.c.FileUtils - Creating directory if it doesn't exist: file:/tmp/junit4858986149037536153/testdb.db/test_table2
21496 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: add_partition : db=testdb tbl=test_table2
21496 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=add_partition : db=testdb tbl=test_table2	
21527 [main] INFO  o.a.h.h.c.FileUtils - Creating directory if it doesn't exist: file:/tmp/junit4858986149037536153/testdb.db/test_table2/city=sunnyvale/state=ca
21538 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Shutting down the object store...
21538 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=Shutting down the object store...	
21539 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Metastore shutdown complete.
21539 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=Metastore shutdown complete.	
21593 [hiveWriterTest] INFO  o.a.h.h.q.s.SessionState - Created local directory: /tmp/d3e5c6fe-6368-4437-9686-90dfe7ea5cc9_resources
21595 [hiveWriterTest] INFO  o.a.h.h.q.s.SessionState - Created HDFS directory: /tmp/hive/travis/d3e5c6fe-6368-4437-9686-90dfe7ea5cc9
21597 [hiveWriterTest] INFO  o.a.h.h.q.s.SessionState - Created local directory: /tmp/travis/d3e5c6fe-6368-4437-9686-90dfe7ea5cc9
21607 [hiveWriterTest] INFO  o.a.h.h.q.s.SessionState - Created HDFS directory: /tmp/hive/travis/d3e5c6fe-6368-4437-9686-90dfe7ea5cc9/_tmp_space.db
21607 [hiveWriterTest] INFO  o.a.h.h.q.s.SessionState - No Tez session required at this point. hive.execution.engine=mr.
21607 [hiveWriterTest] INFO  o.a.h.h.m.HiveMetaStore - 2: get_table : db=testdb tbl=test_table2
21607 [hiveWriterTest] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table : db=testdb tbl=test_table2	
21607 [hiveWriterTest] INFO  o.a.h.h.m.HiveMetaStore - 2: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
21608 [hiveWriterTest] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
21608 [hiveWriterTest] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
21610 [hiveWriterTest] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
21610 [hiveWriterTest] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
21620 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=Driver.run from=org.apache.hadoop.hive.ql.Driver>
21620 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=TimeToSubmit from=org.apache.hadoop.hive.ql.Driver>
21620 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=compile from=org.apache.hadoop.hive.ql.Driver>
21621 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=parse from=org.apache.hadoop.hive.ql.Driver>
21621 [hiveWriterTest] INFO  h.q.p.ParseDriver - Parsing command: use testdb
21621 [hiveWriterTest] INFO  h.q.p.ParseDriver - Parse Completed
21621 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=parse start=1466737124560 end=1466737124560 duration=0 from=org.apache.hadoop.hive.ql.Driver>
21626 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=semanticAnalyze from=org.apache.hadoop.hive.ql.Driver>
21627 [hiveWriterTest] INFO  o.a.h.h.m.HiveMetaStore - 2: get_database: testdb
21627 [hiveWriterTest] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_database: testdb	
21628 [hiveWriterTest] INFO  o.a.h.h.q.Driver - Semantic Analysis Completed
21628 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=semanticAnalyze start=1466737124565 end=1466737124567 duration=2 from=org.apache.hadoop.hive.ql.Driver>
21628 [hiveWriterTest] INFO  o.a.h.h.q.Driver - Returning Hive schema: Schema(fieldSchemas:null, properties:null)
21628 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=compile start=1466737124559 end=1466737124567 duration=8 from=org.apache.hadoop.hive.ql.Driver>
21628 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=acquireReadWriteLocks from=org.apache.hadoop.hive.ql.Driver>
21628 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=acquireReadWriteLocks start=1466737124567 end=1466737124567 duration=0 from=org.apache.hadoop.hive.ql.Driver>
21628 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=Driver.execute from=org.apache.hadoop.hive.ql.Driver>
21628 [hiveWriterTest] INFO  o.a.h.h.q.Driver - Starting command: use testdb
21628 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=TimeToSubmit start=1466737124559 end=1466737124567 duration=8 from=org.apache.hadoop.hive.ql.Driver>
21628 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=runTasks from=org.apache.hadoop.hive.ql.Driver>
21628 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=task.DDL.Stage-0 from=org.apache.hadoop.hive.ql.Driver>
21628 [hiveWriterTest] INFO  o.a.h.h.q.Driver - Starting task [Stage-0:DDL] in serial mode
21629 [hiveWriterTest] INFO  o.a.h.h.m.HiveMetaStore - 2: get_database: testdb
21629 [hiveWriterTest] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_database: testdb	
21629 [hiveWriterTest] INFO  o.a.h.h.m.HiveMetaStore - 2: get_database: testdb
21629 [hiveWriterTest] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_database: testdb	
21630 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=runTasks start=1466737124567 end=1466737124569 duration=2 from=org.apache.hadoop.hive.ql.Driver>
21630 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=Driver.execute start=1466737124567 end=1466737124569 duration=2 from=org.apache.hadoop.hive.ql.Driver>
21630 [hiveWriterTest] INFO  o.a.h.h.q.Driver - OK
21630 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=releaseLocks from=org.apache.hadoop.hive.ql.Driver>
21630 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=releaseLocks start=1466737124569 end=1466737124569 duration=0 from=org.apache.hadoop.hive.ql.Driver>
21630 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=Driver.run start=1466737124559 end=1466737124569 duration=10 from=org.apache.hadoop.hive.ql.Driver>
21631 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=Driver.run from=org.apache.hadoop.hive.ql.Driver>
21631 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=TimeToSubmit from=org.apache.hadoop.hive.ql.Driver>
21631 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=compile from=org.apache.hadoop.hive.ql.Driver>
21631 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=parse from=org.apache.hadoop.hive.ql.Driver>
21631 [hiveWriterTest] INFO  h.q.p.ParseDriver - Parsing command: alter table test_table2 add if not exists partition  ( city='sunnyvale',state='ca' )
21631 [hiveWriterTest] INFO  h.q.p.ParseDriver - Parse Completed
21632 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=parse start=1466737124570 end=1466737124571 duration=1 from=org.apache.hadoop.hive.ql.Driver>
21645 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=semanticAnalyze from=org.apache.hadoop.hive.ql.Driver>
21646 [hiveWriterTest] INFO  o.a.h.h.m.HiveMetaStore - 2: get_table : db=testdb tbl=test_table2
21646 [hiveWriterTest] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table : db=testdb tbl=test_table2	
21660 [hiveWriterTest] INFO  o.a.h.h.q.Driver - Semantic Analysis Completed
21660 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=semanticAnalyze start=1466737124584 end=1466737124599 duration=15 from=org.apache.hadoop.hive.ql.Driver>
21660 [hiveWriterTest] INFO  o.a.h.h.q.Driver - Returning Hive schema: Schema(fieldSchemas:null, properties:null)
21660 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=compile start=1466737124570 end=1466737124599 duration=29 from=org.apache.hadoop.hive.ql.Driver>
21660 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=acquireReadWriteLocks from=org.apache.hadoop.hive.ql.Driver>
21678 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=acquireReadWriteLocks start=1466737124599 end=1466737124617 duration=18 from=org.apache.hadoop.hive.ql.Driver>
21678 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=Driver.execute from=org.apache.hadoop.hive.ql.Driver>
21678 [hiveWriterTest] INFO  o.a.h.h.q.Driver - Starting command: alter table test_table2 add if not exists partition  ( city='sunnyvale',state='ca' )
21678 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=TimeToSubmit start=1466737124570 end=1466737124617 duration=47 from=org.apache.hadoop.hive.ql.Driver>
21678 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=runTasks from=org.apache.hadoop.hive.ql.Driver>
21678 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=task.DDL.Stage-0 from=org.apache.hadoop.hive.ql.Driver>
21680 [hiveWriterTest] INFO  o.a.h.h.q.Driver - Starting task [Stage-0:DDL] in serial mode
21680 [hiveWriterTest] INFO  o.a.h.h.m.HiveMetaStore - 2: get_table : db=testdb tbl=test_table2
21681 [hiveWriterTest] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table : db=testdb tbl=test_table2	
21690 [hiveWriterTest] INFO  o.a.h.h.m.HiveMetaStore - 2: add_partitions
21690 [hiveWriterTest] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=add_partitions	
21740 [hiveWriterTest] INFO  o.a.h.h.m.HiveMetaStore - Not adding partition Partition(values:[sunnyvale, ca], dbName:testdb, tableName:test_table2, createTime:0, lastAccessTime:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:), FieldSchema(name:msg, type:string, comment:)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:1, serdeInfo:SerDeInfo(name:test_table2, serializationLib:org.apache.hadoop.hive.ql.io.orc.OrcSerde, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), parameters:null) as it already exists
21741 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=runTasks start=1466737124617 end=1466737124680 duration=63 from=org.apache.hadoop.hive.ql.Driver>
21742 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=Driver.execute start=1466737124617 end=1466737124681 duration=64 from=org.apache.hadoop.hive.ql.Driver>
21742 [hiveWriterTest] INFO  o.a.h.h.q.Driver - OK
21742 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=releaseLocks from=org.apache.hadoop.hive.ql.Driver>
21789 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=releaseLocks start=1466737124681 end=1466737124728 duration=47 from=org.apache.hadoop.hive.ql.Driver>
21789 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=Driver.run start=1466737124570 end=1466737124728 duration=158 from=org.apache.hadoop.hive.ql.Driver>
21829 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_table : db=testdb tbl=test_table2
21829 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table : db=testdb tbl=test_table2	
21829 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
21830 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
21832 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
21834 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
21834 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
21883 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_partition : db=testdb tbl=test_table2[sunnyvale,ca]
21883 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_partition : db=testdb tbl=test_table2[sunnyvale,ca]	
21897 [hiveWriterTest] INFO  o.a.h.h.m.HiveMetaStore - 2: get_table : db=testdb tbl=test_table2
21897 [hiveWriterTest] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table : db=testdb tbl=test_table2	

-------------------- system-err --------------------
OK
OK

--------------------------------------------------
Looking for errors in ./external/storm-jdbc/target/surefire-reports
Checking ./external/storm-jdbc/target/surefire-reports/TEST-org.apache.storm.jdbc.common.UtilTest.xml
Checking ./external/storm-jdbc/target/surefire-reports/TEST-org.apache.storm.jdbc.common.JdbcClientTest.xml
Checking ./external/storm-jdbc/target/surefire-reports/TEST-org.apache.storm.jdbc.bolt.JdbcLookupBoltTest.xml
Checking ./external/storm-jdbc/target/surefire-reports/TEST-org.apache.storm.jdbc.bolt.JdbcInsertBoltTest.xml
Looking for errors in ./external/storm-kafka/target/surefire-reports
Checking ./external/storm-kafka/target/surefire-reports/TEST-org.apache.storm.kafka.TestStringScheme.xml
Checking ./external/storm-kafka/target/surefire-reports/TEST-org.apache.storm.kafka.StringKeyValueSchemeTest.xml
Checking ./external/storm-kafka/target/surefire-reports/TEST-org.apache.storm.kafka.DynamicBrokersReaderTest.xml
Checking ./external/storm-kafka/target/surefire-reports/TEST-org.apache.storm.kafka.KafkaUtilsTest.xml
Checking ./external/storm-kafka/target/surefire-reports/TEST-org.apache.storm.kafka.KafkaErrorTest.xml
Checking ./external/storm-kafka/target/surefire-reports/TEST-org.apache.storm.kafka.bolt.KafkaBoltTest.xml
--------------------------------------------------
classname: org.apache.storm.kafka.bolt.KafkaBoltTest / testname: executeWithBrokerDown
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.kafka.bolt.KafkaBoltTest.generateTestTuple(KafkaBoltTest.java:301)
	at org.apache.storm.kafka.bolt.KafkaBoltTest.executeWithBrokerDown(KafkaBoltTest.java:266)

-------------------- system-out --------------------
02:56:24.793 [Curator-Framework-0-SendThread(127.0.0.1:36417)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
02:56:24.906 [Curator-Framework-0-SendThread(127.0.0.1:43062)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
02:56:25.097 [Curator-Framework-0-SendThread(127.0.0.1:41699)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
02:56:25.258 [Curator-Framework-0-SendThread(127.0.0.1:51772)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
02:56:25.708 [Curator-Framework-0-SendThread(127.0.0.1:52275)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
02:56:25.818 [main] INFO  o.a.c.f.i.CuratorFrameworkImpl - Starting
02:56:25.820 [main] INFO  k.u.VerifiableProperties - Verifying properties
02:56:25.820 [main] INFO  k.u.VerifiableProperties - Property broker.id is overridden to 0
02:56:25.820 [main] INFO  k.u.VerifiableProperties - Property log.dirs is overridden to /tmp/kafka/logs/kafka-test-60303
02:56:25.820 [main] INFO  k.u.VerifiableProperties - Property port is overridden to 60303
02:56:25.820 [main] INFO  k.u.VerifiableProperties - Property zookeeper.connect is overridden to 127.0.0.1:53020
02:56:25.820 [main] INFO  k.s.KafkaServer - [Kafka Server 0], starting
02:56:25.820 [main] INFO  k.s.KafkaServer - [Kafka Server 0], Connecting to zookeeper on 127.0.0.1:53020
02:56:25.821 [ZkClient-EventThread-833-127.0.0.1:53020] INFO  o.I.z.ZkEventThread - Starting ZkClient event thread.
02:56:25.830 [main-EventThread] INFO  o.a.c.f.s.ConnectionStateManager - State change: CONNECTED
02:56:25.831 [main-EventThread] INFO  o.I.z.ZkClient - zookeeper state changed (SyncConnected)
02:56:25.839 [main] INFO  k.l.LogManager - Log directory '/tmp/kafka/logs/kafka-test-60303' not found, creating it.
02:56:25.840 [main] INFO  k.l.LogManager - Loading logs.
02:56:25.840 [main] INFO  k.l.LogManager - Logs loading complete.
02:56:25.840 [main] INFO  k.l.LogManager - Starting log cleanup with a period of 300000 ms.
02:56:25.840 [main] INFO  k.l.LogManager - Starting log flusher with a default period of 9223372036854775807 ms.
02:56:25.842 [main] INFO  k.n.Acceptor - Awaiting socket connections on 0.0.0.0:60303.
02:56:25.842 [main] INFO  k.n.SocketServer - [Socket Server on Broker 0], Started
02:56:25.844 [main] INFO  k.u.Mx4jLoader$ - Will not load MX4J, mx4j-tools.jar is not in the classpath
02:56:25.845 [main] INFO  k.c.KafkaController - [Controller 0]: Controller starting up
02:56:25.846 [main] INFO  k.s.ZookeeperLeaderElector - 0 successfully elected as leader
02:56:25.846 [main] INFO  k.c.KafkaController - [Controller 0]: Broker 0 starting become controller state transition
02:56:25.847 [main] INFO  k.c.KafkaController - [Controller 0]: Controller 0 incremented epoch to 1
02:56:25.849 [main] INFO  k.c.KafkaController - [Controller 0]: Partitions undergoing preferred replica election: 
02:56:25.849 [main] INFO  k.c.KafkaController - [Controller 0]: Partitions that completed preferred replica election: 
02:56:25.850 [main] INFO  k.c.KafkaController - [Controller 0]: Resuming preferred replica election for partitions: 
02:56:25.850 [main] INFO  k.c.KafkaController - [Controller 0]: Partitions being reassigned: Map()
02:56:25.850 [main] INFO  k.c.KafkaController - [Controller 0]: Partitions already reassigned: List()
02:56:25.850 [main] INFO  k.c.KafkaController - [Controller 0]: Resuming reassignment of partitions: Map()
02:56:25.850 [main] INFO  k.c.KafkaController - [Controller 0]: List of topics to be deleted: 
02:56:25.850 [main] INFO  k.c.KafkaController - [Controller 0]: List of topics ineligible for deletion: 
02:56:25.851 [main] INFO  k.c.KafkaController - [Controller 0]: Currently active brokers in the cluster: Set()
02:56:25.851 [main] INFO  k.c.KafkaController - [Controller 0]: Currently shutting brokers in the cluster: Set()
02:56:25.851 [main] INFO  k.c.KafkaController - [Controller 0]: Current list of topics in the cluster: Set()
02:56:25.851 [main] INFO  k.c.ReplicaStateMachine - [Replica state machine on controller 0]: Started replica state machine with initial state -> Map()
02:56:25.851 [main] INFO  k.c.PartitionStateMachine - [Partition state machine on Controller 0]: Started partition state machine with initial state -> Map()
02:56:25.851 [main] INFO  k.c.KafkaController - [Controller 0]: Broker 0 is ready to serve as the new controller with epoch 1
02:56:25.851 [main] INFO  k.c.KafkaController - [Controller 0]: Starting preferred replica leader election for partitions 
02:56:25.851 [main] INFO  k.c.PartitionStateMachine - [Partition state machine on Controller 0]: Invoking state change to OnlinePartition for partitions 
02:56:25.852 [main] INFO  k.c.KafkaController - [Controller 0]: starting the partition rebalance scheduler
02:56:25.852 [main] INFO  k.c.KafkaController - [Controller 0]: Controller startup complete
02:56:25.853 [ZkClient-EventThread-833-127.0.0.1:53020] INFO  k.s.ZookeeperLeaderElector$LeaderChangeListener - New leader is 0
02:56:25.854 [main] INFO  k.u.ZkUtils$ - Registered broker 0 at path /brokers/ids/0 with address testing-worker-linux-docker-d7119792-3389-linux-9.prod.travis-ci.org:60303.
02:56:25.854 [main] INFO  k.s.KafkaServer - [Kafka Server 0], started
02:56:25.857 [main] INFO  o.a.k.c.p.ProducerConfig - ProducerConfig values: 
	compression.type = none
	metric.reporters = []
	metadata.max.age.ms = 300000
	metadata.fetch.timeout.ms = 1000
	acks = 1
	batch.size = 16384
	reconnect.backoff.ms = 10
	bootstrap.servers = [localhost:60303]
	receive.buffer.bytes = 32768
	retry.backoff.ms = 100
	buffer.memory = 33554432
	timeout.ms = 30000
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	retries = 0
	max.request.size = 1048576
	block.on.buffer.full = true
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
	metrics.sample.window.ms = 30000
	send.buffer.bytes = 131072
	max.in.flight.requests.per.connection = 5
	metrics.num.samples = 2
	linger.ms = 0
	client.id = 

02:56:25.857 [ZkClient-EventThread-833-127.0.0.1:53020] INFO  k.c.ReplicaStateMachine$BrokerChangeListener - [BrokerChangeListener on Controller 0]: Broker change listener fired for path /brokers/ids with children 0
02:56:25.860 [main] INFO  k.s.KafkaServer - [Kafka Server 0], shutting down
02:56:25.860 [main] INFO  k.s.KafkaServer - [Kafka Server 0], Starting controlled shutdown
02:56:25.861 [ZkClient-EventThread-833-127.0.0.1:53020] INFO  k.c.ReplicaStateMachine$BrokerChangeListener - [BrokerChangeListener on Controller 0]: Newly added brokers: 0, deleted brokers: , all live brokers: 0
02:56:25.862 [ZkClient-EventThread-833-127.0.0.1:53020] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Controller 0 connected to id:0,host:testing-worker-linux-docker-d7119792-3389-linux-9.prod.travis-ci.org,port:60303 for sending state change requests
02:56:25.862 [ZkClient-EventThread-833-127.0.0.1:53020] INFO  k.c.KafkaController - [Controller 0]: New broker startup callback for 0
02:56:25.864 [Controller-0-to-broker-0-send-thread] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Starting 
02:56:25.864 [kafka-request-handler-0] INFO  k.c.KafkaController - [Controller 0]: Shutting down broker 0
02:56:25.865 [main] INFO  k.s.KafkaServer - [Kafka Server 0], Controlled shutdown succeeded
02:56:25.865 [main] INFO  k.n.SocketServer - [Socket Server on Broker 0], Shutting down
02:56:25.865 [kafka-network-thread-60303-1] INFO  k.n.Processor - Closing socket connection to /172.17.5.8.
02:56:25.874 [Curator-Framework-0-SendThread(127.0.0.1:38937)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
02:56:25.875 [main] INFO  k.n.SocketServer - [Socket Server on Broker 0], Shutdown completed
02:56:25.875 [main] INFO  k.s.KafkaRequestHandlerPool - [Kafka Request Handler on Broker 0], shutting down
02:56:25.875 [main] INFO  k.s.KafkaRequestHandlerPool - [Kafka Request Handler on Broker 0], shut down completely
02:56:25.893 [Curator-Framework-0-SendThread(127.0.0.1:36417)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
02:56:26.007 [Curator-Framework-0-SendThread(127.0.0.1:43062)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
02:56:26.198 [Curator-Framework-0-SendThread(127.0.0.1:41699)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
02:56:26.243 [main] INFO  k.s.ReplicaManager - [Replica Manager on Broker 0]: Shut down
02:56:26.243 [main] INFO  k.s.ReplicaFetcherManager - [ReplicaFetcherManager on broker 0] shutting down
02:56:26.243 [main] INFO  k.s.ReplicaFetcherManager - [ReplicaFetcherManager on broker 0] shutdown completed
02:56:26.243 [main] INFO  k.s.ReplicaManager - [Replica Manager on Broker 0]: Shut down completely
02:56:26.244 [main] INFO  k.l.LogManager - Shutting down.
02:56:26.244 [main] INFO  k.l.LogManager - Shutdown complete.
02:56:26.244 [main] INFO  k.c.PartitionStateMachine - [Partition state machine on Controller 0]: Stopped partition state machine
02:56:26.244 [main] INFO  k.c.ReplicaStateMachine - [Replica state machine on controller 0]: Stopped replica state machine
02:56:26.244 [main] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Shutting down
02:56:26.244 [Controller-0-to-broker-0-send-thread] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Stopped 
02:56:26.244 [main] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Shutdown completed
02:56:26.244 [ZkClient-EventThread-833-127.0.0.1:53020] INFO  o.I.z.ZkEventThread - Terminate ZkClient event thread.
02:56:26.246 [main] INFO  k.s.KafkaServer - [Kafka Server 0], shut down completed
02:56:26.246 [Curator-Framework-0] INFO  o.a.c.f.i.CuratorFrameworkImpl - backgroundOperationsLoop exiting

--------------------------------------------------
--------------------------------------------------
classname: org.apache.storm.kafka.bolt.KafkaBoltTest / testname: executeWithoutKey
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.kafka.bolt.KafkaBoltTest.generateTestTuple(KafkaBoltTest.java:301)
	at org.apache.storm.kafka.bolt.KafkaBoltTest.executeWithoutKey(KafkaBoltTest.java:255)

-------------------- system-out --------------------
02:56:27.910 [Curator-Framework-0-SendThread(127.0.0.1:52275)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
02:56:28.076 [Curator-Framework-0-SendThread(127.0.0.1:38937)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
02:56:28.094 [Curator-Framework-0-SendThread(127.0.0.1:36417)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
02:56:28.209 [Curator-Framework-0-SendThread(127.0.0.1:43062)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
02:56:28.399 [Curator-Framework-0-SendThread(127.0.0.1:41699)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
02:56:28.561 [Curator-Framework-0-SendThread(127.0.0.1:51772)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
02:56:28.769 [main] INFO  o.a.c.f.i.CuratorFrameworkImpl - Starting
02:56:28.771 [main] INFO  k.u.VerifiableProperties - Verifying properties
02:56:28.771 [main] INFO  k.u.VerifiableProperties - Property broker.id is overridden to 0
02:56:28.771 [main] INFO  k.u.VerifiableProperties - Property log.dirs is overridden to /tmp/kafka/logs/kafka-test-43211
02:56:28.771 [main] INFO  k.u.VerifiableProperties - Property port is overridden to 43211
02:56:28.771 [main] INFO  k.u.VerifiableProperties - Property zookeeper.connect is overridden to 127.0.0.1:40944
02:56:28.771 [main] INFO  k.s.KafkaServer - [Kafka Server 0], starting
02:56:28.771 [main] INFO  k.s.KafkaServer - [Kafka Server 0], Connecting to zookeeper on 127.0.0.1:40944
02:56:28.780 [main-EventThread] INFO  o.a.c.f.s.ConnectionStateManager - State change: CONNECTED
02:56:28.780 [ZkClient-EventThread-905-127.0.0.1:40944] INFO  o.I.z.ZkEventThread - Starting ZkClient event thread.
02:56:28.789 [main-EventThread] INFO  o.I.z.ZkClient - zookeeper state changed (SyncConnected)
02:56:28.798 [main] INFO  k.l.LogManager - Log directory '/tmp/kafka/logs/kafka-test-43211' not found, creating it.
02:56:28.798 [main] INFO  k.l.LogManager - Loading logs.
02:56:28.798 [main] INFO  k.l.LogManager - Logs loading complete.
02:56:28.798 [main] INFO  k.l.LogManager - Starting log cleanup with a period of 300000 ms.
02:56:28.799 [main] INFO  k.l.LogManager - Starting log flusher with a default period of 9223372036854775807 ms.
02:56:28.802 [main] INFO  k.n.Acceptor - Awaiting socket connections on 0.0.0.0:43211.
02:56:28.802 [main] INFO  k.n.SocketServer - [Socket Server on Broker 0], Started
02:56:28.809 [main] INFO  k.u.Mx4jLoader$ - Will not load MX4J, mx4j-tools.jar is not in the classpath
02:56:28.809 [main] INFO  k.c.KafkaController - [Controller 0]: Controller starting up
02:56:28.811 [main] INFO  k.s.ZookeeperLeaderElector - 0 successfully elected as leader
02:56:28.811 [main] INFO  k.c.KafkaController - [Controller 0]: Broker 0 starting become controller state transition
02:56:28.814 [main] INFO  k.c.KafkaController - [Controller 0]: Controller 0 incremented epoch to 1
02:56:28.816 [main] INFO  k.c.KafkaController - [Controller 0]: Partitions undergoing preferred replica election: 
02:56:28.816 [main] INFO  k.c.KafkaController - [Controller 0]: Partitions that completed preferred replica election: 
02:56:28.816 [main] INFO  k.c.KafkaController - [Controller 0]: Resuming preferred replica election for partitions: 
02:56:28.817 [main] INFO  k.c.KafkaController - [Controller 0]: Partitions being reassigned: Map()
02:56:28.817 [main] INFO  k.c.KafkaController - [Controller 0]: Partitions already reassigned: List()
02:56:28.817 [main] INFO  k.c.KafkaController - [Controller 0]: Resuming reassignment of partitions: Map()
02:56:28.817 [main] INFO  k.c.KafkaController - [Controller 0]: List of topics to be deleted: 
02:56:28.817 [main] INFO  k.c.KafkaController - [Controller 0]: List of topics ineligible for deletion: 
02:56:28.817 [main] INFO  k.c.KafkaController - [Controller 0]: Currently active brokers in the cluster: Set()
02:56:28.817 [main] INFO  k.c.KafkaController - [Controller 0]: Currently shutting brokers in the cluster: Set()
02:56:28.817 [main] INFO  k.c.KafkaController - [Controller 0]: Current list of topics in the cluster: Set()
02:56:28.817 [main] INFO  k.c.ReplicaStateMachine - [Replica state machine on controller 0]: Started replica state machine with initial state -> Map()
02:56:28.817 [main] INFO  k.c.PartitionStateMachine - [Partition state machine on Controller 0]: Started partition state machine with initial state -> Map()
02:56:28.817 [main] INFO  k.c.KafkaController - [Controller 0]: Broker 0 is ready to serve as the new controller with epoch 1
02:56:28.817 [main] INFO  k.c.KafkaController - [Controller 0]: Starting preferred replica leader election for partitions 
02:56:28.817 [main] INFO  k.c.PartitionStateMachine - [Partition state machine on Controller 0]: Invoking state change to OnlinePartition for partitions 
02:56:28.822 [main] INFO  k.c.KafkaController - [Controller 0]: starting the partition rebalance scheduler
02:56:28.823 [main] INFO  k.c.KafkaController - [Controller 0]: Controller startup complete
02:56:28.823 [ZkClient-EventThread-905-127.0.0.1:40944] INFO  k.s.ZookeeperLeaderElector$LeaderChangeListener - New leader is 0
02:56:28.825 [main] INFO  k.u.ZkUtils$ - Registered broker 0 at path /brokers/ids/0 with address testing-worker-linux-docker-d7119792-3389-linux-9.prod.travis-ci.org:43211.
02:56:28.825 [main] INFO  k.s.KafkaServer - [Kafka Server 0], started
02:56:28.826 [main] INFO  o.a.k.c.p.ProducerConfig - ProducerConfig values: 
	compression.type = none
	metric.reporters = []
	metadata.max.age.ms = 300000
	metadata.fetch.timeout.ms = 1000
	acks = 1
	batch.size = 16384
	reconnect.backoff.ms = 10
	bootstrap.servers = [localhost:43211]
	receive.buffer.bytes = 32768
	retry.backoff.ms = 100
	buffer.memory = 33554432
	timeout.ms = 30000
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	retries = 0
	max.request.size = 1048576
	block.on.buffer.full = true
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
	metrics.sample.window.ms = 30000
	send.buffer.bytes = 131072
	max.in.flight.requests.per.connection = 5
	metrics.num.samples = 2
	linger.ms = 0
	client.id = 

02:56:28.826 [ZkClient-EventThread-905-127.0.0.1:40944] INFO  k.c.ReplicaStateMachine$BrokerChangeListener - [BrokerChangeListener on Controller 0]: Broker change listener fired for path /brokers/ids with children 0
02:56:28.828 [main] INFO  k.s.KafkaServer - [Kafka Server 0], shutting down
02:56:28.828 [main] INFO  k.s.KafkaServer - [Kafka Server 0], Starting controlled shutdown
02:56:28.829 [ZkClient-EventThread-905-127.0.0.1:40944] INFO  k.c.ReplicaStateMachine$BrokerChangeListener - [BrokerChangeListener on Controller 0]: Newly added brokers: 0, deleted brokers: , all live brokers: 0
02:56:28.829 [ZkClient-EventThread-905-127.0.0.1:40944] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Controller 0 connected to id:0,host:testing-worker-linux-docker-d7119792-3389-linux-9.prod.travis-ci.org,port:43211 for sending state change requests
02:56:28.834 [ZkClient-EventThread-905-127.0.0.1:40944] INFO  k.c.KafkaController - [Controller 0]: New broker startup callback for 0
02:56:28.835 [Controller-0-to-broker-0-send-thread] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Starting 
02:56:28.835 [kafka-request-handler-0] INFO  k.c.KafkaController - [Controller 0]: Shutting down broker 0
02:56:28.835 [main] INFO  k.s.KafkaServer - [Kafka Server 0], Controlled shutdown succeeded
02:56:28.835 [main] INFO  k.n.SocketServer - [Socket Server on Broker 0], Shutting down
02:56:28.835 [kafka-network-thread-43211-1] INFO  k.n.Processor - Closing socket connection to /172.17.5.8.
02:56:28.836 [main] INFO  k.n.SocketServer - [Socket Server on Broker 0], Shutdown completed
02:56:28.836 [main] INFO  k.s.KafkaRequestHandlerPool - [Kafka Request Handler on Broker 0], shutting down
02:56:28.836 [main] INFO  k.s.KafkaRequestHandlerPool - [Kafka Request Handler on Broker 0], shut down completely
02:56:29.011 [Curator-Framework-0-SendThread(127.0.0.1:52275)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
02:56:29.177 [Curator-Framework-0-SendThread(127.0.0.1:38937)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
02:56:29.195 [Curator-Framework-0-SendThread(127.0.0.1:36417)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
02:56:29.208 [main] INFO  k.s.ReplicaManager - [Replica Manager on Broker 0]: Shut down
02:56:29.208 [main] INFO  k.s.ReplicaFetcherManager - [ReplicaFetcherManager on broker 0] shutting down
02:56:29.208 [main] INFO  k.s.ReplicaFetcherManager - [ReplicaFetcherManager on broker 0] shutdown completed
02:56:29.208 [main] INFO  k.s.ReplicaManager - [Replica Manager on Broker 0]: Shut down completely
02:56:29.208 [main] INFO  k.l.LogManager - Shutting down.
02:56:29.209 [main] INFO  k.l.LogManager - Shutdown complete.
02:56:29.209 [main] INFO  k.c.PartitionStateMachine - [Partition state machine on Controller 0]: Stopped partition state machine
02:56:29.209 [main] INFO  k.c.ReplicaStateMachine - [Replica state machine on controller 0]: Stopped replica state machine
02:56:29.209 [main] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Shutting down
02:56:29.209 [Controller-0-to-broker-0-send-thread] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Stopped 
02:56:29.209 [main] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Shutdown completed
02:56:29.209 [ZkClient-EventThread-905-127.0.0.1:40944] INFO  o.I.z.ZkEventThread - Terminate ZkClient event thread.
02:56:29.210 [main] INFO  k.s.KafkaServer - [Kafka Server 0], shut down completed
02:56:29.210 [Curator-Framework-0] INFO  o.a.c.f.i.CuratorFrameworkImpl - backgroundOperationsLoop exiting

--------------------------------------------------
--------------------------------------------------
classname: org.apache.storm.kafka.bolt.KafkaBoltTest / testname: executeWithByteArrayKeyAndMessageFire
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.kafka.bolt.KafkaBoltTest.generateTestTuple(KafkaBoltTest.java:290)
	at org.apache.storm.kafka.bolt.KafkaBoltTest.executeWithByteArrayKeyAndMessageFire(KafkaBoltTest.java:176)

-------------------- system-out --------------------
02:56:29.310 [Curator-Framework-0-SendThread(127.0.0.1:43062)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
02:56:29.500 [Curator-Framework-0-SendThread(127.0.0.1:41699)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
02:56:29.662 [Curator-Framework-0-SendThread(127.0.0.1:51772)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
02:56:30.112 [Curator-Framework-0-SendThread(127.0.0.1:52275)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
02:56:30.245 [main] INFO  o.a.c.f.i.CuratorFrameworkImpl - Starting
02:56:30.246 [main] INFO  k.u.VerifiableProperties - Verifying properties
02:56:30.247 [main] INFO  k.u.VerifiableProperties - Property broker.id is overridden to 0
02:56:30.247 [main] INFO  k.u.VerifiableProperties - Property log.dirs is overridden to /tmp/kafka/logs/kafka-test-41157
02:56:30.247 [main] INFO  k.u.VerifiableProperties - Property port is overridden to 41157
02:56:30.247 [main] INFO  k.u.VerifiableProperties - Property zookeeper.connect is overridden to 127.0.0.1:60887
02:56:30.247 [main] INFO  k.s.KafkaServer - [Kafka Server 0], starting
02:56:30.247 [main] INFO  k.s.KafkaServer - [Kafka Server 0], Connecting to zookeeper on 127.0.0.1:60887
02:56:30.247 [ZkClient-EventThread-941-127.0.0.1:60887] INFO  o.I.z.ZkEventThread - Starting ZkClient event thread.
02:56:30.249 [main-EventThread] INFO  o.I.z.ZkClient - zookeeper state changed (SyncConnected)
02:56:30.250 [main-EventThread] INFO  o.a.c.f.s.ConnectionStateManager - State change: CONNECTED
02:56:30.266 [main] INFO  k.l.LogManager - Log directory '/tmp/kafka/logs/kafka-test-41157' not found, creating it.
02:56:30.266 [main] INFO  k.l.LogManager - Loading logs.
02:56:30.266 [main] INFO  k.l.LogManager - Logs loading complete.
02:56:30.266 [main] INFO  k.l.LogManager - Starting log cleanup with a period of 300000 ms.
02:56:30.267 [main] INFO  k.l.LogManager - Starting log flusher with a default period of 9223372036854775807 ms.
02:56:30.268 [main] INFO  k.n.Acceptor - Awaiting socket connections on 0.0.0.0:41157.
02:56:30.268 [main] INFO  k.n.SocketServer - [Socket Server on Broker 0], Started
02:56:30.271 [main] INFO  k.u.Mx4jLoader$ - Will not load MX4J, mx4j-tools.jar is not in the classpath
02:56:30.271 [main] INFO  k.c.KafkaController - [Controller 0]: Controller starting up
02:56:30.272 [main] INFO  k.s.ZookeeperLeaderElector - 0 successfully elected as leader
02:56:30.272 [main] INFO  k.c.KafkaController - [Controller 0]: Broker 0 starting become controller state transition
02:56:30.274 [main] INFO  k.c.KafkaController - [Controller 0]: Controller 0 incremented epoch to 1
02:56:30.275 [main] INFO  k.c.KafkaController - [Controller 0]: Partitions undergoing preferred replica election: 
02:56:30.275 [main] INFO  k.c.KafkaController - [Controller 0]: Partitions that completed preferred replica election: 
02:56:30.275 [main] INFO  k.c.KafkaController - [Controller 0]: Resuming preferred replica election for partitions: 
02:56:30.276 [main] INFO  k.c.KafkaController - [Controller 0]: Partitions being reassigned: Map()
02:56:30.276 [main] INFO  k.c.KafkaController - [Controller 0]: Partitions already reassigned: List()
02:56:30.276 [main] INFO  k.c.KafkaController - [Controller 0]: Resuming reassignment of partitions: Map()
02:56:30.276 [main] INFO  k.c.KafkaController - [Controller 0]: List of topics to be deleted: 
02:56:30.276 [main] INFO  k.c.KafkaController - [Controller 0]: List of topics ineligible for deletion: 
02:56:30.276 [main] INFO  k.c.KafkaController - [Controller 0]: Currently active brokers in the cluster: Set()
02:56:30.276 [main] INFO  k.c.KafkaController - [Controller 0]: Currently shutting brokers in the cluster: Set()
02:56:30.276 [main] INFO  k.c.KafkaController - [Controller 0]: Current list of topics in the cluster: Set()
02:56:30.276 [main] INFO  k.c.ReplicaStateMachine - [Replica state machine on controller 0]: Started replica state machine with initial state -> Map()
02:56:30.276 [main] INFO  k.c.PartitionStateMachine - [Partition state machine on Controller 0]: Started partition state machine with initial state -> Map()
02:56:30.276 [main] INFO  k.c.KafkaController - [Controller 0]: Broker 0 is ready to serve as the new controller with epoch 1
02:56:30.276 [main] INFO  k.c.KafkaController - [Controller 0]: Starting preferred replica leader election for partitions 
02:56:30.276 [main] INFO  k.c.PartitionStateMachine - [Partition state machine on Controller 0]: Invoking state change to OnlinePartition for partitions 
02:56:30.277 [Curator-Framework-0-SendThread(127.0.0.1:38937)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
02:56:30.277 [main] INFO  k.c.KafkaController - [Controller 0]: starting the partition rebalance scheduler
02:56:30.281 [main] INFO  k.c.KafkaController - [Controller 0]: Controller startup complete
02:56:30.282 [ZkClient-EventThread-941-127.0.0.1:60887] INFO  k.s.ZookeeperLeaderElector$LeaderChangeListener - New leader is 0
02:56:30.283 [main] INFO  k.u.ZkUtils$ - Registered broker 0 at path /brokers/ids/0 with address testing-worker-linux-docker-d7119792-3389-linux-9.prod.travis-ci.org:41157.
02:56:30.283 [main] INFO  k.s.KafkaServer - [Kafka Server 0], started
02:56:30.284 [main] INFO  o.a.k.c.p.ProducerConfig - ProducerConfig values: 
	compression.type = none
	metric.reporters = []
	metadata.max.age.ms = 300000
	metadata.fetch.timeout.ms = 1000
	acks = 1
	batch.size = 16384
	reconnect.backoff.ms = 10
	bootstrap.servers = [localhost:41157]
	receive.buffer.bytes = 32768
	retry.backoff.ms = 100
	buffer.memory = 33554432
	timeout.ms = 30000
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	retries = 0
	max.request.size = 1048576
	block.on.buffer.full = true
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
	metrics.sample.window.ms = 30000
	send.buffer.bytes = 131072
	max.in.flight.requests.per.connection = 5
	metrics.num.samples = 2
	linger.ms = 0
	client.id = 

02:56:30.285 [ZkClient-EventThread-941-127.0.0.1:60887] INFO  k.c.ReplicaStateMachine$BrokerChangeListener - [BrokerChangeListener on Controller 0]: Broker change listener fired for path /brokers/ids with children 0
02:56:30.287 [ZkClient-EventThread-941-127.0.0.1:60887] INFO  k.c.ReplicaStateMachine$BrokerChangeListener - [BrokerChangeListener on Controller 0]: Newly added brokers: 0, deleted brokers: , all live brokers: 0
02:56:30.288 [main] INFO  o.a.k.c.p.ProducerConfig - ProducerConfig values: 
	compression.type = none
	metric.reporters = []
	metadata.max.age.ms = 300000
	metadata.fetch.timeout.ms = 1000
	acks = 1
	batch.size = 16384
	reconnect.backoff.ms = 10
	bootstrap.servers = [localhost:41157]
	receive.buffer.bytes = 32768
	retry.backoff.ms = 100
	buffer.memory = 33554432
	timeout.ms = 30000
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	retries = 0
	max.request.size = 1048576
	block.on.buffer.full = true
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	metrics.sample.window.ms = 30000
	send.buffer.bytes = 131072
	max.in.flight.requests.per.connection = 5
	metrics.num.samples = 2
	linger.ms = 0
	client.id = 

02:56:30.289 [main] INFO  k.s.KafkaServer - [Kafka Server 0], shutting down
02:56:30.289 [main] INFO  k.s.KafkaServer - [Kafka Server 0], Starting controlled shutdown
02:56:30.289 [ZkClient-EventThread-941-127.0.0.1:60887] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Controller 0 connected to id:0,host:testing-worker-linux-docker-d7119792-3389-linux-9.prod.travis-ci.org,port:41157 for sending state change requests
02:56:30.292 [ZkClient-EventThread-941-127.0.0.1:60887] INFO  k.c.KafkaController - [Controller 0]: New broker startup callback for 0
02:56:30.292 [kafka-request-handler-0] INFO  k.c.KafkaController - [Controller 0]: Shutting down broker 0
02:56:30.292 [Controller-0-to-broker-0-send-thread] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Starting 
02:56:30.294 [main] INFO  k.s.KafkaServer - [Kafka Server 0], Controlled shutdown succeeded
02:56:30.294 [main] INFO  k.n.SocketServer - [Socket Server on Broker 0], Shutting down
02:56:30.295 [kafka-network-thread-41157-1] INFO  k.n.Processor - Closing socket connection to /172.17.5.8.
02:56:30.295 [main] INFO  k.n.SocketServer - [Socket Server on Broker 0], Shutdown completed
02:56:30.295 [main] INFO  k.s.KafkaRequestHandlerPool - [Kafka Request Handler on Broker 0], shutting down
02:56:30.295 [Curator-Framework-0-SendThread(127.0.0.1:36417)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
02:56:30.296 [main] INFO  k.s.KafkaRequestHandlerPool - [Kafka Request Handler on Broker 0], shut down completely
02:56:30.411 [Curator-Framework-0-SendThread(127.0.0.1:43062)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
02:56:30.601 [Curator-Framework-0-SendThread(127.0.0.1:41699)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
02:56:30.670 [main] INFO  k.s.ReplicaManager - [Replica Manager on Broker 0]: Shut down
02:56:30.670 [main] INFO  k.s.ReplicaFetcherManager - [ReplicaFetcherManager on broker 0] shutting down
02:56:30.670 [main] INFO  k.s.ReplicaFetcherManager - [ReplicaFetcherManager on broker 0] shutdown completed
02:56:30.670 [main] INFO  k.s.ReplicaManager - [Replica Manager on Broker 0]: Shut down completely
02:56:30.670 [main] INFO  k.l.LogManager - Shutting down.
02:56:30.670 [main] INFO  k.l.LogManager - Shutdown complete.
02:56:30.671 [main] INFO  k.c.PartitionStateMachine - [Partition state machine on Controller 0]: Stopped partition state machine
02:56:30.671 [main] INFO  k.c.ReplicaStateMachine - [Replica state machine on controller 0]: Stopped replica state machine
02:56:30.671 [main] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Shutting down
02:56:30.671 [main] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Shutdown completed
02:56:30.671 [ZkClient-EventThread-941-127.0.0.1:60887] INFO  o.I.z.ZkEventThread - Terminate ZkClient event thread.
02:56:30.671 [Controller-0-to-broker-0-send-thread] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Stopped 
02:56:30.673 [main] INFO  k.s.KafkaServer - [Kafka Server 0], shut down completed
02:56:30.673 [Curator-Framework-0] INFO  o.a.c.f.i.CuratorFrameworkImpl - backgroundOperationsLoop exiting

--------------------------------------------------
--------------------------------------------------
classname: org.apache.storm.kafka.bolt.KafkaBoltTest / testname: executeWithByteArrayKeyAndMessageSync
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.kafka.bolt.KafkaBoltTest.generateTestTuple(KafkaBoltTest.java:290)
	at org.apache.storm.kafka.bolt.KafkaBoltTest.executeWithByteArrayKeyAndMessageSync(KafkaBoltTest.java:131)

-------------------- system-out --------------------
02:56:30.763 [Curator-Framework-0-SendThread(127.0.0.1:51772)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
02:56:31.213 [Curator-Framework-0-SendThread(127.0.0.1:52275)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
02:56:31.378 [Curator-Framework-0-SendThread(127.0.0.1:38937)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
02:56:31.396 [Curator-Framework-0-SendThread(127.0.0.1:36417)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
02:56:31.511 [Curator-Framework-0-SendThread(127.0.0.1:43062)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
02:56:31.680 [main] INFO  o.a.c.f.i.CuratorFrameworkImpl - Starting
02:56:31.681 [main] INFO  k.u.VerifiableProperties - Verifying properties
02:56:31.681 [main] INFO  k.u.VerifiableProperties - Property broker.id is overridden to 0
02:56:31.682 [main] INFO  k.u.VerifiableProperties - Property log.dirs is overridden to /tmp/kafka/logs/kafka-test-50623
02:56:31.682 [main] INFO  k.u.VerifiableProperties - Property port is overridden to 50623
02:56:31.682 [main] INFO  k.u.VerifiableProperties - Property zookeeper.connect is overridden to 127.0.0.1:45922
02:56:31.682 [main] INFO  k.s.KafkaServer - [Kafka Server 0], starting
02:56:31.682 [main] INFO  k.s.KafkaServer - [Kafka Server 0], Connecting to zookeeper on 127.0.0.1:45922
02:56:31.682 [ZkClient-EventThread-978-127.0.0.1:45922] INFO  o.I.z.ZkEventThread - Starting ZkClient event thread.
02:56:31.683 [main-EventThread] INFO  o.a.c.f.s.ConnectionStateManager - State change: CONNECTED
02:56:31.685 [main-EventThread] INFO  o.I.z.ZkClient - zookeeper state changed (SyncConnected)
02:56:31.693 [main] INFO  k.l.LogManager - Log directory '/tmp/kafka/logs/kafka-test-50623' not found, creating it.
02:56:31.693 [main] INFO  k.l.LogManager - Loading logs.
02:56:31.693 [main] INFO  k.l.LogManager - Logs loading complete.
02:56:31.693 [main] INFO  k.l.LogManager - Starting log cleanup with a period of 300000 ms.
02:56:31.694 [main] INFO  k.l.LogManager - Starting log flusher with a default period of 9223372036854775807 ms.
02:56:31.695 [main] INFO  k.n.Acceptor - Awaiting socket connections on 0.0.0.0:50623.
02:56:31.696 [main] INFO  k.n.SocketServer - [Socket Server on Broker 0], Started
02:56:31.698 [main] INFO  k.u.Mx4jLoader$ - Will not load MX4J, mx4j-tools.jar is not in the classpath
02:56:31.698 [main] INFO  k.c.KafkaController - [Controller 0]: Controller starting up
02:56:31.699 [main] INFO  k.s.ZookeeperLeaderElector - 0 successfully elected as leader
02:56:31.699 [main] INFO  k.c.KafkaController - [Controller 0]: Broker 0 starting become controller state transition
02:56:31.701 [main] INFO  k.c.KafkaController - [Controller 0]: Controller 0 incremented epoch to 1
02:56:31.701 [Curator-Framework-0-SendThread(127.0.0.1:41699)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
02:56:31.703 [main] INFO  k.c.KafkaController - [Controller 0]: Partitions undergoing preferred replica election: 
02:56:31.703 [main] INFO  k.c.KafkaController - [Controller 0]: Partitions that completed preferred replica election: 
02:56:31.703 [main] INFO  k.c.KafkaController - [Controller 0]: Resuming preferred replica election for partitions: 
02:56:31.703 [main] INFO  k.c.KafkaController - [Controller 0]: Partitions being reassigned: Map()
02:56:31.703 [main] INFO  k.c.KafkaController - [Controller 0]: Partitions already reassigned: List()
02:56:31.703 [main] INFO  k.c.KafkaController - [Controller 0]: Resuming reassignment of partitions: Map()
02:56:31.703 [main] INFO  k.c.KafkaController - [Controller 0]: List of topics to be deleted: 
02:56:31.703 [main] INFO  k.c.KafkaController - [Controller 0]: List of topics ineligible for deletion: 
02:56:31.704 [main] INFO  k.c.KafkaController - [Controller 0]: Currently active brokers in the cluster: Set()
02:56:31.704 [main] INFO  k.c.KafkaController - [Controller 0]: Currently shutting brokers in the cluster: Set()
02:56:31.704 [main] INFO  k.c.KafkaController - [Controller 0]: Current list of topics in the cluster: Set()
02:56:31.704 [main] INFO  k.c.ReplicaStateMachine - [Replica state machine on controller 0]: Started replica state machine with initial state -> Map()
02:56:31.704 [main] INFO  k.c.PartitionStateMachine - [Partition state machine on Controller 0]: Started partition state machine with initial state -> Map()
02:56:31.704 [main] INFO  k.c.KafkaController - [Controller 0]: Broker 0 is ready to serve as the new controller with epoch 1
02:56:31.704 [main] INFO  k.c.KafkaController - [Controller 0]: Starting preferred replica leader election for partitions 
02:56:31.704 [main] INFO  k.c.PartitionStateMachine - [Partition state machine on Controller 0]: Invoking state change to OnlinePartition for partitions 
02:56:31.705 [main] INFO  k.c.KafkaController - [Controller 0]: starting the partition rebalance scheduler
02:56:31.705 [main] INFO  k.c.KafkaController - [Controller 0]: Controller startup complete
02:56:31.706 [ZkClient-EventThread-978-127.0.0.1:45922] INFO  k.s.ZookeeperLeaderElector$LeaderChangeListener - New leader is 0
02:56:31.707 [main] INFO  k.u.ZkUtils$ - Registered broker 0 at path /brokers/ids/0 with address testing-worker-linux-docker-d7119792-3389-linux-9.prod.travis-ci.org:50623.
02:56:31.707 [main] INFO  k.s.KafkaServer - [Kafka Server 0], started
02:56:31.707 [main] INFO  o.a.k.c.p.ProducerConfig - ProducerConfig values: 
	compression.type = none
	metric.reporters = []
	metadata.max.age.ms = 300000
	metadata.fetch.timeout.ms = 1000
	acks = 1
	batch.size = 16384
	reconnect.backoff.ms = 10
	bootstrap.servers = [localhost:50623]
	receive.buffer.bytes = 32768
	retry.backoff.ms = 100
	buffer.memory = 33554432
	timeout.ms = 30000
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	retries = 0
	max.request.size = 1048576
	block.on.buffer.full = true
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
	metrics.sample.window.ms = 30000
	send.buffer.bytes = 131072
	max.in.flight.requests.per.connection = 5
	metrics.num.samples = 2
	linger.ms = 0
	client.id = 

02:56:31.709 [ZkClient-EventThread-978-127.0.0.1:45922] INFO  k.c.ReplicaStateMachine$BrokerChangeListener - [BrokerChangeListener on Controller 0]: Broker change listener fired for path /brokers/ids with children 0
02:56:31.711 [main] INFO  o.a.k.c.p.ProducerConfig - ProducerConfig values: 
	compression.type = none
	metric.reporters = []
	metadata.max.age.ms = 300000
	metadata.fetch.timeout.ms = 1000
	acks = 1
	batch.size = 16384
	reconnect.backoff.ms = 10
	bootstrap.servers = [localhost:50623]
	receive.buffer.bytes = 32768
	retry.backoff.ms = 100
	buffer.memory = 33554432
	timeout.ms = 30000
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	retries = 0
	max.request.size = 1048576
	block.on.buffer.full = true
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	metrics.sample.window.ms = 30000
	send.buffer.bytes = 131072
	max.in.flight.requests.per.connection = 5
	metrics.num.samples = 2
	linger.ms = 0
	client.id = 

02:56:31.711 [ZkClient-EventThread-978-127.0.0.1:45922] INFO  k.c.ReplicaStateMachine$BrokerChangeListener - [BrokerChangeListener on Controller 0]: Newly added brokers: 0, deleted brokers: , all live brokers: 0
02:56:31.712 [main] INFO  k.s.KafkaServer - [Kafka Server 0], shutting down
02:56:31.712 [main] INFO  k.s.KafkaServer - [Kafka Server 0], Starting controlled shutdown
02:56:31.713 [ZkClient-EventThread-978-127.0.0.1:45922] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Controller 0 connected to id:0,host:testing-worker-linux-docker-d7119792-3389-linux-9.prod.travis-ci.org,port:50623 for sending state change requests
02:56:31.715 [ZkClient-EventThread-978-127.0.0.1:45922] INFO  k.c.KafkaController - [Controller 0]: New broker startup callback for 0
02:56:31.715 [kafka-request-handler-0] INFO  k.c.KafkaController - [Controller 0]: Shutting down broker 0
02:56:31.715 [main] INFO  k.s.KafkaServer - [Kafka Server 0], Controlled shutdown succeeded
02:56:31.715 [main] INFO  k.n.SocketServer - [Socket Server on Broker 0], Shutting down
02:56:31.716 [Controller-0-to-broker-0-send-thread] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Starting 
02:56:31.716 [main] INFO  k.n.SocketServer - [Socket Server on Broker 0], Shutdown completed
02:56:31.716 [main] INFO  k.s.KafkaRequestHandlerPool - [Kafka Request Handler on Broker 0], shutting down
02:56:31.716 [main] INFO  k.s.KafkaRequestHandlerPool - [Kafka Request Handler on Broker 0], shut down completely
02:56:31.863 [Curator-Framework-0-SendThread(127.0.0.1:51772)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
02:56:32.097 [main] INFO  k.s.ReplicaManager - [Replica Manager on Broker 0]: Shut down
02:56:32.097 [main] INFO  k.s.ReplicaFetcherManager - [ReplicaFetcherManager on broker 0] shutting down
02:56:32.097 [main] INFO  k.s.ReplicaFetcherManager - [ReplicaFetcherManager on broker 0] shutdown completed
02:56:32.097 [main] INFO  k.s.ReplicaManager - [Replica Manager on Broker 0]: Shut down completely
02:56:32.097 [main] INFO  k.l.LogManager - Shutting down.
02:56:32.098 [main] INFO  k.l.LogManager - Shutdown complete.
02:56:32.098 [main] INFO  k.c.PartitionStateMachine - [Partition state machine on Controller 0]: Stopped partition state machine
02:56:32.098 [main] INFO  k.c.ReplicaStateMachine - [Replica state machine on controller 0]: Stopped replica state machine
02:56:32.098 [main] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Shutting down
02:56:32.098 [Controller-0-to-broker-0-send-thread] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Stopped 
02:56:32.098 [main] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Shutdown completed
02:56:32.098 [ZkClient-EventThread-978-127.0.0.1:45922] INFO  o.I.z.ZkEventThread - Terminate ZkClient event thread.
02:56:32.099 [main] INFO  k.s.KafkaServer - [Kafka Server 0], shut down completed
02:56:32.099 [Curator-Framework-0] INFO  o.a.c.f.i.CuratorFrameworkImpl - backgroundOperationsLoop exiting

--------------------------------------------------
--------------------------------------------------
classname: org.apache.storm.kafka.bolt.KafkaBoltTest / testname: executeWithByteArrayKeyAndMessageAsync
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.kafka.bolt.KafkaBoltTest.generateTestTuple(KafkaBoltTest.java:290)
	at org.apache.storm.kafka.bolt.KafkaBoltTest.executeWithByteArrayKeyAndMessageAsync(KafkaBoltTest.java:146)

-------------------- system-out --------------------
02:56:32.313 [Curator-Framework-0-SendThread(127.0.0.1:52275)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
02:56:32.479 [Curator-Framework-0-SendThread(127.0.0.1:38937)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
02:56:32.497 [Curator-Framework-0-SendThread(127.0.0.1:36417)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
02:56:32.612 [Curator-Framework-0-SendThread(127.0.0.1:43062)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
02:56:32.802 [Curator-Framework-0-SendThread(127.0.0.1:41699)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
02:56:32.964 [Curator-Framework-0-SendThread(127.0.0.1:51772)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
02:56:33.149 [main] INFO  o.a.c.f.i.CuratorFrameworkImpl - Starting
02:56:33.150 [main] INFO  k.u.VerifiableProperties - Verifying properties
02:56:33.150 [main] INFO  k.u.VerifiableProperties - Property broker.id is overridden to 0
02:56:33.150 [main] INFO  k.u.VerifiableProperties - Property log.dirs is overridden to /tmp/kafka/logs/kafka-test-38608
02:56:33.150 [main] INFO  k.u.VerifiableProperties - Property port is overridden to 38608
02:56:33.151 [main] INFO  k.u.VerifiableProperties - Property zookeeper.connect is overridden to 127.0.0.1:54029
02:56:33.151 [main] INFO  k.s.KafkaServer - [Kafka Server 0], starting
02:56:33.151 [main] INFO  k.s.KafkaServer - [Kafka Server 0], Connecting to zookeeper on 127.0.0.1:54029
02:56:33.152 [ZkClient-EventThread-1015-127.0.0.1:54029] INFO  o.I.z.ZkEventThread - Starting ZkClient event thread.
02:56:33.153 [main-EventThread] INFO  o.a.c.f.s.ConnectionStateManager - State change: CONNECTED
02:56:33.154 [main-EventThread] INFO  o.I.z.ZkClient - zookeeper state changed (SyncConnected)
02:56:33.163 [main] INFO  k.l.LogManager - Log directory '/tmp/kafka/logs/kafka-test-38608' not found, creating it.
02:56:33.163 [main] INFO  k.l.LogManager - Loading logs.
02:56:33.163 [main] INFO  k.l.LogManager - Logs loading complete.
02:56:33.163 [main] INFO  k.l.LogManager - Starting log cleanup with a period of 300000 ms.
02:56:33.163 [main] INFO  k.l.LogManager - Starting log flusher with a default period of 9223372036854775807 ms.
02:56:33.165 [main] INFO  k.n.Acceptor - Awaiting socket connections on 0.0.0.0:38608.
02:56:33.165 [main] INFO  k.n.SocketServer - [Socket Server on Broker 0], Started
02:56:33.167 [main] INFO  k.u.Mx4jLoader$ - Will not load MX4J, mx4j-tools.jar is not in the classpath
02:56:33.167 [main] INFO  k.c.KafkaController - [Controller 0]: Controller starting up
02:56:33.169 [main] INFO  k.s.ZookeeperLeaderElector - 0 successfully elected as leader
02:56:33.169 [main] INFO  k.c.KafkaController - [Controller 0]: Broker 0 starting become controller state transition
02:56:33.170 [main] INFO  k.c.KafkaController - [Controller 0]: Controller 0 incremented epoch to 1
02:56:33.172 [main] INFO  k.c.KafkaController - [Controller 0]: Partitions undergoing preferred replica election: 
02:56:33.172 [main] INFO  k.c.KafkaController - [Controller 0]: Partitions that completed preferred replica election: 
02:56:33.172 [main] INFO  k.c.KafkaController - [Controller 0]: Resuming preferred replica election for partitions: 
02:56:33.172 [main] INFO  k.c.KafkaController - [Controller 0]: Partitions being reassigned: Map()
02:56:33.172 [main] INFO  k.c.KafkaController - [Controller 0]: Partitions already reassigned: List()
02:56:33.173 [main] INFO  k.c.KafkaController - [Controller 0]: Resuming reassignment of partitions: Map()
02:56:33.174 [main] INFO  k.c.KafkaController - [Controller 0]: List of topics to be deleted: 
02:56:33.174 [main] INFO  k.c.KafkaController - [Controller 0]: List of topics ineligible for deletion: 
02:56:33.174 [main] INFO  k.c.KafkaController - [Controller 0]: Currently active brokers in the cluster: Set()
02:56:33.174 [main] INFO  k.c.KafkaController - [Controller 0]: Currently shutting brokers in the cluster: Set()
02:56:33.174 [main] INFO  k.c.KafkaController - [Controller 0]: Current list of topics in the cluster: Set()
02:56:33.174 [main] INFO  k.c.ReplicaStateMachine - [Replica state machine on controller 0]: Started replica state machine with initial state -> Map()
02:56:33.174 [main] INFO  k.c.PartitionStateMachine - [Partition state machine on Controller 0]: Started partition state machine with initial state -> Map()
02:56:33.174 [main] INFO  k.c.KafkaController - [Controller 0]: Broker 0 is ready to serve as the new controller with epoch 1
02:56:33.174 [main] INFO  k.c.KafkaController - [Controller 0]: Starting preferred replica leader election for partitions 
02:56:33.174 [main] INFO  k.c.PartitionStateMachine - [Partition state machine on Controller 0]: Invoking state change to OnlinePartition for partitions 
02:56:33.175 [main] INFO  k.c.KafkaController - [Controller 0]: starting the partition rebalance scheduler
02:56:33.175 [main] INFO  k.c.KafkaController - [Controller 0]: Controller startup complete
02:56:33.176 [ZkClient-EventThread-1015-127.0.0.1:54029] INFO  k.s.ZookeeperLeaderElector$LeaderChangeListener - New leader is 0
02:56:33.178 [main] INFO  k.u.ZkUtils$ - Registered broker 0 at path /brokers/ids/0 with address testing-worker-linux-docker-d7119792-3389-linux-9.prod.travis-ci.org:38608.
02:56:33.178 [main] INFO  k.s.KafkaServer - [Kafka Server 0], started
02:56:33.178 [ZkClient-EventThread-1015-127.0.0.1:54029] INFO  k.c.ReplicaStateMachine$BrokerChangeListener - [BrokerChangeListener on Controller 0]: Broker change listener fired for path /brokers/ids with children 0
02:56:33.180 [ZkClient-EventThread-1015-127.0.0.1:54029] INFO  k.c.ReplicaStateMachine$BrokerChangeListener - [BrokerChangeListener on Controller 0]: Newly added brokers: 0, deleted brokers: , all live brokers: 0
02:56:33.181 [ZkClient-EventThread-1015-127.0.0.1:54029] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Controller 0 connected to id:0,host:testing-worker-linux-docker-d7119792-3389-linux-9.prod.travis-ci.org,port:38608 for sending state change requests
02:56:33.181 [ZkClient-EventThread-1015-127.0.0.1:54029] INFO  k.c.KafkaController - [Controller 0]: New broker startup callback for 0
02:56:33.181 [Controller-0-to-broker-0-send-thread] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Starting 
02:56:33.183 [main] INFO  o.a.k.c.p.ProducerConfig - ProducerConfig values: 
	compression.type = none
	metric.reporters = []
	metadata.max.age.ms = 300000
	metadata.fetch.timeout.ms = 1000
	acks = 1
	batch.size = 16384
	reconnect.backoff.ms = 10
	bootstrap.servers = [localhost:38608]
	receive.buffer.bytes = 32768
	retry.backoff.ms = 100
	buffer.memory = 33554432
	timeout.ms = 30000
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	retries = 0
	max.request.size = 1048576
	block.on.buffer.full = true
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
	metrics.sample.window.ms = 30000
	send.buffer.bytes = 131072
	max.in.flight.requests.per.connection = 5
	metrics.num.samples = 2
	linger.ms = 0
	client.id = 

02:56:33.184 [main] INFO  k.s.KafkaServer - [Kafka Server 0], shutting down
02:56:33.184 [main] INFO  k.s.KafkaServer - [Kafka Server 0], Starting controlled shutdown
02:56:33.188 [kafka-request-handler-0] INFO  k.c.KafkaController - [Controller 0]: Shutting down broker 0
02:56:33.188 [main] INFO  k.s.KafkaServer - [Kafka Server 0], Controlled shutdown succeeded
02:56:33.189 [main] INFO  k.n.SocketServer - [Socket Server on Broker 0], Shutting down
02:56:33.189 [kafka-network-thread-38608-1] INFO  k.n.Processor - Closing socket connection to /172.17.5.8.
02:56:33.189 [main] INFO  k.n.SocketServer - [Socket Server on Broker 0], Shutdown completed
02:56:33.189 [main] INFO  k.s.KafkaRequestHandlerPool - [Kafka Request Handler on Broker 0], shutting down
02:56:33.190 [main] INFO  k.s.KafkaRequestHandlerPool - [Kafka Request Handler on Broker 0], shut down completely
02:56:33.414 [Curator-Framework-0-SendThread(127.0.0.1:52275)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
02:56:33.566 [main] INFO  k.s.ReplicaManager - [Replica Manager on Broker 0]: Shut down
02:56:33.567 [main] INFO  k.s.ReplicaFetcherManager - [ReplicaFetcherManager on broker 0] shutting down
02:56:33.567 [main] INFO  k.s.ReplicaFetcherManager - [ReplicaFetcherManager on broker 0] shutdown completed
02:56:33.567 [main] INFO  k.s.ReplicaManager - [Replica Manager on Broker 0]: Shut down completely
02:56:33.567 [main] INFO  k.l.LogManager - Shutting down.
02:56:33.567 [main] INFO  k.l.LogManager - Shutdown complete.
02:56:33.568 [main] INFO  k.c.PartitionStateMachine - [Partition state machine on Controller 0]: Stopped partition state machine
02:56:33.568 [main] INFO  k.c.ReplicaStateMachine - [Replica state machine on controller 0]: Stopped replica state machine
02:56:33.568 [main] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Shutting down
02:56:33.568 [Controller-0-to-broker-0-send-thread] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Stopped 
02:56:33.568 [main] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Shutdown completed
02:56:33.568 [ZkClient-EventThread-1015-127.0.0.1:54029] INFO  o.I.z.ZkEventThread - Terminate ZkClient event thread.
02:56:33.570 [main] INFO  k.s.KafkaServer - [Kafka Server 0], shut down completed
02:56:33.570 [Curator-Framework-0] INFO  o.a.c.f.i.CuratorFrameworkImpl - backgroundOperationsLoop exiting

--------------------------------------------------
--------------------------------------------------
classname: org.apache.storm.kafka.bolt.KafkaBoltTest / testname: executeWithKey
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.kafka.bolt.KafkaBoltTest.generateTestTuple(KafkaBoltTest.java:290)
	at org.apache.storm.kafka.bolt.KafkaBoltTest.executeWithKey(KafkaBoltTest.java:115)

-------------------- system-out --------------------
02:56:33.584 [Curator-Framework-0-SendThread(127.0.0.1:38937)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
02:56:33.597 [Curator-Framework-0-SendThread(127.0.0.1:36417)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
02:56:33.713 [Curator-Framework-0-SendThread(127.0.0.1:43062)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
02:56:33.903 [Curator-Framework-0-SendThread(127.0.0.1:41699)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
02:56:34.065 [Curator-Framework-0-SendThread(127.0.0.1:51772)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
02:56:34.515 [Curator-Framework-0-SendThread(127.0.0.1:52275)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
02:56:34.576 [main] INFO  o.a.c.f.i.CuratorFrameworkImpl - Starting
02:56:34.578 [main] INFO  k.u.VerifiableProperties - Verifying properties
02:56:34.578 [main] INFO  k.u.VerifiableProperties - Property broker.id is overridden to 0
02:56:34.578 [main] INFO  k.u.VerifiableProperties - Property log.dirs is overridden to /tmp/kafka/logs/kafka-test-36327
02:56:34.578 [main] INFO  k.u.VerifiableProperties - Property port is overridden to 36327
02:56:34.578 [main] INFO  k.u.VerifiableProperties - Property zookeeper.connect is overridden to 127.0.0.1:51847
02:56:34.578 [main] INFO  k.s.KafkaServer - [Kafka Server 0], starting
02:56:34.578 [main] INFO  k.s.KafkaServer - [Kafka Server 0], Connecting to zookeeper on 127.0.0.1:51847
02:56:34.579 [ZkClient-EventThread-1051-127.0.0.1:51847] INFO  o.I.z.ZkEventThread - Starting ZkClient event thread.
02:56:34.587 [main-EventThread] INFO  o.a.c.f.s.ConnectionStateManager - State change: CONNECTED
02:56:34.588 [main-EventThread] INFO  o.I.z.ZkClient - zookeeper state changed (SyncConnected)
02:56:34.598 [main] INFO  k.l.LogManager - Log directory '/tmp/kafka/logs/kafka-test-36327' not found, creating it.
02:56:34.598 [main] INFO  k.l.LogManager - Loading logs.
02:56:34.598 [main] INFO  k.l.LogManager - Logs loading complete.
02:56:34.598 [main] INFO  k.l.LogManager - Starting log cleanup with a period of 300000 ms.
02:56:34.599 [main] INFO  k.l.LogManager - Starting log flusher with a default period of 9223372036854775807 ms.
02:56:34.601 [main] INFO  k.n.Acceptor - Awaiting socket connections on 0.0.0.0:36327.
02:56:34.601 [main] INFO  k.n.SocketServer - [Socket Server on Broker 0], Started
02:56:34.604 [main] INFO  k.u.Mx4jLoader$ - Will not load MX4J, mx4j-tools.jar is not in the classpath
02:56:34.604 [main] INFO  k.c.KafkaController - [Controller 0]: Controller starting up
02:56:34.606 [main] INFO  k.s.ZookeeperLeaderElector - 0 successfully elected as leader
02:56:34.606 [main] INFO  k.c.KafkaController - [Controller 0]: Broker 0 starting become controller state transition
02:56:34.607 [main] INFO  k.c.KafkaController - [Controller 0]: Controller 0 incremented epoch to 1
02:56:34.609 [main] INFO  k.c.KafkaController - [Controller 0]: Partitions undergoing preferred replica election: 
02:56:34.609 [main] INFO  k.c.KafkaController - [Controller 0]: Partitions that completed preferred replica election: 
02:56:34.609 [main] INFO  k.c.KafkaController - [Controller 0]: Resuming preferred replica election for partitions: 
02:56:34.609 [main] INFO  k.c.KafkaController - [Controller 0]: Partitions being reassigned: Map()
02:56:34.609 [main] INFO  k.c.KafkaController - [Controller 0]: Partitions already reassigned: List()
02:56:34.609 [main] INFO  k.c.KafkaController - [Controller 0]: Resuming reassignment of partitions: Map()
02:56:34.610 [main] INFO  k.c.KafkaController - [Controller 0]: List of topics to be deleted: 
02:56:34.610 [main] INFO  k.c.KafkaController - [Controller 0]: List of topics ineligible for deletion: 
02:56:34.610 [main] INFO  k.c.KafkaController - [Controller 0]: Currently active brokers in the cluster: Set()
02:56:34.610 [main] INFO  k.c.KafkaController - [Controller 0]: Currently shutting brokers in the cluster: Set()
02:56:34.610 [main] INFO  k.c.KafkaController - [Controller 0]: Current list of topics in the cluster: Set()
02:56:34.610 [main] INFO  k.c.ReplicaStateMachine - [Replica state machine on controller 0]: Started replica state machine with initial state -> Map()
02:56:34.610 [main] INFO  k.c.PartitionStateMachine - [Partition state machine on Controller 0]: Started partition state machine with initial state -> Map()
02:56:34.610 [main] INFO  k.c.KafkaController - [Controller 0]: Broker 0 is ready to serve as the new controller with epoch 1
02:56:34.610 [main] INFO  k.c.KafkaController - [Controller 0]: Starting preferred replica leader election for partitions 
02:56:34.610 [main] INFO  k.c.PartitionStateMachine - [Partition state machine on Controller 0]: Invoking state change to OnlinePartition for partitions 
02:56:34.611 [main] INFO  k.c.KafkaController - [Controller 0]: starting the partition rebalance scheduler
02:56:34.611 [main] INFO  k.c.KafkaController - [Controller 0]: Controller startup complete
02:56:34.612 [ZkClient-EventThread-1051-127.0.0.1:51847] INFO  k.s.ZookeeperLeaderElector$LeaderChangeListener - New leader is 0
02:56:34.614 [main] INFO  k.u.ZkUtils$ - Registered broker 0 at path /brokers/ids/0 with address testing-worker-linux-docker-d7119792-3389-linux-9.prod.travis-ci.org:36327.
02:56:34.614 [main] INFO  k.s.KafkaServer - [Kafka Server 0], started
02:56:34.614 [main] INFO  o.a.k.c.p.ProducerConfig - ProducerConfig values: 
	compression.type = none
	metric.reporters = []
	metadata.max.age.ms = 300000
	metadata.fetch.timeout.ms = 1000
	acks = 1
	batch.size = 16384
	reconnect.backoff.ms = 10
	bootstrap.servers = [localhost:36327]
	receive.buffer.bytes = 32768
	retry.backoff.ms = 100
	buffer.memory = 33554432
	timeout.ms = 30000
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	retries = 0
	max.request.size = 1048576
	block.on.buffer.full = true
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
	metrics.sample.window.ms = 30000
	send.buffer.bytes = 131072
	max.in.flight.requests.per.connection = 5
	metrics.num.samples = 2
	linger.ms = 0
	client.id = 

02:56:34.616 [ZkClient-EventThread-1051-127.0.0.1:51847] INFO  k.c.ReplicaStateMachine$BrokerChangeListener - [BrokerChangeListener on Controller 0]: Broker change listener fired for path /brokers/ids with children 0
02:56:34.618 [ZkClient-EventThread-1051-127.0.0.1:51847] INFO  k.c.ReplicaStateMachine$BrokerChangeListener - [BrokerChangeListener on Controller 0]: Newly added brokers: 0, deleted brokers: , all live brokers: 0
02:56:34.618 [main] INFO  k.s.KafkaServer - [Kafka Server 0], shutting down
02:56:34.618 [main] INFO  k.s.KafkaServer - [Kafka Server 0], Starting controlled shutdown
02:56:34.618 [ZkClient-EventThread-1051-127.0.0.1:51847] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Controller 0 connected to id:0,host:testing-worker-linux-docker-d7119792-3389-linux-9.prod.travis-ci.org,port:36327 for sending state change requests
02:56:34.621 [ZkClient-EventThread-1051-127.0.0.1:51847] INFO  k.c.KafkaController - [Controller 0]: New broker startup callback for 0
02:56:34.621 [kafka-request-handler-0] INFO  k.c.KafkaController - [Controller 0]: Shutting down broker 0
02:56:34.621 [main] INFO  k.s.KafkaServer - [Kafka Server 0], Controlled shutdown succeeded
02:56:34.622 [main] INFO  k.n.SocketServer - [Socket Server on Broker 0], Shutting down
02:56:34.622 [kafka-network-thread-36327-1] INFO  k.n.Processor - Closing socket connection to /172.17.5.8.
02:56:34.630 [main] INFO  k.n.SocketServer - [Socket Server on Broker 0], Shutdown completed
02:56:34.630 [main] INFO  k.s.KafkaRequestHandlerPool - [Kafka Request Handler on Broker 0], shutting down
02:56:34.631 [main] INFO  k.s.KafkaRequestHandlerPool - [Kafka Request Handler on Broker 0], shut down completely
02:56:34.631 [Controller-0-to-broker-0-send-thread] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Starting 
02:56:34.684 [Curator-Framework-0-SendThread(127.0.0.1:38937)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
02:56:34.698 [Curator-Framework-0-SendThread(127.0.0.1:36417)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
02:56:34.803 [main] INFO  k.s.ReplicaManager - [Replica Manager on Broker 0]: Shut down
02:56:34.803 [main] INFO  k.s.ReplicaFetcherManager - [ReplicaFetcherManager on broker 0] shutting down
02:56:34.803 [main] INFO  k.s.ReplicaFetcherManager - [ReplicaFetcherManager on broker 0] shutdown completed
02:56:34.803 [main] INFO  k.s.ReplicaManager - [Replica Manager on Broker 0]: Shut down completely
02:56:34.803 [main] INFO  k.l.LogManager - Shutting down.
02:56:34.804 [main] INFO  k.l.LogManager - Shutdown complete.
02:56:34.804 [main] INFO  k.c.PartitionStateMachine - [Partition state machine on Controller 0]: Stopped partition state machine
02:56:34.804 [main] INFO  k.c.ReplicaStateMachine - [Replica state machine on controller 0]: Stopped replica state machine
02:56:34.804 [main] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Shutting down
02:56:34.805 [Controller-0-to-broker-0-send-thread] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Stopped 
02:56:34.805 [main] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Shutdown completed
02:56:34.805 [ZkClient-EventThread-1051-127.0.0.1:51847] INFO  o.I.z.ZkEventThread - Terminate ZkClient event thread.
02:56:34.808 [main] INFO  k.s.KafkaServer - [Kafka Server 0], shut down completed
02:56:34.808 [Curator-Framework-0] INFO  o.a.c.f.i.CuratorFrameworkImpl - backgroundOperationsLoop exiting

--------------------------------------------------
--------------------------------------------------
classname: org.apache.storm.kafka.bolt.KafkaBoltTest / testname: executeWithBoltSpecifiedProperties
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.kafka.bolt.KafkaBoltTest.generateTestTuple(KafkaBoltTest.java:290)
	at org.apache.storm.kafka.bolt.KafkaBoltTest.executeWithBoltSpecifiedProperties(KafkaBoltTest.java:199)

-------------------- system-out --------------------
02:56:34.822 [Curator-Framework-0-SendThread(127.0.0.1:43062)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
02:56:35.004 [Curator-Framework-0-SendThread(127.0.0.1:41699)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
02:56:35.166 [Curator-Framework-0-SendThread(127.0.0.1:51772)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
02:56:35.616 [Curator-Framework-0-SendThread(127.0.0.1:52275)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
02:56:35.785 [Curator-Framework-0-SendThread(127.0.0.1:38937)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
02:56:35.798 [Curator-Framework-0-SendThread(127.0.0.1:36417)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
02:56:35.814 [main] INFO  o.a.c.f.i.CuratorFrameworkImpl - Starting
02:56:35.816 [main] INFO  k.u.VerifiableProperties - Verifying properties
02:56:35.816 [main] INFO  k.u.VerifiableProperties - Property broker.id is overridden to 0
02:56:35.816 [main] INFO  k.u.VerifiableProperties - Property log.dirs is overridden to /tmp/kafka/logs/kafka-test-41994
02:56:35.816 [main] INFO  k.u.VerifiableProperties - Property port is overridden to 41994
02:56:35.816 [main] INFO  k.u.VerifiableProperties - Property zookeeper.connect is overridden to 127.0.0.1:50263
02:56:35.816 [main] INFO  k.s.KafkaServer - [Kafka Server 0], starting
02:56:35.816 [main] INFO  k.s.KafkaServer - [Kafka Server 0], Connecting to zookeeper on 127.0.0.1:50263
02:56:35.817 [ZkClient-EventThread-1087-127.0.0.1:50263] INFO  o.I.z.ZkEventThread - Starting ZkClient event thread.
02:56:35.819 [main-EventThread] INFO  o.a.c.f.s.ConnectionStateManager - State change: CONNECTED
02:56:35.819 [main-EventThread] INFO  o.I.z.ZkClient - zookeeper state changed (SyncConnected)
02:56:35.832 [main] INFO  k.l.LogManager - Log directory '/tmp/kafka/logs/kafka-test-41994' not found, creating it.
02:56:35.832 [main] INFO  k.l.LogManager - Loading logs.
02:56:35.832 [main] INFO  k.l.LogManager - Logs loading complete.
02:56:35.832 [main] INFO  k.l.LogManager - Starting log cleanup with a period of 300000 ms.
02:56:35.832 [main] INFO  k.l.LogManager - Starting log flusher with a default period of 9223372036854775807 ms.
02:56:35.833 [main] INFO  k.n.Acceptor - Awaiting socket connections on 0.0.0.0:41994.
02:56:35.834 [main] INFO  k.n.SocketServer - [Socket Server on Broker 0], Started
02:56:35.836 [main] INFO  k.u.Mx4jLoader$ - Will not load MX4J, mx4j-tools.jar is not in the classpath
02:56:35.836 [main] INFO  k.c.KafkaController - [Controller 0]: Controller starting up
02:56:35.838 [main] INFO  k.s.ZookeeperLeaderElector - 0 successfully elected as leader
02:56:35.838 [main] INFO  k.c.KafkaController - [Controller 0]: Broker 0 starting become controller state transition
02:56:35.839 [main] INFO  k.c.KafkaController - [Controller 0]: Controller 0 incremented epoch to 1
02:56:35.843 [main] INFO  k.c.KafkaController - [Controller 0]: Partitions undergoing preferred replica election: 
02:56:35.843 [main] INFO  k.c.KafkaController - [Controller 0]: Partitions that completed preferred replica election: 
02:56:35.843 [main] INFO  k.c.KafkaController - [Controller 0]: Resuming preferred replica election for partitions: 
02:56:35.843 [main] INFO  k.c.KafkaController - [Controller 0]: Partitions being reassigned: Map()
02:56:35.843 [main] INFO  k.c.KafkaController - [Controller 0]: Partitions already reassigned: List()
02:56:35.843 [main] INFO  k.c.KafkaController - [Controller 0]: Resuming reassignment of partitions: Map()
02:56:35.843 [main] INFO  k.c.KafkaController - [Controller 0]: List of topics to be deleted: 
02:56:35.843 [main] INFO  k.c.KafkaController - [Controller 0]: List of topics ineligible for deletion: 
02:56:35.844 [main] INFO  k.c.KafkaController - [Controller 0]: Currently active brokers in the cluster: Set()
02:56:35.844 [main] INFO  k.c.KafkaController - [Controller 0]: Currently shutting brokers in the cluster: Set()
02:56:35.844 [main] INFO  k.c.KafkaController - [Controller 0]: Current list of topics in the cluster: Set()
02:56:35.844 [main] INFO  k.c.ReplicaStateMachine - [Replica state machine on controller 0]: Started replica state machine with initial state -> Map()
02:56:35.844 [main] INFO  k.c.PartitionStateMachine - [Partition state machine on Controller 0]: Started partition state machine with initial state -> Map()
02:56:35.844 [main] INFO  k.c.KafkaController - [Controller 0]: Broker 0 is ready to serve as the new controller with epoch 1
02:56:35.844 [main] INFO  k.c.KafkaController - [Controller 0]: Starting preferred replica leader election for partitions 
02:56:35.844 [main] INFO  k.c.PartitionStateMachine - [Partition state machine on Controller 0]: Invoking state change to OnlinePartition for partitions 
02:56:35.845 [main] INFO  k.c.KafkaController - [Controller 0]: starting the partition rebalance scheduler
02:56:35.845 [main] INFO  k.c.KafkaController - [Controller 0]: Controller startup complete
02:56:35.846 [ZkClient-EventThread-1087-127.0.0.1:50263] INFO  k.s.ZookeeperLeaderElector$LeaderChangeListener - New leader is 0
02:56:35.852 [main] INFO  k.u.ZkUtils$ - Registered broker 0 at path /brokers/ids/0 with address testing-worker-linux-docker-d7119792-3389-linux-9.prod.travis-ci.org:41994.
02:56:35.852 [main] INFO  k.s.KafkaServer - [Kafka Server 0], started
02:56:35.852 [main] INFO  o.a.k.c.p.ProducerConfig - ProducerConfig values: 
	compression.type = none
	metric.reporters = []
	metadata.max.age.ms = 300000
	metadata.fetch.timeout.ms = 1000
	acks = 1
	batch.size = 16384
	reconnect.backoff.ms = 10
	bootstrap.servers = [localhost:41994]
	receive.buffer.bytes = 32768
	retry.backoff.ms = 100
	buffer.memory = 33554432
	timeout.ms = 30000
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	retries = 0
	max.request.size = 1048576
	block.on.buffer.full = true
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
	metrics.sample.window.ms = 30000
	send.buffer.bytes = 131072
	max.in.flight.requests.per.connection = 5
	metrics.num.samples = 2
	linger.ms = 0
	client.id = 

02:56:35.855 [ZkClient-EventThread-1087-127.0.0.1:50263] INFO  k.c.ReplicaStateMachine$BrokerChangeListener - [BrokerChangeListener on Controller 0]: Broker change listener fired for path /brokers/ids with children 0
02:56:35.855 [main] INFO  o.a.k.c.p.ProducerConfig - ProducerConfig values: 
	compression.type = none
	metric.reporters = []
	metadata.max.age.ms = 300000
	metadata.fetch.timeout.ms = 1000
	acks = 1
	batch.size = 16384
	reconnect.backoff.ms = 10
	bootstrap.servers = [localhost:41994]
	receive.buffer.bytes = 32768
	retry.backoff.ms = 100
	buffer.memory = 33554432
	timeout.ms = 30000
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	retries = 0
	max.request.size = 1048576
	block.on.buffer.full = true
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	metrics.sample.window.ms = 30000
	send.buffer.bytes = 131072
	max.in.flight.requests.per.connection = 5
	metrics.num.samples = 2
	linger.ms = 0
	client.id = 

02:56:35.856 [main] INFO  k.s.KafkaServer - [Kafka Server 0], shutting down
02:56:35.856 [main] INFO  k.s.KafkaServer - [Kafka Server 0], Starting controlled shutdown
02:56:35.859 [ZkClient-EventThread-1087-127.0.0.1:50263] INFO  k.c.ReplicaStateMachine$BrokerChangeListener - [BrokerChangeListener on Controller 0]: Newly added brokers: 0, deleted brokers: , all live brokers: 0
02:56:35.859 [ZkClient-EventThread-1087-127.0.0.1:50263] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Controller 0 connected to id:0,host:testing-worker-linux-docker-d7119792-3389-linux-9.prod.travis-ci.org,port:41994 for sending state change requests
02:56:35.864 [ZkClient-EventThread-1087-127.0.0.1:50263] INFO  k.c.KafkaController - [Controller 0]: New broker startup callback for 0
02:56:35.864 [Controller-0-to-broker-0-send-thread] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Starting 
02:56:35.864 [kafka-request-handler-0] INFO  k.c.KafkaController - [Controller 0]: Shutting down broker 0
02:56:35.865 [main] INFO  k.s.KafkaServer - [Kafka Server 0], Controlled shutdown succeeded
02:56:35.865 [main] INFO  k.n.SocketServer - [Socket Server on Broker 0], Shutting down
02:56:35.865 [kafka-network-thread-41994-1] INFO  k.n.Processor - Closing socket connection to /172.17.5.8.
02:56:35.865 [main] INFO  k.n.SocketServer - [Socket Server on Broker 0], Shutdown completed
02:56:35.866 [main] INFO  k.s.KafkaRequestHandlerPool - [Kafka Request Handler on Broker 0], shutting down
02:56:35.866 [main] INFO  k.s.KafkaRequestHandlerPool - [Kafka Request Handler on Broker 0], shut down completely
02:56:35.923 [Curator-Framework-0-SendThread(127.0.0.1:43062)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
02:56:36.105 [Curator-Framework-0-SendThread(127.0.0.1:41699)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
02:56:36.235 [main] INFO  k.s.ReplicaManager - [Replica Manager on Broker 0]: Shut down
02:56:36.235 [main] INFO  k.s.ReplicaFetcherManager - [ReplicaFetcherManager on broker 0] shutting down
02:56:36.235 [main] INFO  k.s.ReplicaFetcherManager - [ReplicaFetcherManager on broker 0] shutdown completed
02:56:36.235 [main] INFO  k.s.ReplicaManager - [Replica Manager on Broker 0]: Shut down completely
02:56:36.235 [main] INFO  k.l.LogManager - Shutting down.
02:56:36.236 [main] INFO  k.l.LogManager - Shutdown complete.
02:56:36.236 [main] INFO  k.c.PartitionStateMachine - [Partition state machine on Controller 0]: Stopped partition state machine
02:56:36.236 [main] INFO  k.c.ReplicaStateMachine - [Replica state machine on controller 0]: Stopped replica state machine
02:56:36.236 [main] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Shutting down
02:56:36.236 [main] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Shutdown completed
02:56:36.237 [ZkClient-EventThread-1087-127.0.0.1:50263] INFO  o.I.z.ZkEventThread - Terminate ZkClient event thread.
02:56:36.237 [Controller-0-to-broker-0-send-thread] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Stopped 
02:56:36.238 [main] INFO  k.s.KafkaServer - [Kafka Server 0], shut down completed
02:56:36.238 [Curator-Framework-0] INFO  o.a.c.f.i.CuratorFrameworkImpl - backgroundOperationsLoop exiting

--------------------------------------------------
Checking ./external/storm-kafka/target/surefire-reports/TEST-org.apache.storm.kafka.ExponentialBackoffMsgRetryManagerTest.xml
Checking ./external/storm-kafka/target/surefire-reports/TEST-org.apache.storm.kafka.ZkCoordinatorTest.xml
Checking ./external/storm-kafka/target/surefire-reports/TEST-org.apache.storm.kafka.TridentKafkaTest.xml
Looking for errors in ./external/storm-redis/target/surefire-reports
Checking ./external/storm-redis/target/surefire-reports/TEST-org.apache.storm.redis.state.DefaultStateSerializerTest.xml
Checking ./external/storm-redis/target/surefire-reports/TEST-org.apache.storm.redis.state.RedisKeyValueStateTest.xml
Checking ./external/storm-redis/target/surefire-reports/TEST-org.apache.storm.redis.state.RedisKeyValueStateProviderTest.xml

travis_time:end:302557db:start=1466736919847263205,finish=1466737186243168249,duration=266395905044[0K
[31;1mThe command "/bin/bash ./dev-tools/travis/travis-script.sh `pwd` $MODULES" exited with 1.[0m
travis_fold:start:cache.2[0Kstore build cache
travis_time:start:099ae6c6[0K
travis_time:end:099ae6c6:start=1466737186247471793,finish=1466737186250854045,duration=3382252[0Ktravis_time:start:0bb3319c[0K[32;1mchange detected (content changed, file is created, or file is deleted):
/home/travis/.m2/repository/eigenbase/eigenbase-properties/1.1.4/eigenbase-properties-1.1.4.pom.lastUpdated
/home/travis/.m2/repository/io/confluent/kafka-avro-serializer/1.0/kafka-avro-serializer-1.0.pom.lastUpdated
/home/travis/.m2/repository/io/confluent/kafka-schema-registry-client/1.0/kafka-schema-registry-client-1.0.pom.lastUpdated
/home/travis/.m2/repository/net/hydromatic/linq4j/0.4/linq4j-0.4.pom.lastUpdated
/home/travis/.m2/repository/net/hydromatic/quidem/0.1.1/quidem-0.1.1.pom.lastUpdated
/home/travis/.m2/repository/org/apache/storm/flux/2.0.0-SNAPSHOT/maven-metadata-local.xml
/home/travis/.m2/repository/org/apache/storm/flux/2.0.0-SNAPSHOT/_remote.repositories
/home/travis/.m2/repository/org/apache/storm/flux-core/2.0.0-SNAPSHOT/flux-core-2.0.0-SNAPSHOT.jar
/home/travis/.m2/repository/org/apache/storm/flux-core/2.0.0-SNAPSHOT/maven-metadata-local.xml
/home/travis/.m2/repository/org/apache/storm/flux-core/2.0.0-SNAPSHOT/_remote.repositories
/home/travis/.m2/repository/org/ap
[0m
[32;1m...
[0m
[32;1mchanges detected, packing new archive[0m
.
.
.
.
.
.
.
[32;1muploading archive[0m

travis_time:end:0bb3319c:start=1466737186254720913,finish=1466737248157690108,duration=61902969195[0Ktravis_fold:end:cache.2[0K
Done. Your build exited with 1.
