Using worker: worker-linux-docker-cb11978e.prod.travis-ci.org:travis-linux-9

travis_fold:start:system_info[0K[33;1mBuild system information[0m
Build language: java
Build group: stable
Build dist: precise
[34m[1mBuild image provisioning date and time[0m
Thu Feb  5 15:09:33 UTC 2015
[34m[1mOperating System Details[0m
Distributor ID:	Ubuntu
Description:	Ubuntu 12.04.5 LTS
Release:	12.04
Codename:	precise
[34m[1mLinux Version[0m
3.13.0-29-generic
[34m[1mCookbooks Version[0m
a68419e https://github.com/travis-ci/travis-cookbooks/tree/a68419e
[34m[1mGCC version[0m
gcc (Ubuntu/Linaro 4.6.3-1ubuntu5) 4.6.3
Copyright (C) 2011 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.

[34m[1mLLVM version[0m
clang version 3.4 (tags/RELEASE_34/final)
Target: x86_64-unknown-linux-gnu
Thread model: posix
[34m[1mPre-installed Ruby versions[0m
ruby-1.9.3-p551
[34m[1mPre-installed Node.js versions[0m
v0.10.36
[34m[1mPre-installed Go versions[0m
1.4.1
[34m[1mRedis version[0m
redis-server 2.8.19
[34m[1mriak version[0m
2.0.2
[34m[1mMongoDB version[0m
MongoDB 2.4.12
[34m[1mCouchDB version[0m
couchdb 1.6.1
[34m[1mNeo4j version[0m
1.9.4
[34m[1mRabbitMQ Version[0m
3.4.3
[34m[1mElasticSearch version[0m
1.4.0
[34m[1mInstalled Sphinx versions[0m
2.0.10
2.1.9
2.2.6
[34m[1mDefault Sphinx version[0m
2.2.6
[34m[1mInstalled Firefox version[0m
firefox 31.0esr
[34m[1mPhantomJS version[0m
1.9.8
[34m[1mant -version[0m
Apache Ant(TM) version 1.8.2 compiled on December 3 2011
[34m[1mmvn -version[0m
Apache Maven 3.2.5 (12a6b3acb947671f09b81f49094c53f426d8cea1; 2014-12-14T17:29:23+00:00)
Maven home: /usr/local/maven
Java version: 1.7.0_76, vendor: Oracle Corporation
Java home: /usr/lib/jvm/java-7-oracle/jre
Default locale: en_US, platform encoding: ANSI_X3.4-1968
OS name: "linux", version: "3.13.0-29-generic", arch: "amd64", family: "unix"
travis_fold:end:system_info[0K
travis_fold:start:fix.CVE-2015-7547[0K$ export DEBIAN_FRONTEND=noninteractive
W: Size of file /var/lib/apt/lists/us.archive.ubuntu.com_ubuntu_dists_precise-backports_multiverse_source_Sources.gz is not what the server reported 5886 5888
W: Size of file /var/lib/apt/lists/ppa.launchpad.net_ubuntugis_ppa_ubuntu_dists_precise_main_binary-amd64_Packages.gz is not what the server reported 36669 36677
W: Size of file /var/lib/apt/lists/ppa.launchpad.net_ubuntugis_ppa_ubuntu_dists_precise_main_binary-i386_Packages.gz is not what the server reported 36729 36733
Reading package lists...
Building dependency tree...
Reading state information...
The following extra packages will be installed:
  libc-bin libc-dev-bin libc6-dev
Suggested packages:
  glibc-doc
The following packages will be upgraded:
  libc-bin libc-dev-bin libc6 libc6-dev
4 upgraded, 0 newly installed, 0 to remove and 247 not upgraded.
Need to get 8,840 kB of archives.
After this operation, 14.3 kB disk space will be freed.
Get:1 http://us.archive.ubuntu.com/ubuntu/ precise-updates/main libc6-dev amd64 2.15-0ubuntu10.15 [2,943 kB]
Get:2 http://us.archive.ubuntu.com/ubuntu/ precise-updates/main libc-dev-bin amd64 2.15-0ubuntu10.15 [84.7 kB]
Get:3 http://us.archive.ubuntu.com/ubuntu/ precise-updates/main libc-bin amd64 2.15-0ubuntu10.15 [1,177 kB]
Get:4 http://us.archive.ubuntu.com/ubuntu/ precise-updates/main libc6 amd64 2.15-0ubuntu10.15 [4,636 kB]
Fetched 8,840 kB in 0s (27.4 MB/s)
Preconfiguring packages ...
(Reading database ... (Reading database ... 5%(Reading database ... 10%(Reading database ... 15%(Reading database ... 20%(Reading database ... 25%(Reading database ... 30%(Reading database ... 35%(Reading database ... 40%(Reading database ... 45%(Reading database ... 50%(Reading database ... 55%(Reading database ... 60%(Reading database ... 65%(Reading database ... 70%(Reading database ... 75%(Reading database ... 80%(Reading database ... 85%(Reading database ... 90%(Reading database ... 95%(Reading database ... 100%(Reading database ... 72019 files and directories currently installed.)
Preparing to replace libc6-dev 2.15-0ubuntu10.10 (using .../libc6-dev_2.15-0ubuntu10.15_amd64.deb) ...
Unpacking replacement libc6-dev ...
Preparing to replace libc-dev-bin 2.15-0ubuntu10.10 (using .../libc-dev-bin_2.15-0ubuntu10.15_amd64.deb) ...
Unpacking replacement libc-dev-bin ...
Preparing to replace libc-bin 2.15-0ubuntu10.10 (using .../libc-bin_2.15-0ubuntu10.15_amd64.deb) ...
Unpacking replacement libc-bin ...
Processing triggers for man-db ...
Setting up libc-bin (2.15-0ubuntu10.15) ...
(Reading database ... (Reading database ... 5%(Reading database ... 10%(Reading database ... 15%(Reading database ... 20%(Reading database ... 25%(Reading database ... 30%(Reading database ... 35%(Reading database ... 40%(Reading database ... 45%(Reading database ... 50%(Reading database ... 55%(Reading database ... 60%(Reading database ... 65%(Reading database ... 70%(Reading database ... 75%(Reading database ... 80%(Reading database ... 85%(Reading database ... 90%(Reading database ... 95%(Reading database ... 100%(Reading database ... 72018 files and directories currently installed.)
Preparing to replace libc6 2.15-0ubuntu10.10 (using .../libc6_2.15-0ubuntu10.15_amd64.deb) ...
Unpacking replacement libc6 ...
Setting up libc6 (2.15-0ubuntu10.15) ...
Setting up libc-dev-bin (2.15-0ubuntu10.15) ...
Setting up libc6-dev (2.15-0ubuntu10.15) ...
Processing triggers for libc-bin ...
ldconfig deferred processing now taking place
travis_fold:end:fix.CVE-2015-7547[0Ktravis_fold:start:git.checkout[0Ktravis_time:start:221917da[0K$ git clone --depth=50 https://github.com/apache/storm.git apache/storm
Cloning into 'apache/storm'...
remote: Counting objects: 19310, done.[K
remote: Compressing objects:   0% (1/7822)   [Kremote: Compressing objects:   1% (79/7822)   [Kremote: Compressing objects:   2% (157/7822)   [Kremote: Compressing objects:   3% (235/7822)   [Kremote: Compressing objects:   4% (313/7822)   [Kremote: Compressing objects:   5% (392/7822)   [Kremote: Compressing objects:   6% (470/7822)   [Kremote: Compressing objects:   7% (548/7822)   [Kremote: Compressing objects:   8% (626/7822)   [Kremote: Compressing objects:   9% (704/7822)   [Kremote: Compressing objects:  10% (783/7822)   [Kremote: Compressing objects:  11% (861/7822)   [Kremote: Compressing objects:  12% (939/7822)   [Kremote: Compressing objects:  13% (1017/7822)   [Kremote: Compressing objects:  14% (1096/7822)   [Kremote: Compressing objects:  15% (1174/7822)   [Kremote: Compressing objects:  16% (1252/7822)   [Kremote: Compressing objects:  17% (1330/7822)   [Kremote: Compressing objects:  18% (1408/7822)   [Kremote: Compressing objects:  19% (1487/7822)   [Kremote: Compressing objects:  20% (1565/7822)   [Kremote: Compressing objects:  21% (1643/7822)   [Kremote: Compressing objects:  22% (1721/7822)   [Kremote: Compressing objects:  23% (1800/7822)   [Kremote: Compressing objects:  24% (1878/7822)   [Kremote: Compressing objects:  25% (1956/7822)   [Kremote: Compressing objects:  26% (2034/7822)   [Kremote: Compressing objects:  27% (2112/7822)   [Kremote: Compressing objects:  28% (2191/7822)   [Kremote: Compressing objects:  29% (2269/7822)   [Kremote: Compressing objects:  30% (2347/7822)   [Kremote: Compressing objects:  31% (2425/7822)   [Kremote: Compressing objects:  32% (2504/7822)   [Kremote: Compressing objects:  33% (2582/7822)   [Kremote: Compressing objects:  34% (2660/7822)   [Kremote: Compressing objects:  35% (2738/7822)   [Kremote: Compressing objects:  36% (2816/7822)   [Kremote: Compressing objects:  37% (2895/7822)   [Kremote: Compressing objects:  38% (2973/7822)   [Kremote: Compressing objects:  39% (3051/7822)   [Kremote: Compressing objects:  40% (3129/7822)   [Kremote: Compressing objects:  41% (3208/7822)   [Kremote: Compressing objects:  42% (3286/7822)   [Kremote: Compressing objects:  43% (3364/7822)   [Kremote: Compressing objects:  44% (3442/7822)   [Kremote: Compressing objects:  45% (3520/7822)   [Kremote: Compressing objects:  46% (3599/7822)   [Kremote: Compressing objects:  47% (3677/7822)   [Kremote: Compressing objects:  48% (3755/7822)   [Kremote: Compressing objects:  49% (3833/7822)   [Kremote: Compressing objects:  50% (3911/7822)   [Kremote: Compressing objects:  51% (3990/7822)   [Kremote: Compressing objects:  52% (4068/7822)   [Kremote: Compressing objects:  53% (4146/7822)   [Kremote: Compressing objects:  54% (4224/7822)   [Kremote: Compressing objects:  55% (4303/7822)   [Kremote: Compressing objects:  56% (4381/7822)   [Kremote: Compressing objects:  57% (4459/7822)   [Kremote: Compressing objects:  58% (4537/7822)   [Kremote: Compressing objects:  59% (4615/7822)   [Kremote: Compressing objects:  60% (4694/7822)   [Kremote: Compressing objects:  61% (4772/7822)   [Kremote: Compressing objects:  62% (4850/7822)   [Kremote: Compressing objects:  63% (4928/7822)   [Kremote: Compressing objects:  64% (5007/7822)   [Kremote: Compressing objects:  65% (5085/7822)   [Kremote: Compressing objects:  66% (5163/7822)   [Kremote: Compressing objects:  67% (5241/7822)   [Kremote: Compressing objects:  68% (5319/7822)   [Kremote: Compressing objects:  69% (5398/7822)   [Kremote: Compressing objects:  70% (5476/7822)   [Kremote: Compressing objects:  71% (5554/7822)   [Kremote: Compressing objects:  72% (5632/7822)   [Kremote: Compressing objects:  73% (5711/7822)   [Kremote: Compressing objects:  74% (5789/7822)   [Kremote: Compressing objects:  75% (5867/7822)   [Kremote: Compressing objects:  76% (5945/7822)   [Kremote: Compressing objects:  77% (6023/7822)   [Kremote: Compressing objects:  78% (6102/7822)   [Kremote: Compressing objects:  79% (6180/7822)   [Kremote: Compressing objects:  80% (6258/7822)   [Kremote: Compressing objects:  81% (6336/7822)   [Kremote: Compressing objects:  82% (6415/7822)   [Kremote: Compressing objects:  83% (6493/7822)   [Kremote: Compressing objects:  84% (6571/7822)   [Kremote: Compressing objects:  85% (6649/7822)   [Kremote: Compressing objects:  86% (6727/7822)   [Kremote: Compressing objects:  87% (6806/7822)   [Kremote: Compressing objects:  88% (6884/7822)   [Kremote: Compressing objects:  89% (6962/7822)   [Kremote: Compressing objects:  90% (7040/7822)   [Kremote: Compressing objects:  91% (7119/7822)   [Kremote: Compressing objects:  92% (7197/7822)   [Kremote: Compressing objects:  93% (7275/7822)   [Kremote: Compressing objects:  94% (7353/7822)   [Kremote: Compressing objects:  95% (7431/7822)   [Kremote: Compressing objects:  96% (7510/7822)   [Kremote: Compressing objects:  97% (7588/7822)   [Kremote: Compressing objects:  98% (7666/7822)   [Kremote: Compressing objects:  99% (7744/7822)   [Kremote: Compressing objects: 100% (7822/7822)   [Kremote: Compressing objects: 100% (7822/7822), done.[K
Receiving objects:   0% (1/19310)   Receiving objects:   1% (194/19310)   Receiving objects:   2% (387/19310)   Receiving objects:   3% (580/19310)   Receiving objects:   4% (773/19310)   Receiving objects:   5% (966/19310)   Receiving objects:   6% (1159/19310)   Receiving objects:   7% (1352/19310)   Receiving objects:   8% (1545/19310)   Receiving objects:   9% (1738/19310)   Receiving objects:  10% (1931/19310)   Receiving objects:  11% (2125/19310)   Receiving objects:  12% (2318/19310)   Receiving objects:  13% (2511/19310)   Receiving objects:  14% (2704/19310)   Receiving objects:  15% (2897/19310)   Receiving objects:  16% (3090/19310)   Receiving objects:  17% (3283/19310)   Receiving objects:  18% (3476/19310)   Receiving objects:  19% (3669/19310)   Receiving objects:  20% (3862/19310)   Receiving objects:  21% (4056/19310)   Receiving objects:  22% (4249/19310)   Receiving objects:  23% (4442/19310)   Receiving objects:  24% (4635/19310)   Receiving objects:  25% (4828/19310)   Receiving objects:  26% (5021/19310)   Receiving objects:  27% (5214/19310)   Receiving objects:  28% (5407/19310)   Receiving objects:  29% (5600/19310)   Receiving objects:  30% (5793/19310)   Receiving objects:  31% (5987/19310)   Receiving objects:  32% (6180/19310)   Receiving objects:  33% (6373/19310)   Receiving objects:  34% (6566/19310)   Receiving objects:  35% (6759/19310)   Receiving objects:  36% (6952/19310)   Receiving objects:  37% (7145/19310)   Receiving objects:  38% (7338/19310)   Receiving objects:  39% (7531/19310)   Receiving objects:  40% (7724/19310)   Receiving objects:  41% (7918/19310)   Receiving objects:  42% (8111/19310)   Receiving objects:  43% (8304/19310)   Receiving objects:  44% (8497/19310)   Receiving objects:  45% (8690/19310)   Receiving objects:  46% (8883/19310)   Receiving objects:  47% (9076/19310)   Receiving objects:  48% (9269/19310)   Receiving objects:  49% (9462/19310)   Receiving objects:  50% (9655/19310)   Receiving objects:  51% (9849/19310)   Receiving objects:  52% (10042/19310)   Receiving objects:  53% (10235/19310)   Receiving objects:  54% (10428/19310)   Receiving objects:  55% (10621/19310)   Receiving objects:  56% (10814/19310)   Receiving objects:  57% (11007/19310)   Receiving objects:  58% (11200/19310)   Receiving objects:  59% (11393/19310)   Receiving objects:  60% (11586/19310)   Receiving objects:  61% (11780/19310)   Receiving objects:  62% (11973/19310)   Receiving objects:  63% (12166/19310)   Receiving objects:  64% (12359/19310)   Receiving objects:  65% (12552/19310)   Receiving objects:  66% (12745/19310)   Receiving objects:  67% (12938/19310)   Receiving objects:  68% (13131/19310)   Receiving objects:  69% (13324/19310)   Receiving objects:  70% (13517/19310)   Receiving objects:  71% (13711/19310)   Receiving objects:  72% (13904/19310)   Receiving objects:  73% (14097/19310)   Receiving objects:  74% (14290/19310)   Receiving objects:  75% (14483/19310)   Receiving objects:  76% (14676/19310)   Receiving objects:  77% (14869/19310)   Receiving objects:  78% (15062/19310)   Receiving objects:  79% (15255/19310)   Receiving objects:  80% (15448/19310)   Receiving objects:  81% (15642/19310)   Receiving objects:  82% (15835/19310)   Receiving objects:  83% (16028/19310)   Receiving objects:  84% (16221/19310)   Receiving objects:  85% (16414/19310)   Receiving objects:  86% (16607/19310)   Receiving objects:  87% (16800/19310)   Receiving objects:  88% (16993/19310)   Receiving objects:  89% (17186/19310)   Receiving objects:  90% (17379/19310)   Receiving objects:  91% (17573/19310)   Receiving objects:  92% (17766/19310)   Receiving objects:  93% (17959/19310)   Receiving objects:  94% (18152/19310)   Receiving objects:  95% (18345/19310)   Receiving objects:  96% (18538/19310)   Receiving objects:  97% (18731/19310)   Receiving objects:  98% (18924/19310)   remote: Total 19310 (delta 9316), reused 16551 (delta 7163), pack-reused 0[K
Receiving objects:  99% (19117/19310)   Receiving objects: 100% (19310/19310)   Receiving objects: 100% (19310/19310), 14.72 MiB | 0 bytes/s, done.
Resolving deltas:   0% (0/9316)   Resolving deltas:   1% (102/9316)   Resolving deltas:   2% (208/9316)   Resolving deltas:   3% (353/9316)   Resolving deltas:   4% (384/9316)   Resolving deltas:   5% (473/9316)   Resolving deltas:   6% (560/9316)   Resolving deltas:   7% (654/9316)   Resolving deltas:   8% (787/9316)   Resolving deltas:   9% (858/9316)   Resolving deltas:  10% (942/9316)   Resolving deltas:  11% (1032/9316)   Resolving deltas:  12% (1141/9316)   Resolving deltas:  13% (1213/9316)   Resolving deltas:  14% (1305/9316)   Resolving deltas:  15% (1404/9316)   Resolving deltas:  16% (1495/9316)   Resolving deltas:  17% (1601/9316)   Resolving deltas:  18% (1680/9316)   Resolving deltas:  19% (1776/9316)   Resolving deltas:  20% (1874/9316)   Resolving deltas:  21% (1957/9316)   Resolving deltas:  22% (2050/9316)   Resolving deltas:  23% (2147/9316)   Resolving deltas:  24% (2273/9316)   Resolving deltas:  25% (2330/9316)   Resolving deltas:  26% (2423/9316)   Resolving deltas:  27% (2528/9316)   Resolving deltas:  28% (2683/9316)   Resolving deltas:  32% (2997/9316)   Resolving deltas:  33% (3076/9316)   Resolving deltas:  34% (3173/9316)   Resolving deltas:  35% (3278/9316)   Resolving deltas:  36% (3358/9316)   Resolving deltas:  39% (3645/9316)   Resolving deltas:  40% (3728/9316)   Resolving deltas:  41% (3833/9316)   Resolving deltas:  42% (3917/9316)   Resolving deltas:  43% (4008/9316)   Resolving deltas:  44% (4119/9316)   Resolving deltas:  45% (4195/9316)   Resolving deltas:  46% (4303/9316)   Resolving deltas:  47% (4387/9316)   Resolving deltas:  48% (4474/9316)   Resolving deltas:  49% (4566/9316)   Resolving deltas:  50% (4662/9316)   Resolving deltas:  51% (4771/9316)   Resolving deltas:  52% (4848/9316)   Resolving deltas:  53% (4940/9316)   Resolving deltas:  54% (5037/9316)   Resolving deltas:  55% (5124/9316)   Resolving deltas:  56% (5221/9316)   Resolving deltas:  57% (5315/9316)   Resolving deltas:  58% (5405/9316)   Resolving deltas:  59% (5500/9316)   Resolving deltas:  60% (5591/9316)   Resolving deltas:  61% (5732/9316)   Resolving deltas:  62% (5776/9316)   Resolving deltas:  63% (5880/9316)   Resolving deltas:  64% (5964/9316)   Resolving deltas:  65% (6084/9316)   Resolving deltas:  66% (6154/9316)   Resolving deltas:  67% (6246/9316)   Resolving deltas:  68% (6337/9316)   Resolving deltas:  75% (7050/9316)   Resolving deltas:  76% (7094/9316)   Resolving deltas:  78% (7274/9316)   Resolving deltas:  79% (7362/9316)   Resolving deltas:  80% (7456/9316)   Resolving deltas:  81% (7548/9316)   Resolving deltas:  82% (7642/9316)   Resolving deltas:  83% (7767/9316)   Resolving deltas:  84% (7841/9316)   Resolving deltas:  85% (7919/9316)   Resolving deltas:  86% (8012/9316)   Resolving deltas:  87% (8105/9316)   Resolving deltas:  88% (8199/9316)   Resolving deltas:  89% (8292/9316)   Resolving deltas:  90% (8385/9316)   Resolving deltas:  92% (8641/9316)   Resolving deltas:  93% (8672/9316)   Resolving deltas:  94% (8761/9316)   Resolving deltas:  95% (8855/9316)   Resolving deltas:  96% (8945/9316)   Resolving deltas:  97% (9052/9316)   Resolving deltas:  98% (9130/9316)   Resolving deltas:  99% (9231/9316)   Resolving deltas: 100% (9316/9316)   Resolving deltas: 100% (9316/9316), done.
Checking connectivity... done.

travis_time:end:221917da:start=1466779556150239606,finish=1466779558390557258,duration=2240317652[0K$ cd apache/storm
travis_time:start:01d1bd4c[0K$ git fetch origin +refs/pull/1515/merge:
remote: Counting objects: 14, done.[K
remote: Compressing objects:   9% (1/11)   [Kremote: Compressing objects:  18% (2/11)   [Kremote: Compressing objects:  27% (3/11)   [Kremote: Compressing objects:  36% (4/11)   [Kremote: Compressing objects:  45% (5/11)   [Kremote: Compressing objects:  54% (6/11)   [Kremote: Compressing objects:  63% (7/11)   [Kremote: Compressing objects:  72% (8/11)   [Kremote: Compressing objects:  81% (9/11)   [Kremote: Compressing objects:  90% (10/11)   [Kremote: Compressing objects: 100% (11/11)   [Kremote: Compressing objects: 100% (11/11), done.[K
remote: Total 14 (delta 7), reused 5 (delta 0), pack-reused 0[K
Unpacking objects:   7% (1/14)   Unpacking objects:  14% (2/14)   Unpacking objects:  21% (3/14)   Unpacking objects:  28% (4/14)   Unpacking objects:  35% (5/14)   Unpacking objects:  42% (6/14)   Unpacking objects:  50% (7/14)   Unpacking objects:  57% (8/14)   Unpacking objects:  64% (9/14)   Unpacking objects:  71% (10/14)   Unpacking objects:  78% (11/14)   Unpacking objects:  85% (12/14)   Unpacking objects:  92% (13/14)   Unpacking objects: 100% (14/14)   Unpacking objects: 100% (14/14), done.
From https://github.com/apache/storm
 * branch            refs/pull/1515/merge -> FETCH_HEAD

travis_time:end:01d1bd4c:start=1466779558394609756,finish=1466779558720726073,duration=326116317[0K$ git checkout -qf FETCH_HEAD
travis_fold:end:git.checkout[0K
[33;1mThis job is running on container-based infrastructure, which does not allow use of 'sudo', setuid and setguid executables.[0m
[33;1mIf you require sudo, add 'sudo: required' to your .travis.yml[0m
[33;1mSee https://docs.travis-ci.com/user/workers/container-based-infrastructure/ for details.[0m

[33;1mSetting environment variables from .travis.yml[0m
$ export MODULES='!storm-core'

$ jdk_switcher use oraclejdk8
Switching to Oracle JDK8 (java-8-oracle), JAVA_HOME will be set to /usr/lib/jvm/java-8-oracle
travis_fold:start:cache.1[0KSetting up build cache
$ export CASHER_DIR=$HOME/.casher
travis_time:start:240cd014[0K$ Installing caching utilities

travis_time:end:240cd014:start=1466779560494070206,finish=1466779560602162836,duration=108092630[0Ktravis_time:start:1e845938[0K
travis_time:end:1e845938:start=1466779560607374650,finish=1466779560611052449,duration=3677799[0Ktravis_time:start:314e5c34[0K[32;1mattempting to download cache archive[0m
[32;1mfetching PR.1515/cache-linux-precise-0b3559b5dae2b3e32156c6d840f23972d650aa066922e3318f7e5db8246681fa--jdk-oraclejdk8.tgz[0m
[32;1mfound cache[0m

travis_time:end:314e5c34:start=1466779560615329826,finish=1466779572506900792,duration=11891570966[0Ktravis_time:start:0121fb00[0K
travis_time:end:0121fb00:start=1466779572511204605,finish=1466779572515060803,duration=3856198[0Ktravis_time:start:1856b95c[0K[32;1madding /home/travis/.m2/repository to cache[0m
[32;1madding /home/travis/.rvm to cache[0m
[32;1madding /home/travis/.nvm to cache[0m

travis_time:end:1856b95c:start=1466779572519635323,finish=1466779583595017342,duration=11075382019[0Ktravis_fold:end:cache.1[0K$ java -Xmx32m -version
java version "1.8.0_31"
Java(TM) SE Runtime Environment (build 1.8.0_31-b13)
Java HotSpot(TM) 64-Bit Server VM (build 25.31-b07, mixed mode)
$ javac -J-Xmx32m -version
javac 1.8.0_31
travis_fold:start:before_install.1[0Ktravis_time:start:06dd21e8[0K$ rvm use 2.1.5 --install
[32mUsing /home/travis/.rvm/gems/ruby-2.1.5[0m

travis_time:end:06dd21e8:start=1466779583979299952,finish=1466779584228481872,duration=249181920[0Ktravis_fold:end:before_install.1[0Ktravis_fold:start:before_install.2[0Ktravis_time:start:0198b16f[0K$ nvm install 0.12.2
v0.12.2 is already installed.
Now using node v0.12.2

travis_time:end:0198b16f:start=1466779584232875469,finish=1466779584485819073,duration=252943604[0Ktravis_fold:end:before_install.2[0Ktravis_fold:start:before_install.3[0Ktravis_time:start:025be22e[0K$ nvm use 0.12.2
Now using node v0.12.2

travis_time:end:025be22e:start=1466779584490106009,finish=1466779584549791874,duration=59685865[0Ktravis_fold:end:before_install.3[0Ktravis_fold:start:install[0Ktravis_time:start:0dfe3680[0K$ /bin/bash ./dev-tools/travis/travis-install.sh `pwd`
Python version :   Python 2.7.3
Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=192m; support was removed in 8.0
Maven version  :   Apache Maven 3.2.5 (12a6b3acb947671f09b81f49094c53f426d8cea1; 2014-12-14T17:29:23+00:00) Maven home: /usr/local/maven Java version: 1.8.0_31, vendor: Oracle Corporation Java home: /usr/lib/jvm/java-8-oracle/jre Default locale: en_US, platform encoding: UTF-8 OS name: "linux", version: "3.13.0-40-generic", arch: "amd64", family: "unix"
['mvn', 'clean', 'install', '-DskipTests', '-Pnative', '--batch-mode'] writing to install.txt
1 seconds 2 log lines11 seconds 175 log lines22 seconds 246 log lines71 seconds 324 log lines85 seconds 331 log lines97 seconds 409 log lines122 seconds 653 log lines134 seconds 796 log lines147 seconds 866 log lines160 seconds 884 log lines174 seconds 963 log lines184 seconds 1111 log lines194 seconds 1315 log lines210 seconds 1466 log lines222 seconds 1654 log lines232 seconds 1817 log lines243 seconds 2172 log lines253 seconds 2220 log lines267 seconds 2366 log lines282 seconds 2668 log lines282 seconds 2669 log lines
['mvn', 'clean', 'install', '-DskipTests', '-Pnative', '--batch-mode'] done 0

travis_time:end:0dfe3680:start=1466779584554364370,finish=1466779867517332036,duration=282962967666[0Ktravis_fold:end:install[0Ktravis_time:start:2d4d2080[0K$ /bin/bash ./dev-tools/travis/travis-script.sh `pwd` $MODULES
Python version :   Python 2.7.3
Ruby version   :   ruby 2.1.5p273 (2014-11-13 revision 48405) [x86_64-linux]
NodeJs version :   v0.12.2
Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=192m; support was removed in 8.0
Maven version  :   Apache Maven 3.2.5 (12a6b3acb947671f09b81f49094c53f426d8cea1; 2014-12-14T17:29:23+00:00) Maven home: /usr/local/maven Java version: 1.8.0_31, vendor: Oracle Corporation Java home: /usr/lib/jvm/java-8-oracle/jre Default locale: en_US, platform encoding: UTF-8 OS name: "linux", version: "3.13.0-40-generic", arch: "amd64", family: "unix"
Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=192m; support was removed in 8.0
[INFO] Scanning for projects...
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Build Order:
[INFO] 
[INFO] Storm
[INFO] multilang-javascript
[INFO] multilang-python
[INFO] multilang-ruby
[INFO] maven-shade-clojure-transformer
[INFO] storm-maven-plugins
[INFO] storm-rename-hack
[INFO] storm-kafka
[INFO] storm-hdfs
[INFO] storm-hbase
[INFO] storm-hive
[INFO] storm-jdbc
[INFO] storm-redis
[INFO] storm-eventhubs
[INFO] flux
[INFO] flux-wrappers
[INFO] flux-core
[INFO] flux-examples
[INFO] storm-sql-runtime
[INFO] storm-sql-core
[INFO] storm-sql-kafka
[INFO] sql
[INFO] storm-elasticsearch
[INFO] storm-solr
[INFO] storm-metrics
[INFO] storm-cassandra
[INFO] storm-mqtt-parent
[INFO] storm-mqtt
[INFO] storm-mqtt-examples
[INFO] storm-mongodb
[INFO] storm-clojure
[INFO] storm-starter
[INFO] storm-kafka-client
[INFO] storm-opentsdb
[INFO] storm-kafka-monitor
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Storm 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ storm ---
[INFO] 
[INFO] --- apache-rat-plugin:0.11:check (default) @ storm ---
[INFO] 51 implicit excludes (use -debug for more details).
[INFO] Exclude: **/target/**
[INFO] Exclude: **/*.iml
[INFO] Exclude: **/.idea/**
[INFO] Exclude: **/metastore_db/**
[INFO] Exclude: **/build/**
[INFO] Exclude: **/logs/**
[INFO] Exclude: **/CHANGELOG.md
[INFO] Exclude: **/README.md
[INFO] Exclude: **/README.markdown
[INFO] Exclude: **/DEVELOPER.md
[INFO] Exclude: **/BYLAWS.md
[INFO] Exclude: **/STORM-UI-REST-API.md
[INFO] Exclude: SECURITY.md
[INFO] Exclude: VERSION
[INFO] Exclude: TODO
[INFO] Exclude: **/src/py/**
[INFO] Exclude: **/src/ui/public/js/jquery.dataTables.1.10.4.min.js
[INFO] Exclude: **/src/ui/public/css/jquery.dataTables.1.10.4.min.css
[INFO] Exclude: **/src/ui/public/images/*
[INFO] Exclude: **/src/ui/public/js/bootstrap-3.3.1.min.js
[INFO] Exclude: **/src/ui/public/css/bootstrap-3.3.1.min.css
[INFO] Exclude: **/src/ui/public/js/dataTables.bootstrap.min.js
[INFO] Exclude: **/src/ui/public/css/dataTables.bootstrap.css
[INFO] Exclude: **/src/ui/public/js/jsonFormatter.min.js
[INFO] Exclude: **/src/ui/public/css/jsonFormatter.min.css
[INFO] Exclude: **/src/ui/public/js/jquery-1.11.1.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.cookies.2.2.0.min.js
[INFO] Exclude: **/src/ui/public/js/moment.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.blockUI.min.js
[INFO] Exclude: **/src/ui/public/js/url.min.js
[INFO] Exclude: **/src/ui/public/js/arbor.js
[INFO] Exclude: **/src/ui/public/js/arbor-graphics.js
[INFO] Exclude: **/src/ui/public/js/arbor-tween.js
[INFO] Exclude: **/src/ui/public/js/jquery.mustache.js
[INFO] Exclude: **/src/ui/public/js/typeahead.jquery.min.js
[INFO] Exclude: **/dependency-reduced-pom.xml
[INFO] Exclude: **/docs/**
[INFO] Exclude: **/.git/**
[INFO] Exclude: **/derby.log
[INFO] Exclude: **/src/dev/**
[INFO] Exclude: **/src/codegen/config.fmpp
[INFO] Exclude: **/src/codegen/data/Parser.tdd
[INFO] Exclude: **/src/test/resources/FixedAvroSerializer.config
[INFO] 1856 resources included (use -debug for more details)
Warning:  org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
Compiler warnings:
  WARNING:  'org.apache.xerces.jaxp.SAXParserImpl: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.'
Warning:  org.apache.xerces.parsers.SAXParser: Feature 'http://javax.xml.XMLConstants/feature/secure-processing' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
[INFO] Rat check: Summary of files. Unapproved: 0 unknown: 0 generated: 0 approved: 1843 licence.
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building multilang-javascript 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ multilang-javascript ---
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ multilang-javascript ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 1 resource
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ multilang-javascript ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ multilang-javascript ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/storm-multilang/javascript/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ multilang-javascript ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-surefire-plugin:2.18.1:test (default-test) @ multilang-javascript ---
[INFO] 
[INFO] --- apache-rat-plugin:0.11:check (default) @ multilang-javascript ---
[INFO] 51 implicit excludes (use -debug for more details).
[INFO] Exclude: **/target/**
[INFO] Exclude: **/*.iml
[INFO] Exclude: **/.idea/**
[INFO] Exclude: **/metastore_db/**
[INFO] Exclude: **/build/**
[INFO] Exclude: **/logs/**
[INFO] Exclude: **/CHANGELOG.md
[INFO] Exclude: **/README.md
[INFO] Exclude: **/README.markdown
[INFO] Exclude: **/DEVELOPER.md
[INFO] Exclude: **/BYLAWS.md
[INFO] Exclude: **/STORM-UI-REST-API.md
[INFO] Exclude: SECURITY.md
[INFO] Exclude: VERSION
[INFO] Exclude: TODO
[INFO] Exclude: **/src/py/**
[INFO] Exclude: **/src/ui/public/js/jquery.dataTables.1.10.4.min.js
[INFO] Exclude: **/src/ui/public/css/jquery.dataTables.1.10.4.min.css
[INFO] Exclude: **/src/ui/public/images/*
[INFO] Exclude: **/src/ui/public/js/bootstrap-3.3.1.min.js
[INFO] Exclude: **/src/ui/public/css/bootstrap-3.3.1.min.css
[INFO] Exclude: **/src/ui/public/js/dataTables.bootstrap.min.js
[INFO] Exclude: **/src/ui/public/css/dataTables.bootstrap.css
[INFO] Exclude: **/src/ui/public/js/jsonFormatter.min.js
[INFO] Exclude: **/src/ui/public/css/jsonFormatter.min.css
[INFO] Exclude: **/src/ui/public/js/jquery-1.11.1.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.cookies.2.2.0.min.js
[INFO] Exclude: **/src/ui/public/js/moment.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.blockUI.min.js
[INFO] Exclude: **/src/ui/public/js/url.min.js
[INFO] Exclude: **/src/ui/public/js/arbor.js
[INFO] Exclude: **/src/ui/public/js/arbor-graphics.js
[INFO] Exclude: **/src/ui/public/js/arbor-tween.js
[INFO] Exclude: **/src/ui/public/js/jquery.mustache.js
[INFO] Exclude: **/src/ui/public/js/typeahead.jquery.min.js
[INFO] Exclude: **/dependency-reduced-pom.xml
[INFO] Exclude: **/docs/**
[INFO] Exclude: **/.git/**
[INFO] Exclude: **/derby.log
[INFO] Exclude: **/src/dev/**
[INFO] Exclude: **/src/codegen/config.fmpp
[INFO] Exclude: **/src/codegen/data/Parser.tdd
[INFO] Exclude: **/src/test/resources/FixedAvroSerializer.config
[INFO] 2 resources included (use -debug for more details)
Warning:  org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
Compiler warnings:
  WARNING:  'org.apache.xerces.jaxp.SAXParserImpl: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.'
Warning:  org.apache.xerces.parsers.SAXParser: Feature 'http://javax.xml.XMLConstants/feature/secure-processing' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
[INFO] Rat check: Summary of files. Unapproved: 0 unknown: 0 generated: 0 approved: 2 licence.
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building multilang-python 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ multilang-python ---
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ multilang-python ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 1 resource
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ multilang-python ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ multilang-python ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/storm-multilang/python/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ multilang-python ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-surefire-plugin:2.18.1:test (default-test) @ multilang-python ---
[INFO] 
[INFO] --- apache-rat-plugin:0.11:check (default) @ multilang-python ---
[INFO] 51 implicit excludes (use -debug for more details).
[INFO] Exclude: **/target/**
[INFO] Exclude: **/*.iml
[INFO] Exclude: **/.idea/**
[INFO] Exclude: **/metastore_db/**
[INFO] Exclude: **/build/**
[INFO] Exclude: **/logs/**
[INFO] Exclude: **/CHANGELOG.md
[INFO] Exclude: **/README.md
[INFO] Exclude: **/README.markdown
[INFO] Exclude: **/DEVELOPER.md
[INFO] Exclude: **/BYLAWS.md
[INFO] Exclude: **/STORM-UI-REST-API.md
[INFO] Exclude: SECURITY.md
[INFO] Exclude: VERSION
[INFO] Exclude: TODO
[INFO] Exclude: **/src/py/**
[INFO] Exclude: **/src/ui/public/js/jquery.dataTables.1.10.4.min.js
[INFO] Exclude: **/src/ui/public/css/jquery.dataTables.1.10.4.min.css
[INFO] Exclude: **/src/ui/public/images/*
[INFO] Exclude: **/src/ui/public/js/bootstrap-3.3.1.min.js
[INFO] Exclude: **/src/ui/public/css/bootstrap-3.3.1.min.css
[INFO] Exclude: **/src/ui/public/js/dataTables.bootstrap.min.js
[INFO] Exclude: **/src/ui/public/css/dataTables.bootstrap.css
[INFO] Exclude: **/src/ui/public/js/jsonFormatter.min.js
[INFO] Exclude: **/src/ui/public/css/jsonFormatter.min.css
[INFO] Exclude: **/src/ui/public/js/jquery-1.11.1.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.cookies.2.2.0.min.js
[INFO] Exclude: **/src/ui/public/js/moment.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.blockUI.min.js
[INFO] Exclude: **/src/ui/public/js/url.min.js
[INFO] Exclude: **/src/ui/public/js/arbor.js
[INFO] Exclude: **/src/ui/public/js/arbor-graphics.js
[INFO] Exclude: **/src/ui/public/js/arbor-tween.js
[INFO] Exclude: **/src/ui/public/js/jquery.mustache.js
[INFO] Exclude: **/src/ui/public/js/typeahead.jquery.min.js
[INFO] Exclude: **/dependency-reduced-pom.xml
[INFO] Exclude: **/docs/**
[INFO] Exclude: **/.git/**
[INFO] Exclude: **/derby.log
[INFO] Exclude: **/src/dev/**
[INFO] Exclude: **/src/codegen/config.fmpp
[INFO] Exclude: **/src/codegen/data/Parser.tdd
[INFO] Exclude: **/src/test/resources/FixedAvroSerializer.config
[INFO] 2 resources included (use -debug for more details)
Warning:  org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
Compiler warnings:
  WARNING:  'org.apache.xerces.jaxp.SAXParserImpl: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.'
Warning:  org.apache.xerces.parsers.SAXParser: Feature 'http://javax.xml.XMLConstants/feature/secure-processing' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
[INFO] Rat check: Summary of files. Unapproved: 0 unknown: 0 generated: 0 approved: 2 licence.
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building multilang-ruby 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ multilang-ruby ---
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ multilang-ruby ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 1 resource
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ multilang-ruby ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ multilang-ruby ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/storm-multilang/ruby/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ multilang-ruby ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-surefire-plugin:2.18.1:test (default-test) @ multilang-ruby ---
[INFO] 
[INFO] --- apache-rat-plugin:0.11:check (default) @ multilang-ruby ---
[INFO] 51 implicit excludes (use -debug for more details).
[INFO] Exclude: **/target/**
[INFO] Exclude: **/*.iml
[INFO] Exclude: **/.idea/**
[INFO] Exclude: **/metastore_db/**
[INFO] Exclude: **/build/**
[INFO] Exclude: **/logs/**
[INFO] Exclude: **/CHANGELOG.md
[INFO] Exclude: **/README.md
[INFO] Exclude: **/README.markdown
[INFO] Exclude: **/DEVELOPER.md
[INFO] Exclude: **/BYLAWS.md
[INFO] Exclude: **/STORM-UI-REST-API.md
[INFO] Exclude: SECURITY.md
[INFO] Exclude: VERSION
[INFO] Exclude: TODO
[INFO] Exclude: **/src/py/**
[INFO] Exclude: **/src/ui/public/js/jquery.dataTables.1.10.4.min.js
[INFO] Exclude: **/src/ui/public/css/jquery.dataTables.1.10.4.min.css
[INFO] Exclude: **/src/ui/public/images/*
[INFO] Exclude: **/src/ui/public/js/bootstrap-3.3.1.min.js
[INFO] Exclude: **/src/ui/public/css/bootstrap-3.3.1.min.css
[INFO] Exclude: **/src/ui/public/js/dataTables.bootstrap.min.js
[INFO] Exclude: **/src/ui/public/css/dataTables.bootstrap.css
[INFO] Exclude: **/src/ui/public/js/jsonFormatter.min.js
[INFO] Exclude: **/src/ui/public/css/jsonFormatter.min.css
[INFO] Exclude: **/src/ui/public/js/jquery-1.11.1.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.cookies.2.2.0.min.js
[INFO] Exclude: **/src/ui/public/js/moment.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.blockUI.min.js
[INFO] Exclude: **/src/ui/public/js/url.min.js
[INFO] Exclude: **/src/ui/public/js/arbor.js
[INFO] Exclude: **/src/ui/public/js/arbor-graphics.js
[INFO] Exclude: **/src/ui/public/js/arbor-tween.js
[INFO] Exclude: **/src/ui/public/js/jquery.mustache.js
[INFO] Exclude: **/src/ui/public/js/typeahead.jquery.min.js
[INFO] Exclude: **/dependency-reduced-pom.xml
[INFO] Exclude: **/docs/**
[INFO] Exclude: **/.git/**
[INFO] Exclude: **/derby.log
[INFO] Exclude: **/src/dev/**
[INFO] Exclude: **/src/codegen/config.fmpp
[INFO] Exclude: **/src/codegen/data/Parser.tdd
[INFO] Exclude: **/src/test/resources/FixedAvroSerializer.config
[INFO] 2 resources included (use -debug for more details)
Warning:  org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
Compiler warnings:
  WARNING:  'org.apache.xerces.jaxp.SAXParserImpl: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.'
Warning:  org.apache.xerces.parsers.SAXParser: Feature 'http://javax.xml.XMLConstants/feature/secure-processing' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
[INFO] Rat check: Summary of files. Unapproved: 0 unknown: 0 generated: 0 approved: 2 licence.
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building maven-shade-clojure-transformer 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ maven-shade-clojure-transformer ---
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ maven-shade-clojure-transformer ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/storm-buildtools/maven-shade-clojure-transformer/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ maven-shade-clojure-transformer ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ maven-shade-clojure-transformer ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/storm-buildtools/maven-shade-clojure-transformer/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ maven-shade-clojure-transformer ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-surefire-plugin:2.18.1:test (default-test) @ maven-shade-clojure-transformer ---
[INFO] 
[INFO] --- apache-rat-plugin:0.11:check (default) @ maven-shade-clojure-transformer ---
[INFO] 51 implicit excludes (use -debug for more details).
[INFO] Exclude: **/target/**
[INFO] Exclude: **/*.iml
[INFO] Exclude: **/.idea/**
[INFO] Exclude: **/metastore_db/**
[INFO] Exclude: **/build/**
[INFO] Exclude: **/logs/**
[INFO] Exclude: **/CHANGELOG.md
[INFO] Exclude: **/README.md
[INFO] Exclude: **/README.markdown
[INFO] Exclude: **/DEVELOPER.md
[INFO] Exclude: **/BYLAWS.md
[INFO] Exclude: **/STORM-UI-REST-API.md
[INFO] Exclude: SECURITY.md
[INFO] Exclude: VERSION
[INFO] Exclude: TODO
[INFO] Exclude: **/src/py/**
[INFO] Exclude: **/src/ui/public/js/jquery.dataTables.1.10.4.min.js
[INFO] Exclude: **/src/ui/public/css/jquery.dataTables.1.10.4.min.css
[INFO] Exclude: **/src/ui/public/images/*
[INFO] Exclude: **/src/ui/public/js/bootstrap-3.3.1.min.js
[INFO] Exclude: **/src/ui/public/css/bootstrap-3.3.1.min.css
[INFO] Exclude: **/src/ui/public/js/dataTables.bootstrap.min.js
[INFO] Exclude: **/src/ui/public/css/dataTables.bootstrap.css
[INFO] Exclude: **/src/ui/public/js/jsonFormatter.min.js
[INFO] Exclude: **/src/ui/public/css/jsonFormatter.min.css
[INFO] Exclude: **/src/ui/public/js/jquery-1.11.1.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.cookies.2.2.0.min.js
[INFO] Exclude: **/src/ui/public/js/moment.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.blockUI.min.js
[INFO] Exclude: **/src/ui/public/js/url.min.js
[INFO] Exclude: **/src/ui/public/js/arbor.js
[INFO] Exclude: **/src/ui/public/js/arbor-graphics.js
[INFO] Exclude: **/src/ui/public/js/arbor-tween.js
[INFO] Exclude: **/src/ui/public/js/jquery.mustache.js
[INFO] Exclude: **/src/ui/public/js/typeahead.jquery.min.js
[INFO] Exclude: **/dependency-reduced-pom.xml
[INFO] Exclude: **/docs/**
[INFO] Exclude: **/.git/**
[INFO] Exclude: **/derby.log
[INFO] Exclude: **/src/dev/**
[INFO] Exclude: **/src/codegen/config.fmpp
[INFO] Exclude: **/src/codegen/data/Parser.tdd
[INFO] Exclude: **/src/test/resources/FixedAvroSerializer.config
[INFO] 2 resources included (use -debug for more details)
Warning:  org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
Compiler warnings:
  WARNING:  'org.apache.xerces.jaxp.SAXParserImpl: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.'
Warning:  org.apache.xerces.parsers.SAXParser: Feature 'http://javax.xml.XMLConstants/feature/secure-processing' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
[INFO] Rat check: Summary of files. Unapproved: 0 unknown: 0 generated: 0 approved: 2 licence.
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building storm-maven-plugins 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ storm-maven-plugins ---
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ storm-maven-plugins ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/storm-buildtools/storm-maven-plugins/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ storm-maven-plugins ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-plugin-plugin:3.0:descriptor (default-descriptor) @ storm-maven-plugins ---
[INFO] Using 'UTF-8' encoding to read mojo metadata.
[INFO] Applying mojo extractor for language: java
[INFO] Mojo extractor for language: java found 0 mojo descriptors.
[INFO] Applying mojo extractor for language: bsh
[INFO] Mojo extractor for language: bsh found 0 mojo descriptors.
[INFO] Applying mojo extractor for language: java-annotations
[INFO] Mojo extractor for language: java-annotations found 1 mojo descriptors.
[INFO] 
[INFO] --- maven-plugin-plugin:3.0:descriptor (mojo-descriptor) @ storm-maven-plugins ---
[INFO] Using 'UTF-8' encoding to read mojo metadata.
[INFO] Applying mojo extractor for language: java
[INFO] Mojo extractor for language: java found 0 mojo descriptors.
[INFO] Applying mojo extractor for language: bsh
[INFO] Mojo extractor for language: bsh found 0 mojo descriptors.
[INFO] Applying mojo extractor for language: java-annotations
[INFO] Mojo extractor for language: java-annotations found 1 mojo descriptors.
[INFO] 
[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ storm-maven-plugins ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/storm-buildtools/storm-maven-plugins/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ storm-maven-plugins ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-surefire-plugin:2.18.1:test (default-test) @ storm-maven-plugins ---
[INFO] 
[INFO] --- apache-rat-plugin:0.11:check (default) @ storm-maven-plugins ---
[INFO] 51 implicit excludes (use -debug for more details).
[INFO] Exclude: **/target/**
[INFO] Exclude: **/*.iml
[INFO] Exclude: **/.idea/**
[INFO] Exclude: **/metastore_db/**
[INFO] Exclude: **/build/**
[INFO] Exclude: **/logs/**
[INFO] Exclude: **/CHANGELOG.md
[INFO] Exclude: **/README.md
[INFO] Exclude: **/README.markdown
[INFO] Exclude: **/DEVELOPER.md
[INFO] Exclude: **/BYLAWS.md
[INFO] Exclude: **/STORM-UI-REST-API.md
[INFO] Exclude: SECURITY.md
[INFO] Exclude: VERSION
[INFO] Exclude: TODO
[INFO] Exclude: **/src/py/**
[INFO] Exclude: **/src/ui/public/js/jquery.dataTables.1.10.4.min.js
[INFO] Exclude: **/src/ui/public/css/jquery.dataTables.1.10.4.min.css
[INFO] Exclude: **/src/ui/public/images/*
[INFO] Exclude: **/src/ui/public/js/bootstrap-3.3.1.min.js
[INFO] Exclude: **/src/ui/public/css/bootstrap-3.3.1.min.css
[INFO] Exclude: **/src/ui/public/js/dataTables.bootstrap.min.js
[INFO] Exclude: **/src/ui/public/css/dataTables.bootstrap.css
[INFO] Exclude: **/src/ui/public/js/jsonFormatter.min.js
[INFO] Exclude: **/src/ui/public/css/jsonFormatter.min.css
[INFO] Exclude: **/src/ui/public/js/jquery-1.11.1.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.cookies.2.2.0.min.js
[INFO] Exclude: **/src/ui/public/js/moment.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.blockUI.min.js
[INFO] Exclude: **/src/ui/public/js/url.min.js
[INFO] Exclude: **/src/ui/public/js/arbor.js
[INFO] Exclude: **/src/ui/public/js/arbor-graphics.js
[INFO] Exclude: **/src/ui/public/js/arbor-tween.js
[INFO] Exclude: **/src/ui/public/js/jquery.mustache.js
[INFO] Exclude: **/src/ui/public/js/typeahead.jquery.min.js
[INFO] Exclude: **/dependency-reduced-pom.xml
[INFO] Exclude: **/docs/**
[INFO] Exclude: **/.git/**
[INFO] Exclude: **/derby.log
[INFO] Exclude: **/src/dev/**
[INFO] Exclude: **/src/codegen/config.fmpp
[INFO] Exclude: **/src/codegen/data/Parser.tdd
[INFO] Exclude: **/src/test/resources/FixedAvroSerializer.config
[INFO] 3 resources included (use -debug for more details)
Warning:  org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
Compiler warnings:
  WARNING:  'org.apache.xerces.jaxp.SAXParserImpl: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.'
Warning:  org.apache.xerces.parsers.SAXParser: Feature 'http://javax.xml.XMLConstants/feature/secure-processing' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
[INFO] Rat check: Summary of files. Unapproved: 0 unknown: 0 generated: 0 approved: 3 licence.
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building storm-rename-hack 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] Downloading: https://repository.apache.org/snapshots/org/apache/storm/storm-core/2.0.0-SNAPSHOT/maven-metadata.xml
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ storm-rename-hack ---
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ storm-rename-hack ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/storm-rename-hack/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ storm-rename-hack ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ storm-rename-hack ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/storm-rename-hack/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ storm-rename-hack ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-surefire-plugin:2.18.1:test (default-test) @ storm-rename-hack ---
[INFO] 
[INFO] --- apache-rat-plugin:0.11:check (default) @ storm-rename-hack ---
[INFO] 51 implicit excludes (use -debug for more details).
[INFO] Exclude: **/target/**
[INFO] Exclude: **/*.iml
[INFO] Exclude: **/.idea/**
[INFO] Exclude: **/metastore_db/**
[INFO] Exclude: **/build/**
[INFO] Exclude: **/logs/**
[INFO] Exclude: **/CHANGELOG.md
[INFO] Exclude: **/README.md
[INFO] Exclude: **/README.markdown
[INFO] Exclude: **/DEVELOPER.md
[INFO] Exclude: **/BYLAWS.md
[INFO] Exclude: **/STORM-UI-REST-API.md
[INFO] Exclude: SECURITY.md
[INFO] Exclude: VERSION
[INFO] Exclude: TODO
[INFO] Exclude: **/src/py/**
[INFO] Exclude: **/src/ui/public/js/jquery.dataTables.1.10.4.min.js
[INFO] Exclude: **/src/ui/public/css/jquery.dataTables.1.10.4.min.css
[INFO] Exclude: **/src/ui/public/images/*
[INFO] Exclude: **/src/ui/public/js/bootstrap-3.3.1.min.js
[INFO] Exclude: **/src/ui/public/css/bootstrap-3.3.1.min.css
[INFO] Exclude: **/src/ui/public/js/dataTables.bootstrap.min.js
[INFO] Exclude: **/src/ui/public/css/dataTables.bootstrap.css
[INFO] Exclude: **/src/ui/public/js/jsonFormatter.min.js
[INFO] Exclude: **/src/ui/public/css/jsonFormatter.min.css
[INFO] Exclude: **/src/ui/public/js/jquery-1.11.1.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.cookies.2.2.0.min.js
[INFO] Exclude: **/src/ui/public/js/moment.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.blockUI.min.js
[INFO] Exclude: **/src/ui/public/js/url.min.js
[INFO] Exclude: **/src/ui/public/js/arbor.js
[INFO] Exclude: **/src/ui/public/js/arbor-graphics.js
[INFO] Exclude: **/src/ui/public/js/arbor-tween.js
[INFO] Exclude: **/src/ui/public/js/jquery.mustache.js
[INFO] Exclude: **/src/ui/public/js/typeahead.jquery.min.js
[INFO] Exclude: **/dependency-reduced-pom.xml
[INFO] Exclude: **/docs/**
[INFO] Exclude: **/.git/**
[INFO] Exclude: **/derby.log
[INFO] Exclude: **/src/dev/**
[INFO] Exclude: **/src/codegen/config.fmpp
[INFO] Exclude: **/src/codegen/data/Parser.tdd
[INFO] Exclude: **/src/test/resources/FixedAvroSerializer.config
[INFO] 10 resources included (use -debug for more details)
Warning:  org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
Compiler warnings:
  WARNING:  'org.apache.xerces.jaxp.SAXParserImpl: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.'
Warning:  org.apache.xerces.parsers.SAXParser: Feature 'http://javax.xml.XMLConstants/feature/secure-processing' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
[INFO] Rat check: Summary of files. Unapproved: 0 unknown: 0 generated: 0 approved: 10 licence.
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building storm-kafka 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.2.201409121644:prepare-agent (jacoco-initialize) @ storm-kafka ---
[INFO] argLine set to -javaagent:/home/travis/.m2/repository/org/jacoco/org.jacoco.agent/0.7.2.201409121644/org.jacoco.agent-0.7.2.201409121644-runtime.jar=destfile=/home/travis/build/apache/storm/external/storm-kafka/target/jacoco.exec
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ storm-kafka ---
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ storm-kafka ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/storm-kafka/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ storm-kafka ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ storm-kafka ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/storm-kafka/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ storm-kafka ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:2.18.1:test (default-test) @ storm-kafka ---
[INFO] Surefire report directory: /home/travis/build/apache/storm/external/storm-kafka/target/surefire-reports
[INFO] parallel='none', perCoreThreadCount=true, threadCount=0, useUnlimitedThreads=false, threadCountSuites=0, threadCountClasses=0, threadCountMethods=0, parallelOptimized=true

-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running org.apache.storm.kafka.TestStringScheme
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.842 sec - in org.apache.storm.kafka.TestStringScheme
Running org.apache.storm.kafka.StringKeyValueSchemeTest
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.065 sec - in org.apache.storm.kafka.StringKeyValueSchemeTest
Running org.apache.storm.kafka.DynamicBrokersReaderTest
Tests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 7.365 sec - in org.apache.storm.kafka.DynamicBrokersReaderTest
Running org.apache.storm.kafka.KafkaUtilsTest
Tests run: 18, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 37.524 sec - in org.apache.storm.kafka.KafkaUtilsTest
Running org.apache.storm.kafka.KafkaErrorTest
Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.004 sec - in org.apache.storm.kafka.KafkaErrorTest
Running org.apache.storm.kafka.bolt.KafkaBoltTest
Tests run: 8, Failures: 0, Errors: 7, Skipped: 0, Time elapsed: 11.962 sec <<< FAILURE! - in org.apache.storm.kafka.bolt.KafkaBoltTest
executeWithBrokerDown(org.apache.storm.kafka.bolt.KafkaBoltTest)  Time elapsed: 1.545 sec  <<< ERROR!
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.kafka.bolt.KafkaBoltTest.generateTestTuple(KafkaBoltTest.java:301)
	at org.apache.storm.kafka.bolt.KafkaBoltTest.executeWithBrokerDown(KafkaBoltTest.java:266)

executeWithoutKey(org.apache.storm.kafka.bolt.KafkaBoltTest)  Time elapsed: 1.45 sec  <<< ERROR!
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.kafka.bolt.KafkaBoltTest.generateTestTuple(KafkaBoltTest.java:301)
	at org.apache.storm.kafka.bolt.KafkaBoltTest.executeWithoutKey(KafkaBoltTest.java:255)

executeWithByteArrayKeyAndMessageFire(org.apache.storm.kafka.bolt.KafkaBoltTest)  Time elapsed: 1.497 sec  <<< ERROR!
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.kafka.bolt.KafkaBoltTest.generateTestTuple(KafkaBoltTest.java:290)
	at org.apache.storm.kafka.bolt.KafkaBoltTest.executeWithByteArrayKeyAndMessageFire(KafkaBoltTest.java:176)

executeWithByteArrayKeyAndMessageSync(org.apache.storm.kafka.bolt.KafkaBoltTest)  Time elapsed: 1.589 sec  <<< ERROR!
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.kafka.bolt.KafkaBoltTest.generateTestTuple(KafkaBoltTest.java:290)
	at org.apache.storm.kafka.bolt.KafkaBoltTest.executeWithByteArrayKeyAndMessageSync(KafkaBoltTest.java:131)

executeWithByteArrayKeyAndMessageAsync(org.apache.storm.kafka.bolt.KafkaBoltTest)  Time elapsed: 1.441 sec  <<< ERROR!
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.kafka.bolt.KafkaBoltTest.generateTestTuple(KafkaBoltTest.java:290)
	at org.apache.storm.kafka.bolt.KafkaBoltTest.executeWithByteArrayKeyAndMessageAsync(KafkaBoltTest.java:146)

executeWithKey(org.apache.storm.kafka.bolt.KafkaBoltTest)  Time elapsed: 1.475 sec  <<< ERROR!
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.kafka.bolt.KafkaBoltTest.generateTestTuple(KafkaBoltTest.java:290)
	at org.apache.storm.kafka.bolt.KafkaBoltTest.executeWithKey(KafkaBoltTest.java:115)

executeWithBoltSpecifiedProperties(org.apache.storm.kafka.bolt.KafkaBoltTest)  Time elapsed: 1.454 sec  <<< ERROR!
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.kafka.bolt.KafkaBoltTest.generateTestTuple(KafkaBoltTest.java:290)
	at org.apache.storm.kafka.bolt.KafkaBoltTest.executeWithBoltSpecifiedProperties(KafkaBoltTest.java:199)

Running org.apache.storm.kafka.ExponentialBackoffMsgRetryManagerTest
Tests run: 12, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 2.734 sec - in org.apache.storm.kafka.ExponentialBackoffMsgRetryManagerTest
Running org.apache.storm.kafka.ZkCoordinatorTest
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 5.521 sec - in org.apache.storm.kafka.ZkCoordinatorTest
Running org.apache.storm.kafka.TridentKafkaTest
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.644 sec - in org.apache.storm.kafka.TridentKafkaTest

Results :

Tests in error: 
  KafkaBoltTest.executeWithBoltSpecifiedProperties:199->generateTestTuple:290 » IllegalArgument
  KafkaBoltTest.executeWithBrokerDown:266->generateTestTuple:301 » IllegalArgument
  KafkaBoltTest.executeWithByteArrayKeyAndMessageAsync:146->generateTestTuple:290 » IllegalArgument
  KafkaBoltTest.executeWithByteArrayKeyAndMessageFire:176->generateTestTuple:290 » IllegalArgument
  KafkaBoltTest.executeWithByteArrayKeyAndMessageSync:131->generateTestTuple:290 » IllegalArgument
  KafkaBoltTest.executeWithKey:115->generateTestTuple:290 » IllegalArgument Spou...
  KafkaBoltTest.executeWithoutKey:255->generateTestTuple:301 » IllegalArgument S...

Tests run: 57, Failures: 0, Errors: 7, Skipped: 0

[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building storm-hdfs 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ storm-hdfs ---
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ storm-hdfs ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/storm-hdfs/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:2.5.1:compile (default-compile) @ storm-hdfs ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ storm-hdfs ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 2 resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:2.5.1:testCompile (default-testCompile) @ storm-hdfs ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:2.18.1:test (default-test) @ storm-hdfs ---
[INFO] Surefire report directory: /home/travis/build/apache/storm/external/storm-hdfs/target/surefire-reports
[INFO] parallel='none', perCoreThreadCount=true, threadCount=0, useUnlimitedThreads=false, threadCountSuites=0, threadCountClasses=0, threadCountMethods=0, parallelOptimized=true

-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running org.apache.storm.hdfs.trident.HdfsStateTest
Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 3.197 sec - in org.apache.storm.hdfs.trident.HdfsStateTest
Running org.apache.storm.hdfs.trident.format.TestSimpleFileNameFormat
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.323 sec - in org.apache.storm.hdfs.trident.format.TestSimpleFileNameFormat
Running org.apache.storm.hdfs.bolt.TestWritersMap
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.005 sec - in org.apache.storm.hdfs.bolt.TestWritersMap
Running org.apache.storm.hdfs.bolt.TestHdfsBolt
Tests run: 6, Failures: 0, Errors: 6, Skipped: 0, Time elapsed: 0.431 sec <<< FAILURE! - in org.apache.storm.hdfs.bolt.TestHdfsBolt
testTwoTuplesTwoFiles(org.apache.storm.hdfs.bolt.TestHdfsBolt)  Time elapsed: 0.012 sec  <<< ERROR!
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hdfs.bolt.TestHdfsBolt.generateTestTuple(TestHdfsBolt.java:243)
	at org.apache.storm.hdfs.bolt.TestHdfsBolt.<init>(TestHdfsBolt.java:69)

testPartitionedOutput(org.apache.storm.hdfs.bolt.TestHdfsBolt)  Time elapsed: 0 sec  <<< ERROR!
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hdfs.bolt.TestHdfsBolt.generateTestTuple(TestHdfsBolt.java:243)
	at org.apache.storm.hdfs.bolt.TestHdfsBolt.<init>(TestHdfsBolt.java:69)

testTwoTuplesOneFile(org.apache.storm.hdfs.bolt.TestHdfsBolt)  Time elapsed: 0.001 sec  <<< ERROR!
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hdfs.bolt.TestHdfsBolt.generateTestTuple(TestHdfsBolt.java:243)
	at org.apache.storm.hdfs.bolt.TestHdfsBolt.<init>(TestHdfsBolt.java:69)

testFailedSync(org.apache.storm.hdfs.bolt.TestHdfsBolt)  Time elapsed: 0.001 sec  <<< ERROR!
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hdfs.bolt.TestHdfsBolt.generateTestTuple(TestHdfsBolt.java:243)
	at org.apache.storm.hdfs.bolt.TestHdfsBolt.<init>(TestHdfsBolt.java:69)

testTickTuples(org.apache.storm.hdfs.bolt.TestHdfsBolt)  Time elapsed: 0.001 sec  <<< ERROR!
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hdfs.bolt.TestHdfsBolt.generateTestTuple(TestHdfsBolt.java:243)
	at org.apache.storm.hdfs.bolt.TestHdfsBolt.<init>(TestHdfsBolt.java:69)

testFailureFilecount(org.apache.storm.hdfs.bolt.TestHdfsBolt)  Time elapsed: 0.001 sec  <<< ERROR!
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hdfs.bolt.TestHdfsBolt.generateTestTuple(TestHdfsBolt.java:243)
	at org.apache.storm.hdfs.bolt.TestHdfsBolt.<init>(TestHdfsBolt.java:69)

Running org.apache.storm.hdfs.bolt.AvroGenericRecordBoltTest
Tests run: 5, Failures: 0, Errors: 5, Skipped: 0, Time elapsed: 0.025 sec <<< FAILURE! - in org.apache.storm.hdfs.bolt.AvroGenericRecordBoltTest
schemaThrashing(org.apache.storm.hdfs.bolt.AvroGenericRecordBoltTest)  Time elapsed: 0 sec  <<< ERROR!
java.lang.ExceptionInInitializerError: null
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hdfs.bolt.AvroGenericRecordBoltTest.generateTestTuple(AvroGenericRecordBoltTest.java:220)
	at org.apache.storm.hdfs.bolt.AvroGenericRecordBoltTest.<clinit>(AvroGenericRecordBoltTest.java:95)

forwardSchemaChangeWorks(org.apache.storm.hdfs.bolt.AvroGenericRecordBoltTest)  Time elapsed: 0 sec  <<< ERROR!
java.lang.NoClassDefFoundError: Could not initialize class org.apache.storm.hdfs.bolt.AvroGenericRecordBoltTest
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:408)
	at org.junit.runners.BlockJUnit4ClassRunner.createTest(BlockJUnit4ClassRunner.java:195)
	at org.junit.runners.BlockJUnit4ClassRunner$1.runReflectiveCall(BlockJUnit4ClassRunner.java:244)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.BlockJUnit4ClassRunner.methodBlock(BlockJUnit4ClassRunner.java:241)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:108)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:78)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:54)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:144)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:203)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:155)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:103)

multipleTuplesOneFile(org.apache.storm.hdfs.bolt.AvroGenericRecordBoltTest)  Time elapsed: 0 sec  <<< ERROR!
java.lang.NoClassDefFoundError: Could not initialize class org.apache.storm.hdfs.bolt.AvroGenericRecordBoltTest
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:408)
	at org.junit.runners.BlockJUnit4ClassRunner.createTest(BlockJUnit4ClassRunner.java:195)
	at org.junit.runners.BlockJUnit4ClassRunner$1.runReflectiveCall(BlockJUnit4ClassRunner.java:244)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.BlockJUnit4ClassRunner.methodBlock(BlockJUnit4ClassRunner.java:241)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:108)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:78)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:54)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:144)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:203)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:155)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:103)

multipleTuplesMutliplesFiles(org.apache.storm.hdfs.bolt.AvroGenericRecordBoltTest)  Time elapsed: 0 sec  <<< ERROR!
java.lang.NoClassDefFoundError: Could not initialize class org.apache.storm.hdfs.bolt.AvroGenericRecordBoltTest
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:408)
	at org.junit.runners.BlockJUnit4ClassRunner.createTest(BlockJUnit4ClassRunner.java:195)
	at org.junit.runners.BlockJUnit4ClassRunner$1.runReflectiveCall(BlockJUnit4ClassRunner.java:244)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.BlockJUnit4ClassRunner.methodBlock(BlockJUnit4ClassRunner.java:241)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:108)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:78)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:54)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:144)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:203)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:155)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:103)

backwardSchemaChangeWorks(org.apache.storm.hdfs.bolt.AvroGenericRecordBoltTest)  Time elapsed: 0.024 sec  <<< ERROR!
java.lang.NoClassDefFoundError: Could not initialize class org.apache.storm.hdfs.bolt.AvroGenericRecordBoltTest
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:408)
	at org.junit.runners.BlockJUnit4ClassRunner.createTest(BlockJUnit4ClassRunner.java:195)
	at org.junit.runners.BlockJUnit4ClassRunner$1.runReflectiveCall(BlockJUnit4ClassRunner.java:244)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.BlockJUnit4ClassRunner.methodBlock(BlockJUnit4ClassRunner.java:241)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:108)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:78)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:54)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:144)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:203)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:155)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:103)

Running org.apache.storm.hdfs.bolt.format.TestSimpleFileNameFormat
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.023 sec - in org.apache.storm.hdfs.bolt.format.TestSimpleFileNameFormat
Running org.apache.storm.hdfs.bolt.TestSequenceFileBolt
Tests run: 3, Failures: 0, Errors: 3, Skipped: 0, Time elapsed: 7.394 sec <<< FAILURE! - in org.apache.storm.hdfs.bolt.TestSequenceFileBolt
testTwoTuplesTwoFiles(org.apache.storm.hdfs.bolt.TestSequenceFileBolt)  Time elapsed: 0 sec  <<< ERROR!
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hdfs.bolt.TestSequenceFileBolt.generateTestTuple(TestSequenceFileBolt.java:161)
	at org.apache.storm.hdfs.bolt.TestSequenceFileBolt.<init>(TestSequenceFileBolt.java:68)

testTwoTuplesOneFile(org.apache.storm.hdfs.bolt.TestSequenceFileBolt)  Time elapsed: 0 sec  <<< ERROR!
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hdfs.bolt.TestSequenceFileBolt.generateTestTuple(TestSequenceFileBolt.java:161)
	at org.apache.storm.hdfs.bolt.TestSequenceFileBolt.<init>(TestSequenceFileBolt.java:68)

testFailedSync(org.apache.storm.hdfs.bolt.TestSequenceFileBolt)  Time elapsed: 0 sec  <<< ERROR!
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hdfs.bolt.TestSequenceFileBolt.generateTestTuple(TestSequenceFileBolt.java:161)
	at org.apache.storm.hdfs.bolt.TestSequenceFileBolt.<init>(TestSequenceFileBolt.java:68)

Running org.apache.storm.hdfs.spout.TestDirLock
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 20.155 sec - in org.apache.storm.hdfs.spout.TestDirLock
Running org.apache.storm.hdfs.spout.TestHdfsSemantics
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.507 sec - in org.apache.storm.hdfs.spout.TestHdfsSemantics
Running org.apache.storm.hdfs.spout.TestProgressTracker
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.957 sec - in org.apache.storm.hdfs.spout.TestProgressTracker
Running org.apache.storm.hdfs.spout.TestFileLock
Tests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 33.159 sec - in org.apache.storm.hdfs.spout.TestFileLock
Running org.apache.storm.hdfs.spout.TestHdfsSpout
Tests run: 10, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 10.959 sec - in org.apache.storm.hdfs.spout.TestHdfsSpout
Running org.apache.storm.hdfs.blobstore.HdfsBlobStoreImplTest
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.791 sec - in org.apache.storm.hdfs.blobstore.HdfsBlobStoreImplTest
Running org.apache.storm.hdfs.blobstore.BlobStoreTest
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 13.004 sec - in org.apache.storm.hdfs.blobstore.BlobStoreTest
Running org.apache.storm.hdfs.avro.TestFixedAvroSerializer
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.001 sec - in org.apache.storm.hdfs.avro.TestFixedAvroSerializer
Running org.apache.storm.hdfs.avro.TestGenericAvroSerializer
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.01 sec - in org.apache.storm.hdfs.avro.TestGenericAvroSerializer

Results :

Tests in error: 
  AvroGenericRecordBoltTest.backwardSchemaChangeWorks » NoClassDefFound Could no...
  AvroGenericRecordBoltTest.forwardSchemaChangeWorks » NoClassDefFound Could not...
  AvroGenericRecordBoltTest.multipleTuplesMutliplesFiles » NoClassDefFound Could...
  AvroGenericRecordBoltTest.multipleTuplesOneFile » NoClassDefFound Could not in...
  AvroGenericRecordBoltTest.schemaThrashing » ExceptionInInitializer
  TestHdfsBolt.<init>:69->generateTestTuple:243 » IllegalArgument Spouts is not ...
  TestHdfsBolt.<init>:69->generateTestTuple:243 » IllegalArgument Spouts is not ...
  TestHdfsBolt.<init>:69->generateTestTuple:243 » IllegalArgument Spouts is not ...
  TestHdfsBolt.<init>:69->generateTestTuple:243 » IllegalArgument Spouts is not ...
  TestHdfsBolt.<init>:69->generateTestTuple:243 » IllegalArgument Spouts is not ...
  TestHdfsBolt.<init>:69->generateTestTuple:243 » IllegalArgument Spouts is not ...
  TestSequenceFileBolt.<init>:68->generateTestTuple:161 » IllegalArgument Spouts...
  TestSequenceFileBolt.<init>:68->generateTestTuple:161 » IllegalArgument Spouts...
  TestSequenceFileBolt.<init>:68->generateTestTuple:161 » IllegalArgument Spouts...

Tests run: 60, Failures: 0, Errors: 14, Skipped: 0

[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building storm-hbase 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ storm-hbase ---
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ storm-hbase ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/storm-hbase/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ storm-hbase ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ storm-hbase ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/storm-hbase/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ storm-hbase ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:2.18.1:test (default-test) @ storm-hbase ---
[INFO] 
[INFO] --- apache-rat-plugin:0.11:check (default) @ storm-hbase ---
[INFO] 51 implicit excludes (use -debug for more details).
[INFO] Exclude: **/target/**
[INFO] Exclude: **/*.iml
[INFO] Exclude: **/.idea/**
[INFO] Exclude: **/metastore_db/**
[INFO] Exclude: **/build/**
[INFO] Exclude: **/logs/**
[INFO] Exclude: **/CHANGELOG.md
[INFO] Exclude: **/README.md
[INFO] Exclude: **/README.markdown
[INFO] Exclude: **/DEVELOPER.md
[INFO] Exclude: **/BYLAWS.md
[INFO] Exclude: **/STORM-UI-REST-API.md
[INFO] Exclude: SECURITY.md
[INFO] Exclude: VERSION
[INFO] Exclude: TODO
[INFO] Exclude: **/src/py/**
[INFO] Exclude: **/src/ui/public/js/jquery.dataTables.1.10.4.min.js
[INFO] Exclude: **/src/ui/public/css/jquery.dataTables.1.10.4.min.css
[INFO] Exclude: **/src/ui/public/images/*
[INFO] Exclude: **/src/ui/public/js/bootstrap-3.3.1.min.js
[INFO] Exclude: **/src/ui/public/css/bootstrap-3.3.1.min.css
[INFO] Exclude: **/src/ui/public/js/dataTables.bootstrap.min.js
[INFO] Exclude: **/src/ui/public/css/dataTables.bootstrap.css
[INFO] Exclude: **/src/ui/public/js/jsonFormatter.min.js
[INFO] Exclude: **/src/ui/public/css/jsonFormatter.min.css
[INFO] Exclude: **/src/ui/public/js/jquery-1.11.1.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.cookies.2.2.0.min.js
[INFO] Exclude: **/src/ui/public/js/moment.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.blockUI.min.js
[INFO] Exclude: **/src/ui/public/js/url.min.js
[INFO] Exclude: **/src/ui/public/js/arbor.js
[INFO] Exclude: **/src/ui/public/js/arbor-graphics.js
[INFO] Exclude: **/src/ui/public/js/arbor-tween.js
[INFO] Exclude: **/src/ui/public/js/jquery.mustache.js
[INFO] Exclude: **/src/ui/public/js/typeahead.jquery.min.js
[INFO] Exclude: **/dependency-reduced-pom.xml
[INFO] Exclude: **/docs/**
[INFO] Exclude: **/.git/**
[INFO] Exclude: **/derby.log
[INFO] Exclude: **/src/dev/**
[INFO] Exclude: **/src/codegen/config.fmpp
[INFO] Exclude: **/src/codegen/data/Parser.tdd
[INFO] Exclude: **/src/test/resources/FixedAvroSerializer.config
[INFO] 35 resources included (use -debug for more details)
Warning:  org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
Compiler warnings:
  WARNING:  'org.apache.xerces.jaxp.SAXParserImpl: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.'
Warning:  org.apache.xerces.parsers.SAXParser: Feature 'http://javax.xml.XMLConstants/feature/secure-processing' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
[INFO] Rat check: Summary of files. Unapproved: 0 unknown: 0 generated: 0 approved: 35 licence.
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building storm-hive 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ storm-hive ---
[INFO] Downloading: https://oss.sonatype.org/content/repositories/releases/org/pentaho/pentaho-aggdesigner-algorithm/5.1.3-jhyde/pentaho-aggdesigner-algorithm-5.1.3-jhyde.pom
[INFO] Downloading: https://repository.apache.org/releases/org/pentaho/pentaho-aggdesigner-algorithm/5.1.3-jhyde/pentaho-aggdesigner-algorithm-5.1.3-jhyde.pom
[INFO] Downloading: https://oss.sonatype.org/content/repositories/releases/net/hydromatic/linq4j/0.4/linq4j-0.4.pom
[INFO] Downloading: https://repository.apache.org/releases/net/hydromatic/linq4j/0.4/linq4j-0.4.pom
[INFO] Downloading: https://oss.sonatype.org/content/repositories/releases/net/hydromatic/quidem/0.1.1/quidem-0.1.1.pom
[INFO] Downloading: https://repository.apache.org/releases/net/hydromatic/quidem/0.1.1/quidem-0.1.1.pom
[INFO] Downloading: https://oss.sonatype.org/content/repositories/releases/eigenbase/eigenbase-properties/1.1.4/eigenbase-properties-1.1.4.pom
[INFO] Downloading: https://repository.apache.org/releases/eigenbase/eigenbase-properties/1.1.4/eigenbase-properties-1.1.4.pom
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ storm-hive ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/storm-hive/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ storm-hive ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ storm-hive ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/storm-hive/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ storm-hive ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (cleanup) @ storm-hive ---
[INFO] 
[INFO] --- maven-surefire-plugin:2.18.1:test (default-test) @ storm-hive ---
[INFO] Surefire report directory: /home/travis/build/apache/storm/external/storm-hive/target/surefire-reports
[INFO] parallel='none', perCoreThreadCount=true, threadCount=0, useUnlimitedThreads=false, threadCountSuites=0, threadCountClasses=0, threadCountMethods=0, parallelOptimized=true

-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running org.apache.storm.hive.bolt.TestHiveBolt
Tests run: 11, Failures: 0, Errors: 9, Skipped: 0, Time elapsed: 18.251 sec <<< FAILURE! - in org.apache.storm.hive.bolt.TestHiveBolt
testMultiPartitionTuples(org.apache.storm.hive.bolt.TestHiveBolt)  Time elapsed: 1.779 sec  <<< ERROR!
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hive.bolt.TestHiveBolt.generateTestTuple(TestHiveBolt.java:447)
	at org.apache.storm.hive.bolt.TestHiveBolt.testMultiPartitionTuples(TestHiveBolt.java:411)

testNoAcksUntilFlushed(org.apache.storm.hive.bolt.TestHiveBolt)  Time elapsed: 2.8 sec  <<< ERROR!
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hive.bolt.TestHiveBolt.generateTestTuple(TestHiveBolt.java:447)
	at org.apache.storm.hive.bolt.TestHiveBolt.testNoAcksUntilFlushed(TestHiveBolt.java:297)

testData(org.apache.storm.hive.bolt.TestHiveBolt)  Time elapsed: 2.704 sec  <<< ERROR!
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hive.bolt.TestHiveBolt.generateTestTuple(TestHiveBolt.java:447)
	at org.apache.storm.hive.bolt.TestHiveBolt.testData(TestHiveBolt.java:251)

testWithoutPartitions(org.apache.storm.hive.bolt.TestHiveBolt)  Time elapsed: 1.393 sec  <<< ERROR!
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hive.bolt.TestHiveBolt.generateTestTuple(TestHiveBolt.java:447)
	at org.apache.storm.hive.bolt.TestHiveBolt.testWithoutPartitions(TestHiveBolt.java:194)

testJsonWriter(org.apache.storm.hive.bolt.TestHiveBolt)  Time elapsed: 0.544 sec  <<< ERROR!
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hive.bolt.TestHiveBolt.generateTestTuple(TestHiveBolt.java:447)
	at org.apache.storm.hive.bolt.TestHiveBolt.testJsonWriter(TestHiveBolt.java:274)

testTickTuple(org.apache.storm.hive.bolt.TestHiveBolt)  Time elapsed: 0.45 sec  <<< ERROR!
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hive.bolt.TestHiveBolt.generateTestTuple(TestHiveBolt.java:447)
	at org.apache.storm.hive.bolt.TestHiveBolt.testTickTuple(TestHiveBolt.java:352)

testWithTimeformat(org.apache.storm.hive.bolt.TestHiveBolt)  Time elapsed: 0.842 sec  <<< ERROR!
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hive.bolt.TestHiveBolt.generateTestTuple(TestHiveBolt.java:447)
	at org.apache.storm.hive.bolt.TestHiveBolt.testWithTimeformat(TestHiveBolt.java:230)

testWithByteArrayIdandMessage(org.apache.storm.hive.bolt.TestHiveBolt)  Time elapsed: 0.474 sec  <<< ERROR!
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hive.bolt.TestHiveBolt.generateTestTuple(TestHiveBolt.java:447)
	at org.apache.storm.hive.bolt.TestHiveBolt.testWithByteArrayIdandMessage(TestHiveBolt.java:161)

testNoAcksIfFlushFails(org.apache.storm.hive.bolt.TestHiveBolt)  Time elapsed: 0.85 sec  <<< ERROR!
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hive.bolt.TestHiveBolt.generateTestTuple(TestHiveBolt.java:447)
	at org.apache.storm.hive.bolt.TestHiveBolt.testNoAcksIfFlushFails(TestHiveBolt.java:327)

Running org.apache.storm.hive.common.TestHiveWriter
Tests run: 3, Failures: 0, Errors: 2, Skipped: 0, Time elapsed: 4.062 sec <<< FAILURE! - in org.apache.storm.hive.common.TestHiveWriter
testWriteBasic(org.apache.storm.hive.common.TestHiveWriter)  Time elapsed: 0.868 sec  <<< ERROR!
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hive.common.TestHiveWriter.generateTestTuple(TestHiveWriter.java:164)
	at org.apache.storm.hive.common.TestHiveWriter.writeTuples(TestHiveWriter.java:179)
	at org.apache.storm.hive.common.TestHiveWriter.testWriteBasic(TestHiveWriter.java:127)

testWriteMultiFlush(org.apache.storm.hive.common.TestHiveWriter)  Time elapsed: 1.096 sec  <<< ERROR!
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hive.common.TestHiveWriter.generateTestTuple(TestHiveWriter.java:164)
	at org.apache.storm.hive.common.TestHiveWriter.testWriteMultiFlush(TestHiveWriter.java:142)


Results :

Tests in error: 
  TestHiveBolt.testData:251->generateTestTuple:447 » IllegalArgument Spouts is n...
  TestHiveBolt.testJsonWriter:274->generateTestTuple:447 » IllegalArgument Spout...
  TestHiveBolt.testMultiPartitionTuples:411->generateTestTuple:447 » IllegalArgument
  TestHiveBolt.testNoAcksIfFlushFails:327->generateTestTuple:447 » IllegalArgument
  TestHiveBolt.testNoAcksUntilFlushed:297->generateTestTuple:447 » IllegalArgument
  TestHiveBolt.testTickTuple:352->generateTestTuple:447 » IllegalArgument Spouts...
  TestHiveBolt.testWithByteArrayIdandMessage:161->generateTestTuple:447 » IllegalArgument
  TestHiveBolt.testWithTimeformat:230->generateTestTuple:447 » IllegalArgument S...
  TestHiveBolt.testWithoutPartitions:194->generateTestTuple:447 » IllegalArgument
  TestHiveWriter.testWriteBasic:127->writeTuples:179->generateTestTuple:164 » IllegalArgument
  TestHiveWriter.testWriteMultiFlush:142->generateTestTuple:164 » IllegalArgument

Tests run: 14, Failures: 0, Errors: 11, Skipped: 0

[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building storm-jdbc 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ storm-jdbc ---
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ storm-jdbc ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/storm-jdbc/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ storm-jdbc ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ storm-jdbc ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/storm-jdbc/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- sql-maven-plugin:1.5:execute (create-db) @ storm-jdbc ---
[INFO] Executing file: /tmp/test.21014852sql
[INFO] 1 of 1 SQL statements executed successfully
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ storm-jdbc ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:2.18.1:test (default-test) @ storm-jdbc ---
[INFO] Surefire report directory: /home/travis/build/apache/storm/external/storm-jdbc/target/surefire-reports
[INFO] parallel='none', perCoreThreadCount=true, threadCount=0, useUnlimitedThreads=false, threadCountSuites=0, threadCountClasses=0, threadCountMethods=0, parallelOptimized=true

-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running org.apache.storm.jdbc.common.UtilTest
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.006 sec - in org.apache.storm.jdbc.common.UtilTest
Running org.apache.storm.jdbc.common.JdbcClientTest
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.213 sec - in org.apache.storm.jdbc.common.JdbcClientTest
Running org.apache.storm.jdbc.bolt.JdbcLookupBoltTest
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.007 sec - in org.apache.storm.jdbc.bolt.JdbcLookupBoltTest
Running org.apache.storm.jdbc.bolt.JdbcInsertBoltTest
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.002 sec - in org.apache.storm.jdbc.bolt.JdbcInsertBoltTest

Results :

Tests run: 5, Failures: 0, Errors: 0, Skipped: 0

[INFO] 
[INFO] --- apache-rat-plugin:0.11:check (default) @ storm-jdbc ---
[INFO] 51 implicit excludes (use -debug for more details).
[INFO] Exclude: **/target/**
[INFO] Exclude: **/*.iml
[INFO] Exclude: **/.idea/**
[INFO] Exclude: **/metastore_db/**
[INFO] Exclude: **/build/**
[INFO] Exclude: **/logs/**
[INFO] Exclude: **/CHANGELOG.md
[INFO] Exclude: **/README.md
[INFO] Exclude: **/README.markdown
[INFO] Exclude: **/DEVELOPER.md
[INFO] Exclude: **/BYLAWS.md
[INFO] Exclude: **/STORM-UI-REST-API.md
[INFO] Exclude: SECURITY.md
[INFO] Exclude: VERSION
[INFO] Exclude: TODO
[INFO] Exclude: **/src/py/**
[INFO] Exclude: **/src/ui/public/js/jquery.dataTables.1.10.4.min.js
[INFO] Exclude: **/src/ui/public/css/jquery.dataTables.1.10.4.min.css
[INFO] Exclude: **/src/ui/public/images/*
[INFO] Exclude: **/src/ui/public/js/bootstrap-3.3.1.min.js
[INFO] Exclude: **/src/ui/public/css/bootstrap-3.3.1.min.css
[INFO] Exclude: **/src/ui/public/js/dataTables.bootstrap.min.js
[INFO] Exclude: **/src/ui/public/css/dataTables.bootstrap.css
[INFO] Exclude: **/src/ui/public/js/jsonFormatter.min.js
[INFO] Exclude: **/src/ui/public/css/jsonFormatter.min.css
[INFO] Exclude: **/src/ui/public/js/jquery-1.11.1.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.cookies.2.2.0.min.js
[INFO] Exclude: **/src/ui/public/js/moment.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.blockUI.min.js
[INFO] Exclude: **/src/ui/public/js/url.min.js
[INFO] Exclude: **/src/ui/public/js/arbor.js
[INFO] Exclude: **/src/ui/public/js/arbor-graphics.js
[INFO] Exclude: **/src/ui/public/js/arbor-tween.js
[INFO] Exclude: **/src/ui/public/js/jquery.mustache.js
[INFO] Exclude: **/src/ui/public/js/typeahead.jquery.min.js
[INFO] Exclude: **/dependency-reduced-pom.xml
[INFO] Exclude: **/docs/**
[INFO] Exclude: **/.git/**
[INFO] Exclude: **/derby.log
[INFO] Exclude: **/src/dev/**
[INFO] Exclude: **/src/codegen/config.fmpp
[INFO] Exclude: **/src/codegen/data/Parser.tdd
[INFO] Exclude: **/src/test/resources/FixedAvroSerializer.config
[INFO] 26 resources included (use -debug for more details)
Warning:  org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
Compiler warnings:
  WARNING:  'org.apache.xerces.jaxp.SAXParserImpl: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.'
Warning:  org.apache.xerces.parsers.SAXParser: Feature 'http://javax.xml.XMLConstants/feature/secure-processing' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
[INFO] Rat check: Summary of files. Unapproved: 0 unknown: 0 generated: 0 approved: 26 licence.
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building storm-redis 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ storm-redis ---
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ storm-redis ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/storm-redis/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ storm-redis ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ storm-redis ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/storm-redis/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ storm-redis ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:2.18.1:test (default-test) @ storm-redis ---
[INFO] Surefire report directory: /home/travis/build/apache/storm/external/storm-redis/target/surefire-reports
[INFO] parallel='none', perCoreThreadCount=true, threadCount=0, useUnlimitedThreads=false, threadCountSuites=0, threadCountClasses=0, threadCountMethods=0, parallelOptimized=true

-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running org.apache.storm.redis.state.DefaultStateSerializerTest
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.161 sec - in org.apache.storm.redis.state.DefaultStateSerializerTest
Running org.apache.storm.redis.state.RedisKeyValueStateTest
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.115 sec - in org.apache.storm.redis.state.RedisKeyValueStateTest
Running org.apache.storm.redis.state.RedisKeyValueStateProviderTest
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.271 sec - in org.apache.storm.redis.state.RedisKeyValueStateProviderTest

Results :

Tests run: 5, Failures: 0, Errors: 0, Skipped: 0

[INFO] 
[INFO] --- apache-rat-plugin:0.11:check (default) @ storm-redis ---
[INFO] 51 implicit excludes (use -debug for more details).
[INFO] Exclude: **/target/**
[INFO] Exclude: **/*.iml
[INFO] Exclude: **/.idea/**
[INFO] Exclude: **/metastore_db/**
[INFO] Exclude: **/build/**
[INFO] Exclude: **/logs/**
[INFO] Exclude: **/CHANGELOG.md
[INFO] Exclude: **/README.md
[INFO] Exclude: **/README.markdown
[INFO] Exclude: **/DEVELOPER.md
[INFO] Exclude: **/BYLAWS.md
[INFO] Exclude: **/STORM-UI-REST-API.md
[INFO] Exclude: SECURITY.md
[INFO] Exclude: VERSION
[INFO] Exclude: TODO
[INFO] Exclude: **/src/py/**
[INFO] Exclude: **/src/ui/public/js/jquery.dataTables.1.10.4.min.js
[INFO] Exclude: **/src/ui/public/css/jquery.dataTables.1.10.4.min.css
[INFO] Exclude: **/src/ui/public/images/*
[INFO] Exclude: **/src/ui/public/js/bootstrap-3.3.1.min.js
[INFO] Exclude: **/src/ui/public/css/bootstrap-3.3.1.min.css
[INFO] Exclude: **/src/ui/public/js/dataTables.bootstrap.min.js
[INFO] Exclude: **/src/ui/public/css/dataTables.bootstrap.css
[INFO] Exclude: **/src/ui/public/js/jsonFormatter.min.js
[INFO] Exclude: **/src/ui/public/css/jsonFormatter.min.css
[INFO] Exclude: **/src/ui/public/js/jquery-1.11.1.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.cookies.2.2.0.min.js
[INFO] Exclude: **/src/ui/public/js/moment.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.blockUI.min.js
[INFO] Exclude: **/src/ui/public/js/url.min.js
[INFO] Exclude: **/src/ui/public/js/arbor.js
[INFO] Exclude: **/src/ui/public/js/arbor-graphics.js
[INFO] Exclude: **/src/ui/public/js/arbor-tween.js
[INFO] Exclude: **/src/ui/public/js/jquery.mustache.js
[INFO] Exclude: **/src/ui/public/js/typeahead.jquery.min.js
[INFO] Exclude: **/dependency-reduced-pom.xml
[INFO] Exclude: **/docs/**
[INFO] Exclude: **/.git/**
[INFO] Exclude: **/derby.log
[INFO] Exclude: **/src/dev/**
[INFO] Exclude: **/src/codegen/config.fmpp
[INFO] Exclude: **/src/codegen/data/Parser.tdd
[INFO] Exclude: **/src/test/resources/FixedAvroSerializer.config
[INFO] 44 resources included (use -debug for more details)
Warning:  org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
Compiler warnings:
  WARNING:  'org.apache.xerces.jaxp.SAXParserImpl: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.'
Warning:  org.apache.xerces.parsers.SAXParser: Feature 'http://javax.xml.XMLConstants/feature/secure-processing' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
[INFO] Rat check: Summary of files. Unapproved: 0 unknown: 0 generated: 0 approved: 44 licence.
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building storm-eventhubs 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ storm-eventhubs ---
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ storm-eventhubs ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 1 resource
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ storm-eventhubs ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ storm-eventhubs ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/storm-eventhubs/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ storm-eventhubs ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:2.18.1:test (default-test) @ storm-eventhubs ---
[INFO] Surefire report directory: /home/travis/build/apache/storm/external/storm-eventhubs/target/surefire-reports
[INFO] parallel='none', perCoreThreadCount=true, threadCount=0, useUnlimitedThreads=false, threadCountSuites=0, threadCountClasses=0, threadCountMethods=0, parallelOptimized=true

-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running org.apache.storm.eventhubs.trident.TestTransactionalTridentEmitter
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.592 sec - in org.apache.storm.eventhubs.trident.TestTransactionalTridentEmitter
Running org.apache.storm.eventhubs.spout.TestPartitionManager
Tests run: 8, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.022 sec - in org.apache.storm.eventhubs.spout.TestPartitionManager
Running org.apache.storm.eventhubs.spout.TestEventData
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.001 sec - in org.apache.storm.eventhubs.spout.TestEventData
Running org.apache.storm.eventhubs.spout.TestEventHubSpout
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 2.016 sec - in org.apache.storm.eventhubs.spout.TestEventHubSpout

Results :

Tests run: 14, Failures: 0, Errors: 0, Skipped: 0

[INFO] 
[INFO] --- apache-rat-plugin:0.11:check (default) @ storm-eventhubs ---
[INFO] 51 implicit excludes (use -debug for more details).
[INFO] Exclude: **/target/**
[INFO] Exclude: **/*.iml
[INFO] Exclude: **/.idea/**
[INFO] Exclude: **/metastore_db/**
[INFO] Exclude: **/build/**
[INFO] Exclude: **/logs/**
[INFO] Exclude: **/CHANGELOG.md
[INFO] Exclude: **/README.md
[INFO] Exclude: **/README.markdown
[INFO] Exclude: **/DEVELOPER.md
[INFO] Exclude: **/BYLAWS.md
[INFO] Exclude: **/STORM-UI-REST-API.md
[INFO] Exclude: SECURITY.md
[INFO] Exclude: VERSION
[INFO] Exclude: TODO
[INFO] Exclude: **/src/py/**
[INFO] Exclude: **/src/ui/public/js/jquery.dataTables.1.10.4.min.js
[INFO] Exclude: **/src/ui/public/css/jquery.dataTables.1.10.4.min.css
[INFO] Exclude: **/src/ui/public/images/*
[INFO] Exclude: **/src/ui/public/js/bootstrap-3.3.1.min.js
[INFO] Exclude: **/src/ui/public/css/bootstrap-3.3.1.min.css
[INFO] Exclude: **/src/ui/public/js/dataTables.bootstrap.min.js
[INFO] Exclude: **/src/ui/public/css/dataTables.bootstrap.css
[INFO] Exclude: **/src/ui/public/js/jsonFormatter.min.js
[INFO] Exclude: **/src/ui/public/css/jsonFormatter.min.css
[INFO] Exclude: **/src/ui/public/js/jquery-1.11.1.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.cookies.2.2.0.min.js
[INFO] Exclude: **/src/ui/public/js/moment.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.blockUI.min.js
[INFO] Exclude: **/src/ui/public/js/url.min.js
[INFO] Exclude: **/src/ui/public/js/arbor.js
[INFO] Exclude: **/src/ui/public/js/arbor-graphics.js
[INFO] Exclude: **/src/ui/public/js/arbor-tween.js
[INFO] Exclude: **/src/ui/public/js/jquery.mustache.js
[INFO] Exclude: **/src/ui/public/js/typeahead.jquery.min.js
[INFO] Exclude: **/dependency-reduced-pom.xml
[INFO] Exclude: **/docs/**
[INFO] Exclude: **/.git/**
[INFO] Exclude: **/derby.log
[INFO] Exclude: **/src/dev/**
[INFO] Exclude: **/src/codegen/config.fmpp
[INFO] Exclude: **/src/codegen/data/Parser.tdd
[INFO] Exclude: **/src/test/resources/FixedAvroSerializer.config
[INFO] 52 resources included (use -debug for more details)
Warning:  org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
Compiler warnings:
  WARNING:  'org.apache.xerces.jaxp.SAXParserImpl: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.'
Warning:  org.apache.xerces.parsers.SAXParser: Feature 'http://javax.xml.XMLConstants/feature/secure-processing' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
[INFO] Rat check: Summary of files. Unapproved: 0 unknown: 0 generated: 0 approved: 52 licence.
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building flux 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ flux ---
[INFO] 
[INFO] --- apache-rat-plugin:0.11:check (default) @ flux ---
[INFO] 51 implicit excludes (use -debug for more details).
[INFO] Exclude: **/target/**
[INFO] Exclude: **/*.iml
[INFO] Exclude: **/.idea/**
[INFO] Exclude: **/metastore_db/**
[INFO] Exclude: **/build/**
[INFO] Exclude: **/logs/**
[INFO] Exclude: **/CHANGELOG.md
[INFO] Exclude: **/README.md
[INFO] Exclude: **/README.markdown
[INFO] Exclude: **/DEVELOPER.md
[INFO] Exclude: **/BYLAWS.md
[INFO] Exclude: **/STORM-UI-REST-API.md
[INFO] Exclude: SECURITY.md
[INFO] Exclude: VERSION
[INFO] Exclude: TODO
[INFO] Exclude: **/src/py/**
[INFO] Exclude: **/src/ui/public/js/jquery.dataTables.1.10.4.min.js
[INFO] Exclude: **/src/ui/public/css/jquery.dataTables.1.10.4.min.css
[INFO] Exclude: **/src/ui/public/images/*
[INFO] Exclude: **/src/ui/public/js/bootstrap-3.3.1.min.js
[INFO] Exclude: **/src/ui/public/css/bootstrap-3.3.1.min.css
[INFO] Exclude: **/src/ui/public/js/dataTables.bootstrap.min.js
[INFO] Exclude: **/src/ui/public/css/dataTables.bootstrap.css
[INFO] Exclude: **/src/ui/public/js/jsonFormatter.min.js
[INFO] Exclude: **/src/ui/public/css/jsonFormatter.min.css
[INFO] Exclude: **/src/ui/public/js/jquery-1.11.1.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.cookies.2.2.0.min.js
[INFO] Exclude: **/src/ui/public/js/moment.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.blockUI.min.js
[INFO] Exclude: **/src/ui/public/js/url.min.js
[INFO] Exclude: **/src/ui/public/js/arbor.js
[INFO] Exclude: **/src/ui/public/js/arbor-graphics.js
[INFO] Exclude: **/src/ui/public/js/arbor-tween.js
[INFO] Exclude: **/src/ui/public/js/jquery.mustache.js
[INFO] Exclude: **/src/ui/public/js/typeahead.jquery.min.js
[INFO] Exclude: **/dependency-reduced-pom.xml
[INFO] Exclude: **/docs/**
[INFO] Exclude: **/.git/**
[INFO] Exclude: **/derby.log
[INFO] Exclude: **/src/dev/**
[INFO] Exclude: **/src/codegen/config.fmpp
[INFO] Exclude: **/src/codegen/data/Parser.tdd
[INFO] Exclude: **/src/test/resources/FixedAvroSerializer.config
[INFO] 69 resources included (use -debug for more details)
Warning:  org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
Compiler warnings:
  WARNING:  'org.apache.xerces.jaxp.SAXParserImpl: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.'
Warning:  org.apache.xerces.parsers.SAXParser: Feature 'http://javax.xml.XMLConstants/feature/secure-processing' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
[INFO] Rat check: Summary of files. Unapproved: 0 unknown: 0 generated: 0 approved: 68 licence.
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building flux-wrappers 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ flux-wrappers ---
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ flux-wrappers ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 2 resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.3:compile (default-compile) @ flux-wrappers ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ flux-wrappers ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/flux/flux-wrappers/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.3:testCompile (default-testCompile) @ flux-wrappers ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-surefire-plugin:2.18.1:test (default-test) @ flux-wrappers ---
[INFO] 
[INFO] --- apache-rat-plugin:0.11:check (default) @ flux-wrappers ---
[INFO] 51 implicit excludes (use -debug for more details).
[INFO] Exclude: **/target/**
[INFO] Exclude: **/*.iml
[INFO] Exclude: **/.idea/**
[INFO] Exclude: **/metastore_db/**
[INFO] Exclude: **/build/**
[INFO] Exclude: **/logs/**
[INFO] Exclude: **/CHANGELOG.md
[INFO] Exclude: **/README.md
[INFO] Exclude: **/README.markdown
[INFO] Exclude: **/DEVELOPER.md
[INFO] Exclude: **/BYLAWS.md
[INFO] Exclude: **/STORM-UI-REST-API.md
[INFO] Exclude: SECURITY.md
[INFO] Exclude: VERSION
[INFO] Exclude: TODO
[INFO] Exclude: **/src/py/**
[INFO] Exclude: **/src/ui/public/js/jquery.dataTables.1.10.4.min.js
[INFO] Exclude: **/src/ui/public/css/jquery.dataTables.1.10.4.min.css
[INFO] Exclude: **/src/ui/public/images/*
[INFO] Exclude: **/src/ui/public/js/bootstrap-3.3.1.min.js
[INFO] Exclude: **/src/ui/public/css/bootstrap-3.3.1.min.css
[INFO] Exclude: **/src/ui/public/js/dataTables.bootstrap.min.js
[INFO] Exclude: **/src/ui/public/css/dataTables.bootstrap.css
[INFO] Exclude: **/src/ui/public/js/jsonFormatter.min.js
[INFO] Exclude: **/src/ui/public/css/jsonFormatter.min.css
[INFO] Exclude: **/src/ui/public/js/jquery-1.11.1.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.cookies.2.2.0.min.js
[INFO] Exclude: **/src/ui/public/js/moment.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.blockUI.min.js
[INFO] Exclude: **/src/ui/public/js/url.min.js
[INFO] Exclude: **/src/ui/public/js/arbor.js
[INFO] Exclude: **/src/ui/public/js/arbor-graphics.js
[INFO] Exclude: **/src/ui/public/js/arbor-tween.js
[INFO] Exclude: **/src/ui/public/js/jquery.mustache.js
[INFO] Exclude: **/src/ui/public/js/typeahead.jquery.min.js
[INFO] Exclude: **/dependency-reduced-pom.xml
[INFO] Exclude: **/docs/**
[INFO] Exclude: **/.git/**
[INFO] Exclude: **/derby.log
[INFO] Exclude: **/src/dev/**
[INFO] Exclude: **/src/codegen/config.fmpp
[INFO] Exclude: **/src/codegen/data/Parser.tdd
[INFO] Exclude: **/src/test/resources/FixedAvroSerializer.config
[INFO] 6 resources included (use -debug for more details)
Warning:  org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
Compiler warnings:
  WARNING:  'org.apache.xerces.jaxp.SAXParserImpl: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.'
Warning:  org.apache.xerces.parsers.SAXParser: Feature 'http://javax.xml.XMLConstants/feature/secure-processing' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
[INFO] Rat check: Summary of files. Unapproved: 0 unknown: 0 generated: 0 approved: 6 licence.
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Skipping flux-core
[INFO] This project has been banned from the build due to previous failures.
[INFO] ------------------------------------------------------------------------
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Skipping flux-examples
[INFO] This project has been banned from the build due to previous failures.
[INFO] ------------------------------------------------------------------------
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building storm-sql-runtime 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ storm-sql-runtime ---
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ storm-sql-runtime ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/sql/storm-sql-runtime/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ storm-sql-runtime ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ storm-sql-runtime ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/sql/storm-sql-runtime/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ storm-sql-runtime ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:2.18.1:test (default-test) @ storm-sql-runtime ---
[INFO] Surefire report directory: /home/travis/build/apache/storm/external/sql/storm-sql-runtime/target/surefire-reports
[INFO] parallel='none', perCoreThreadCount=true, threadCount=0, useUnlimitedThreads=false, threadCountSuites=0, threadCountClasses=0, threadCountMethods=0, parallelOptimized=true

-------------------------------------------------------
 T E S T S
-------------------------------------------------------

Results :

Tests run: 0, Failures: 0, Errors: 0, Skipped: 0

[INFO] 
[INFO] --- apache-rat-plugin:0.11:check (default) @ storm-sql-runtime ---
[INFO] 51 implicit excludes (use -debug for more details).
[INFO] Exclude: **/target/**
[INFO] Exclude: **/*.iml
[INFO] Exclude: **/.idea/**
[INFO] Exclude: **/metastore_db/**
[INFO] Exclude: **/build/**
[INFO] Exclude: **/logs/**
[INFO] Exclude: **/CHANGELOG.md
[INFO] Exclude: **/README.md
[INFO] Exclude: **/README.markdown
[INFO] Exclude: **/DEVELOPER.md
[INFO] Exclude: **/BYLAWS.md
[INFO] Exclude: **/STORM-UI-REST-API.md
[INFO] Exclude: SECURITY.md
[INFO] Exclude: VERSION
[INFO] Exclude: TODO
[INFO] Exclude: **/src/py/**
[INFO] Exclude: **/src/ui/public/js/jquery.dataTables.1.10.4.min.js
[INFO] Exclude: **/src/ui/public/css/jquery.dataTables.1.10.4.min.css
[INFO] Exclude: **/src/ui/public/images/*
[INFO] Exclude: **/src/ui/public/js/bootstrap-3.3.1.min.js
[INFO] Exclude: **/src/ui/public/css/bootstrap-3.3.1.min.css
[INFO] Exclude: **/src/ui/public/js/dataTables.bootstrap.min.js
[INFO] Exclude: **/src/ui/public/css/dataTables.bootstrap.css
[INFO] Exclude: **/src/ui/public/js/jsonFormatter.min.js
[INFO] Exclude: **/src/ui/public/css/jsonFormatter.min.css
[INFO] Exclude: **/src/ui/public/js/jquery-1.11.1.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.cookies.2.2.0.min.js
[INFO] Exclude: **/src/ui/public/js/moment.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.blockUI.min.js
[INFO] Exclude: **/src/ui/public/js/url.min.js
[INFO] Exclude: **/src/ui/public/js/arbor.js
[INFO] Exclude: **/src/ui/public/js/arbor-graphics.js
[INFO] Exclude: **/src/ui/public/js/arbor-tween.js
[INFO] Exclude: **/src/ui/public/js/jquery.mustache.js
[INFO] Exclude: **/src/ui/public/js/typeahead.jquery.min.js
[INFO] Exclude: **/dependency-reduced-pom.xml
[INFO] Exclude: **/docs/**
[INFO] Exclude: **/.git/**
[INFO] Exclude: **/derby.log
[INFO] Exclude: **/src/dev/**
[INFO] Exclude: **/src/codegen/config.fmpp
[INFO] Exclude: **/src/codegen/data/Parser.tdd
[INFO] Exclude: **/src/test/resources/FixedAvroSerializer.config
[INFO] 15 resources included (use -debug for more details)
Warning:  org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
Compiler warnings:
  WARNING:  'org.apache.xerces.jaxp.SAXParserImpl: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.'
Warning:  org.apache.xerces.parsers.SAXParser: Feature 'http://javax.xml.XMLConstants/feature/secure-processing' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
[INFO] Rat check: Summary of files. Unapproved: 0 unknown: 0 generated: 0 approved: 15 licence.
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building storm-sql-core 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-resources-plugin:2.5:copy-resources (copy-fmpp-resources) @ storm-sql-core ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 4 resources
[INFO] 
[INFO] --- maven-dependency-plugin:2.8:unpack (unpack-parser-template) @ storm-sql-core ---
[INFO] Configured Artifact: org.apache.calcite:calcite-core:?:jar
[INFO] Unpacking /home/travis/.m2/repository/org/apache/calcite/calcite-core/1.4.0-incubating/calcite-core-1.4.0-incubating.jar to /home/travis/build/apache/storm/external/sql/storm-sql-core/target with includes "**/Parser.jj" and excludes ""
[INFO] 
[INFO] --- fmpp-maven-plugin:1.0:generate (generate-fmpp-sources) @ storm-sql-core ---
- Executing: Parser.jj
log4j:WARN No appenders could be found for logger (freemarker.cache).
log4j:WARN Please initialize the log4j system properly.
[INFO] Done
[INFO] 
[INFO] --- javacc-maven-plugin:2.4:javacc (javacc) @ storm-sql-core ---
Java Compiler Compiler Version 4.0 (Parser Generator)
(type "javacc" with no arguments for help)
Reading from file /home/travis/build/apache/storm/external/sql/storm-sql-core/target/generated-sources/javacc/Parser.jj . . .
Note: UNICODE_INPUT option is specified. Please make sure you create the parser/lexer using a Reader with the correct character encoding.
Warning: Lookahead adequacy checking not being performed since option LOOKAHEAD is more than 1.  Set option FORCE_LA_CHECK to true to force checking.
Parser generated with 0 errors and 1 warnings.
[INFO] Processed 1 grammar
[INFO] 
[INFO] --- maven-resources-plugin:2.5:copy-resources (copy-java-sources) @ storm-sql-core ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 19 resources
[INFO] Copying 8 resources
[INFO] Copying 8 resources
[INFO] 
[INFO] --- build-helper-maven-plugin:1.5:add-source (add-generated-sources) @ storm-sql-core ---
[INFO] Source directory: /home/travis/build/apache/storm/external/sql/storm-sql-core/target/generated-sources added.
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ storm-sql-core ---
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ storm-sql-core ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/sql/storm-sql-core/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ storm-sql-core ---
[INFO] Changes detected - recompiling the module!
[INFO] Compiling 26 source files to /home/travis/build/apache/storm/external/sql/storm-sql-core/target/classes
[WARNING] /home/travis/build/apache/storm/external/sql/storm-sql-core/target/generated-sources/org/apache/storm/sql/parser/impl/StormParserImpl.java: Some input files use or override a deprecated API.
[WARNING] /home/travis/build/apache/storm/external/sql/storm-sql-core/target/generated-sources/org/apache/storm/sql/parser/impl/StormParserImpl.java: Recompile with -Xlint:deprecation for details.
[WARNING] /home/travis/build/apache/storm/external/sql/storm-sql-core/target/generated-sources/org/apache/storm/sql/parser/impl/StormParserImpl.java: /home/travis/build/apache/storm/external/sql/storm-sql-core/target/generated-sources/org/apache/storm/sql/parser/impl/StormParserImpl.java uses unchecked or unsafe operations.
[WARNING] /home/travis/build/apache/storm/external/sql/storm-sql-core/target/generated-sources/org/apache/storm/sql/parser/impl/StormParserImpl.java: Recompile with -Xlint:unchecked for details.
[INFO] 
[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ storm-sql-core ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/sql/storm-sql-core/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ storm-sql-core ---
[INFO] Changes detected - recompiling the module!
[INFO] Compiling 15 source files to /home/travis/build/apache/storm/external/sql/storm-sql-core/target/test-classes
[WARNING] /home/travis/build/apache/storm/external/sql/storm-sql-core/target/generated-sources/org/apache/storm/sql/parser/impl/StormParserImpl.java: /home/travis/build/apache/storm/external/sql/storm-sql-core/target/generated-sources/org/apache/storm/sql/parser/impl/StormParserImpl.java uses or overrides a deprecated API.
[WARNING] /home/travis/build/apache/storm/external/sql/storm-sql-core/target/generated-sources/org/apache/storm/sql/parser/impl/StormParserImpl.java: Recompile with -Xlint:deprecation for details.
[WARNING] /home/travis/build/apache/storm/external/sql/storm-sql-core/target/generated-sources/org/apache/storm/sql/parser/impl/StormParserImpl.java: /home/travis/build/apache/storm/external/sql/storm-sql-core/target/generated-sources/org/apache/storm/sql/parser/impl/StormParserImpl.java uses unchecked or unsafe operations.
[WARNING] /home/travis/build/apache/storm/external/sql/storm-sql-core/target/generated-sources/org/apache/storm/sql/parser/impl/StormParserImpl.java: Recompile with -Xlint:unchecked for details.
[INFO] 
[INFO] --- maven-surefire-plugin:2.18.1:test (default-test) @ storm-sql-core ---
[INFO] Surefire report directory: /home/travis/build/apache/storm/external/sql/storm-sql-core/target/surefire-reports
[INFO] parallel='none', perCoreThreadCount=true, threadCount=0, useUnlimitedThreads=false, threadCountSuites=0, threadCountClasses=0, threadCountMethods=0, parallelOptimized=true

-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running org.apache.storm.sql.parser.TestSqlParser
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.452 sec - in org.apache.storm.sql.parser.TestSqlParser
Running org.apache.storm.sql.compiler.backends.standalone.TestRelNodeCompiler
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.889 sec - in org.apache.storm.sql.compiler.backends.standalone.TestRelNodeCompiler
Running org.apache.storm.sql.compiler.backends.standalone.TestPlanCompiler
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 3.594 sec - in org.apache.storm.sql.compiler.backends.standalone.TestPlanCompiler
Running org.apache.storm.sql.compiler.backends.trident.TestPlanCompiler
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 34.338 sec - in org.apache.storm.sql.compiler.backends.trident.TestPlanCompiler
Running org.apache.storm.sql.compiler.TestExprCompiler
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.028 sec - in org.apache.storm.sql.compiler.TestExprCompiler
Running org.apache.storm.sql.compiler.TestExprSemantic
Tests run: 10, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 2.881 sec - in org.apache.storm.sql.compiler.TestExprSemantic
Running org.apache.storm.sql.TestStormSql
Tests run: 15, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 3.854 sec - in org.apache.storm.sql.TestStormSql

Results :

Tests run: 39, Failures: 0, Errors: 0, Skipped: 0

[INFO] 
[INFO] --- apache-rat-plugin:0.11:check (default) @ storm-sql-core ---
[INFO] 51 implicit excludes (use -debug for more details).
[INFO] Exclude: **/target/**
[INFO] Exclude: **/*.iml
[INFO] Exclude: **/.idea/**
[INFO] Exclude: **/metastore_db/**
[INFO] Exclude: **/build/**
[INFO] Exclude: **/logs/**
[INFO] Exclude: **/CHANGELOG.md
[INFO] Exclude: **/README.md
[INFO] Exclude: **/README.markdown
[INFO] Exclude: **/DEVELOPER.md
[INFO] Exclude: **/BYLAWS.md
[INFO] Exclude: **/STORM-UI-REST-API.md
[INFO] Exclude: SECURITY.md
[INFO] Exclude: VERSION
[INFO] Exclude: TODO
[INFO] Exclude: **/src/py/**
[INFO] Exclude: **/src/ui/public/js/jquery.dataTables.1.10.4.min.js
[INFO] Exclude: **/src/ui/public/css/jquery.dataTables.1.10.4.min.css
[INFO] Exclude: **/src/ui/public/images/*
[INFO] Exclude: **/src/ui/public/js/bootstrap-3.3.1.min.js
[INFO] Exclude: **/src/ui/public/css/bootstrap-3.3.1.min.css
[INFO] Exclude: **/src/ui/public/js/dataTables.bootstrap.min.js
[INFO] Exclude: **/src/ui/public/css/dataTables.bootstrap.css
[INFO] Exclude: **/src/ui/public/js/jsonFormatter.min.js
[INFO] Exclude: **/src/ui/public/css/jsonFormatter.min.css
[INFO] Exclude: **/src/ui/public/js/jquery-1.11.1.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.cookies.2.2.0.min.js
[INFO] Exclude: **/src/ui/public/js/moment.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.blockUI.min.js
[INFO] Exclude: **/src/ui/public/js/url.min.js
[INFO] Exclude: **/src/ui/public/js/arbor.js
[INFO] Exclude: **/src/ui/public/js/arbor-graphics.js
[INFO] Exclude: **/src/ui/public/js/arbor-tween.js
[INFO] Exclude: **/src/ui/public/js/jquery.mustache.js
[INFO] Exclude: **/src/ui/public/js/typeahead.jquery.min.js
[INFO] Exclude: **/dependency-reduced-pom.xml
[INFO] Exclude: **/docs/**
[INFO] Exclude: **/.git/**
[INFO] Exclude: **/derby.log
[INFO] Exclude: **/src/dev/**
[INFO] Exclude: **/src/codegen/config.fmpp
[INFO] Exclude: **/src/codegen/data/Parser.tdd
[INFO] Exclude: **/src/test/resources/FixedAvroSerializer.config
[INFO] 30 resources included (use -debug for more details)
Warning:  org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
Compiler warnings:
  WARNING:  'org.apache.xerces.jaxp.SAXParserImpl: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.'
Warning:  org.apache.xerces.parsers.SAXParser: Feature 'http://javax.xml.XMLConstants/feature/secure-processing' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
[INFO] Rat check: Summary of files. Unapproved: 0 unknown: 0 generated: 0 approved: 30 licence.
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Skipping storm-sql-kafka
[INFO] This project has been banned from the build due to previous failures.
[INFO] ------------------------------------------------------------------------
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building sql 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ sql ---
[INFO] 
[INFO] --- apache-rat-plugin:0.11:check (default) @ sql ---
[INFO] 51 implicit excludes (use -debug for more details).
[INFO] Exclude: **/target/**
[INFO] Exclude: **/*.iml
[INFO] Exclude: **/.idea/**
[INFO] Exclude: **/metastore_db/**
[INFO] Exclude: **/build/**
[INFO] Exclude: **/logs/**
[INFO] Exclude: **/CHANGELOG.md
[INFO] Exclude: **/README.md
[INFO] Exclude: **/README.markdown
[INFO] Exclude: **/DEVELOPER.md
[INFO] Exclude: **/BYLAWS.md
[INFO] Exclude: **/STORM-UI-REST-API.md
[INFO] Exclude: SECURITY.md
[INFO] Exclude: VERSION
[INFO] Exclude: TODO
[INFO] Exclude: **/src/py/**
[INFO] Exclude: **/src/ui/public/js/jquery.dataTables.1.10.4.min.js
[INFO] Exclude: **/src/ui/public/css/jquery.dataTables.1.10.4.min.css
[INFO] Exclude: **/src/ui/public/images/*
[INFO] Exclude: **/src/ui/public/js/bootstrap-3.3.1.min.js
[INFO] Exclude: **/src/ui/public/css/bootstrap-3.3.1.min.css
[INFO] Exclude: **/src/ui/public/js/dataTables.bootstrap.min.js
[INFO] Exclude: **/src/ui/public/css/dataTables.bootstrap.css
[INFO] Exclude: **/src/ui/public/js/jsonFormatter.min.js
[INFO] Exclude: **/src/ui/public/css/jsonFormatter.min.css
[INFO] Exclude: **/src/ui/public/js/jquery-1.11.1.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.cookies.2.2.0.min.js
[INFO] Exclude: **/src/ui/public/js/moment.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.blockUI.min.js
[INFO] Exclude: **/src/ui/public/js/url.min.js
[INFO] Exclude: **/src/ui/public/js/arbor.js
[INFO] Exclude: **/src/ui/public/js/arbor-graphics.js
[INFO] Exclude: **/src/ui/public/js/arbor-tween.js
[INFO] Exclude: **/src/ui/public/js/jquery.mustache.js
[INFO] Exclude: **/src/ui/public/js/typeahead.jquery.min.js
[INFO] Exclude: **/dependency-reduced-pom.xml
[INFO] Exclude: **/docs/**
[INFO] Exclude: **/.git/**
[INFO] Exclude: **/derby.log
[INFO] Exclude: **/src/dev/**
[INFO] Exclude: **/src/codegen/config.fmpp
[INFO] Exclude: **/src/codegen/data/Parser.tdd
[INFO] Exclude: **/src/test/resources/FixedAvroSerializer.config
[INFO] 53 resources included (use -debug for more details)
Warning:  org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
Compiler warnings:
  WARNING:  'org.apache.xerces.jaxp.SAXParserImpl: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.'
Warning:  org.apache.xerces.parsers.SAXParser: Feature 'http://javax.xml.XMLConstants/feature/secure-processing' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
[INFO] Rat check: Summary of files. Unapproved: 0 unknown: 0 generated: 0 approved: 53 licence.
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building storm-elasticsearch 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ storm-elasticsearch ---
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ storm-elasticsearch ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/storm-elasticsearch/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ storm-elasticsearch ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ storm-elasticsearch ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/storm-elasticsearch/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ storm-elasticsearch ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (cleanup) @ storm-elasticsearch ---
[INFO] 
[INFO] --- maven-surefire-plugin:2.18.1:test (default-test) @ storm-elasticsearch ---
[INFO] Surefire report directory: /home/travis/build/apache/storm/external/storm-elasticsearch/target/surefire-reports
[INFO] parallel='none', perCoreThreadCount=true, threadCount=0, useUnlimitedThreads=false, threadCountSuites=0, threadCountClasses=0, threadCountMethods=0, parallelOptimized=true

-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running org.apache.storm.elasticsearch.bolt.EsLookupBoltTest
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.651 sec - in org.apache.storm.elasticsearch.bolt.EsLookupBoltTest
Running org.apache.storm.elasticsearch.trident.EsStateFactoryTest
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.027 sec - in org.apache.storm.elasticsearch.trident.EsStateFactoryTest
Running org.apache.storm.elasticsearch.common.EsConfigTest
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.066 sec - in org.apache.storm.elasticsearch.common.EsConfigTest
Running org.apache.storm.elasticsearch.common.TransportAddressesTest
Tests run: 9, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.02 sec - in org.apache.storm.elasticsearch.common.TransportAddressesTest

Results :

Tests run: 18, Failures: 0, Errors: 0, Skipped: 0

[INFO] 
[INFO] --- apache-rat-plugin:0.11:check (default) @ storm-elasticsearch ---
[INFO] 51 implicit excludes (use -debug for more details).
[INFO] Exclude: **/target/**
[INFO] Exclude: **/*.iml
[INFO] Exclude: **/.idea/**
[INFO] Exclude: **/metastore_db/**
[INFO] Exclude: **/build/**
[INFO] Exclude: **/logs/**
[INFO] Exclude: **/CHANGELOG.md
[INFO] Exclude: **/README.md
[INFO] Exclude: **/README.markdown
[INFO] Exclude: **/DEVELOPER.md
[INFO] Exclude: **/BYLAWS.md
[INFO] Exclude: **/STORM-UI-REST-API.md
[INFO] Exclude: SECURITY.md
[INFO] Exclude: VERSION
[INFO] Exclude: TODO
[INFO] Exclude: **/src/py/**
[INFO] Exclude: **/src/ui/public/js/jquery.dataTables.1.10.4.min.js
[INFO] Exclude: **/src/ui/public/css/jquery.dataTables.1.10.4.min.css
[INFO] Exclude: **/src/ui/public/images/*
[INFO] Exclude: **/src/ui/public/js/bootstrap-3.3.1.min.js
[INFO] Exclude: **/src/ui/public/css/bootstrap-3.3.1.min.css
[INFO] Exclude: **/src/ui/public/js/dataTables.bootstrap.min.js
[INFO] Exclude: **/src/ui/public/css/dataTables.bootstrap.css
[INFO] Exclude: **/src/ui/public/js/jsonFormatter.min.js
[INFO] Exclude: **/src/ui/public/css/jsonFormatter.min.css
[INFO] Exclude: **/src/ui/public/js/jquery-1.11.1.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.cookies.2.2.0.min.js
[INFO] Exclude: **/src/ui/public/js/moment.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.blockUI.min.js
[INFO] Exclude: **/src/ui/public/js/url.min.js
[INFO] Exclude: **/src/ui/public/js/arbor.js
[INFO] Exclude: **/src/ui/public/js/arbor-graphics.js
[INFO] Exclude: **/src/ui/public/js/arbor-tween.js
[INFO] Exclude: **/src/ui/public/js/jquery.mustache.js
[INFO] Exclude: **/src/ui/public/js/typeahead.jquery.min.js
[INFO] Exclude: **/dependency-reduced-pom.xml
[INFO] Exclude: **/docs/**
[INFO] Exclude: **/.git/**
[INFO] Exclude: **/derby.log
[INFO] Exclude: **/src/dev/**
[INFO] Exclude: **/src/codegen/config.fmpp
[INFO] Exclude: **/src/codegen/data/Parser.tdd
[INFO] Exclude: **/src/test/resources/FixedAvroSerializer.config
[INFO] 28 resources included (use -debug for more details)
Warning:  org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
Compiler warnings:
  WARNING:  'org.apache.xerces.jaxp.SAXParserImpl: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.'
Warning:  org.apache.xerces.parsers.SAXParser: Feature 'http://javax.xml.XMLConstants/feature/secure-processing' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
[INFO] Rat check: Summary of files. Unapproved: 0 unknown: 0 generated: 0 approved: 28 licence.
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building storm-solr 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ storm-solr ---
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ storm-solr ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/storm-solr/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ storm-solr ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ storm-solr ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/storm-solr/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ storm-solr ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:2.18.1:test (default-test) @ storm-solr ---
[INFO] Surefire report directory: /home/travis/build/apache/storm/external/storm-solr/target/surefire-reports
[INFO] parallel='none', perCoreThreadCount=true, threadCount=0, useUnlimitedThreads=false, threadCountSuites=0, threadCountClasses=0, threadCountMethods=0, parallelOptimized=true

-------------------------------------------------------
 T E S T S
-------------------------------------------------------

Results :

Tests run: 0, Failures: 0, Errors: 0, Skipped: 0

[INFO] 
[INFO] --- apache-rat-plugin:0.11:check (default) @ storm-solr ---
[INFO] 51 implicit excludes (use -debug for more details).
[INFO] Exclude: **/target/**
[INFO] Exclude: **/*.iml
[INFO] Exclude: **/.idea/**
[INFO] Exclude: **/metastore_db/**
[INFO] Exclude: **/build/**
[INFO] Exclude: **/logs/**
[INFO] Exclude: **/CHANGELOG.md
[INFO] Exclude: **/README.md
[INFO] Exclude: **/README.markdown
[INFO] Exclude: **/DEVELOPER.md
[INFO] Exclude: **/BYLAWS.md
[INFO] Exclude: **/STORM-UI-REST-API.md
[INFO] Exclude: SECURITY.md
[INFO] Exclude: VERSION
[INFO] Exclude: TODO
[INFO] Exclude: **/src/py/**
[INFO] Exclude: **/src/ui/public/js/jquery.dataTables.1.10.4.min.js
[INFO] Exclude: **/src/ui/public/css/jquery.dataTables.1.10.4.min.css
[INFO] Exclude: **/src/ui/public/images/*
[INFO] Exclude: **/src/ui/public/js/bootstrap-3.3.1.min.js
[INFO] Exclude: **/src/ui/public/css/bootstrap-3.3.1.min.css
[INFO] Exclude: **/src/ui/public/js/dataTables.bootstrap.min.js
[INFO] Exclude: **/src/ui/public/css/dataTables.bootstrap.css
[INFO] Exclude: **/src/ui/public/js/jsonFormatter.min.js
[INFO] Exclude: **/src/ui/public/css/jsonFormatter.min.css
[INFO] Exclude: **/src/ui/public/js/jquery-1.11.1.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.cookies.2.2.0.min.js
[INFO] Exclude: **/src/ui/public/js/moment.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.blockUI.min.js
[INFO] Exclude: **/src/ui/public/js/url.min.js
[INFO] Exclude: **/src/ui/public/js/arbor.js
[INFO] Exclude: **/src/ui/public/js/arbor-graphics.js
[INFO] Exclude: **/src/ui/public/js/arbor-tween.js
[INFO] Exclude: **/src/ui/public/js/jquery.mustache.js
[INFO] Exclude: **/src/ui/public/js/typeahead.jquery.min.js
[INFO] Exclude: **/dependency-reduced-pom.xml
[INFO] Exclude: **/docs/**
[INFO] Exclude: **/.git/**
[INFO] Exclude: **/derby.log
[INFO] Exclude: **/src/dev/**
[INFO] Exclude: **/src/codegen/config.fmpp
[INFO] Exclude: **/src/codegen/data/Parser.tdd
[INFO] Exclude: **/src/test/resources/FixedAvroSerializer.config
[INFO] 27 resources included (use -debug for more details)
Warning:  org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
Compiler warnings:
  WARNING:  'org.apache.xerces.jaxp.SAXParserImpl: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.'
Warning:  org.apache.xerces.parsers.SAXParser: Feature 'http://javax.xml.XMLConstants/feature/secure-processing' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
[INFO] Rat check: Summary of files. Unapproved: 0 unknown: 0 generated: 0 approved: 27 licence.
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building storm-metrics 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-antrun-plugin:1.6:run (prepare) @ storm-metrics ---
[WARNING] Parameter tasks is deprecated, use target instead
[INFO] Executing tasks

main:
     [echo] Downloading sigar native binaries...
      [get] Destination already exists (skipping): /home/travis/.m2/repository/org/fusesource/sigar/1.6.4/hyperic-sigar-1.6.4.zip
    [unzip] Expanding: /home/travis/.m2/repository/org/fusesource/sigar/1.6.4/hyperic-sigar-1.6.4.zip into /home/travis/build/apache/storm/external/storm-metrics/target/classes/resources
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ storm-metrics ---
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ storm-metrics ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/storm-metrics/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ storm-metrics ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ storm-metrics ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/storm-metrics/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ storm-metrics ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-surefire-plugin:2.18.1:test (default-test) @ storm-metrics ---
[INFO] 
[INFO] --- apache-rat-plugin:0.11:check (default) @ storm-metrics ---
[INFO] 51 implicit excludes (use -debug for more details).
[INFO] Exclude: **/target/**
[INFO] Exclude: **/*.iml
[INFO] Exclude: **/.idea/**
[INFO] Exclude: **/metastore_db/**
[INFO] Exclude: **/build/**
[INFO] Exclude: **/logs/**
[INFO] Exclude: **/CHANGELOG.md
[INFO] Exclude: **/README.md
[INFO] Exclude: **/README.markdown
[INFO] Exclude: **/DEVELOPER.md
[INFO] Exclude: **/BYLAWS.md
[INFO] Exclude: **/STORM-UI-REST-API.md
[INFO] Exclude: SECURITY.md
[INFO] Exclude: VERSION
[INFO] Exclude: TODO
[INFO] Exclude: **/src/py/**
[INFO] Exclude: **/src/ui/public/js/jquery.dataTables.1.10.4.min.js
[INFO] Exclude: **/src/ui/public/css/jquery.dataTables.1.10.4.min.css
[INFO] Exclude: **/src/ui/public/images/*
[INFO] Exclude: **/src/ui/public/js/bootstrap-3.3.1.min.js
[INFO] Exclude: **/src/ui/public/css/bootstrap-3.3.1.min.css
[INFO] Exclude: **/src/ui/public/js/dataTables.bootstrap.min.js
[INFO] Exclude: **/src/ui/public/css/dataTables.bootstrap.css
[INFO] Exclude: **/src/ui/public/js/jsonFormatter.min.js
[INFO] Exclude: **/src/ui/public/css/jsonFormatter.min.css
[INFO] Exclude: **/src/ui/public/js/jquery-1.11.1.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.cookies.2.2.0.min.js
[INFO] Exclude: **/src/ui/public/js/moment.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.blockUI.min.js
[INFO] Exclude: **/src/ui/public/js/url.min.js
[INFO] Exclude: **/src/ui/public/js/arbor.js
[INFO] Exclude: **/src/ui/public/js/arbor-graphics.js
[INFO] Exclude: **/src/ui/public/js/arbor-tween.js
[INFO] Exclude: **/src/ui/public/js/jquery.mustache.js
[INFO] Exclude: **/src/ui/public/js/typeahead.jquery.min.js
[INFO] Exclude: **/dependency-reduced-pom.xml
[INFO] Exclude: **/docs/**
[INFO] Exclude: **/.git/**
[INFO] Exclude: **/derby.log
[INFO] Exclude: **/src/dev/**
[INFO] Exclude: **/src/codegen/config.fmpp
[INFO] Exclude: **/src/codegen/data/Parser.tdd
[INFO] Exclude: **/src/test/resources/FixedAvroSerializer.config
[INFO] 3 resources included (use -debug for more details)
Warning:  org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
Compiler warnings:
  WARNING:  'org.apache.xerces.jaxp.SAXParserImpl: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.'
Warning:  org.apache.xerces.parsers.SAXParser: Feature 'http://javax.xml.XMLConstants/feature/secure-processing' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
[INFO] Rat check: Summary of files. Unapproved: 0 unknown: 0 generated: 0 approved: 3 licence.
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building storm-cassandra 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ storm-cassandra ---
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ storm-cassandra ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/storm-cassandra/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ storm-cassandra ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ storm-cassandra ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 1 resource
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ storm-cassandra ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:2.18.1:test (default-test) @ storm-cassandra ---
[INFO] 
[INFO] --- apache-rat-plugin:0.11:check (default) @ storm-cassandra ---
[INFO] 51 implicit excludes (use -debug for more details).
[INFO] Exclude: **/target/**
[INFO] Exclude: **/*.iml
[INFO] Exclude: **/.idea/**
[INFO] Exclude: **/metastore_db/**
[INFO] Exclude: **/build/**
[INFO] Exclude: **/logs/**
[INFO] Exclude: **/CHANGELOG.md
[INFO] Exclude: **/README.md
[INFO] Exclude: **/README.markdown
[INFO] Exclude: **/DEVELOPER.md
[INFO] Exclude: **/BYLAWS.md
[INFO] Exclude: **/STORM-UI-REST-API.md
[INFO] Exclude: SECURITY.md
[INFO] Exclude: VERSION
[INFO] Exclude: TODO
[INFO] Exclude: **/src/py/**
[INFO] Exclude: **/src/ui/public/js/jquery.dataTables.1.10.4.min.js
[INFO] Exclude: **/src/ui/public/css/jquery.dataTables.1.10.4.min.css
[INFO] Exclude: **/src/ui/public/images/*
[INFO] Exclude: **/src/ui/public/js/bootstrap-3.3.1.min.js
[INFO] Exclude: **/src/ui/public/css/bootstrap-3.3.1.min.css
[INFO] Exclude: **/src/ui/public/js/dataTables.bootstrap.min.js
[INFO] Exclude: **/src/ui/public/css/dataTables.bootstrap.css
[INFO] Exclude: **/src/ui/public/js/jsonFormatter.min.js
[INFO] Exclude: **/src/ui/public/css/jsonFormatter.min.css
[INFO] Exclude: **/src/ui/public/js/jquery-1.11.1.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.cookies.2.2.0.min.js
[INFO] Exclude: **/src/ui/public/js/moment.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.blockUI.min.js
[INFO] Exclude: **/src/ui/public/js/url.min.js
[INFO] Exclude: **/src/ui/public/js/arbor.js
[INFO] Exclude: **/src/ui/public/js/arbor-graphics.js
[INFO] Exclude: **/src/ui/public/js/arbor-tween.js
[INFO] Exclude: **/src/ui/public/js/jquery.mustache.js
[INFO] Exclude: **/src/ui/public/js/typeahead.jquery.min.js
[INFO] Exclude: **/dependency-reduced-pom.xml
[INFO] Exclude: **/docs/**
[INFO] Exclude: **/.git/**
[INFO] Exclude: **/derby.log
[INFO] Exclude: **/src/dev/**
[INFO] Exclude: **/src/codegen/config.fmpp
[INFO] Exclude: **/src/codegen/data/Parser.tdd
[INFO] Exclude: **/src/test/resources/FixedAvroSerializer.config
[INFO] 50 resources included (use -debug for more details)
Warning:  org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
Compiler warnings:
  WARNING:  'org.apache.xerces.jaxp.SAXParserImpl: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.'
Warning:  org.apache.xerces.parsers.SAXParser: Feature 'http://javax.xml.XMLConstants/feature/secure-processing' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
[INFO] Rat check: Summary of files. Unapproved: 0 unknown: 0 generated: 0 approved: 50 licence.
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building storm-mqtt-parent 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ storm-mqtt-parent ---
[INFO] 
[INFO] --- apache-rat-plugin:0.11:check (default) @ storm-mqtt-parent ---
[INFO] 51 implicit excludes (use -debug for more details).
[INFO] Exclude: **/target/**
[INFO] Exclude: **/*.iml
[INFO] Exclude: **/.idea/**
[INFO] Exclude: **/metastore_db/**
[INFO] Exclude: **/build/**
[INFO] Exclude: **/logs/**
[INFO] Exclude: **/CHANGELOG.md
[INFO] Exclude: **/README.md
[INFO] Exclude: **/README.markdown
[INFO] Exclude: **/DEVELOPER.md
[INFO] Exclude: **/BYLAWS.md
[INFO] Exclude: **/STORM-UI-REST-API.md
[INFO] Exclude: SECURITY.md
[INFO] Exclude: VERSION
[INFO] Exclude: TODO
[INFO] Exclude: **/src/py/**
[INFO] Exclude: **/src/ui/public/js/jquery.dataTables.1.10.4.min.js
[INFO] Exclude: **/src/ui/public/css/jquery.dataTables.1.10.4.min.css
[INFO] Exclude: **/src/ui/public/images/*
[INFO] Exclude: **/src/ui/public/js/bootstrap-3.3.1.min.js
[INFO] Exclude: **/src/ui/public/css/bootstrap-3.3.1.min.css
[INFO] Exclude: **/src/ui/public/js/dataTables.bootstrap.min.js
[INFO] Exclude: **/src/ui/public/css/dataTables.bootstrap.css
[INFO] Exclude: **/src/ui/public/js/jsonFormatter.min.js
[INFO] Exclude: **/src/ui/public/css/jsonFormatter.min.css
[INFO] Exclude: **/src/ui/public/js/jquery-1.11.1.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.cookies.2.2.0.min.js
[INFO] Exclude: **/src/ui/public/js/moment.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.blockUI.min.js
[INFO] Exclude: **/src/ui/public/js/url.min.js
[INFO] Exclude: **/src/ui/public/js/arbor.js
[INFO] Exclude: **/src/ui/public/js/arbor-graphics.js
[INFO] Exclude: **/src/ui/public/js/arbor-tween.js
[INFO] Exclude: **/src/ui/public/js/jquery.mustache.js
[INFO] Exclude: **/src/ui/public/js/typeahead.jquery.min.js
[INFO] Exclude: **/dependency-reduced-pom.xml
[INFO] Exclude: **/docs/**
[INFO] Exclude: **/.git/**
[INFO] Exclude: **/derby.log
[INFO] Exclude: **/src/dev/**
[INFO] Exclude: **/src/codegen/config.fmpp
[INFO] Exclude: **/src/codegen/data/Parser.tdd
[INFO] Exclude: **/src/test/resources/FixedAvroSerializer.config
[INFO] 25 resources included (use -debug for more details)
Warning:  org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
Compiler warnings:
  WARNING:  'org.apache.xerces.jaxp.SAXParserImpl: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.'
Warning:  org.apache.xerces.parsers.SAXParser: Feature 'http://javax.xml.XMLConstants/feature/secure-processing' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
[INFO] Rat check: Summary of files. Unapproved: 0 unknown: 0 generated: 0 approved: 25 licence.
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building storm-mqtt 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ storm-mqtt ---
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ storm-mqtt ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/storm-mqtt/core/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ storm-mqtt ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ storm-mqtt ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/storm-mqtt/core/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ storm-mqtt ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:2.18.1:test (default-test) @ storm-mqtt ---
[WARNING] The parameter forkMode is deprecated since version 2.14. Use forkCount and reuseForks instead.
[INFO] Surefire report directory: /home/travis/build/apache/storm/external/storm-mqtt/core/target/surefire-reports
[INFO] parallel='none', perCoreThreadCount=true, threadCount=0, useUnlimitedThreads=false, threadCountSuites=0, threadCountClasses=0, threadCountMethods=0, parallelOptimized=true

-------------------------------------------------------
 T E S T S
-------------------------------------------------------

Results :

Tests run: 0, Failures: 0, Errors: 0, Skipped: 0

[INFO] 
[INFO] --- apache-rat-plugin:0.11:check (default) @ storm-mqtt ---
[INFO] 51 implicit excludes (use -debug for more details).
[INFO] Exclude: **/target/**
[INFO] Exclude: **/*.iml
[INFO] Exclude: **/.idea/**
[INFO] Exclude: **/metastore_db/**
[INFO] Exclude: **/build/**
[INFO] Exclude: **/logs/**
[INFO] Exclude: **/CHANGELOG.md
[INFO] Exclude: **/README.md
[INFO] Exclude: **/README.markdown
[INFO] Exclude: **/DEVELOPER.md
[INFO] Exclude: **/BYLAWS.md
[INFO] Exclude: **/STORM-UI-REST-API.md
[INFO] Exclude: SECURITY.md
[INFO] Exclude: VERSION
[INFO] Exclude: TODO
[INFO] Exclude: **/src/py/**
[INFO] Exclude: **/src/ui/public/js/jquery.dataTables.1.10.4.min.js
[INFO] Exclude: **/src/ui/public/css/jquery.dataTables.1.10.4.min.css
[INFO] Exclude: **/src/ui/public/images/*
[INFO] Exclude: **/src/ui/public/js/bootstrap-3.3.1.min.js
[INFO] Exclude: **/src/ui/public/css/bootstrap-3.3.1.min.css
[INFO] Exclude: **/src/ui/public/js/dataTables.bootstrap.min.js
[INFO] Exclude: **/src/ui/public/css/dataTables.bootstrap.css
[INFO] Exclude: **/src/ui/public/js/jsonFormatter.min.js
[INFO] Exclude: **/src/ui/public/css/jsonFormatter.min.css
[INFO] Exclude: **/src/ui/public/js/jquery-1.11.1.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.cookies.2.2.0.min.js
[INFO] Exclude: **/src/ui/public/js/moment.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.blockUI.min.js
[INFO] Exclude: **/src/ui/public/js/url.min.js
[INFO] Exclude: **/src/ui/public/js/arbor.js
[INFO] Exclude: **/src/ui/public/js/arbor-graphics.js
[INFO] Exclude: **/src/ui/public/js/arbor-tween.js
[INFO] Exclude: **/src/ui/public/js/jquery.mustache.js
[INFO] Exclude: **/src/ui/public/js/typeahead.jquery.min.js
[INFO] Exclude: **/dependency-reduced-pom.xml
[INFO] Exclude: **/docs/**
[INFO] Exclude: **/.git/**
[INFO] Exclude: **/derby.log
[INFO] Exclude: **/src/dev/**
[INFO] Exclude: **/src/codegen/config.fmpp
[INFO] Exclude: **/src/codegen/data/Parser.tdd
[INFO] Exclude: **/src/test/resources/FixedAvroSerializer.config
[INFO] 18 resources included (use -debug for more details)
Warning:  org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
Compiler warnings:
  WARNING:  'org.apache.xerces.jaxp.SAXParserImpl: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.'
Warning:  org.apache.xerces.parsers.SAXParser: Feature 'http://javax.xml.XMLConstants/feature/secure-processing' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
[INFO] Rat check: Summary of files. Unapproved: 0 unknown: 0 generated: 0 approved: 18 licence.
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Skipping storm-mqtt-examples
[INFO] This project has been banned from the build due to previous failures.
[INFO] ------------------------------------------------------------------------
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building storm-mongodb 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ storm-mongodb ---
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ storm-mongodb ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/storm-mongodb/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ storm-mongodb ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ storm-mongodb ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/storm-mongodb/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ storm-mongodb ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:2.18.1:test (default-test) @ storm-mongodb ---
[INFO] 
[INFO] --- apache-rat-plugin:0.11:check (default) @ storm-mongodb ---
[INFO] 51 implicit excludes (use -debug for more details).
[INFO] Exclude: **/target/**
[INFO] Exclude: **/*.iml
[INFO] Exclude: **/.idea/**
[INFO] Exclude: **/metastore_db/**
[INFO] Exclude: **/build/**
[INFO] Exclude: **/logs/**
[INFO] Exclude: **/CHANGELOG.md
[INFO] Exclude: **/README.md
[INFO] Exclude: **/README.markdown
[INFO] Exclude: **/DEVELOPER.md
[INFO] Exclude: **/BYLAWS.md
[INFO] Exclude: **/STORM-UI-REST-API.md
[INFO] Exclude: SECURITY.md
[INFO] Exclude: VERSION
[INFO] Exclude: TODO
[INFO] Exclude: **/src/py/**
[INFO] Exclude: **/src/ui/public/js/jquery.dataTables.1.10.4.min.js
[INFO] Exclude: **/src/ui/public/css/jquery.dataTables.1.10.4.min.css
[INFO] Exclude: **/src/ui/public/images/*
[INFO] Exclude: **/src/ui/public/js/bootstrap-3.3.1.min.js
[INFO] Exclude: **/src/ui/public/css/bootstrap-3.3.1.min.css
[INFO] Exclude: **/src/ui/public/js/dataTables.bootstrap.min.js
[INFO] Exclude: **/src/ui/public/css/dataTables.bootstrap.css
[INFO] Exclude: **/src/ui/public/js/jsonFormatter.min.js
[INFO] Exclude: **/src/ui/public/css/jsonFormatter.min.css
[INFO] Exclude: **/src/ui/public/js/jquery-1.11.1.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.cookies.2.2.0.min.js
[INFO] Exclude: **/src/ui/public/js/moment.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.blockUI.min.js
[INFO] Exclude: **/src/ui/public/js/url.min.js
[INFO] Exclude: **/src/ui/public/js/arbor.js
[INFO] Exclude: **/src/ui/public/js/arbor-graphics.js
[INFO] Exclude: **/src/ui/public/js/arbor-tween.js
[INFO] Exclude: **/src/ui/public/js/jquery.mustache.js
[INFO] Exclude: **/src/ui/public/js/typeahead.jquery.min.js
[INFO] Exclude: **/dependency-reduced-pom.xml
[INFO] Exclude: **/docs/**
[INFO] Exclude: **/.git/**
[INFO] Exclude: **/derby.log
[INFO] Exclude: **/src/dev/**
[INFO] Exclude: **/src/codegen/config.fmpp
[INFO] Exclude: **/src/codegen/data/Parser.tdd
[INFO] Exclude: **/src/test/resources/FixedAvroSerializer.config
[INFO] 18 resources included (use -debug for more details)
Warning:  org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
Compiler warnings:
  WARNING:  'org.apache.xerces.jaxp.SAXParserImpl: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.'
Warning:  org.apache.xerces.parsers.SAXParser: Feature 'http://javax.xml.XMLConstants/feature/secure-processing' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
[INFO] Rat check: Summary of files. Unapproved: 0 unknown: 0 generated: 0 approved: 18 licence.
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building storm-clojure 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ storm-clojure ---
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ storm-clojure ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/storm-clojure/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ storm-clojure ---
[INFO] No sources to compile
[INFO] 
[INFO] --- clojure-maven-plugin:1.7.1:compile (compile) @ storm-clojure ---
[INFO] 
[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ storm-clojure ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/storm-clojure/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ storm-clojure ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-surefire-plugin:2.18.1:test (default-test) @ storm-clojure ---
[INFO] 
[INFO] --- apache-rat-plugin:0.11:check (default) @ storm-clojure ---
[INFO] 51 implicit excludes (use -debug for more details).
[INFO] Exclude: **/target/**
[INFO] Exclude: **/*.iml
[INFO] Exclude: **/.idea/**
[INFO] Exclude: **/metastore_db/**
[INFO] Exclude: **/build/**
[INFO] Exclude: **/logs/**
[INFO] Exclude: **/CHANGELOG.md
[INFO] Exclude: **/README.md
[INFO] Exclude: **/README.markdown
[INFO] Exclude: **/DEVELOPER.md
[INFO] Exclude: **/BYLAWS.md
[INFO] Exclude: **/STORM-UI-REST-API.md
[INFO] Exclude: SECURITY.md
[INFO] Exclude: VERSION
[INFO] Exclude: TODO
[INFO] Exclude: **/src/py/**
[INFO] Exclude: **/src/ui/public/js/jquery.dataTables.1.10.4.min.js
[INFO] Exclude: **/src/ui/public/css/jquery.dataTables.1.10.4.min.css
[INFO] Exclude: **/src/ui/public/images/*
[INFO] Exclude: **/src/ui/public/js/bootstrap-3.3.1.min.js
[INFO] Exclude: **/src/ui/public/css/bootstrap-3.3.1.min.css
[INFO] Exclude: **/src/ui/public/js/dataTables.bootstrap.min.js
[INFO] Exclude: **/src/ui/public/css/dataTables.bootstrap.css
[INFO] Exclude: **/src/ui/public/js/jsonFormatter.min.js
[INFO] Exclude: **/src/ui/public/css/jsonFormatter.min.css
[INFO] Exclude: **/src/ui/public/js/jquery-1.11.1.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.cookies.2.2.0.min.js
[INFO] Exclude: **/src/ui/public/js/moment.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.blockUI.min.js
[INFO] Exclude: **/src/ui/public/js/url.min.js
[INFO] Exclude: **/src/ui/public/js/arbor.js
[INFO] Exclude: **/src/ui/public/js/arbor-graphics.js
[INFO] Exclude: **/src/ui/public/js/arbor-tween.js
[INFO] Exclude: **/src/ui/public/js/jquery.mustache.js
[INFO] Exclude: **/src/ui/public/js/typeahead.jquery.min.js
[INFO] Exclude: **/dependency-reduced-pom.xml
[INFO] Exclude: **/docs/**
[INFO] Exclude: **/.git/**
[INFO] Exclude: **/derby.log
[INFO] Exclude: **/src/dev/**
[INFO] Exclude: **/src/codegen/config.fmpp
[INFO] Exclude: **/src/codegen/data/Parser.tdd
[INFO] Exclude: **/src/test/resources/FixedAvroSerializer.config
[INFO] 4 resources included (use -debug for more details)
Warning:  org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
Compiler warnings:
  WARNING:  'org.apache.xerces.jaxp.SAXParserImpl: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.'
Warning:  org.apache.xerces.parsers.SAXParser: Feature 'http://javax.xml.XMLConstants/feature/secure-processing' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
[INFO] Rat check: Summary of files. Unapproved: 0 unknown: 0 generated: 0 approved: 4 licence.
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Skipping storm-starter
[INFO] This project has been banned from the build due to previous failures.
[INFO] ------------------------------------------------------------------------
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building storm-kafka-client 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ storm-kafka-client ---
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ storm-kafka-client ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/storm-kafka-client/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ storm-kafka-client ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ storm-kafka-client ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/storm-kafka-client/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ storm-kafka-client ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:2.18.1:test (default-test) @ storm-kafka-client ---
[INFO] 
[INFO] --- apache-rat-plugin:0.11:check (default) @ storm-kafka-client ---
[INFO] 51 implicit excludes (use -debug for more details).
[INFO] Exclude: **/target/**
[INFO] Exclude: **/*.iml
[INFO] Exclude: **/.idea/**
[INFO] Exclude: **/metastore_db/**
[INFO] Exclude: **/build/**
[INFO] Exclude: **/logs/**
[INFO] Exclude: **/CHANGELOG.md
[INFO] Exclude: **/README.md
[INFO] Exclude: **/README.markdown
[INFO] Exclude: **/DEVELOPER.md
[INFO] Exclude: **/BYLAWS.md
[INFO] Exclude: **/STORM-UI-REST-API.md
[INFO] Exclude: SECURITY.md
[INFO] Exclude: VERSION
[INFO] Exclude: TODO
[INFO] Exclude: **/src/py/**
[INFO] Exclude: **/src/ui/public/js/jquery.dataTables.1.10.4.min.js
[INFO] Exclude: **/src/ui/public/css/jquery.dataTables.1.10.4.min.css
[INFO] Exclude: **/src/ui/public/images/*
[INFO] Exclude: **/src/ui/public/js/bootstrap-3.3.1.min.js
[INFO] Exclude: **/src/ui/public/css/bootstrap-3.3.1.min.css
[INFO] Exclude: **/src/ui/public/js/dataTables.bootstrap.min.js
[INFO] Exclude: **/src/ui/public/css/dataTables.bootstrap.css
[INFO] Exclude: **/src/ui/public/js/jsonFormatter.min.js
[INFO] Exclude: **/src/ui/public/css/jsonFormatter.min.css
[INFO] Exclude: **/src/ui/public/js/jquery-1.11.1.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.cookies.2.2.0.min.js
[INFO] Exclude: **/src/ui/public/js/moment.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.blockUI.min.js
[INFO] Exclude: **/src/ui/public/js/url.min.js
[INFO] Exclude: **/src/ui/public/js/arbor.js
[INFO] Exclude: **/src/ui/public/js/arbor-graphics.js
[INFO] Exclude: **/src/ui/public/js/arbor-tween.js
[INFO] Exclude: **/src/ui/public/js/jquery.mustache.js
[INFO] Exclude: **/src/ui/public/js/typeahead.jquery.min.js
[INFO] Exclude: **/dependency-reduced-pom.xml
[INFO] Exclude: **/docs/**
[INFO] Exclude: **/.git/**
[INFO] Exclude: **/derby.log
[INFO] Exclude: **/src/dev/**
[INFO] Exclude: **/src/codegen/config.fmpp
[INFO] Exclude: **/src/codegen/data/Parser.tdd
[INFO] Exclude: **/src/test/resources/FixedAvroSerializer.config
[INFO] 14 resources included (use -debug for more details)
Warning:  org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
Compiler warnings:
  WARNING:  'org.apache.xerces.jaxp.SAXParserImpl: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.'
Warning:  org.apache.xerces.parsers.SAXParser: Feature 'http://javax.xml.XMLConstants/feature/secure-processing' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
[INFO] Rat check: Summary of files. Unapproved: 0 unknown: 0 generated: 0 approved: 14 licence.
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building storm-opentsdb 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ storm-opentsdb ---
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ storm-opentsdb ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/storm-opentsdb/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ storm-opentsdb ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ storm-opentsdb ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/storm-opentsdb/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ storm-opentsdb ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:2.18.1:test (default-test) @ storm-opentsdb ---
[INFO] 
[INFO] --- apache-rat-plugin:0.11:check (default) @ storm-opentsdb ---
[INFO] 51 implicit excludes (use -debug for more details).
[INFO] Exclude: **/target/**
[INFO] Exclude: **/*.iml
[INFO] Exclude: **/.idea/**
[INFO] Exclude: **/metastore_db/**
[INFO] Exclude: **/build/**
[INFO] Exclude: **/logs/**
[INFO] Exclude: **/CHANGELOG.md
[INFO] Exclude: **/README.md
[INFO] Exclude: **/README.markdown
[INFO] Exclude: **/DEVELOPER.md
[INFO] Exclude: **/BYLAWS.md
[INFO] Exclude: **/STORM-UI-REST-API.md
[INFO] Exclude: SECURITY.md
[INFO] Exclude: VERSION
[INFO] Exclude: TODO
[INFO] Exclude: **/src/py/**
[INFO] Exclude: **/src/ui/public/js/jquery.dataTables.1.10.4.min.js
[INFO] Exclude: **/src/ui/public/css/jquery.dataTables.1.10.4.min.css
[INFO] Exclude: **/src/ui/public/images/*
[INFO] Exclude: **/src/ui/public/js/bootstrap-3.3.1.min.js
[INFO] Exclude: **/src/ui/public/css/bootstrap-3.3.1.min.css
[INFO] Exclude: **/src/ui/public/js/dataTables.bootstrap.min.js
[INFO] Exclude: **/src/ui/public/css/dataTables.bootstrap.css
[INFO] Exclude: **/src/ui/public/js/jsonFormatter.min.js
[INFO] Exclude: **/src/ui/public/css/jsonFormatter.min.css
[INFO] Exclude: **/src/ui/public/js/jquery-1.11.1.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.cookies.2.2.0.min.js
[INFO] Exclude: **/src/ui/public/js/moment.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.blockUI.min.js
[INFO] Exclude: **/src/ui/public/js/url.min.js
[INFO] Exclude: **/src/ui/public/js/arbor.js
[INFO] Exclude: **/src/ui/public/js/arbor-graphics.js
[INFO] Exclude: **/src/ui/public/js/arbor-tween.js
[INFO] Exclude: **/src/ui/public/js/jquery.mustache.js
[INFO] Exclude: **/src/ui/public/js/typeahead.jquery.min.js
[INFO] Exclude: **/dependency-reduced-pom.xml
[INFO] Exclude: **/docs/**
[INFO] Exclude: **/.git/**
[INFO] Exclude: **/derby.log
[INFO] Exclude: **/src/dev/**
[INFO] Exclude: **/src/codegen/config.fmpp
[INFO] Exclude: **/src/codegen/data/Parser.tdd
[INFO] Exclude: **/src/test/resources/FixedAvroSerializer.config
[INFO] 14 resources included (use -debug for more details)
Warning:  org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
Compiler warnings:
  WARNING:  'org.apache.xerces.jaxp.SAXParserImpl: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.'
Warning:  org.apache.xerces.parsers.SAXParser: Feature 'http://javax.xml.XMLConstants/feature/secure-processing' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
[INFO] Rat check: Summary of files. Unapproved: 0 unknown: 0 generated: 0 approved: 14 licence.
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building storm-kafka-monitor 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ storm-kafka-monitor ---
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ storm-kafka-monitor ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/storm-kafka-monitor/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ storm-kafka-monitor ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ storm-kafka-monitor ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/storm-kafka-monitor/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ storm-kafka-monitor ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-surefire-plugin:2.18.1:test (default-test) @ storm-kafka-monitor ---
[INFO] 
[INFO] --- apache-rat-plugin:0.11:check (default) @ storm-kafka-monitor ---
[INFO] 51 implicit excludes (use -debug for more details).
[INFO] Exclude: **/target/**
[INFO] Exclude: **/*.iml
[INFO] Exclude: **/.idea/**
[INFO] Exclude: **/metastore_db/**
[INFO] Exclude: **/build/**
[INFO] Exclude: **/logs/**
[INFO] Exclude: **/CHANGELOG.md
[INFO] Exclude: **/README.md
[INFO] Exclude: **/README.markdown
[INFO] Exclude: **/DEVELOPER.md
[INFO] Exclude: **/BYLAWS.md
[INFO] Exclude: **/STORM-UI-REST-API.md
[INFO] Exclude: SECURITY.md
[INFO] Exclude: VERSION
[INFO] Exclude: TODO
[INFO] Exclude: **/src/py/**
[INFO] Exclude: **/src/ui/public/js/jquery.dataTables.1.10.4.min.js
[INFO] Exclude: **/src/ui/public/css/jquery.dataTables.1.10.4.min.css
[INFO] Exclude: **/src/ui/public/images/*
[INFO] Exclude: **/src/ui/public/js/bootstrap-3.3.1.min.js
[INFO] Exclude: **/src/ui/public/css/bootstrap-3.3.1.min.css
[INFO] Exclude: **/src/ui/public/js/dataTables.bootstrap.min.js
[INFO] Exclude: **/src/ui/public/css/dataTables.bootstrap.css
[INFO] Exclude: **/src/ui/public/js/jsonFormatter.min.js
[INFO] Exclude: **/src/ui/public/css/jsonFormatter.min.css
[INFO] Exclude: **/src/ui/public/js/jquery-1.11.1.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.cookies.2.2.0.min.js
[INFO] Exclude: **/src/ui/public/js/moment.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.blockUI.min.js
[INFO] Exclude: **/src/ui/public/js/url.min.js
[INFO] Exclude: **/src/ui/public/js/arbor.js
[INFO] Exclude: **/src/ui/public/js/arbor-graphics.js
[INFO] Exclude: **/src/ui/public/js/arbor-tween.js
[INFO] Exclude: **/src/ui/public/js/jquery.mustache.js
[INFO] Exclude: **/src/ui/public/js/typeahead.jquery.min.js
[INFO] Exclude: **/dependency-reduced-pom.xml
[INFO] Exclude: **/docs/**
[INFO] Exclude: **/.git/**
[INFO] Exclude: **/derby.log
[INFO] Exclude: **/src/dev/**
[INFO] Exclude: **/src/codegen/config.fmpp
[INFO] Exclude: **/src/codegen/data/Parser.tdd
[INFO] Exclude: **/src/test/resources/FixedAvroSerializer.config
[INFO] 5 resources included (use -debug for more details)
Warning:  org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
Compiler warnings:
  WARNING:  'org.apache.xerces.jaxp.SAXParserImpl: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.'
Warning:  org.apache.xerces.parsers.SAXParser: Feature 'http://javax.xml.XMLConstants/feature/secure-processing' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
[INFO] Rat check: Summary of files. Unapproved: 0 unknown: 0 generated: 0 approved: 5 licence.
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO] 
[INFO] Storm .............................................. SUCCESS [  7.017 s]
[INFO] multilang-javascript ............................... SUCCESS [  1.193 s]
[INFO] multilang-python ................................... SUCCESS [  0.180 s]
[INFO] multilang-ruby ..................................... SUCCESS [  0.191 s]
[INFO] maven-shade-clojure-transformer .................... SUCCESS [  3.006 s]
[INFO] storm-maven-plugins ................................ SUCCESS [  4.873 s]
[INFO] storm-rename-hack .................................. SUCCESS [  3.243 s]
[INFO] storm-kafka ........................................ FAILURE [01:12 min]
[INFO] storm-hdfs ......................................... FAILURE [01:42 min]
[INFO] storm-hbase ........................................ SUCCESS [  8.621 s]
[INFO] storm-hive ......................................... FAILURE [ 45.220 s]
[INFO] storm-jdbc ......................................... SUCCESS [  2.748 s]
[INFO] storm-redis ........................................ SUCCESS [  2.520 s]
[INFO] storm-eventhubs .................................... SUCCESS [  3.780 s]
[INFO] flux ............................................... SUCCESS [  0.150 s]
[INFO] flux-wrappers ...................................... SUCCESS [  0.548 s]
[INFO] flux-core .......................................... SKIPPED
[INFO] flux-examples ...................................... SKIPPED
[INFO] storm-sql-runtime .................................. SUCCESS [  0.477 s]
[INFO] storm-sql-core ..................................... SUCCESS [01:14 min]
[INFO] storm-sql-kafka .................................... SKIPPED
[INFO] sql ................................................ SUCCESS [  0.145 s]
[INFO] storm-elasticsearch ................................ SUCCESS [  3.658 s]
[INFO] storm-solr ......................................... SUCCESS [  4.034 s]
[INFO] storm-metrics ...................................... SUCCESS [  1.350 s]
[INFO] storm-cassandra .................................... SUCCESS [  0.651 s]
[INFO] storm-mqtt-parent .................................. SUCCESS [  0.328 s]
[INFO] storm-mqtt ......................................... SUCCESS [  2.944 s]
[INFO] storm-mqtt-examples ................................ SKIPPED
[INFO] storm-mongodb ...................................... SUCCESS [  0.244 s]
[INFO] storm-clojure ...................................... SUCCESS [  3.603 s]
[INFO] storm-starter ...................................... SKIPPED
[INFO] storm-kafka-client ................................. SUCCESS [  0.190 s]
[INFO] storm-opentsdb ..................................... SUCCESS [  1.585 s]
[INFO] storm-kafka-monitor ................................ SUCCESS [  0.247 s]
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 05:55 min
[INFO] Finished at: 2016-06-24T14:57:05+00:00
[INFO] Final Memory: 83M/457M
[INFO] ------------------------------------------------------------------------
[WARNING] The requested profile "native" could not be activated because it does not exist.
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:2.18.1:test (default-test) on project storm-kafka: There are test failures.
[ERROR] 
[ERROR] Please refer to /home/travis/build/apache/storm/external/storm-kafka/target/surefire-reports for the individual test results.
[ERROR] -> [Help 1]
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:2.18.1:test (default-test) on project storm-hdfs: There are test failures.
[ERROR] 
[ERROR] Please refer to /home/travis/build/apache/storm/external/storm-hdfs/target/surefire-reports for the individual test results.
[ERROR] -> [Help 1]
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:2.18.1:test (default-test) on project storm-hive: There are test failures.
[ERROR] 
[ERROR] Please refer to /home/travis/build/apache/storm/external/storm-hive/target/surefire-reports for the individual test results.
[ERROR] -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException
[ERROR] 
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn <goals> -rf :storm-kafka
Looking for errors in ./external/sql/storm-sql-core/target/surefire-reports
Checking ./external/sql/storm-sql-core/target/surefire-reports/TEST-org.apache.storm.sql.parser.TestSqlParser.xml
Checking ./external/sql/storm-sql-core/target/surefire-reports/TEST-org.apache.storm.sql.compiler.backends.standalone.TestRelNodeCompiler.xml
Checking ./external/sql/storm-sql-core/target/surefire-reports/TEST-org.apache.storm.sql.compiler.backends.standalone.TestPlanCompiler.xml
Checking ./external/sql/storm-sql-core/target/surefire-reports/TEST-org.apache.storm.sql.compiler.backends.trident.TestPlanCompiler.xml
Checking ./external/sql/storm-sql-core/target/surefire-reports/TEST-org.apache.storm.sql.compiler.TestExprCompiler.xml
Checking ./external/sql/storm-sql-core/target/surefire-reports/TEST-org.apache.storm.sql.compiler.TestExprSemantic.xml
Checking ./external/sql/storm-sql-core/target/surefire-reports/TEST-org.apache.storm.sql.TestStormSql.xml
Looking for errors in ./external/storm-elasticsearch/target/surefire-reports
Checking ./external/storm-elasticsearch/target/surefire-reports/TEST-org.apache.storm.elasticsearch.bolt.EsLookupBoltTest.xml
Checking ./external/storm-elasticsearch/target/surefire-reports/TEST-org.apache.storm.elasticsearch.trident.EsStateFactoryTest.xml
Checking ./external/storm-elasticsearch/target/surefire-reports/TEST-org.apache.storm.elasticsearch.common.EsConfigTest.xml
Checking ./external/storm-elasticsearch/target/surefire-reports/TEST-org.apache.storm.elasticsearch.common.TransportAddressesTest.xml
Looking for errors in ./external/storm-eventhubs/target/surefire-reports
Checking ./external/storm-eventhubs/target/surefire-reports/TEST-org.apache.storm.eventhubs.trident.TestTransactionalTridentEmitter.xml
Checking ./external/storm-eventhubs/target/surefire-reports/TEST-org.apache.storm.eventhubs.spout.TestPartitionManager.xml
Checking ./external/storm-eventhubs/target/surefire-reports/TEST-org.apache.storm.eventhubs.spout.TestEventData.xml
Checking ./external/storm-eventhubs/target/surefire-reports/TEST-org.apache.storm.eventhubs.spout.TestEventHubSpout.xml
Looking for errors in ./external/storm-hdfs/target/surefire-reports
Checking ./external/storm-hdfs/target/surefire-reports/TEST-org.apache.storm.hdfs.trident.HdfsStateTest.xml
Checking ./external/storm-hdfs/target/surefire-reports/TEST-org.apache.storm.hdfs.trident.format.TestSimpleFileNameFormat.xml
Checking ./external/storm-hdfs/target/surefire-reports/TEST-org.apache.storm.hdfs.bolt.TestWritersMap.xml
Checking ./external/storm-hdfs/target/surefire-reports/TEST-org.apache.storm.hdfs.bolt.TestHdfsBolt.xml
--------------------------------------------------
classname: org.apache.storm.hdfs.bolt.TestHdfsBolt / testname: testTwoTuplesTwoFiles
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hdfs.bolt.TestHdfsBolt.generateTestTuple(TestHdfsBolt.java:243)
	at org.apache.storm.hdfs.bolt.TestHdfsBolt.<init>(TestHdfsBolt.java:69)

--------------------------------------------------
--------------------------------------------------
classname: org.apache.storm.hdfs.bolt.TestHdfsBolt / testname: testPartitionedOutput
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hdfs.bolt.TestHdfsBolt.generateTestTuple(TestHdfsBolt.java:243)
	at org.apache.storm.hdfs.bolt.TestHdfsBolt.<init>(TestHdfsBolt.java:69)

--------------------------------------------------
--------------------------------------------------
classname: org.apache.storm.hdfs.bolt.TestHdfsBolt / testname: testTwoTuplesOneFile
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hdfs.bolt.TestHdfsBolt.generateTestTuple(TestHdfsBolt.java:243)
	at org.apache.storm.hdfs.bolt.TestHdfsBolt.<init>(TestHdfsBolt.java:69)

--------------------------------------------------
--------------------------------------------------
classname: org.apache.storm.hdfs.bolt.TestHdfsBolt / testname: testFailedSync
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hdfs.bolt.TestHdfsBolt.generateTestTuple(TestHdfsBolt.java:243)
	at org.apache.storm.hdfs.bolt.TestHdfsBolt.<init>(TestHdfsBolt.java:69)

--------------------------------------------------
--------------------------------------------------
classname: org.apache.storm.hdfs.bolt.TestHdfsBolt / testname: testTickTuples
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hdfs.bolt.TestHdfsBolt.generateTestTuple(TestHdfsBolt.java:243)
	at org.apache.storm.hdfs.bolt.TestHdfsBolt.<init>(TestHdfsBolt.java:69)

--------------------------------------------------
--------------------------------------------------
classname: org.apache.storm.hdfs.bolt.TestHdfsBolt / testname: testFailureFilecount
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hdfs.bolt.TestHdfsBolt.generateTestTuple(TestHdfsBolt.java:243)
	at org.apache.storm.hdfs.bolt.TestHdfsBolt.<init>(TestHdfsBolt.java:69)

--------------------------------------------------
Checking ./external/storm-hdfs/target/surefire-reports/TEST-org.apache.storm.hdfs.bolt.AvroGenericRecordBoltTest.xml
--------------------------------------------------
classname: org.apache.storm.hdfs.bolt.AvroGenericRecordBoltTest / testname: schemaThrashing
java.lang.ExceptionInInitializerError: null
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hdfs.bolt.AvroGenericRecordBoltTest.generateTestTuple(AvroGenericRecordBoltTest.java:220)
	at org.apache.storm.hdfs.bolt.AvroGenericRecordBoltTest.<clinit>(AvroGenericRecordBoltTest.java:95)

--------------------------------------------------
--------------------------------------------------
classname: org.apache.storm.hdfs.bolt.AvroGenericRecordBoltTest / testname: forwardSchemaChangeWorks
java.lang.NoClassDefFoundError: Could not initialize class org.apache.storm.hdfs.bolt.AvroGenericRecordBoltTest
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:408)
	at org.junit.runners.BlockJUnit4ClassRunner.createTest(BlockJUnit4ClassRunner.java:195)
	at org.junit.runners.BlockJUnit4ClassRunner$1.runReflectiveCall(BlockJUnit4ClassRunner.java:244)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.BlockJUnit4ClassRunner.methodBlock(BlockJUnit4ClassRunner.java:241)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:108)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:78)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:54)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:144)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:203)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:155)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:103)

--------------------------------------------------
--------------------------------------------------
classname: org.apache.storm.hdfs.bolt.AvroGenericRecordBoltTest / testname: multipleTuplesOneFile
java.lang.NoClassDefFoundError: Could not initialize class org.apache.storm.hdfs.bolt.AvroGenericRecordBoltTest
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:408)
	at org.junit.runners.BlockJUnit4ClassRunner.createTest(BlockJUnit4ClassRunner.java:195)
	at org.junit.runners.BlockJUnit4ClassRunner$1.runReflectiveCall(BlockJUnit4ClassRunner.java:244)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.BlockJUnit4ClassRunner.methodBlock(BlockJUnit4ClassRunner.java:241)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:108)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:78)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:54)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:144)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:203)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:155)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:103)

--------------------------------------------------
--------------------------------------------------
classname: org.apache.storm.hdfs.bolt.AvroGenericRecordBoltTest / testname: multipleTuplesMutliplesFiles
java.lang.NoClassDefFoundError: Could not initialize class org.apache.storm.hdfs.bolt.AvroGenericRecordBoltTest
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:408)
	at org.junit.runners.BlockJUnit4ClassRunner.createTest(BlockJUnit4ClassRunner.java:195)
	at org.junit.runners.BlockJUnit4ClassRunner$1.runReflectiveCall(BlockJUnit4ClassRunner.java:244)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.BlockJUnit4ClassRunner.methodBlock(BlockJUnit4ClassRunner.java:241)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:108)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:78)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:54)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:144)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:203)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:155)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:103)

--------------------------------------------------
--------------------------------------------------
classname: org.apache.storm.hdfs.bolt.AvroGenericRecordBoltTest / testname: backwardSchemaChangeWorks
java.lang.NoClassDefFoundError: Could not initialize class org.apache.storm.hdfs.bolt.AvroGenericRecordBoltTest
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:408)
	at org.junit.runners.BlockJUnit4ClassRunner.createTest(BlockJUnit4ClassRunner.java:195)
	at org.junit.runners.BlockJUnit4ClassRunner$1.runReflectiveCall(BlockJUnit4ClassRunner.java:244)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.BlockJUnit4ClassRunner.methodBlock(BlockJUnit4ClassRunner.java:241)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:108)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:78)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:54)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:144)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:203)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:155)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:103)

--------------------------------------------------
Checking ./external/storm-hdfs/target/surefire-reports/TEST-org.apache.storm.hdfs.bolt.format.TestSimpleFileNameFormat.xml
Checking ./external/storm-hdfs/target/surefire-reports/TEST-org.apache.storm.hdfs.bolt.TestSequenceFileBolt.xml
--------------------------------------------------
classname: org.apache.storm.hdfs.bolt.TestSequenceFileBolt / testname: testTwoTuplesTwoFiles
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hdfs.bolt.TestSequenceFileBolt.generateTestTuple(TestSequenceFileBolt.java:161)
	at org.apache.storm.hdfs.bolt.TestSequenceFileBolt.<init>(TestSequenceFileBolt.java:68)

--------------------------------------------------
--------------------------------------------------
classname: org.apache.storm.hdfs.bolt.TestSequenceFileBolt / testname: testTwoTuplesOneFile
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hdfs.bolt.TestSequenceFileBolt.generateTestTuple(TestSequenceFileBolt.java:161)
	at org.apache.storm.hdfs.bolt.TestSequenceFileBolt.<init>(TestSequenceFileBolt.java:68)

--------------------------------------------------
--------------------------------------------------
classname: org.apache.storm.hdfs.bolt.TestSequenceFileBolt / testname: testFailedSync
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hdfs.bolt.TestSequenceFileBolt.generateTestTuple(TestSequenceFileBolt.java:161)
	at org.apache.storm.hdfs.bolt.TestSequenceFileBolt.<init>(TestSequenceFileBolt.java:68)

--------------------------------------------------
Checking ./external/storm-hdfs/target/surefire-reports/TEST-org.apache.storm.hdfs.spout.TestDirLock.xml
Checking ./external/storm-hdfs/target/surefire-reports/TEST-org.apache.storm.hdfs.spout.TestHdfsSemantics.xml
Checking ./external/storm-hdfs/target/surefire-reports/TEST-org.apache.storm.hdfs.spout.TestProgressTracker.xml
Checking ./external/storm-hdfs/target/surefire-reports/TEST-org.apache.storm.hdfs.spout.TestFileLock.xml
Checking ./external/storm-hdfs/target/surefire-reports/TEST-org.apache.storm.hdfs.spout.TestHdfsSpout.xml
Checking ./external/storm-hdfs/target/surefire-reports/TEST-org.apache.storm.hdfs.blobstore.HdfsBlobStoreImplTest.xml
Checking ./external/storm-hdfs/target/surefire-reports/TEST-org.apache.storm.hdfs.blobstore.BlobStoreTest.xml
Checking ./external/storm-hdfs/target/surefire-reports/TEST-org.apache.storm.hdfs.avro.TestFixedAvroSerializer.xml
Checking ./external/storm-hdfs/target/surefire-reports/TEST-org.apache.storm.hdfs.avro.TestGenericAvroSerializer.xml
Looking for errors in ./external/storm-hive/target/surefire-reports
Checking ./external/storm-hive/target/surefire-reports/TEST-org.apache.storm.hive.bolt.TestHiveBolt.xml
--------------------------------------------------
classname: org.apache.storm.hive.bolt.TestHiveBolt / testname: testMultiPartitionTuples
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hive.bolt.TestHiveBolt.generateTestTuple(TestHiveBolt.java:447)
	at org.apache.storm.hive.bolt.TestHiveBolt.testMultiPartitionTuples(TestHiveBolt.java:411)

-------------------- system-out --------------------
6511 [main] WARN  o.a.h.u.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
6630 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
6661 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
6867 [main] INFO  D.Persistence - Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
6867 [main] INFO  D.Persistence - Property datanucleus.cache.level2 unknown - will be ignored
7931 [main] INFO  o.a.h.h.m.ObjectStore - Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
8081 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
9820 [main] INFO  D.Datastore - The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
9821 [main] INFO  D.Datastore - The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
11775 [main] INFO  D.Datastore - The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
11775 [main] INFO  D.Datastore - The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
12422 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
12565 [main] WARN  o.a.h.h.m.ObjectStore - Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 0.14.0
12776 [main] WARN  o.a.h.h.m.ObjectStore - Failed to get database default, returning NoSuchObjectException
13094 [main] INFO  o.a.h.h.m.HiveMetaStore - Added admin role in metastore
13098 [main] INFO  o.a.h.h.m.HiveMetaStore - Added public role in metastore
13234 [main] INFO  o.a.h.h.m.HiveMetaStore - No user is added in admin role, since config is empty
13446 [main] INFO  o.a.h.h.q.s.SessionState - Created HDFS directory: /tmp/hive/travis
13469 [main] INFO  o.a.h.h.q.s.SessionState - Created local directory: /tmp/travis
13473 [main] INFO  o.a.h.h.q.s.SessionState - Created local directory: /tmp/c5b11d40-793e-4c43-9766-98a36f7453af_resources
13476 [main] INFO  o.a.h.h.q.s.SessionState - Created HDFS directory: /tmp/hive/travis/c5b11d40-793e-4c43-9766-98a36f7453af
13484 [main] INFO  o.a.h.h.q.s.SessionState - Created local directory: /tmp/travis/c5b11d40-793e-4c43-9766-98a36f7453af
13488 [main] INFO  o.a.h.h.q.s.SessionState - Created HDFS directory: /tmp/hive/travis/c5b11d40-793e-4c43-9766-98a36f7453af/_tmp_space.db
13489 [main] INFO  o.a.h.h.q.s.SessionState - No Tez session required at this point. hive.execution.engine=mr.
13622 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
13623 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
13630 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
13631 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
13631 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_table_names_by_filter: db = testdb, filter = 
13632 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table_names_by_filter: db = testdb, filter = 	
13667 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_database: testdb
13667 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_database: testdb	
13668 [main] WARN  o.a.h.h.m.ObjectStore - Failed to get database testdb, returning NoSuchObjectException
13670 [main] ERROR o.a.h.h.m.RetryingHMSHandler - NoSuchObjectException(message:testdb)
	at org.apache.hadoop.hive.metastore.ObjectStore.getDatabase(ObjectStore.java:539)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:483)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:98)
	at com.sun.proxy.$Proxy24.getDatabase(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_database_core(HiveMetaStore.java:914)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_database(HiveMetaStore.java:888)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:483)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:102)
	at com.sun.proxy.$Proxy26.get_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getDatabase(HiveMetaStoreClient.java:1043)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:657)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:644)
	at org.apache.storm.hive.bolt.HiveSetupUtil.dropDB(HiveSetupUtil.java:166)
	at org.apache.storm.hive.bolt.TestHiveBolt.setup(TestHiveBolt.java:119)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:483)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:108)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:78)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:54)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:144)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:203)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:155)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:103)

13671 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Shutting down the object store...
13671 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=Shutting down the object store...	
13671 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Metastore shutdown complete.
13671 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=Metastore shutdown complete.	
13672 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: create_database: Database(name:testdb, description:null, locationUri:raw:///tmp/junit6631721644367953094/testdb.db, parameters:null)
13672 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=create_database: Database(name:testdb, description:null, locationUri:raw:///tmp/junit6631721644367953094/testdb.db, parameters:null)	
13672 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
13674 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
13675 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
13677 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
13678 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
13680 [main] WARN  o.a.h.h.m.ObjectStore - Failed to get database testdb, returning NoSuchObjectException
13699 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: create_table: Table(tableName:test_table, dbName:testdb, owner:null, createTime:0, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:), FieldSchema(name:msg, type:string, comment:)], location:raw:///tmp/junit6631721644367953094/testdb.db/test_table, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:1, serdeInfo:SerDeInfo(name:test_table, serializationLib:org.apache.hadoop.hive.ql.io.orc.OrcSerde, parameters:{serialization.format=1}), bucketCols:[], sortCols:null, parameters:null), partitionKeys:[FieldSchema(name:city, type:string, comment:), FieldSchema(name:state, type:string, comment:)], parameters:{}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)
13699 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=create_table: Table(tableName:test_table, dbName:testdb, owner:null, createTime:0, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:), FieldSchema(name:msg, type:string, comment:)], location:raw:///tmp/junit6631721644367953094/testdb.db/test_table, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:1, serdeInfo:SerDeInfo(name:test_table, serializationLib:org.apache.hadoop.hive.ql.io.orc.OrcSerde, parameters:{serialization.format=1}), bucketCols:[], sortCols:null, parameters:null), partitionKeys:[FieldSchema(name:city, type:string, comment:), FieldSchema(name:state, type:string, comment:)], parameters:{}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
13723 [main] WARN  o.a.h.h.m.HiveMetaStore - Location: raw:///tmp/junit6631721644367953094/testdb.db/test_table specified for non-external table:test_table
13726 [main] INFO  o.a.h.h.c.FileUtils - Creating directory if it doesn't exist: raw:/tmp/junit6631721644367953094/testdb.db/test_table
14030 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: add_partition : db=testdb tbl=test_table
14030 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=add_partition : db=testdb tbl=test_table	
14184 [main] INFO  o.a.h.h.c.FileUtils - Creating directory if it doesn't exist: raw:/tmp/junit6631721644367953094/testdb.db/test_table/city=sunnyvale/state=ca
14321 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Shutting down the object store...
14321 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=Shutting down the object store...	
14322 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Metastore shutdown complete.
14322 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=Metastore shutdown complete.	
done
14329 [main] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=compile from=org.apache.hadoop.hive.ql.Driver>
14356 [main] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=parse from=org.apache.hadoop.hive.ql.Driver>
14359 [main] INFO  h.q.p.ParseDriver - Parsing command: select * from testdb.test_table
14693 [main] INFO  h.q.p.ParseDriver - Parse Completed
14695 [main] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=parse start=1466780100653 end=1466780100992 duration=339 from=org.apache.hadoop.hive.ql.Driver>
14745 [main] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=semanticAnalyze from=org.apache.hadoop.hive.ql.Driver>
14790 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Starting Semantic Analysis
14791 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Completed phase 1 of Semantic Analysis
14791 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Get metadata for source tables
14792 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_table : db=testdb tbl=test_table
14792 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table : db=testdb tbl=test_table	
14792 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
14793 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
14794 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
14796 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
14797 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
14845 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Get metadata for subqueries
14853 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Get metadata for destination tables
14869 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Completed getting MetaData in Semantic Analysis
14981 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Set stats collection dir : file:/tmp/travis/c5b11d40-793e-4c43-9766-98a36f7453af/hive_2016-06-24_14-55-00_652_48814799233923419-1/-ext-10002
15078 [main] INFO  o.a.h.h.q.p.OpProcFactory - Processing for FS(2)
15078 [main] INFO  o.a.h.h.q.p.OpProcFactory - Processing for SEL(1)
15079 [main] INFO  o.a.h.h.q.p.OpProcFactory - Processing for TS(0)
15101 [main] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=partition-retrieving from=org.apache.hadoop.hive.ql.optimizer.ppr.PartitionPruner>
15101 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_partitions : db=testdb tbl=test_table
15102 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_partitions : db=testdb tbl=test_table	
15172 [main] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=partition-retrieving start=1466780101398 end=1466780101469 duration=71 from=org.apache.hadoop.hive.ql.optimizer.ppr.PartitionPruner>
15210 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Completed plan generation
15211 [main] INFO  o.a.h.h.q.Driver - Semantic Analysis Completed
15211 [main] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=semanticAnalyze start=1466780101042 end=1466780101508 duration=466 from=org.apache.hadoop.hive.ql.Driver>
15233 [main] INFO  o.a.h.h.q.e.TableScanOperator - Initializing Self TS[0]
15233 [main] INFO  o.a.h.h.q.e.TableScanOperator - Operator 0 TS initialized
15233 [main] INFO  o.a.h.h.q.e.TableScanOperator - Initializing children of 0 TS
15233 [main] INFO  o.a.h.h.q.e.SelectOperator - Initializing child 1 SEL
15233 [main] INFO  o.a.h.h.q.e.SelectOperator - Initializing Self SEL[1]
15237 [main] INFO  o.a.h.h.q.e.SelectOperator - SELECT struct<id:int,msg:string,city:string,state:string>
15237 [main] INFO  o.a.h.h.q.e.SelectOperator - Operator 1 SEL initialized
15237 [main] INFO  o.a.h.h.q.e.SelectOperator - Initializing children of 1 SEL
15237 [main] INFO  o.a.h.h.q.e.ListSinkOperator - Initializing child 3 OP
15237 [main] INFO  o.a.h.h.q.e.ListSinkOperator - Initializing Self OP[3]
15240 [main] INFO  o.a.h.h.q.e.ListSinkOperator - Operator 3 OP initialized
15240 [main] INFO  o.a.h.h.q.e.ListSinkOperator - Initialization Done 3 OP
15240 [main] INFO  o.a.h.h.q.e.SelectOperator - Initialization Done 1 SEL
15240 [main] INFO  o.a.h.h.q.e.TableScanOperator - Initialization Done 0 TS
15243 [main] INFO  o.a.h.h.q.Driver - Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:test_table.id, type:int, comment:null), FieldSchema(name:test_table.msg, type:string, comment:null), FieldSchema(name:test_table.city, type:string, comment:null), FieldSchema(name:test_table.state, type:string, comment:null)], properties:null)
15243 [main] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=compile start=1466780100626 end=1466780101540 duration=914 from=org.apache.hadoop.hive.ql.Driver>

-------------------- system-err --------------------
Unable to drop index HL_TXNID_INDEX Index 'HL_TXNID_INDEX' does not exist.
Unable to drop table TXN_COMPONENTS: 'DROP TABLE' cannot be performed on 'TXN_COMPONENTS' because it does not exist.
Unable to drop table COMPLETED_TXN_COMPONENTS: 'DROP TABLE' cannot be performed on 'COMPLETED_TXN_COMPONENTS' because it does not exist.
Unable to drop table TXNS: 'DROP TABLE' cannot be performed on 'TXNS' because it does not exist.
Unable to drop table NEXT_TXN_ID: 'DROP TABLE' cannot be performed on 'NEXT_TXN_ID' because it does not exist.
Unable to drop table HIVE_LOCKS: 'DROP TABLE' cannot be performed on 'HIVE_LOCKS' because it does not exist.
Unable to drop table NEXT_LOCK_ID: 'DROP TABLE' cannot be performed on 'NEXT_LOCK_ID' because it does not exist.
Unable to drop table COMPACTION_QUEUE: 'DROP TABLE' cannot be performed on 'COMPACTION_QUEUE' because it does not exist.
Unable to drop table NEXT_COMPACTION_QUEUE_ID: 'DROP TABLE' cannot be performed on 'NEXT_COMPACTION_QUEUE_ID' because it does not exist.

--------------------------------------------------
--------------------------------------------------
classname: org.apache.storm.hive.bolt.TestHiveBolt / testname: testNoAcksUntilFlushed
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hive.bolt.TestHiveBolt.generateTestTuple(TestHiveBolt.java:447)
	at org.apache.storm.hive.bolt.TestHiveBolt.testNoAcksUntilFlushed(TestHiveBolt.java:297)

-------------------- system-out --------------------
15965 [main] INFO  o.a.h.h.q.s.SessionState - Created local directory: /tmp/b25a019c-4d50-4f64-8022-45043c27c93c_resources
15973 [main] INFO  o.a.h.h.q.s.SessionState - Created HDFS directory: /tmp/hive/travis/b25a019c-4d50-4f64-8022-45043c27c93c
15989 [main] INFO  o.a.h.h.q.s.SessionState - Created local directory: /tmp/travis/b25a019c-4d50-4f64-8022-45043c27c93c
16006 [main] INFO  o.a.h.h.q.s.SessionState - Created HDFS directory: /tmp/hive/travis/b25a019c-4d50-4f64-8022-45043c27c93c/_tmp_space.db
16006 [main] INFO  o.a.h.h.q.s.SessionState - No Tez session required at this point. hive.execution.engine=mr.
16009 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
16010 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
16011 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
16012 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
16012 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_table_names_by_filter: db = testdb, filter = 
16012 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table_names_by_filter: db = testdb, filter = 	
16033 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_table : db=testdb tbl=test_table
16033 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table : db=testdb tbl=test_table	
16048 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: drop_table : db=testdb tbl=test_table
16067 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=drop_table : db=testdb tbl=test_table	
16084 [main] INFO  D.Datastore - The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
16084 [main] INFO  D.Datastore - The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
16412 [main] INFO  D.Datastore - The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
16412 [main] INFO  D.Datastore - The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
16705 [main] INFO  D.Datastore - The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
16719 [main] INFO  D.Datastore - The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
17159 [main] INFO  D.Datastore - The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
17159 [main] INFO  D.Datastore - The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
17253 [main] INFO  D.Datastore - The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
17253 [main] INFO  D.Datastore - The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
17723 [main] INFO  h.m.hivemetastoressimpl - deleting  raw:/tmp/junit6631721644367953094/testdb.db/test_table
17723 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_database: testdb
17724 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_database: testdb	
17757 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: drop_database: testdb
17757 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=drop_database: testdb	
17759 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_all_tables: db=testdb
17759 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_all_tables: db=testdb	
17776 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_functions: db=testdb pat=*
17776 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_functions: db=testdb pat=*	
17780 [main] INFO  D.Datastore - The class "org.apache.hadoop.hive.metastore.model.MResourceUri" is tagged as "embedded-only" so does not have its own datastore table.
17924 [main] INFO  o.a.h.h.m.ObjectStore - Dropping database testdb along with all tables
18141 [main] INFO  h.m.hivemetastoressimpl - deleting  raw:/tmp/junit6631721644367953094/testdb.db
18142 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
18143 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
18179 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
18180 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
18180 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: create_database: Database(name:testdb, description:null, locationUri:raw:///tmp/junit3884178349047559963/testdb.db, parameters:null)
18180 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=create_database: Database(name:testdb, description:null, locationUri:raw:///tmp/junit3884178349047559963/testdb.db, parameters:null)	
18183 [main] WARN  o.a.h.h.m.ObjectStore - Failed to get database testdb, returning NoSuchObjectException
18233 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: create_table: Table(tableName:test_table, dbName:testdb, owner:null, createTime:0, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:), FieldSchema(name:msg, type:string, comment:)], location:raw:///tmp/junit3884178349047559963/testdb.db/test_table, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:1, serdeInfo:SerDeInfo(name:test_table, serializationLib:org.apache.hadoop.hive.ql.io.orc.OrcSerde, parameters:{serialization.format=1}), bucketCols:[], sortCols:null, parameters:null), partitionKeys:[FieldSchema(name:city, type:string, comment:), FieldSchema(name:state, type:string, comment:)], parameters:{}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)
18233 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=create_table: Table(tableName:test_table, dbName:testdb, owner:null, createTime:0, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:), FieldSchema(name:msg, type:string, comment:)], location:raw:///tmp/junit3884178349047559963/testdb.db/test_table, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:1, serdeInfo:SerDeInfo(name:test_table, serializationLib:org.apache.hadoop.hive.ql.io.orc.OrcSerde, parameters:{serialization.format=1}), bucketCols:[], sortCols:null, parameters:null), partitionKeys:[FieldSchema(name:city, type:string, comment:), FieldSchema(name:state, type:string, comment:)], parameters:{}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
18259 [main] WARN  o.a.h.h.m.HiveMetaStore - Location: raw:///tmp/junit3884178349047559963/testdb.db/test_table specified for non-external table:test_table
18285 [main] INFO  o.a.h.h.c.FileUtils - Creating directory if it doesn't exist: raw:/tmp/junit3884178349047559963/testdb.db/test_table
18542 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: add_partition : db=testdb tbl=test_table
18543 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=add_partition : db=testdb tbl=test_table	
18751 [main] INFO  o.a.h.h.c.FileUtils - Creating directory if it doesn't exist: raw:/tmp/junit3884178349047559963/testdb.db/test_table/city=sunnyvale/state=ca
18802 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Shutting down the object store...
18802 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=Shutting down the object store...	
18802 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Metastore shutdown complete.
18802 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=Metastore shutdown complete.	
done

--------------------------------------------------
--------------------------------------------------
classname: org.apache.storm.hive.bolt.TestHiveBolt / testname: testData
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hive.bolt.TestHiveBolt.generateTestTuple(TestHiveBolt.java:447)
	at org.apache.storm.hive.bolt.TestHiveBolt.testData(TestHiveBolt.java:251)

-------------------- system-out --------------------
19185 [main] INFO  o.a.h.h.q.s.SessionState - Created local directory: /tmp/cef77429-a7f7-4f9e-99f7-767600823052_resources
19188 [main] INFO  o.a.h.h.q.s.SessionState - Created HDFS directory: /tmp/hive/travis/cef77429-a7f7-4f9e-99f7-767600823052
19190 [main] INFO  o.a.h.h.q.s.SessionState - Created local directory: /tmp/travis/cef77429-a7f7-4f9e-99f7-767600823052
19220 [main] INFO  o.a.h.h.q.s.SessionState - Created HDFS directory: /tmp/hive/travis/cef77429-a7f7-4f9e-99f7-767600823052/_tmp_space.db
19221 [main] INFO  o.a.h.h.q.s.SessionState - No Tez session required at this point. hive.execution.engine=mr.
19222 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_table_names_by_filter: db = testdb, filter = 
19222 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table_names_by_filter: db = testdb, filter = 	
19222 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
19223 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
19224 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
19260 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
19273 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
19281 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_table : db=testdb tbl=test_table
19281 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table : db=testdb tbl=test_table	
19304 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: drop_table : db=testdb tbl=test_table
19312 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=drop_table : db=testdb tbl=test_table	
20195 [main] INFO  h.m.hivemetastoressimpl - deleting  raw:/tmp/junit3884178349047559963/testdb.db/test_table
20196 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_database: testdb
20196 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_database: testdb	
20202 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: drop_database: testdb
20202 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=drop_database: testdb	
20203 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_all_tables: db=testdb
20204 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_all_tables: db=testdb	
20207 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_functions: db=testdb pat=*
20207 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_functions: db=testdb pat=*	
21644 [main] INFO  o.a.h.h.m.ObjectStore - Dropping database testdb along with all tables
21668 [main] INFO  h.m.hivemetastoressimpl - deleting  raw:/tmp/junit3884178349047559963/testdb.db
21669 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
21670 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
21671 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
21672 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
21672 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: create_database: Database(name:testdb, description:null, locationUri:raw:///tmp/junit1108104806544683564/testdb.db, parameters:null)
21672 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=create_database: Database(name:testdb, description:null, locationUri:raw:///tmp/junit1108104806544683564/testdb.db, parameters:null)	
21673 [main] WARN  o.a.h.h.m.ObjectStore - Failed to get database testdb, returning NoSuchObjectException
21677 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: create_table: Table(tableName:test_table, dbName:testdb, owner:null, createTime:0, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:), FieldSchema(name:msg, type:string, comment:)], location:raw:///tmp/junit1108104806544683564/testdb.db/test_table, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:1, serdeInfo:SerDeInfo(name:test_table, serializationLib:org.apache.hadoop.hive.ql.io.orc.OrcSerde, parameters:{serialization.format=1}), bucketCols:[], sortCols:null, parameters:null), partitionKeys:[FieldSchema(name:city, type:string, comment:), FieldSchema(name:state, type:string, comment:)], parameters:{}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)
21677 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=create_table: Table(tableName:test_table, dbName:testdb, owner:null, createTime:0, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:), FieldSchema(name:msg, type:string, comment:)], location:raw:///tmp/junit1108104806544683564/testdb.db/test_table, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:1, serdeInfo:SerDeInfo(name:test_table, serializationLib:org.apache.hadoop.hive.ql.io.orc.OrcSerde, parameters:{serialization.format=1}), bucketCols:[], sortCols:null, parameters:null), partitionKeys:[FieldSchema(name:city, type:string, comment:), FieldSchema(name:state, type:string, comment:)], parameters:{}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
21678 [main] WARN  o.a.h.h.m.HiveMetaStore - Location: raw:///tmp/junit1108104806544683564/testdb.db/test_table specified for non-external table:test_table
21678 [main] INFO  o.a.h.h.c.FileUtils - Creating directory if it doesn't exist: raw:/tmp/junit1108104806544683564/testdb.db/test_table
21754 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: add_partition : db=testdb tbl=test_table
21755 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=add_partition : db=testdb tbl=test_table	
21815 [main] INFO  o.a.h.h.c.FileUtils - Creating directory if it doesn't exist: raw:/tmp/junit1108104806544683564/testdb.db/test_table/city=sunnyvale/state=ca
21919 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Shutting down the object store...
21919 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=Shutting down the object store...	
21919 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Metastore shutdown complete.
21919 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=Metastore shutdown complete.	
done

--------------------------------------------------
--------------------------------------------------
classname: org.apache.storm.hive.bolt.TestHiveBolt / testname: testWithoutPartitions
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hive.bolt.TestHiveBolt.generateTestTuple(TestHiveBolt.java:447)
	at org.apache.storm.hive.bolt.TestHiveBolt.testWithoutPartitions(TestHiveBolt.java:194)

-------------------- system-out --------------------
22389 [main] INFO  o.a.h.h.q.s.SessionState - Created local directory: /tmp/83dc9aa8-f0ce-4cc3-9045-c63b06b37620_resources
22402 [main] INFO  o.a.h.h.q.s.SessionState - Created HDFS directory: /tmp/hive/travis/83dc9aa8-f0ce-4cc3-9045-c63b06b37620
22425 [main] INFO  o.a.h.h.q.s.SessionState - Created local directory: /tmp/travis/83dc9aa8-f0ce-4cc3-9045-c63b06b37620
22446 [main] INFO  o.a.h.h.q.s.SessionState - Created HDFS directory: /tmp/hive/travis/83dc9aa8-f0ce-4cc3-9045-c63b06b37620/_tmp_space.db
22446 [main] INFO  o.a.h.h.q.s.SessionState - No Tez session required at this point. hive.execution.engine=mr.
22450 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_table_names_by_filter: db = testdb, filter = 
22450 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table_names_by_filter: db = testdb, filter = 	
22450 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
22451 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
22452 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
22455 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
22455 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
22473 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_table : db=testdb tbl=test_table
22473 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table : db=testdb tbl=test_table	
22492 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: drop_table : db=testdb tbl=test_table
22494 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=drop_table : db=testdb tbl=test_table	
23304 [main] INFO  h.m.hivemetastoressimpl - deleting  raw:/tmp/junit1108104806544683564/testdb.db/test_table
23305 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_database: testdb
23305 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_database: testdb	
23314 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: drop_database: testdb
23314 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=drop_database: testdb	
23318 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_all_tables: db=testdb
23318 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_all_tables: db=testdb	
23324 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_functions: db=testdb pat=*
23325 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_functions: db=testdb pat=*	
23344 [main] INFO  o.a.h.h.m.ObjectStore - Dropping database testdb along with all tables
23370 [main] INFO  h.m.hivemetastoressimpl - deleting  raw:/tmp/junit1108104806544683564/testdb.db
23373 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
23380 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
23410 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
23410 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
23411 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: create_database: Database(name:testdb, description:null, locationUri:raw:///tmp/junit1104605902549751506/testdb.db, parameters:null)
23412 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=create_database: Database(name:testdb, description:null, locationUri:raw:///tmp/junit1104605902549751506/testdb.db, parameters:null)	
23417 [main] WARN  o.a.h.h.m.ObjectStore - Failed to get database testdb, returning NoSuchObjectException
23426 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: create_table: Table(tableName:test_table, dbName:testdb, owner:null, createTime:0, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:), FieldSchema(name:msg, type:string, comment:)], location:raw:///tmp/junit1104605902549751506/testdb.db/test_table, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:1, serdeInfo:SerDeInfo(name:test_table, serializationLib:org.apache.hadoop.hive.ql.io.orc.OrcSerde, parameters:{serialization.format=1}), bucketCols:[], sortCols:null, parameters:null), partitionKeys:[FieldSchema(name:city, type:string, comment:), FieldSchema(name:state, type:string, comment:)], parameters:{}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)
23426 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=create_table: Table(tableName:test_table, dbName:testdb, owner:null, createTime:0, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:), FieldSchema(name:msg, type:string, comment:)], location:raw:///tmp/junit1104605902549751506/testdb.db/test_table, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:1, serdeInfo:SerDeInfo(name:test_table, serializationLib:org.apache.hadoop.hive.ql.io.orc.OrcSerde, parameters:{serialization.format=1}), bucketCols:[], sortCols:null, parameters:null), partitionKeys:[FieldSchema(name:city, type:string, comment:), FieldSchema(name:state, type:string, comment:)], parameters:{}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
23431 [main] WARN  o.a.h.h.m.HiveMetaStore - Location: raw:///tmp/junit1104605902549751506/testdb.db/test_table specified for non-external table:test_table
23433 [main] INFO  o.a.h.h.c.FileUtils - Creating directory if it doesn't exist: raw:/tmp/junit1104605902549751506/testdb.db/test_table
23532 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: add_partition : db=testdb tbl=test_table
23541 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=add_partition : db=testdb tbl=test_table	
23595 [main] INFO  o.a.h.h.c.FileUtils - Creating directory if it doesn't exist: raw:/tmp/junit1104605902549751506/testdb.db/test_table/city=sunnyvale/state=ca
23630 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Shutting down the object store...
23630 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=Shutting down the object store...	
23630 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Metastore shutdown complete.
23631 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=Metastore shutdown complete.	
done
23631 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_table_names_by_filter: db = testdb1, filter = 
23631 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table_names_by_filter: db = testdb1, filter = 	
23631 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
23632 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
23633 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
23634 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
23634 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
23636 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_database: testdb1
23636 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_database: testdb1	
23636 [main] WARN  o.a.h.h.m.ObjectStore - Failed to get database testdb1, returning NoSuchObjectException
23637 [main] ERROR o.a.h.h.m.RetryingHMSHandler - NoSuchObjectException(message:testdb1)
	at org.apache.hadoop.hive.metastore.ObjectStore.getDatabase(ObjectStore.java:539)
	at sun.reflect.GeneratedMethodAccessor10.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:483)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:98)
	at com.sun.proxy.$Proxy24.getDatabase(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_database_core(HiveMetaStore.java:914)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_database(HiveMetaStore.java:888)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:483)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:102)
	at com.sun.proxy.$Proxy26.get_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getDatabase(HiveMetaStoreClient.java:1043)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:657)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:644)
	at org.apache.storm.hive.bolt.HiveSetupUtil.dropDB(HiveSetupUtil.java:166)
	at org.apache.storm.hive.bolt.TestHiveBolt.testWithoutPartitions(TestHiveBolt.java:175)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:483)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:108)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:78)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:54)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:144)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:203)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:155)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:103)

23637 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Shutting down the object store...
23637 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=Shutting down the object store...	
23637 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Metastore shutdown complete.
23637 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=Metastore shutdown complete.	
23638 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: create_database: Database(name:testdb1, description:null, locationUri:raw:///tmp/junit1104605902549751506/testdb.db, parameters:null)
23638 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=create_database: Database(name:testdb1, description:null, locationUri:raw:///tmp/junit1104605902549751506/testdb.db, parameters:null)	
23638 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
23639 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
23639 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
23645 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
23645 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
23648 [main] WARN  o.a.h.h.m.ObjectStore - Failed to get database testdb1, returning NoSuchObjectException
23673 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: create_table: Table(tableName:test_table1, dbName:testdb1, owner:null, createTime:0, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:), FieldSchema(name:msg, type:string, comment:)], location:raw:///tmp/junit1104605902549751506/testdb.db/test_table1, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:1, serdeInfo:SerDeInfo(name:test_table1, serializationLib:org.apache.hadoop.hive.ql.io.orc.OrcSerde, parameters:{serialization.format=1}), bucketCols:[], sortCols:null, parameters:null), partitionKeys:null, parameters:{}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)
23674 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=create_table: Table(tableName:test_table1, dbName:testdb1, owner:null, createTime:0, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:), FieldSchema(name:msg, type:string, comment:)], location:raw:///tmp/junit1104605902549751506/testdb.db/test_table1, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:1, serdeInfo:SerDeInfo(name:test_table1, serializationLib:org.apache.hadoop.hive.ql.io.orc.OrcSerde, parameters:{serialization.format=1}), bucketCols:[], sortCols:null, parameters:null), partitionKeys:null, parameters:{}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
23678 [main] WARN  o.a.h.h.m.HiveMetaStore - Location: raw:///tmp/junit1104605902549751506/testdb.db/test_table1 specified for non-external table:test_table1
23678 [main] INFO  o.a.h.h.c.FileUtils - Creating directory if it doesn't exist: raw:/tmp/junit1104605902549751506/testdb.db/test_table1
23731 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Shutting down the object store...
23732 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=Shutting down the object store...	
23732 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Metastore shutdown complete.
23732 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=Metastore shutdown complete.	
23732 [main] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=compile from=org.apache.hadoop.hive.ql.Driver>
23733 [main] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=parse from=org.apache.hadoop.hive.ql.Driver>
23733 [main] INFO  h.q.p.ParseDriver - Parsing command: select * from testdb1.test_table1
23734 [main] INFO  h.q.p.ParseDriver - Parse Completed
23734 [main] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=parse start=1466780110030 end=1466780110031 duration=1 from=org.apache.hadoop.hive.ql.Driver>
23758 [main] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=semanticAnalyze from=org.apache.hadoop.hive.ql.Driver>
23758 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Starting Semantic Analysis
23758 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Completed phase 1 of Semantic Analysis
23758 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Get metadata for source tables
23759 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_table : db=testdb1 tbl=test_table1
23759 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table : db=testdb1 tbl=test_table1	
23759 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
23760 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
23761 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
23769 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
23770 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
23778 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Get metadata for subqueries
23808 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Get metadata for destination tables
23828 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Completed getting MetaData in Semantic Analysis
23829 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Set stats collection dir : file:/tmp/travis/83dc9aa8-f0ce-4cc3-9045-c63b06b37620/hive_2016-06-24_14-55-10_030_3320730121614872812-1/-ext-10002
23830 [main] INFO  o.a.h.h.q.p.OpProcFactory - Processing for FS(6)
23830 [main] INFO  o.a.h.h.q.p.OpProcFactory - Processing for SEL(5)
23830 [main] INFO  o.a.h.h.q.p.OpProcFactory - Processing for TS(4)
23832 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Completed plan generation
23832 [main] INFO  o.a.h.h.q.Driver - Semantic Analysis Completed
23832 [main] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=semanticAnalyze start=1466780110055 end=1466780110129 duration=74 from=org.apache.hadoop.hive.ql.Driver>
23832 [main] INFO  o.a.h.h.q.e.TableScanOperator - Initializing Self TS[4]
23832 [main] INFO  o.a.h.h.q.e.TableScanOperator - Operator 4 TS initialized
23832 [main] INFO  o.a.h.h.q.e.TableScanOperator - Initializing children of 4 TS
23832 [main] INFO  o.a.h.h.q.e.SelectOperator - Initializing child 5 SEL
23832 [main] INFO  o.a.h.h.q.e.SelectOperator - Initializing Self SEL[5]
23838 [main] INFO  o.a.h.h.q.e.SelectOperator - SELECT struct<id:int,msg:string>
23839 [main] INFO  o.a.h.h.q.e.SelectOperator - Operator 5 SEL initialized
23839 [main] INFO  o.a.h.h.q.e.SelectOperator - Initializing children of 5 SEL
23839 [main] INFO  o.a.h.h.q.e.ListSinkOperator - Initializing child 7 OP
23839 [main] INFO  o.a.h.h.q.e.ListSinkOperator - Initializing Self OP[7]
23839 [main] INFO  o.a.h.h.q.e.ListSinkOperator - Operator 7 OP initialized
23839 [main] INFO  o.a.h.h.q.e.ListSinkOperator - Initialization Done 7 OP
23839 [main] INFO  o.a.h.h.q.e.SelectOperator - Initialization Done 5 SEL
23839 [main] INFO  o.a.h.h.q.e.TableScanOperator - Initialization Done 4 TS
23839 [main] INFO  o.a.h.h.q.Driver - Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:test_table1.id, type:int, comment:null), FieldSchema(name:test_table1.msg, type:string, comment:null)], properties:null)
23839 [main] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=compile start=1466780110029 end=1466780110136 duration=107 from=org.apache.hadoop.hive.ql.Driver>

--------------------------------------------------
--------------------------------------------------
classname: org.apache.storm.hive.bolt.TestHiveBolt / testname: testJsonWriter
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hive.bolt.TestHiveBolt.generateTestTuple(TestHiveBolt.java:447)
	at org.apache.storm.hive.bolt.TestHiveBolt.testJsonWriter(TestHiveBolt.java:274)

-------------------- system-out --------------------
25269 [main] INFO  o.a.h.h.q.s.SessionState - Created local directory: /tmp/5c7d138b-4e80-4c63-9801-21e37c3f8e26_resources
25271 [main] INFO  o.a.h.h.q.s.SessionState - Created HDFS directory: /tmp/hive/travis/5c7d138b-4e80-4c63-9801-21e37c3f8e26
25275 [main] INFO  o.a.h.h.q.s.SessionState - Created local directory: /tmp/travis/5c7d138b-4e80-4c63-9801-21e37c3f8e26
25277 [main] INFO  o.a.h.h.q.s.SessionState - Created HDFS directory: /tmp/hive/travis/5c7d138b-4e80-4c63-9801-21e37c3f8e26/_tmp_space.db
25278 [main] INFO  o.a.h.h.q.s.SessionState - No Tez session required at this point. hive.execution.engine=mr.
25279 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_table_names_by_filter: db = testdb, filter = 
25279 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table_names_by_filter: db = testdb, filter = 	
25279 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
25280 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
25281 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
25283 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
25283 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
25297 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_table : db=testdb tbl=test_table
25297 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table : db=testdb tbl=test_table	
25330 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: drop_table : db=testdb tbl=test_table
25330 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=drop_table : db=testdb tbl=test_table	
25558 [main] INFO  h.m.hivemetastoressimpl - deleting  raw:/tmp/junit3521699689089105284/testdb.db/test_table
25558 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_database: testdb
25559 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_database: testdb	
25577 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: drop_database: testdb
25577 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=drop_database: testdb	
25578 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_all_tables: db=testdb
25578 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_all_tables: db=testdb	
25583 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_functions: db=testdb pat=*
25583 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_functions: db=testdb pat=*	
25597 [main] INFO  o.a.h.h.m.ObjectStore - Dropping database testdb along with all tables
25619 [main] INFO  h.m.hivemetastoressimpl - deleting  raw:/tmp/junit3521699689089105284/testdb.db
25620 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
25620 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
25623 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
25623 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
25624 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: create_database: Database(name:testdb, description:null, locationUri:raw:///tmp/junit206063848967774232/testdb.db, parameters:null)
25624 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=create_database: Database(name:testdb, description:null, locationUri:raw:///tmp/junit206063848967774232/testdb.db, parameters:null)	
25625 [main] WARN  o.a.h.h.m.ObjectStore - Failed to get database testdb, returning NoSuchObjectException
25629 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: create_table: Table(tableName:test_table, dbName:testdb, owner:null, createTime:0, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:), FieldSchema(name:msg, type:string, comment:)], location:raw:///tmp/junit206063848967774232/testdb.db/test_table, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:1, serdeInfo:SerDeInfo(name:test_table, serializationLib:org.apache.hadoop.hive.ql.io.orc.OrcSerde, parameters:{serialization.format=1}), bucketCols:[], sortCols:null, parameters:null), partitionKeys:[FieldSchema(name:city, type:string, comment:), FieldSchema(name:state, type:string, comment:)], parameters:{}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)
25629 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=create_table: Table(tableName:test_table, dbName:testdb, owner:null, createTime:0, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:), FieldSchema(name:msg, type:string, comment:)], location:raw:///tmp/junit206063848967774232/testdb.db/test_table, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:1, serdeInfo:SerDeInfo(name:test_table, serializationLib:org.apache.hadoop.hive.ql.io.orc.OrcSerde, parameters:{serialization.format=1}), bucketCols:[], sortCols:null, parameters:null), partitionKeys:[FieldSchema(name:city, type:string, comment:), FieldSchema(name:state, type:string, comment:)], parameters:{}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
25630 [main] WARN  o.a.h.h.m.HiveMetaStore - Location: raw:///tmp/junit206063848967774232/testdb.db/test_table specified for non-external table:test_table
25630 [main] INFO  o.a.h.h.c.FileUtils - Creating directory if it doesn't exist: raw:/tmp/junit206063848967774232/testdb.db/test_table
25674 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: add_partition : db=testdb tbl=test_table
25674 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=add_partition : db=testdb tbl=test_table	
25768 [main] INFO  o.a.h.h.c.FileUtils - Creating directory if it doesn't exist: raw:/tmp/junit206063848967774232/testdb.db/test_table/city=sunnyvale/state=ca
25808 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Shutting down the object store...
25808 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=Shutting down the object store...	
25808 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Metastore shutdown complete.
25808 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=Metastore shutdown complete.	
done

--------------------------------------------------
--------------------------------------------------
classname: org.apache.storm.hive.bolt.TestHiveBolt / testname: testTickTuple
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hive.bolt.TestHiveBolt.generateTestTuple(TestHiveBolt.java:447)
	at org.apache.storm.hive.bolt.TestHiveBolt.testTickTuple(TestHiveBolt.java:352)

-------------------- system-out --------------------
27491 [main] INFO  o.a.h.h.q.s.SessionState - Created local directory: /tmp/ca94216d-32c7-4715-922a-042fafb5bee5_resources
27493 [main] INFO  o.a.h.h.q.s.SessionState - Created HDFS directory: /tmp/hive/travis/ca94216d-32c7-4715-922a-042fafb5bee5
27494 [main] INFO  o.a.h.h.q.s.SessionState - Created local directory: /tmp/travis/ca94216d-32c7-4715-922a-042fafb5bee5
27496 [main] INFO  o.a.h.h.q.s.SessionState - Created HDFS directory: /tmp/hive/travis/ca94216d-32c7-4715-922a-042fafb5bee5/_tmp_space.db
27496 [main] INFO  o.a.h.h.q.s.SessionState - No Tez session required at this point. hive.execution.engine=mr.
27497 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_table_names_by_filter: db = testdb, filter = 
27498 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table_names_by_filter: db = testdb, filter = 	
27498 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
27498 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
27499 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
27500 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
27501 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
27507 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_table : db=testdb tbl=test_table
27507 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table : db=testdb tbl=test_table	
27515 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: drop_table : db=testdb tbl=test_table
27516 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=drop_table : db=testdb tbl=test_table	
27734 [main] INFO  h.m.hivemetastoressimpl - deleting  raw:/tmp/junit8623162165602050541/testdb.db/test_table
27735 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_database: testdb
27735 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_database: testdb	
27741 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: drop_database: testdb
27741 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=drop_database: testdb	
27742 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_all_tables: db=testdb
27742 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_all_tables: db=testdb	
27745 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_functions: db=testdb pat=*
27745 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_functions: db=testdb pat=*	
27750 [main] INFO  o.a.h.h.m.ObjectStore - Dropping database testdb along with all tables
27769 [main] INFO  h.m.hivemetastoressimpl - deleting  raw:/tmp/junit8623162165602050541/testdb.db
27770 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
27771 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
27773 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
27773 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
27773 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: create_database: Database(name:testdb, description:null, locationUri:raw:///tmp/junit9146112252694876496/testdb.db, parameters:null)
27773 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=create_database: Database(name:testdb, description:null, locationUri:raw:///tmp/junit9146112252694876496/testdb.db, parameters:null)	
27774 [main] WARN  o.a.h.h.m.ObjectStore - Failed to get database testdb, returning NoSuchObjectException
27779 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: create_table: Table(tableName:test_table, dbName:testdb, owner:null, createTime:0, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:), FieldSchema(name:msg, type:string, comment:)], location:raw:///tmp/junit9146112252694876496/testdb.db/test_table, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:1, serdeInfo:SerDeInfo(name:test_table, serializationLib:org.apache.hadoop.hive.ql.io.orc.OrcSerde, parameters:{serialization.format=1}), bucketCols:[], sortCols:null, parameters:null), partitionKeys:[FieldSchema(name:city, type:string, comment:), FieldSchema(name:state, type:string, comment:)], parameters:{}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)
27779 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=create_table: Table(tableName:test_table, dbName:testdb, owner:null, createTime:0, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:), FieldSchema(name:msg, type:string, comment:)], location:raw:///tmp/junit9146112252694876496/testdb.db/test_table, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:1, serdeInfo:SerDeInfo(name:test_table, serializationLib:org.apache.hadoop.hive.ql.io.orc.OrcSerde, parameters:{serialization.format=1}), bucketCols:[], sortCols:null, parameters:null), partitionKeys:[FieldSchema(name:city, type:string, comment:), FieldSchema(name:state, type:string, comment:)], parameters:{}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
27781 [main] WARN  o.a.h.h.m.HiveMetaStore - Location: raw:///tmp/junit9146112252694876496/testdb.db/test_table specified for non-external table:test_table
27781 [main] INFO  o.a.h.h.c.FileUtils - Creating directory if it doesn't exist: raw:/tmp/junit9146112252694876496/testdb.db/test_table
27837 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: add_partition : db=testdb tbl=test_table
27838 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=add_partition : db=testdb tbl=test_table	
27899 [main] INFO  o.a.h.h.c.FileUtils - Creating directory if it doesn't exist: raw:/tmp/junit9146112252694876496/testdb.db/test_table/city=sunnyvale/state=ca
27925 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Shutting down the object store...
27925 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=Shutting down the object store...	
27925 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Metastore shutdown complete.
27925 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=Metastore shutdown complete.	
done

--------------------------------------------------
--------------------------------------------------
classname: org.apache.storm.hive.bolt.TestHiveBolt / testname: testWithTimeformat
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hive.bolt.TestHiveBolt.generateTestTuple(TestHiveBolt.java:447)
	at org.apache.storm.hive.bolt.TestHiveBolt.testWithTimeformat(TestHiveBolt.java:230)

-------------------- system-out --------------------
28294 [main] INFO  o.a.h.h.q.s.SessionState - Created local directory: /tmp/60186529-211e-487a-ae74-a9a7307218fe_resources
28296 [main] INFO  o.a.h.h.q.s.SessionState - Created HDFS directory: /tmp/hive/travis/60186529-211e-487a-ae74-a9a7307218fe
28298 [main] INFO  o.a.h.h.q.s.SessionState - Created local directory: /tmp/travis/60186529-211e-487a-ae74-a9a7307218fe
28306 [main] INFO  o.a.h.h.q.s.SessionState - Created HDFS directory: /tmp/hive/travis/60186529-211e-487a-ae74-a9a7307218fe/_tmp_space.db
28309 [main] INFO  o.a.h.h.q.s.SessionState - No Tez session required at this point. hive.execution.engine=mr.
28311 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_table_names_by_filter: db = testdb, filter = 
28311 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table_names_by_filter: db = testdb, filter = 	
28311 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
28312 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
28313 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
28317 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
28317 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
28329 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_table : db=testdb tbl=test_table
28344 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table : db=testdb tbl=test_table	
28354 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: drop_table : db=testdb tbl=test_table
28354 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=drop_table : db=testdb tbl=test_table	
28739 [main] INFO  h.m.hivemetastoressimpl - deleting  raw:/tmp/junit9146112252694876496/testdb.db/test_table
28739 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_database: testdb
28739 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_database: testdb	
28745 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: drop_database: testdb
28745 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=drop_database: testdb	
28746 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_all_tables: db=testdb
28746 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_all_tables: db=testdb	
28749 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_functions: db=testdb pat=*
28749 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_functions: db=testdb pat=*	
28753 [main] INFO  o.a.h.h.m.ObjectStore - Dropping database testdb along with all tables
28767 [main] INFO  h.m.hivemetastoressimpl - deleting  raw:/tmp/junit9146112252694876496/testdb.db
28769 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
28769 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
28776 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
28776 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
28777 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: create_database: Database(name:testdb, description:null, locationUri:raw:///tmp/junit4529692089802623592/testdb.db, parameters:null)
28777 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=create_database: Database(name:testdb, description:null, locationUri:raw:///tmp/junit4529692089802623592/testdb.db, parameters:null)	
28777 [main] WARN  o.a.h.h.m.ObjectStore - Failed to get database testdb, returning NoSuchObjectException
28811 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: create_table: Table(tableName:test_table, dbName:testdb, owner:null, createTime:0, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:), FieldSchema(name:msg, type:string, comment:)], location:raw:///tmp/junit4529692089802623592/testdb.db/test_table, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:1, serdeInfo:SerDeInfo(name:test_table, serializationLib:org.apache.hadoop.hive.ql.io.orc.OrcSerde, parameters:{serialization.format=1}), bucketCols:[], sortCols:null, parameters:null), partitionKeys:[FieldSchema(name:city, type:string, comment:), FieldSchema(name:state, type:string, comment:)], parameters:{}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)
28811 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=create_table: Table(tableName:test_table, dbName:testdb, owner:null, createTime:0, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:), FieldSchema(name:msg, type:string, comment:)], location:raw:///tmp/junit4529692089802623592/testdb.db/test_table, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:1, serdeInfo:SerDeInfo(name:test_table, serializationLib:org.apache.hadoop.hive.ql.io.orc.OrcSerde, parameters:{serialization.format=1}), bucketCols:[], sortCols:null, parameters:null), partitionKeys:[FieldSchema(name:city, type:string, comment:), FieldSchema(name:state, type:string, comment:)], parameters:{}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
28812 [main] WARN  o.a.h.h.m.HiveMetaStore - Location: raw:///tmp/junit4529692089802623592/testdb.db/test_table specified for non-external table:test_table
28812 [main] INFO  o.a.h.h.c.FileUtils - Creating directory if it doesn't exist: raw:/tmp/junit4529692089802623592/testdb.db/test_table
28908 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: add_partition : db=testdb tbl=test_table
28908 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=add_partition : db=testdb tbl=test_table	
28956 [main] INFO  o.a.h.h.c.FileUtils - Creating directory if it doesn't exist: raw:/tmp/junit4529692089802623592/testdb.db/test_table/city=sunnyvale/state=ca
28980 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Shutting down the object store...
28980 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=Shutting down the object store...	
28980 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Metastore shutdown complete.
28980 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=Metastore shutdown complete.	
done
28980 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_table_names_by_filter: db = testdb1, filter = 
28980 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table_names_by_filter: db = testdb1, filter = 	
28980 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
28981 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
28982 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
28983 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
28983 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
29000 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_table : db=testdb1 tbl=test_table1
29000 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table : db=testdb1 tbl=test_table1	
29007 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: drop_table : db=testdb1 tbl=test_table1
29008 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=drop_table : db=testdb1 tbl=test_table1	
29032 [main] INFO  h.m.hivemetastoressimpl - deleting  raw:/tmp/junit1104605902549751506/testdb.db/test_table1
29032 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_database: testdb1
29032 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_database: testdb1	
29033 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: drop_database: testdb1
29035 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=drop_database: testdb1	
29035 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_all_tables: db=testdb1
29035 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_all_tables: db=testdb1	
29036 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_functions: db=testdb1 pat=*
29036 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_functions: db=testdb1 pat=*	
29036 [main] INFO  o.a.h.h.m.ObjectStore - Dropping database testdb1 along with all tables
29060 [main] INFO  h.m.hivemetastoressimpl - deleting  raw:/tmp/junit1104605902549751506/testdb.db
29061 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
29062 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
29064 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
29064 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
29065 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: create_database: Database(name:testdb1, description:null, locationUri:raw:///tmp/junit4529692089802623592/testdb.db, parameters:null)
29065 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=create_database: Database(name:testdb1, description:null, locationUri:raw:///tmp/junit4529692089802623592/testdb.db, parameters:null)	
29066 [main] WARN  o.a.h.h.m.ObjectStore - Failed to get database testdb1, returning NoSuchObjectException
29068 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: create_table: Table(tableName:test_table1, dbName:testdb1, owner:null, createTime:0, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:), FieldSchema(name:msg, type:string, comment:)], location:raw:///tmp/junit4529692089802623592/testdb.db/test_table1, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:1, serdeInfo:SerDeInfo(name:test_table1, serializationLib:org.apache.hadoop.hive.ql.io.orc.OrcSerde, parameters:{serialization.format=1}), bucketCols:[], sortCols:null, parameters:null), partitionKeys:[FieldSchema(name:dt, type:string, comment:)], parameters:{}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)
29074 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=create_table: Table(tableName:test_table1, dbName:testdb1, owner:null, createTime:0, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:), FieldSchema(name:msg, type:string, comment:)], location:raw:///tmp/junit4529692089802623592/testdb.db/test_table1, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:1, serdeInfo:SerDeInfo(name:test_table1, serializationLib:org.apache.hadoop.hive.ql.io.orc.OrcSerde, parameters:{serialization.format=1}), bucketCols:[], sortCols:null, parameters:null), partitionKeys:[FieldSchema(name:dt, type:string, comment:)], parameters:{}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
29075 [main] WARN  o.a.h.h.m.HiveMetaStore - Location: raw:///tmp/junit4529692089802623592/testdb.db/test_table1 specified for non-external table:test_table1
29076 [main] INFO  o.a.h.h.c.FileUtils - Creating directory if it doesn't exist: raw:/tmp/junit4529692089802623592/testdb.db/test_table1
29094 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Shutting down the object store...
29094 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=Shutting down the object store...	
29094 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Metastore shutdown complete.
29094 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=Metastore shutdown complete.	
29095 [main] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=compile from=org.apache.hadoop.hive.ql.Driver>
29096 [main] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=parse from=org.apache.hadoop.hive.ql.Driver>
29096 [main] INFO  h.q.p.ParseDriver - Parsing command: select * from testdb1.test_table1
29096 [main] INFO  h.q.p.ParseDriver - Parse Completed
29096 [main] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=parse start=1466780115393 end=1466780115393 duration=0 from=org.apache.hadoop.hive.ql.Driver>
29105 [main] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=semanticAnalyze from=org.apache.hadoop.hive.ql.Driver>
29105 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Starting Semantic Analysis
29105 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Completed phase 1 of Semantic Analysis
29105 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Get metadata for source tables
29105 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_table : db=testdb1 tbl=test_table1
29105 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table : db=testdb1 tbl=test_table1	
29106 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
29106 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
29107 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
29108 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
29108 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
29121 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Get metadata for subqueries
29121 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Get metadata for destination tables
29124 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Completed getting MetaData in Semantic Analysis
29125 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Set stats collection dir : file:/tmp/travis/60186529-211e-487a-ae74-a9a7307218fe/hive_2016-06-24_14-55-15_392_353643774580584976-1/-ext-10002
29126 [main] INFO  o.a.h.h.q.p.OpProcFactory - Processing for FS(10)
29126 [main] INFO  o.a.h.h.q.p.OpProcFactory - Processing for SEL(9)
29126 [main] INFO  o.a.h.h.q.p.OpProcFactory - Processing for TS(8)
29127 [main] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=partition-retrieving from=org.apache.hadoop.hive.ql.optimizer.ppr.PartitionPruner>
29127 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_partitions : db=testdb1 tbl=test_table1
29127 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_partitions : db=testdb1 tbl=test_table1	
29132 [main] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=partition-retrieving start=1466780115424 end=1466780115429 duration=5 from=org.apache.hadoop.hive.ql.optimizer.ppr.PartitionPruner>
29149 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Completed plan generation
29149 [main] INFO  o.a.h.h.q.Driver - Semantic Analysis Completed
29149 [main] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=semanticAnalyze start=1466780115402 end=1466780115446 duration=44 from=org.apache.hadoop.hive.ql.Driver>
29149 [main] INFO  o.a.h.h.q.e.TableScanOperator - Initializing Self TS[8]
29150 [main] INFO  o.a.h.h.q.e.TableScanOperator - Operator 8 TS initialized
29150 [main] INFO  o.a.h.h.q.e.TableScanOperator - Initializing children of 8 TS
29150 [main] INFO  o.a.h.h.q.e.SelectOperator - Initializing child 9 SEL
29150 [main] INFO  o.a.h.h.q.e.SelectOperator - Initializing Self SEL[9]
29150 [main] INFO  o.a.h.h.q.e.SelectOperator - SELECT struct<id:int,msg:string,dt:string>
29150 [main] INFO  o.a.h.h.q.e.SelectOperator - Operator 9 SEL initialized
29150 [main] INFO  o.a.h.h.q.e.SelectOperator - Initializing children of 9 SEL
29150 [main] INFO  o.a.h.h.q.e.ListSinkOperator - Initializing child 11 OP
29150 [main] INFO  o.a.h.h.q.e.ListSinkOperator - Initializing Self OP[11]
29150 [main] INFO  o.a.h.h.q.e.ListSinkOperator - Operator 11 OP initialized
29150 [main] INFO  o.a.h.h.q.e.ListSinkOperator - Initialization Done 11 OP
29150 [main] INFO  o.a.h.h.q.e.SelectOperator - Initialization Done 9 SEL
29150 [main] INFO  o.a.h.h.q.e.TableScanOperator - Initialization Done 8 TS
29150 [main] INFO  o.a.h.h.q.Driver - Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:test_table1.id, type:int, comment:null), FieldSchema(name:test_table1.msg, type:string, comment:null), FieldSchema(name:test_table1.dt, type:string, comment:null)], properties:null)
29151 [main] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=compile start=1466780115392 end=1466780115448 duration=56 from=org.apache.hadoop.hive.ql.Driver>

--------------------------------------------------
--------------------------------------------------
classname: org.apache.storm.hive.bolt.TestHiveBolt / testname: testWithByteArrayIdandMessage
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hive.bolt.TestHiveBolt.generateTestTuple(TestHiveBolt.java:447)
	at org.apache.storm.hive.bolt.TestHiveBolt.testWithByteArrayIdandMessage(TestHiveBolt.java:161)

-------------------- system-out --------------------
29610 [main] INFO  o.a.h.h.q.s.SessionState - Created local directory: /tmp/8de4b0af-b941-4c43-b276-a205db9779c0_resources
29624 [main] INFO  o.a.h.h.q.s.SessionState - Created HDFS directory: /tmp/hive/travis/8de4b0af-b941-4c43-b276-a205db9779c0
29628 [main] INFO  o.a.h.h.q.s.SessionState - Created local directory: /tmp/travis/8de4b0af-b941-4c43-b276-a205db9779c0
29630 [main] INFO  o.a.h.h.q.s.SessionState - Created HDFS directory: /tmp/hive/travis/8de4b0af-b941-4c43-b276-a205db9779c0/_tmp_space.db
29630 [main] INFO  o.a.h.h.q.s.SessionState - No Tez session required at this point. hive.execution.engine=mr.
29632 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
29633 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
29634 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
29635 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
29635 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_table_names_by_filter: db = testdb, filter = 
29635 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table_names_by_filter: db = testdb, filter = 	
29635 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_table : db=testdb tbl=test_table
29635 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table : db=testdb tbl=test_table	
29645 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: drop_table : db=testdb tbl=test_table
29645 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=drop_table : db=testdb tbl=test_table	
29845 [main] INFO  h.m.hivemetastoressimpl - deleting  raw:/tmp/junit4529692089802623592/testdb.db/test_table
29845 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_database: testdb
29845 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_database: testdb	
29850 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: drop_database: testdb
29850 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=drop_database: testdb	
29851 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_all_tables: db=testdb
29851 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_all_tables: db=testdb	
29878 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_functions: db=testdb pat=*
29878 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_functions: db=testdb pat=*	
29882 [main] INFO  o.a.h.h.m.ObjectStore - Dropping database testdb along with all tables
29899 [main] INFO  h.m.hivemetastoressimpl - deleting  raw:/tmp/junit4529692089802623592/testdb.db
29900 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
29901 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
29902 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
29902 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
29902 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: create_database: Database(name:testdb, description:null, locationUri:raw:///tmp/junit645422386249345777/testdb.db, parameters:null)
29903 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=create_database: Database(name:testdb, description:null, locationUri:raw:///tmp/junit645422386249345777/testdb.db, parameters:null)	
29903 [main] WARN  o.a.h.h.m.ObjectStore - Failed to get database testdb, returning NoSuchObjectException
29908 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: create_table: Table(tableName:test_table, dbName:testdb, owner:null, createTime:0, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:), FieldSchema(name:msg, type:string, comment:)], location:raw:///tmp/junit645422386249345777/testdb.db/test_table, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:1, serdeInfo:SerDeInfo(name:test_table, serializationLib:org.apache.hadoop.hive.ql.io.orc.OrcSerde, parameters:{serialization.format=1}), bucketCols:[], sortCols:null, parameters:null), partitionKeys:[FieldSchema(name:city, type:string, comment:), FieldSchema(name:state, type:string, comment:)], parameters:{}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)
29909 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=create_table: Table(tableName:test_table, dbName:testdb, owner:null, createTime:0, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:), FieldSchema(name:msg, type:string, comment:)], location:raw:///tmp/junit645422386249345777/testdb.db/test_table, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:1, serdeInfo:SerDeInfo(name:test_table, serializationLib:org.apache.hadoop.hive.ql.io.orc.OrcSerde, parameters:{serialization.format=1}), bucketCols:[], sortCols:null, parameters:null), partitionKeys:[FieldSchema(name:city, type:string, comment:), FieldSchema(name:state, type:string, comment:)], parameters:{}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
29910 [main] WARN  o.a.h.h.m.HiveMetaStore - Location: raw:///tmp/junit645422386249345777/testdb.db/test_table specified for non-external table:test_table
29910 [main] INFO  o.a.h.h.c.FileUtils - Creating directory if it doesn't exist: raw:/tmp/junit645422386249345777/testdb.db/test_table
29974 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: add_partition : db=testdb tbl=test_table
29974 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=add_partition : db=testdb tbl=test_table	
30013 [main] INFO  o.a.h.h.c.FileUtils - Creating directory if it doesn't exist: raw:/tmp/junit645422386249345777/testdb.db/test_table/city=sunnyvale/state=ca
30033 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Shutting down the object store...
30033 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=Shutting down the object store...	
30033 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Metastore shutdown complete.
30033 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=Metastore shutdown complete.	
done
30034 [main] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=compile from=org.apache.hadoop.hive.ql.Driver>
30034 [main] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=parse from=org.apache.hadoop.hive.ql.Driver>
30034 [main] INFO  h.q.p.ParseDriver - Parsing command: select * from testdb.test_table
30034 [main] INFO  h.q.p.ParseDriver - Parse Completed
30034 [main] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=parse start=1466780116331 end=1466780116331 duration=0 from=org.apache.hadoop.hive.ql.Driver>
30042 [main] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=semanticAnalyze from=org.apache.hadoop.hive.ql.Driver>
30042 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Starting Semantic Analysis
30042 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Completed phase 1 of Semantic Analysis
30042 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Get metadata for source tables
30042 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_table : db=testdb tbl=test_table
30042 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table : db=testdb tbl=test_table	
30042 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
30043 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
30044 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
30045 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
30045 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
30057 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Get metadata for subqueries
30057 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Get metadata for destination tables
30061 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Completed getting MetaData in Semantic Analysis
30062 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Set stats collection dir : file:/tmp/travis/8de4b0af-b941-4c43-b276-a205db9779c0/hive_2016-06-24_14-55-16_331_388578236221017079-1/-ext-10002
30063 [main] INFO  o.a.h.h.q.p.OpProcFactory - Processing for FS(14)
30063 [main] INFO  o.a.h.h.q.p.OpProcFactory - Processing for SEL(13)
30063 [main] INFO  o.a.h.h.q.p.OpProcFactory - Processing for TS(12)
30064 [main] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=partition-retrieving from=org.apache.hadoop.hive.ql.optimizer.ppr.PartitionPruner>
30064 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_partitions : db=testdb tbl=test_table
30064 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_partitions : db=testdb tbl=test_table	
30096 [main] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=partition-retrieving start=1466780116361 end=1466780116393 duration=32 from=org.apache.hadoop.hive.ql.optimizer.ppr.PartitionPruner>
30099 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Completed plan generation
30099 [main] INFO  o.a.h.h.q.Driver - Semantic Analysis Completed
30099 [main] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=semanticAnalyze start=1466780116339 end=1466780116396 duration=57 from=org.apache.hadoop.hive.ql.Driver>
30099 [main] INFO  o.a.h.h.q.e.TableScanOperator - Initializing Self TS[12]
30099 [main] INFO  o.a.h.h.q.e.TableScanOperator - Operator 12 TS initialized
30099 [main] INFO  o.a.h.h.q.e.TableScanOperator - Initializing children of 12 TS
30100 [main] INFO  o.a.h.h.q.e.SelectOperator - Initializing child 13 SEL
30100 [main] INFO  o.a.h.h.q.e.SelectOperator - Initializing Self SEL[13]
30100 [main] INFO  o.a.h.h.q.e.SelectOperator - SELECT struct<id:int,msg:string,city:string,state:string>
30100 [main] INFO  o.a.h.h.q.e.SelectOperator - Operator 13 SEL initialized
30100 [main] INFO  o.a.h.h.q.e.SelectOperator - Initializing children of 13 SEL
30100 [main] INFO  o.a.h.h.q.e.ListSinkOperator - Initializing child 15 OP
30100 [main] INFO  o.a.h.h.q.e.ListSinkOperator - Initializing Self OP[15]
30100 [main] INFO  o.a.h.h.q.e.ListSinkOperator - Operator 15 OP initialized
30100 [main] INFO  o.a.h.h.q.e.ListSinkOperator - Initialization Done 15 OP
30100 [main] INFO  o.a.h.h.q.e.SelectOperator - Initialization Done 13 SEL
30100 [main] INFO  o.a.h.h.q.e.TableScanOperator - Initialization Done 12 TS
30100 [main] INFO  o.a.h.h.q.Driver - Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:test_table.id, type:int, comment:null), FieldSchema(name:test_table.msg, type:string, comment:null), FieldSchema(name:test_table.city, type:string, comment:null), FieldSchema(name:test_table.state, type:string, comment:null)], properties:null)
30100 [main] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=compile start=1466780116331 end=1466780116397 duration=66 from=org.apache.hadoop.hive.ql.Driver>

--------------------------------------------------
--------------------------------------------------
classname: org.apache.storm.hive.bolt.TestHiveBolt / testname: testNoAcksIfFlushFails
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hive.bolt.TestHiveBolt.generateTestTuple(TestHiveBolt.java:447)
	at org.apache.storm.hive.bolt.TestHiveBolt.testNoAcksIfFlushFails(TestHiveBolt.java:327)

-------------------- system-out --------------------
30384 [main] INFO  o.a.h.h.q.s.SessionState - Created local directory: /tmp/f60bd08d-1013-4a7a-b802-bf989c97868a_resources
30385 [main] INFO  o.a.h.h.q.s.SessionState - Created HDFS directory: /tmp/hive/travis/f60bd08d-1013-4a7a-b802-bf989c97868a
30387 [main] INFO  o.a.h.h.q.s.SessionState - Created local directory: /tmp/travis/f60bd08d-1013-4a7a-b802-bf989c97868a
30389 [main] INFO  o.a.h.h.q.s.SessionState - Created HDFS directory: /tmp/hive/travis/f60bd08d-1013-4a7a-b802-bf989c97868a/_tmp_space.db
30389 [main] INFO  o.a.h.h.q.s.SessionState - No Tez session required at this point. hive.execution.engine=mr.
30393 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
30393 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
30395 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
30395 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
30395 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_table_names_by_filter: db = testdb, filter = 
30395 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table_names_by_filter: db = testdb, filter = 	
30399 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_table : db=testdb tbl=test_table
30399 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table : db=testdb tbl=test_table	
30415 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: drop_table : db=testdb tbl=test_table
30415 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=drop_table : db=testdb tbl=test_table	
30736 [main] INFO  h.m.hivemetastoressimpl - deleting  raw:/tmp/junit645422386249345777/testdb.db/test_table
30736 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_database: testdb
30736 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_database: testdb	
30742 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: drop_database: testdb
30743 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=drop_database: testdb	
30743 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_all_tables: db=testdb
30743 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_all_tables: db=testdb	
30748 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_functions: db=testdb pat=*
30748 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_functions: db=testdb pat=*	
30753 [main] INFO  o.a.h.h.m.ObjectStore - Dropping database testdb along with all tables
30797 [main] INFO  h.m.hivemetastoressimpl - deleting  raw:/tmp/junit645422386249345777/testdb.db
30798 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
30799 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
30800 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
30801 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
30801 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: create_database: Database(name:testdb, description:null, locationUri:raw:///tmp/junit3431472091226529681/testdb.db, parameters:null)
30801 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=create_database: Database(name:testdb, description:null, locationUri:raw:///tmp/junit3431472091226529681/testdb.db, parameters:null)	
30801 [main] WARN  o.a.h.h.m.ObjectStore - Failed to get database testdb, returning NoSuchObjectException
30826 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: create_table: Table(tableName:test_table, dbName:testdb, owner:null, createTime:0, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:), FieldSchema(name:msg, type:string, comment:)], location:raw:///tmp/junit3431472091226529681/testdb.db/test_table, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:1, serdeInfo:SerDeInfo(name:test_table, serializationLib:org.apache.hadoop.hive.ql.io.orc.OrcSerde, parameters:{serialization.format=1}), bucketCols:[], sortCols:null, parameters:null), partitionKeys:[FieldSchema(name:city, type:string, comment:), FieldSchema(name:state, type:string, comment:)], parameters:{}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)
30826 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=create_table: Table(tableName:test_table, dbName:testdb, owner:null, createTime:0, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:), FieldSchema(name:msg, type:string, comment:)], location:raw:///tmp/junit3431472091226529681/testdb.db/test_table, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:1, serdeInfo:SerDeInfo(name:test_table, serializationLib:org.apache.hadoop.hive.ql.io.orc.OrcSerde, parameters:{serialization.format=1}), bucketCols:[], sortCols:null, parameters:null), partitionKeys:[FieldSchema(name:city, type:string, comment:), FieldSchema(name:state, type:string, comment:)], parameters:{}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
30827 [main] WARN  o.a.h.h.m.HiveMetaStore - Location: raw:///tmp/junit3431472091226529681/testdb.db/test_table specified for non-external table:test_table
30828 [main] INFO  o.a.h.h.c.FileUtils - Creating directory if it doesn't exist: raw:/tmp/junit3431472091226529681/testdb.db/test_table
30939 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: add_partition : db=testdb tbl=test_table
30940 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=add_partition : db=testdb tbl=test_table	
31010 [main] INFO  o.a.h.h.c.FileUtils - Creating directory if it doesn't exist: raw:/tmp/junit3431472091226529681/testdb.db/test_table/city=sunnyvale/state=ca
31112 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Shutting down the object store...
31112 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=Shutting down the object store...	
31113 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Metastore shutdown complete.
31113 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=Metastore shutdown complete.	
done

--------------------------------------------------
Checking ./external/storm-hive/target/surefire-reports/TEST-org.apache.storm.hive.common.TestHiveWriter.xml
--------------------------------------------------
classname: org.apache.storm.hive.common.TestHiveWriter / testname: testWriteBasic
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hive.common.TestHiveWriter.generateTestTuple(TestHiveWriter.java:164)
	at org.apache.storm.hive.common.TestHiveWriter.writeTuples(TestHiveWriter.java:179)
	at org.apache.storm.hive.common.TestHiveWriter.testWriteBasic(TestHiveWriter.java:127)

-------------------- system-out --------------------
31713 [main] INFO  o.a.h.h.q.s.SessionState - Created local directory: /tmp/085dde2d-83c5-458f-976b-43636bfb144b_resources
31730 [main] INFO  o.a.h.h.q.s.SessionState - Created HDFS directory: /tmp/hive/travis/085dde2d-83c5-458f-976b-43636bfb144b
31748 [main] INFO  o.a.h.h.q.s.SessionState - Created local directory: /tmp/travis/085dde2d-83c5-458f-976b-43636bfb144b
31751 [main] INFO  o.a.h.h.q.s.SessionState - Created HDFS directory: /tmp/hive/travis/085dde2d-83c5-458f-976b-43636bfb144b/_tmp_space.db
31751 [main] INFO  o.a.h.h.q.s.SessionState - No Tez session required at this point. hive.execution.engine=mr.
31752 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_table_names_by_filter: db = testdb, filter = 
31752 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table_names_by_filter: db = testdb, filter = 	
31752 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
31753 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
31754 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
31756 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
31756 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
31765 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_table : db=testdb tbl=test_table
31765 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table : db=testdb tbl=test_table	
31821 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: drop_table : db=testdb tbl=test_table
31821 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=drop_table : db=testdb tbl=test_table	
32105 [main] INFO  h.m.hivemetastoressimpl - deleting  raw:/tmp/junit3431472091226529681/testdb.db/test_table
32105 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_database: testdb
32105 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_database: testdb	
32110 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: drop_database: testdb
32110 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=drop_database: testdb	
32110 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_all_tables: db=testdb
32110 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_all_tables: db=testdb	
32113 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_functions: db=testdb pat=*
32113 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_functions: db=testdb pat=*	
32117 [main] INFO  o.a.h.h.m.ObjectStore - Dropping database testdb along with all tables
32128 [main] INFO  h.m.hivemetastoressimpl - deleting  raw:/tmp/junit3431472091226529681/testdb.db
32129 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
32129 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
32130 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
32131 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
32131 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: create_database: Database(name:testdb, description:null, locationUri:/tmp/junit3560580963807820623/testdb.db, parameters:null)
32131 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=create_database: Database(name:testdb, description:null, locationUri:/tmp/junit3560580963807820623/testdb.db, parameters:null)	
32131 [main] WARN  o.a.h.h.m.ObjectStore - Failed to get database testdb, returning NoSuchObjectException
32132 [main] INFO  o.a.h.h.c.FileUtils - Creating directory if it doesn't exist: file:/tmp/junit3560580963807820623/testdb.db
32136 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: create_table: Table(tableName:test_table2, dbName:testdb, owner:null, createTime:0, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:), FieldSchema(name:msg, type:string, comment:)], location:/tmp/junit3560580963807820623/testdb.db/test_table2, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:1, serdeInfo:SerDeInfo(name:test_table2, serializationLib:org.apache.hadoop.hive.ql.io.orc.OrcSerde, parameters:{serialization.format=1}), bucketCols:[], sortCols:null, parameters:null), partitionKeys:[FieldSchema(name:city, type:string, comment:), FieldSchema(name:state, type:string, comment:)], parameters:{}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)
32136 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=create_table: Table(tableName:test_table2, dbName:testdb, owner:null, createTime:0, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:), FieldSchema(name:msg, type:string, comment:)], location:/tmp/junit3560580963807820623/testdb.db/test_table2, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:1, serdeInfo:SerDeInfo(name:test_table2, serializationLib:org.apache.hadoop.hive.ql.io.orc.OrcSerde, parameters:{serialization.format=1}), bucketCols:[], sortCols:null, parameters:null), partitionKeys:[FieldSchema(name:city, type:string, comment:), FieldSchema(name:state, type:string, comment:)], parameters:{}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
32137 [main] WARN  o.a.h.h.m.HiveMetaStore - Location: /tmp/junit3560580963807820623/testdb.db/test_table2 specified for non-external table:test_table2
32137 [main] INFO  o.a.h.h.c.FileUtils - Creating directory if it doesn't exist: file:/tmp/junit3560580963807820623/testdb.db/test_table2
32162 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: add_partition : db=testdb tbl=test_table2
32162 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=add_partition : db=testdb tbl=test_table2	
32196 [main] INFO  o.a.h.h.c.FileUtils - Creating directory if it doesn't exist: file:/tmp/junit3560580963807820623/testdb.db/test_table2/city=sunnyvale/state=ca
32230 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Shutting down the object store...
32231 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=Shutting down the object store...	
32231 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Metastore shutdown complete.
32231 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=Metastore shutdown complete.	
32292 [hiveWriterTest] INFO  o.a.h.h.q.s.SessionState - Created local directory: /tmp/1b0318d9-7bae-4fdc-bc95-326af508c87b_resources
32294 [hiveWriterTest] INFO  o.a.h.h.q.s.SessionState - Created HDFS directory: /tmp/hive/travis/1b0318d9-7bae-4fdc-bc95-326af508c87b
32297 [hiveWriterTest] INFO  o.a.h.h.q.s.SessionState - Created local directory: /tmp/travis/1b0318d9-7bae-4fdc-bc95-326af508c87b
32314 [hiveWriterTest] INFO  o.a.h.h.q.s.SessionState - Created HDFS directory: /tmp/hive/travis/1b0318d9-7bae-4fdc-bc95-326af508c87b/_tmp_space.db
32315 [hiveWriterTest] INFO  o.a.h.h.q.s.SessionState - No Tez session required at this point. hive.execution.engine=mr.
32316 [hiveWriterTest] INFO  o.a.h.h.m.HiveMetaStore - 1: get_table : db=testdb tbl=test_table2
32316 [hiveWriterTest] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table : db=testdb tbl=test_table2	
32316 [hiveWriterTest] INFO  o.a.h.h.m.HiveMetaStore - 1: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
32317 [hiveWriterTest] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
32319 [hiveWriterTest] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
32333 [hiveWriterTest] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
32333 [hiveWriterTest] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
32372 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=Driver.run from=org.apache.hadoop.hive.ql.Driver>
32372 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=TimeToSubmit from=org.apache.hadoop.hive.ql.Driver>
32372 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=compile from=org.apache.hadoop.hive.ql.Driver>
32372 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=parse from=org.apache.hadoop.hive.ql.Driver>
32373 [hiveWriterTest] INFO  h.q.p.ParseDriver - Parsing command: use testdb
32374 [hiveWriterTest] INFO  h.q.p.ParseDriver - Parse Completed
32374 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=parse start=1466780118669 end=1466780118671 duration=2 from=org.apache.hadoop.hive.ql.Driver>
32398 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=semanticAnalyze from=org.apache.hadoop.hive.ql.Driver>
32408 [hiveWriterTest] INFO  o.a.h.h.m.HiveMetaStore - 1: get_database: testdb
32408 [hiveWriterTest] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_database: testdb	
32429 [hiveWriterTest] INFO  o.a.h.h.q.Driver - Semantic Analysis Completed
32429 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=semanticAnalyze start=1466780118695 end=1466780118726 duration=31 from=org.apache.hadoop.hive.ql.Driver>
32429 [hiveWriterTest] INFO  o.a.h.h.q.Driver - Returning Hive schema: Schema(fieldSchemas:null, properties:null)
32429 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=compile start=1466780118669 end=1466780118726 duration=57 from=org.apache.hadoop.hive.ql.Driver>
32429 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=acquireReadWriteLocks from=org.apache.hadoop.hive.ql.Driver>
32431 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=acquireReadWriteLocks start=1466780118726 end=1466780118728 duration=2 from=org.apache.hadoop.hive.ql.Driver>
32432 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=Driver.execute from=org.apache.hadoop.hive.ql.Driver>
32432 [hiveWriterTest] INFO  o.a.h.h.q.Driver - Starting command: use testdb
32434 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=TimeToSubmit start=1466780118669 end=1466780118731 duration=62 from=org.apache.hadoop.hive.ql.Driver>
32434 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=runTasks from=org.apache.hadoop.hive.ql.Driver>
32435 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=task.DDL.Stage-0 from=org.apache.hadoop.hive.ql.Driver>
32438 [hiveWriterTest] INFO  o.a.h.h.q.Driver - Starting task [Stage-0:DDL] in serial mode
32439 [hiveWriterTest] INFO  o.a.h.h.m.HiveMetaStore - 1: get_database: testdb
32439 [hiveWriterTest] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_database: testdb	
32440 [hiveWriterTest] INFO  o.a.h.h.m.HiveMetaStore - 1: get_database: testdb
32440 [hiveWriterTest] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_database: testdb	
32441 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=runTasks start=1466780118731 end=1466780118738 duration=7 from=org.apache.hadoop.hive.ql.Driver>
32441 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=Driver.execute start=1466780118729 end=1466780118738 duration=9 from=org.apache.hadoop.hive.ql.Driver>
32442 [hiveWriterTest] INFO  o.a.h.h.q.Driver - OK
32442 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=releaseLocks from=org.apache.hadoop.hive.ql.Driver>
32442 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=releaseLocks start=1466780118739 end=1466780118739 duration=0 from=org.apache.hadoop.hive.ql.Driver>
32442 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=Driver.run start=1466780118669 end=1466780118739 duration=70 from=org.apache.hadoop.hive.ql.Driver>
32443 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=Driver.run from=org.apache.hadoop.hive.ql.Driver>
32443 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=TimeToSubmit from=org.apache.hadoop.hive.ql.Driver>
32443 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=compile from=org.apache.hadoop.hive.ql.Driver>
32443 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=parse from=org.apache.hadoop.hive.ql.Driver>
32443 [hiveWriterTest] INFO  h.q.p.ParseDriver - Parsing command: alter table test_table2 add if not exists partition  ( city='sunnyvale',state='ca' )
32446 [hiveWriterTest] INFO  h.q.p.ParseDriver - Parse Completed
32447 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=parse start=1466780118740 end=1466780118744 duration=4 from=org.apache.hadoop.hive.ql.Driver>
32449 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=semanticAnalyze from=org.apache.hadoop.hive.ql.Driver>
32450 [hiveWriterTest] INFO  o.a.h.h.m.HiveMetaStore - 1: get_table : db=testdb tbl=test_table2
32450 [hiveWriterTest] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table : db=testdb tbl=test_table2	
32469 [hiveWriterTest] INFO  o.a.h.h.q.Driver - Semantic Analysis Completed
32469 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=semanticAnalyze start=1466780118746 end=1466780118766 duration=20 from=org.apache.hadoop.hive.ql.Driver>
32469 [hiveWriterTest] INFO  o.a.h.h.q.Driver - Returning Hive schema: Schema(fieldSchemas:null, properties:null)
32470 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=compile start=1466780118740 end=1466780118767 duration=27 from=org.apache.hadoop.hive.ql.Driver>
32470 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=acquireReadWriteLocks from=org.apache.hadoop.hive.ql.Driver>
32523 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=acquireReadWriteLocks start=1466780118767 end=1466780118820 duration=53 from=org.apache.hadoop.hive.ql.Driver>
32523 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=Driver.execute from=org.apache.hadoop.hive.ql.Driver>
32523 [hiveWriterTest] INFO  o.a.h.h.q.Driver - Starting command: alter table test_table2 add if not exists partition  ( city='sunnyvale',state='ca' )
32524 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=TimeToSubmit start=1466780118740 end=1466780118821 duration=81 from=org.apache.hadoop.hive.ql.Driver>
32524 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=runTasks from=org.apache.hadoop.hive.ql.Driver>
32524 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=task.DDL.Stage-0 from=org.apache.hadoop.hive.ql.Driver>
32524 [hiveWriterTest] INFO  o.a.h.h.q.Driver - Starting task [Stage-0:DDL] in serial mode
32524 [hiveWriterTest] INFO  o.a.h.h.m.HiveMetaStore - 1: get_table : db=testdb tbl=test_table2
32525 [hiveWriterTest] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table : db=testdb tbl=test_table2	
32544 [hiveWriterTest] INFO  o.a.h.h.m.HiveMetaStore - 1: add_partitions
32544 [hiveWriterTest] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=add_partitions	
32551 [hiveWriterTest] INFO  o.a.h.h.m.HiveMetaStore - Not adding partition Partition(values:[sunnyvale, ca], dbName:testdb, tableName:test_table2, createTime:0, lastAccessTime:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:), FieldSchema(name:msg, type:string, comment:)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:1, serdeInfo:SerDeInfo(name:test_table2, serializationLib:org.apache.hadoop.hive.ql.io.orc.OrcSerde, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), parameters:null) as it already exists
32552 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=runTasks start=1466780118821 end=1466780118849 duration=28 from=org.apache.hadoop.hive.ql.Driver>
32552 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=Driver.execute start=1466780118820 end=1466780118849 duration=29 from=org.apache.hadoop.hive.ql.Driver>
32553 [hiveWriterTest] INFO  o.a.h.h.q.Driver - OK
32553 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=releaseLocks from=org.apache.hadoop.hive.ql.Driver>
32560 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=releaseLocks start=1466780118850 end=1466780118857 duration=7 from=org.apache.hadoop.hive.ql.Driver>
32560 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=Driver.run start=1466780118740 end=1466780118857 duration=117 from=org.apache.hadoop.hive.ql.Driver>
32607 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_table : db=testdb tbl=test_table2
32607 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table : db=testdb tbl=test_table2	
32607 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
32608 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
32609 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
32610 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
32610 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
32624 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_partition : db=testdb tbl=test_table2[sunnyvale,ca]
32624 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_partition : db=testdb tbl=test_table2[sunnyvale,ca]	
32681 [hiveWriterTest] INFO  o.a.h.h.m.HiveMetaStore - 1: get_table : db=testdb tbl=test_table2
32681 [hiveWriterTest] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table : db=testdb tbl=test_table2	

-------------------- system-err --------------------
OK
OK

--------------------------------------------------
--------------------------------------------------
classname: org.apache.storm.hive.common.TestHiveWriter / testname: testWriteMultiFlush
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hive.common.TestHiveWriter.generateTestTuple(TestHiveWriter.java:164)
	at org.apache.storm.hive.common.TestHiveWriter.testWriteMultiFlush(TestHiveWriter.java:142)

-------------------- system-out --------------------
33084 [main] INFO  o.a.h.h.q.s.SessionState - Created local directory: /tmp/cb33f43e-3cb6-4354-88be-b7fd834cceaa_resources
33085 [main] INFO  o.a.h.h.q.s.SessionState - Created HDFS directory: /tmp/hive/travis/cb33f43e-3cb6-4354-88be-b7fd834cceaa
33087 [main] INFO  o.a.h.h.q.s.SessionState - Created local directory: /tmp/travis/cb33f43e-3cb6-4354-88be-b7fd834cceaa
33088 [main] INFO  o.a.h.h.q.s.SessionState - Created HDFS directory: /tmp/hive/travis/cb33f43e-3cb6-4354-88be-b7fd834cceaa/_tmp_space.db
33089 [main] INFO  o.a.h.h.q.s.SessionState - No Tez session required at this point. hive.execution.engine=mr.
33089 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
33090 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
33091 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
33091 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
33091 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_table_names_by_filter: db = testdb, filter = 
33091 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table_names_by_filter: db = testdb, filter = 	
33095 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_table : db=testdb tbl=test_table2
33095 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table : db=testdb tbl=test_table2	
33142 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: drop_table : db=testdb tbl=test_table2
33142 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=drop_table : db=testdb tbl=test_table2	
33441 [main] INFO  h.m.hivemetastoressimpl - deleting  file:/tmp/junit3560580963807820623/testdb.db/test_table2
33441 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_database: testdb
33441 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_database: testdb	
33459 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: drop_database: testdb
33459 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=drop_database: testdb	
33460 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_all_tables: db=testdb
33463 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_all_tables: db=testdb	
33497 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_functions: db=testdb pat=*
33497 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_functions: db=testdb pat=*	
33503 [main] INFO  o.a.h.h.m.ObjectStore - Dropping database testdb along with all tables
33527 [main] INFO  h.m.hivemetastoressimpl - deleting  file:/tmp/junit3560580963807820623/testdb.db
33528 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
33529 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
33535 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
33535 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
33540 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: create_database: Database(name:testdb, description:null, locationUri:/tmp/junit4926701054415080257/testdb.db, parameters:null)
33540 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=create_database: Database(name:testdb, description:null, locationUri:/tmp/junit4926701054415080257/testdb.db, parameters:null)	
33542 [main] WARN  o.a.h.h.m.ObjectStore - Failed to get database testdb, returning NoSuchObjectException
33543 [main] INFO  o.a.h.h.c.FileUtils - Creating directory if it doesn't exist: file:/tmp/junit4926701054415080257/testdb.db
33556 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: create_table: Table(tableName:test_table2, dbName:testdb, owner:null, createTime:0, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:), FieldSchema(name:msg, type:string, comment:)], location:/tmp/junit4926701054415080257/testdb.db/test_table2, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:1, serdeInfo:SerDeInfo(name:test_table2, serializationLib:org.apache.hadoop.hive.ql.io.orc.OrcSerde, parameters:{serialization.format=1}), bucketCols:[], sortCols:null, parameters:null), partitionKeys:[FieldSchema(name:city, type:string, comment:), FieldSchema(name:state, type:string, comment:)], parameters:{}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)
33556 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=create_table: Table(tableName:test_table2, dbName:testdb, owner:null, createTime:0, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:), FieldSchema(name:msg, type:string, comment:)], location:/tmp/junit4926701054415080257/testdb.db/test_table2, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:1, serdeInfo:SerDeInfo(name:test_table2, serializationLib:org.apache.hadoop.hive.ql.io.orc.OrcSerde, parameters:{serialization.format=1}), bucketCols:[], sortCols:null, parameters:null), partitionKeys:[FieldSchema(name:city, type:string, comment:), FieldSchema(name:state, type:string, comment:)], parameters:{}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
33557 [main] WARN  o.a.h.h.m.HiveMetaStore - Location: /tmp/junit4926701054415080257/testdb.db/test_table2 specified for non-external table:test_table2
33557 [main] INFO  o.a.h.h.c.FileUtils - Creating directory if it doesn't exist: file:/tmp/junit4926701054415080257/testdb.db/test_table2
33605 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: add_partition : db=testdb tbl=test_table2
33605 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=add_partition : db=testdb tbl=test_table2	
33673 [main] INFO  o.a.h.h.c.FileUtils - Creating directory if it doesn't exist: file:/tmp/junit4926701054415080257/testdb.db/test_table2/city=sunnyvale/state=ca
33699 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Shutting down the object store...
33700 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=Shutting down the object store...	
33700 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Metastore shutdown complete.
33700 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=Metastore shutdown complete.	
33794 [hiveWriterTest] INFO  o.a.h.h.q.s.SessionState - Created local directory: /tmp/cf935603-c614-4897-a576-1433b9e617f6_resources
33810 [hiveWriterTest] INFO  o.a.h.h.q.s.SessionState - Created HDFS directory: /tmp/hive/travis/cf935603-c614-4897-a576-1433b9e617f6
33824 [hiveWriterTest] INFO  o.a.h.h.q.s.SessionState - Created local directory: /tmp/travis/cf935603-c614-4897-a576-1433b9e617f6
33843 [hiveWriterTest] INFO  o.a.h.h.q.s.SessionState - Created HDFS directory: /tmp/hive/travis/cf935603-c614-4897-a576-1433b9e617f6/_tmp_space.db
33844 [hiveWriterTest] INFO  o.a.h.h.q.s.SessionState - No Tez session required at this point. hive.execution.engine=mr.
33844 [hiveWriterTest] INFO  o.a.h.h.m.HiveMetaStore - 2: get_table : db=testdb tbl=test_table2
33844 [hiveWriterTest] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table : db=testdb tbl=test_table2	
33844 [hiveWriterTest] INFO  o.a.h.h.m.HiveMetaStore - 2: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
33845 [hiveWriterTest] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
33846 [hiveWriterTest] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
33848 [hiveWriterTest] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
33848 [hiveWriterTest] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
33864 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=Driver.run from=org.apache.hadoop.hive.ql.Driver>
33864 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=TimeToSubmit from=org.apache.hadoop.hive.ql.Driver>
33864 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=compile from=org.apache.hadoop.hive.ql.Driver>
33864 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=parse from=org.apache.hadoop.hive.ql.Driver>
33864 [hiveWriterTest] INFO  h.q.p.ParseDriver - Parsing command: use testdb
33865 [hiveWriterTest] INFO  h.q.p.ParseDriver - Parse Completed
33865 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=parse start=1466780120161 end=1466780120162 duration=1 from=org.apache.hadoop.hive.ql.Driver>
33874 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=semanticAnalyze from=org.apache.hadoop.hive.ql.Driver>
33874 [hiveWriterTest] INFO  o.a.h.h.m.HiveMetaStore - 2: get_database: testdb
33874 [hiveWriterTest] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_database: testdb	
33876 [hiveWriterTest] INFO  o.a.h.h.q.Driver - Semantic Analysis Completed
33876 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=semanticAnalyze start=1466780120171 end=1466780120173 duration=2 from=org.apache.hadoop.hive.ql.Driver>
33876 [hiveWriterTest] INFO  o.a.h.h.q.Driver - Returning Hive schema: Schema(fieldSchemas:null, properties:null)
33876 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=compile start=1466780120161 end=1466780120173 duration=12 from=org.apache.hadoop.hive.ql.Driver>
33876 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=acquireReadWriteLocks from=org.apache.hadoop.hive.ql.Driver>
33876 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=acquireReadWriteLocks start=1466780120173 end=1466780120173 duration=0 from=org.apache.hadoop.hive.ql.Driver>
33876 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=Driver.execute from=org.apache.hadoop.hive.ql.Driver>
33876 [hiveWriterTest] INFO  o.a.h.h.q.Driver - Starting command: use testdb
33878 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=TimeToSubmit start=1466780120161 end=1466780120175 duration=14 from=org.apache.hadoop.hive.ql.Driver>
33878 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=runTasks from=org.apache.hadoop.hive.ql.Driver>
33878 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=task.DDL.Stage-0 from=org.apache.hadoop.hive.ql.Driver>
33878 [hiveWriterTest] INFO  o.a.h.h.q.Driver - Starting task [Stage-0:DDL] in serial mode
33879 [hiveWriterTest] INFO  o.a.h.h.m.HiveMetaStore - 2: get_database: testdb
33879 [hiveWriterTest] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_database: testdb	
33884 [hiveWriterTest] INFO  o.a.h.h.m.HiveMetaStore - 2: get_database: testdb
33884 [hiveWriterTest] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_database: testdb	
33885 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=runTasks start=1466780120175 end=1466780120182 duration=7 from=org.apache.hadoop.hive.ql.Driver>
33885 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=Driver.execute start=1466780120173 end=1466780120182 duration=9 from=org.apache.hadoop.hive.ql.Driver>
33885 [hiveWriterTest] INFO  o.a.h.h.q.Driver - OK
33885 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=releaseLocks from=org.apache.hadoop.hive.ql.Driver>
33885 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=releaseLocks start=1466780120182 end=1466780120182 duration=0 from=org.apache.hadoop.hive.ql.Driver>
33885 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=Driver.run start=1466780120160 end=1466780120182 duration=22 from=org.apache.hadoop.hive.ql.Driver>
33886 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=Driver.run from=org.apache.hadoop.hive.ql.Driver>
33886 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=TimeToSubmit from=org.apache.hadoop.hive.ql.Driver>
33886 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=compile from=org.apache.hadoop.hive.ql.Driver>
33886 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=parse from=org.apache.hadoop.hive.ql.Driver>
33886 [hiveWriterTest] INFO  h.q.p.ParseDriver - Parsing command: alter table test_table2 add if not exists partition  ( city='sunnyvale',state='ca' )
33905 [hiveWriterTest] INFO  h.q.p.ParseDriver - Parse Completed
33905 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=parse start=1466780120183 end=1466780120202 duration=19 from=org.apache.hadoop.hive.ql.Driver>
33912 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=semanticAnalyze from=org.apache.hadoop.hive.ql.Driver>
33913 [hiveWriterTest] INFO  o.a.h.h.m.HiveMetaStore - 2: get_table : db=testdb tbl=test_table2
33913 [hiveWriterTest] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table : db=testdb tbl=test_table2	
33957 [hiveWriterTest] INFO  o.a.h.h.q.Driver - Semantic Analysis Completed
33957 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=semanticAnalyze start=1466780120209 end=1466780120254 duration=45 from=org.apache.hadoop.hive.ql.Driver>
33957 [hiveWriterTest] INFO  o.a.h.h.q.Driver - Returning Hive schema: Schema(fieldSchemas:null, properties:null)
33957 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=compile start=1466780120183 end=1466780120254 duration=71 from=org.apache.hadoop.hive.ql.Driver>
33957 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=acquireReadWriteLocks from=org.apache.hadoop.hive.ql.Driver>
33992 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=acquireReadWriteLocks start=1466780120254 end=1466780120289 duration=35 from=org.apache.hadoop.hive.ql.Driver>
33992 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=Driver.execute from=org.apache.hadoop.hive.ql.Driver>
33993 [hiveWriterTest] INFO  o.a.h.h.q.Driver - Starting command: alter table test_table2 add if not exists partition  ( city='sunnyvale',state='ca' )
33993 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=TimeToSubmit start=1466780120183 end=1466780120290 duration=107 from=org.apache.hadoop.hive.ql.Driver>
33993 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=runTasks from=org.apache.hadoop.hive.ql.Driver>
33993 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=task.DDL.Stage-0 from=org.apache.hadoop.hive.ql.Driver>
33993 [hiveWriterTest] INFO  o.a.h.h.q.Driver - Starting task [Stage-0:DDL] in serial mode
33993 [hiveWriterTest] INFO  o.a.h.h.m.HiveMetaStore - 2: get_table : db=testdb tbl=test_table2
33993 [hiveWriterTest] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table : db=testdb tbl=test_table2	
34000 [hiveWriterTest] INFO  o.a.h.h.m.HiveMetaStore - 2: add_partitions
34000 [hiveWriterTest] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=add_partitions	
34006 [hiveWriterTest] INFO  o.a.h.h.m.HiveMetaStore - Not adding partition Partition(values:[sunnyvale, ca], dbName:testdb, tableName:test_table2, createTime:0, lastAccessTime:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:), FieldSchema(name:msg, type:string, comment:)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:1, serdeInfo:SerDeInfo(name:test_table2, serializationLib:org.apache.hadoop.hive.ql.io.orc.OrcSerde, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), parameters:null) as it already exists
34016 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=runTasks start=1466780120290 end=1466780120313 duration=23 from=org.apache.hadoop.hive.ql.Driver>
34016 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=Driver.execute start=1466780120289 end=1466780120313 duration=24 from=org.apache.hadoop.hive.ql.Driver>
34016 [hiveWriterTest] INFO  o.a.h.h.q.Driver - OK
34016 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=releaseLocks from=org.apache.hadoop.hive.ql.Driver>
34029 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=releaseLocks start=1466780120313 end=1466780120326 duration=13 from=org.apache.hadoop.hive.ql.Driver>
34029 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=Driver.run start=1466780120183 end=1466780120326 duration=143 from=org.apache.hadoop.hive.ql.Driver>
34066 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_table : db=testdb tbl=test_table2
34067 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table : db=testdb tbl=test_table2	
34067 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
34067 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
34068 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
34069 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
34070 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
34086 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_partition : db=testdb tbl=test_table2[sunnyvale,ca]
34086 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_partition : db=testdb tbl=test_table2[sunnyvale,ca]	
34125 [hiveWriterTest] INFO  o.a.h.h.m.HiveMetaStore - 2: get_table : db=testdb tbl=test_table2
34125 [hiveWriterTest] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table : db=testdb tbl=test_table2	

-------------------- system-err --------------------
OK
OK

--------------------------------------------------
Looking for errors in ./external/storm-jdbc/target/surefire-reports
Checking ./external/storm-jdbc/target/surefire-reports/TEST-org.apache.storm.jdbc.common.UtilTest.xml
Checking ./external/storm-jdbc/target/surefire-reports/TEST-org.apache.storm.jdbc.common.JdbcClientTest.xml
Checking ./external/storm-jdbc/target/surefire-reports/TEST-org.apache.storm.jdbc.bolt.JdbcLookupBoltTest.xml
Checking ./external/storm-jdbc/target/surefire-reports/TEST-org.apache.storm.jdbc.bolt.JdbcInsertBoltTest.xml
Looking for errors in ./external/storm-kafka/target/surefire-reports
Checking ./external/storm-kafka/target/surefire-reports/TEST-org.apache.storm.kafka.TestStringScheme.xml
Checking ./external/storm-kafka/target/surefire-reports/TEST-org.apache.storm.kafka.StringKeyValueSchemeTest.xml
Checking ./external/storm-kafka/target/surefire-reports/TEST-org.apache.storm.kafka.DynamicBrokersReaderTest.xml
Checking ./external/storm-kafka/target/surefire-reports/TEST-org.apache.storm.kafka.KafkaUtilsTest.xml
Checking ./external/storm-kafka/target/surefire-reports/TEST-org.apache.storm.kafka.KafkaErrorTest.xml
Checking ./external/storm-kafka/target/surefire-reports/TEST-org.apache.storm.kafka.bolt.KafkaBoltTest.xml
--------------------------------------------------
classname: org.apache.storm.kafka.bolt.KafkaBoltTest / testname: executeWithBrokerDown
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.kafka.bolt.KafkaBoltTest.generateTestTuple(KafkaBoltTest.java:301)
	at org.apache.storm.kafka.bolt.KafkaBoltTest.executeWithBrokerDown(KafkaBoltTest.java:266)

-------------------- system-out --------------------
14:52:23.910 [Curator-Framework-0-SendThread(127.0.0.1:38865)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
14:52:23.910 [Curator-Framework-0-SendThread(127.0.0.1:56833)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
14:52:23.990 [Curator-Framework-0-SendThread(127.0.0.1:51624)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
14:52:24.011 [Curator-Framework-0-SendThread(127.0.0.1:33293)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
14:52:24.111 [Curator-Framework-0-SendThread(127.0.0.1:35858)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
14:52:24.207 [Curator-Framework-0-SendThread(127.0.0.1:45954)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
14:52:24.480 [main] INFO  o.a.c.f.i.CuratorFrameworkImpl - Starting
14:52:24.482 [main] INFO  k.u.VerifiableProperties - Verifying properties
14:52:24.482 [main] INFO  k.u.VerifiableProperties - Property broker.id is overridden to 0
14:52:24.482 [main] INFO  k.u.VerifiableProperties - Property log.dirs is overridden to /tmp/kafka/logs/kafka-test-49486
14:52:24.482 [main] INFO  k.u.VerifiableProperties - Property port is overridden to 49486
14:52:24.482 [main] INFO  k.u.VerifiableProperties - Property zookeeper.connect is overridden to 127.0.0.1:34328
14:52:24.482 [main] INFO  k.s.KafkaServer - [Kafka Server 0], starting
14:52:24.482 [main] INFO  k.s.KafkaServer - [Kafka Server 0], Connecting to zookeeper on 127.0.0.1:34328
14:52:24.486 [ZkClient-EventThread-831-127.0.0.1:34328] INFO  o.I.z.ZkEventThread - Starting ZkClient event thread.
14:52:24.496 [main-EventThread] INFO  o.a.c.f.s.ConnectionStateManager - State change: CONNECTED
14:52:24.499 [main-EventThread] INFO  o.I.z.ZkClient - zookeeper state changed (SyncConnected)
14:52:24.553 [main] INFO  k.l.LogManager - Log directory '/tmp/kafka/logs/kafka-test-49486' not found, creating it.
14:52:24.553 [main] INFO  k.l.LogManager - Loading logs.
14:52:24.554 [main] INFO  k.l.LogManager - Logs loading complete.
14:52:24.554 [main] INFO  k.l.LogManager - Starting log cleanup with a period of 300000 ms.
14:52:24.554 [main] INFO  k.l.LogManager - Starting log flusher with a default period of 9223372036854775807 ms.
14:52:24.556 [main] INFO  k.n.Acceptor - Awaiting socket connections on 0.0.0.0:49486.
14:52:24.556 [main] INFO  k.n.SocketServer - [Socket Server on Broker 0], Started
14:52:24.559 [main] INFO  k.u.Mx4jLoader$ - Will not load MX4J, mx4j-tools.jar is not in the classpath
14:52:24.559 [main] INFO  k.c.KafkaController - [Controller 0]: Controller starting up
14:52:24.562 [main] INFO  k.s.ZookeeperLeaderElector - 0 successfully elected as leader
14:52:24.562 [main] INFO  k.c.KafkaController - [Controller 0]: Broker 0 starting become controller state transition
14:52:24.565 [main] INFO  k.c.KafkaController - [Controller 0]: Controller 0 incremented epoch to 1
14:52:24.567 [main] INFO  k.c.KafkaController - [Controller 0]: Partitions undergoing preferred replica election: 
14:52:24.567 [main] INFO  k.c.KafkaController - [Controller 0]: Partitions that completed preferred replica election: 
14:52:24.567 [main] INFO  k.c.KafkaController - [Controller 0]: Resuming preferred replica election for partitions: 
14:52:24.568 [main] INFO  k.c.KafkaController - [Controller 0]: Partitions being reassigned: Map()
14:52:24.568 [main] INFO  k.c.KafkaController - [Controller 0]: Partitions already reassigned: List()
14:52:24.568 [main] INFO  k.c.KafkaController - [Controller 0]: Resuming reassignment of partitions: Map()
14:52:24.568 [main] INFO  k.c.KafkaController - [Controller 0]: List of topics to be deleted: 
14:52:24.568 [main] INFO  k.c.KafkaController - [Controller 0]: List of topics ineligible for deletion: 
14:52:24.568 [main] INFO  k.c.KafkaController - [Controller 0]: Currently active brokers in the cluster: Set()
14:52:24.568 [main] INFO  k.c.KafkaController - [Controller 0]: Currently shutting brokers in the cluster: Set()
14:52:24.568 [main] INFO  k.c.KafkaController - [Controller 0]: Current list of topics in the cluster: Set()
14:52:24.569 [main] INFO  k.c.ReplicaStateMachine - [Replica state machine on controller 0]: Started replica state machine with initial state -> Map()
14:52:24.569 [main] INFO  k.c.PartitionStateMachine - [Partition state machine on Controller 0]: Started partition state machine with initial state -> Map()
14:52:24.569 [main] INFO  k.c.KafkaController - [Controller 0]: Broker 0 is ready to serve as the new controller with epoch 1
14:52:24.569 [main] INFO  k.c.KafkaController - [Controller 0]: Starting preferred replica leader election for partitions 
14:52:24.569 [main] INFO  k.c.PartitionStateMachine - [Partition state machine on Controller 0]: Invoking state change to OnlinePartition for partitions 
14:52:24.572 [main] INFO  k.c.KafkaController - [Controller 0]: starting the partition rebalance scheduler
14:52:24.572 [main] INFO  k.c.KafkaController - [Controller 0]: Controller startup complete
14:52:24.573 [ZkClient-EventThread-831-127.0.0.1:34328] INFO  k.s.ZookeeperLeaderElector$LeaderChangeListener - New leader is 0
14:52:24.600 [main] INFO  k.u.ZkUtils$ - Registered broker 0 at path /brokers/ids/0 with address testing-worker-linux-docker-cb11978e-3394-linux-9.prod.travis-ci.org:49486.
14:52:24.601 [main] INFO  k.s.KafkaServer - [Kafka Server 0], started
14:52:24.602 [ZkClient-EventThread-831-127.0.0.1:34328] INFO  k.c.ReplicaStateMachine$BrokerChangeListener - [BrokerChangeListener on Controller 0]: Broker change listener fired for path /brokers/ids with children 0
14:52:24.605 [ZkClient-EventThread-831-127.0.0.1:34328] INFO  k.c.ReplicaStateMachine$BrokerChangeListener - [BrokerChangeListener on Controller 0]: Newly added brokers: 0, deleted brokers: , all live brokers: 0
14:52:24.606 [ZkClient-EventThread-831-127.0.0.1:34328] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Controller 0 connected to id:0,host:testing-worker-linux-docker-cb11978e-3394-linux-9.prod.travis-ci.org,port:49486 for sending state change requests
14:52:24.607 [main] INFO  o.a.k.c.p.ProducerConfig - ProducerConfig values: 
	compression.type = none
	metric.reporters = []
	metadata.max.age.ms = 300000
	metadata.fetch.timeout.ms = 1000
	acks = 1
	batch.size = 16384
	reconnect.backoff.ms = 10
	bootstrap.servers = [localhost:49486]
	receive.buffer.bytes = 32768
	retry.backoff.ms = 100
	buffer.memory = 33554432
	timeout.ms = 30000
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	retries = 0
	max.request.size = 1048576
	block.on.buffer.full = true
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
	metrics.sample.window.ms = 30000
	send.buffer.bytes = 131072
	max.in.flight.requests.per.connection = 5
	metrics.num.samples = 2
	linger.ms = 0
	client.id = 

14:52:24.611 [ZkClient-EventThread-831-127.0.0.1:34328] INFO  k.c.KafkaController - [Controller 0]: New broker startup callback for 0
14:52:24.611 [main] INFO  k.s.KafkaServer - [Kafka Server 0], shutting down
14:52:24.611 [main] INFO  k.s.KafkaServer - [Kafka Server 0], Starting controlled shutdown
14:52:24.611 [Controller-0-to-broker-0-send-thread] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Starting 
14:52:24.615 [kafka-request-handler-0] INFO  k.c.KafkaController - [Controller 0]: Shutting down broker 0
14:52:24.615 [main] INFO  k.s.KafkaServer - [Kafka Server 0], Controlled shutdown succeeded
14:52:24.616 [main] INFO  k.n.SocketServer - [Socket Server on Broker 0], Shutting down
14:52:24.616 [kafka-network-thread-49486-1] INFO  k.n.Processor - Closing socket connection to /172.17.9.115.
14:52:24.616 [main] INFO  k.n.SocketServer - [Socket Server on Broker 0], Shutdown completed
14:52:24.617 [main] INFO  k.s.KafkaRequestHandlerPool - [Kafka Request Handler on Broker 0], shutting down
14:52:24.618 [main] INFO  k.s.KafkaRequestHandlerPool - [Kafka Request Handler on Broker 0], shut down completely
14:52:24.958 [main] INFO  k.s.ReplicaManager - [Replica Manager on Broker 0]: Shut down
14:52:24.958 [main] INFO  k.s.ReplicaFetcherManager - [ReplicaFetcherManager on broker 0] shutting down
14:52:24.958 [main] INFO  k.s.ReplicaFetcherManager - [ReplicaFetcherManager on broker 0] shutdown completed
14:52:24.958 [main] INFO  k.s.ReplicaManager - [Replica Manager on Broker 0]: Shut down completely
14:52:24.958 [main] INFO  k.l.LogManager - Shutting down.
14:52:24.958 [main] INFO  k.l.LogManager - Shutdown complete.
14:52:24.959 [main] INFO  k.c.PartitionStateMachine - [Partition state machine on Controller 0]: Stopped partition state machine
14:52:24.959 [main] INFO  k.c.ReplicaStateMachine - [Replica state machine on controller 0]: Stopped replica state machine
14:52:24.959 [main] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Shutting down
14:52:24.959 [main] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Shutdown completed
14:52:24.959 [ZkClient-EventThread-831-127.0.0.1:34328] INFO  o.I.z.ZkEventThread - Terminate ZkClient event thread.
14:52:24.960 [Controller-0-to-broker-0-send-thread] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Stopped 
14:52:24.961 [main] INFO  k.s.KafkaServer - [Kafka Server 0], shut down completed
14:52:24.963 [Curator-Framework-0] INFO  o.a.c.f.i.CuratorFrameworkImpl - backgroundOperationsLoop exiting

--------------------------------------------------
--------------------------------------------------
classname: org.apache.storm.kafka.bolt.KafkaBoltTest / testname: executeWithoutKey
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.kafka.bolt.KafkaBoltTest.generateTestTuple(KafkaBoltTest.java:301)
	at org.apache.storm.kafka.bolt.KafkaBoltTest.executeWithoutKey(KafkaBoltTest.java:255)

-------------------- system-out --------------------
14:52:27.213 [Curator-Framework-0-SendThread(127.0.0.1:38865)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
14:52:27.213 [Curator-Framework-0-SendThread(127.0.0.1:56833)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
14:52:27.293 [Curator-Framework-0-SendThread(127.0.0.1:51624)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
14:52:27.313 [Curator-Framework-0-SendThread(127.0.0.1:33293)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
14:52:27.414 [Curator-Framework-0-SendThread(127.0.0.1:35858)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
14:52:27.490 [main] INFO  o.a.c.f.i.CuratorFrameworkImpl - Starting
14:52:27.495 [main] INFO  k.u.VerifiableProperties - Verifying properties
14:52:27.495 [main] INFO  k.u.VerifiableProperties - Property broker.id is overridden to 0
14:52:27.495 [main] INFO  k.u.VerifiableProperties - Property log.dirs is overridden to /tmp/kafka/logs/kafka-test-56546
14:52:27.495 [main] INFO  k.u.VerifiableProperties - Property port is overridden to 56546
14:52:27.495 [main] INFO  k.u.VerifiableProperties - Property zookeeper.connect is overridden to 127.0.0.1:45484
14:52:27.495 [main] INFO  k.s.KafkaServer - [Kafka Server 0], starting
14:52:27.495 [main] INFO  k.s.KafkaServer - [Kafka Server 0], Connecting to zookeeper on 127.0.0.1:45484
14:52:27.496 [ZkClient-EventThread-903-127.0.0.1:45484] INFO  o.I.z.ZkEventThread - Starting ZkClient event thread.
14:52:27.497 [main-EventThread] INFO  o.a.c.f.s.ConnectionStateManager - State change: CONNECTED
14:52:27.497 [main-EventThread] INFO  o.I.z.ZkClient - zookeeper state changed (SyncConnected)
14:52:27.507 [main] INFO  k.l.LogManager - Log directory '/tmp/kafka/logs/kafka-test-56546' not found, creating it.
14:52:27.507 [main] INFO  k.l.LogManager - Loading logs.
14:52:27.508 [main] INFO  k.l.LogManager - Logs loading complete.
14:52:27.508 [main] INFO  k.l.LogManager - Starting log cleanup with a period of 300000 ms.
14:52:27.509 [main] INFO  k.l.LogManager - Starting log flusher with a default period of 9223372036854775807 ms.
14:52:27.510 [Curator-Framework-0-SendThread(127.0.0.1:45954)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
14:52:27.511 [main] INFO  k.n.Acceptor - Awaiting socket connections on 0.0.0.0:56546.
14:52:27.512 [main] INFO  k.n.SocketServer - [Socket Server on Broker 0], Started
14:52:27.520 [main] INFO  k.u.Mx4jLoader$ - Will not load MX4J, mx4j-tools.jar is not in the classpath
14:52:27.520 [main] INFO  k.c.KafkaController - [Controller 0]: Controller starting up
14:52:27.522 [main] INFO  k.s.ZookeeperLeaderElector - 0 successfully elected as leader
14:52:27.522 [main] INFO  k.c.KafkaController - [Controller 0]: Broker 0 starting become controller state transition
14:52:27.523 [main] INFO  k.c.KafkaController - [Controller 0]: Controller 0 incremented epoch to 1
14:52:27.527 [main] INFO  k.c.KafkaController - [Controller 0]: Partitions undergoing preferred replica election: 
14:52:27.527 [main] INFO  k.c.KafkaController - [Controller 0]: Partitions that completed preferred replica election: 
14:52:27.527 [main] INFO  k.c.KafkaController - [Controller 0]: Resuming preferred replica election for partitions: 
14:52:27.528 [main] INFO  k.c.KafkaController - [Controller 0]: Partitions being reassigned: Map()
14:52:27.528 [main] INFO  k.c.KafkaController - [Controller 0]: Partitions already reassigned: List()
14:52:27.528 [main] INFO  k.c.KafkaController - [Controller 0]: Resuming reassignment of partitions: Map()
14:52:27.528 [main] INFO  k.c.KafkaController - [Controller 0]: List of topics to be deleted: 
14:52:27.528 [main] INFO  k.c.KafkaController - [Controller 0]: List of topics ineligible for deletion: 
14:52:27.528 [main] INFO  k.c.KafkaController - [Controller 0]: Currently active brokers in the cluster: Set()
14:52:27.528 [main] INFO  k.c.KafkaController - [Controller 0]: Currently shutting brokers in the cluster: Set()
14:52:27.528 [main] INFO  k.c.KafkaController - [Controller 0]: Current list of topics in the cluster: Set()
14:52:27.528 [main] INFO  k.c.ReplicaStateMachine - [Replica state machine on controller 0]: Started replica state machine with initial state -> Map()
14:52:27.528 [main] INFO  k.c.PartitionStateMachine - [Partition state machine on Controller 0]: Started partition state machine with initial state -> Map()
14:52:27.528 [main] INFO  k.c.KafkaController - [Controller 0]: Broker 0 is ready to serve as the new controller with epoch 1
14:52:27.528 [main] INFO  k.c.KafkaController - [Controller 0]: Starting preferred replica leader election for partitions 
14:52:27.528 [main] INFO  k.c.PartitionStateMachine - [Partition state machine on Controller 0]: Invoking state change to OnlinePartition for partitions 
14:52:27.529 [main] INFO  k.c.KafkaController - [Controller 0]: starting the partition rebalance scheduler
14:52:27.529 [main] INFO  k.c.KafkaController - [Controller 0]: Controller startup complete
14:52:27.530 [ZkClient-EventThread-903-127.0.0.1:45484] INFO  k.s.ZookeeperLeaderElector$LeaderChangeListener - New leader is 0
14:52:27.536 [main] INFO  k.u.ZkUtils$ - Registered broker 0 at path /brokers/ids/0 with address testing-worker-linux-docker-cb11978e-3394-linux-9.prod.travis-ci.org:56546.
14:52:27.536 [main] INFO  k.s.KafkaServer - [Kafka Server 0], started
14:52:27.536 [main] INFO  o.a.k.c.p.ProducerConfig - ProducerConfig values: 
	compression.type = none
	metric.reporters = []
	metadata.max.age.ms = 300000
	metadata.fetch.timeout.ms = 1000
	acks = 1
	batch.size = 16384
	reconnect.backoff.ms = 10
	bootstrap.servers = [localhost:56546]
	receive.buffer.bytes = 32768
	retry.backoff.ms = 100
	buffer.memory = 33554432
	timeout.ms = 30000
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	retries = 0
	max.request.size = 1048576
	block.on.buffer.full = true
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
	metrics.sample.window.ms = 30000
	send.buffer.bytes = 131072
	max.in.flight.requests.per.connection = 5
	metrics.num.samples = 2
	linger.ms = 0
	client.id = 

14:52:27.538 [main] INFO  k.s.KafkaServer - [Kafka Server 0], shutting down
14:52:27.538 [main] INFO  k.s.KafkaServer - [Kafka Server 0], Starting controlled shutdown
14:52:27.539 [ZkClient-EventThread-903-127.0.0.1:45484] INFO  k.c.ReplicaStateMachine$BrokerChangeListener - [BrokerChangeListener on Controller 0]: Broker change listener fired for path /brokers/ids with children 0
14:52:27.542 [ZkClient-EventThread-903-127.0.0.1:45484] INFO  k.c.ReplicaStateMachine$BrokerChangeListener - [BrokerChangeListener on Controller 0]: Newly added brokers: 0, deleted brokers: , all live brokers: 0
14:52:27.542 [ZkClient-EventThread-903-127.0.0.1:45484] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Controller 0 connected to id:0,host:testing-worker-linux-docker-cb11978e-3394-linux-9.prod.travis-ci.org,port:56546 for sending state change requests
14:52:27.545 [ZkClient-EventThread-903-127.0.0.1:45484] INFO  k.c.KafkaController - [Controller 0]: New broker startup callback for 0
14:52:27.546 [kafka-request-handler-0] INFO  k.c.KafkaController - [Controller 0]: Shutting down broker 0
14:52:27.546 [Controller-0-to-broker-0-send-thread] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Starting 
14:52:27.546 [main] INFO  k.s.KafkaServer - [Kafka Server 0], Controlled shutdown succeeded
14:52:27.546 [main] INFO  k.n.SocketServer - [Socket Server on Broker 0], Shutting down
14:52:27.546 [kafka-network-thread-56546-1] INFO  k.n.Processor - Closing socket connection to /172.17.9.115.
14:52:27.547 [main] INFO  k.n.SocketServer - [Socket Server on Broker 0], Shutdown completed
14:52:27.547 [main] INFO  k.s.KafkaRequestHandlerPool - [Kafka Request Handler on Broker 0], shutting down
14:52:27.547 [main] INFO  k.s.KafkaRequestHandlerPool - [Kafka Request Handler on Broker 0], shut down completely
14:52:27.919 [main] INFO  k.s.ReplicaManager - [Replica Manager on Broker 0]: Shut down
14:52:27.919 [main] INFO  k.s.ReplicaFetcherManager - [ReplicaFetcherManager on broker 0] shutting down
14:52:27.919 [main] INFO  k.s.ReplicaFetcherManager - [ReplicaFetcherManager on broker 0] shutdown completed
14:52:27.919 [main] INFO  k.s.ReplicaManager - [Replica Manager on Broker 0]: Shut down completely
14:52:27.919 [main] INFO  k.l.LogManager - Shutting down.
14:52:27.920 [main] INFO  k.l.LogManager - Shutdown complete.
14:52:27.920 [main] INFO  k.c.PartitionStateMachine - [Partition state machine on Controller 0]: Stopped partition state machine
14:52:27.920 [main] INFO  k.c.ReplicaStateMachine - [Replica state machine on controller 0]: Stopped replica state machine
14:52:27.920 [main] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Shutting down
14:52:27.920 [main] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Shutdown completed
14:52:27.920 [ZkClient-EventThread-903-127.0.0.1:45484] INFO  o.I.z.ZkEventThread - Terminate ZkClient event thread.
14:52:27.920 [Controller-0-to-broker-0-send-thread] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Stopped 
14:52:27.933 [main] INFO  k.s.KafkaServer - [Kafka Server 0], shut down completed
14:52:27.933 [Curator-Framework-0] INFO  o.a.c.f.i.CuratorFrameworkImpl - backgroundOperationsLoop exiting

--------------------------------------------------
--------------------------------------------------
classname: org.apache.storm.kafka.bolt.KafkaBoltTest / testname: executeWithByteArrayKeyAndMessageFire
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.kafka.bolt.KafkaBoltTest.generateTestTuple(KafkaBoltTest.java:290)
	at org.apache.storm.kafka.bolt.KafkaBoltTest.executeWithByteArrayKeyAndMessageFire(KafkaBoltTest.java:176)

-------------------- system-out --------------------
14:52:28.314 [Curator-Framework-0-SendThread(127.0.0.1:38865)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
14:52:28.315 [Curator-Framework-0-SendThread(127.0.0.1:56833)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
14:52:28.394 [Curator-Framework-0-SendThread(127.0.0.1:51624)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
14:52:28.414 [Curator-Framework-0-SendThread(127.0.0.1:33293)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
14:52:28.515 [Curator-Framework-0-SendThread(127.0.0.1:35858)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
14:52:28.611 [Curator-Framework-0-SendThread(127.0.0.1:45954)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
14:52:28.994 [main] INFO  o.a.c.f.i.CuratorFrameworkImpl - Starting
14:52:28.995 [main] INFO  k.u.VerifiableProperties - Verifying properties
14:52:28.995 [main] INFO  k.u.VerifiableProperties - Property broker.id is overridden to 0
14:52:28.995 [main] INFO  k.u.VerifiableProperties - Property log.dirs is overridden to /tmp/kafka/logs/kafka-test-35529
14:52:28.995 [main] INFO  k.u.VerifiableProperties - Property port is overridden to 35529
14:52:28.995 [main] INFO  k.u.VerifiableProperties - Property zookeeper.connect is overridden to 127.0.0.1:46619
14:52:28.996 [main] INFO  k.s.KafkaServer - [Kafka Server 0], starting
14:52:28.996 [main] INFO  k.s.KafkaServer - [Kafka Server 0], Connecting to zookeeper on 127.0.0.1:46619
14:52:28.996 [ZkClient-EventThread-939-127.0.0.1:46619] INFO  o.I.z.ZkEventThread - Starting ZkClient event thread.
14:52:28.999 [main-EventThread] INFO  o.a.c.f.s.ConnectionStateManager - State change: CONNECTED
14:52:29.000 [main-EventThread] INFO  o.I.z.ZkClient - zookeeper state changed (SyncConnected)
14:52:29.017 [main] INFO  k.l.LogManager - Log directory '/tmp/kafka/logs/kafka-test-35529' not found, creating it.
14:52:29.017 [main] INFO  k.l.LogManager - Loading logs.
14:52:29.017 [main] INFO  k.l.LogManager - Logs loading complete.
14:52:29.017 [main] INFO  k.l.LogManager - Starting log cleanup with a period of 300000 ms.
14:52:29.017 [main] INFO  k.l.LogManager - Starting log flusher with a default period of 9223372036854775807 ms.
14:52:29.019 [main] INFO  k.n.Acceptor - Awaiting socket connections on 0.0.0.0:35529.
14:52:29.020 [main] INFO  k.n.SocketServer - [Socket Server on Broker 0], Started
14:52:29.030 [main] INFO  k.u.Mx4jLoader$ - Will not load MX4J, mx4j-tools.jar is not in the classpath
14:52:29.030 [main] INFO  k.c.KafkaController - [Controller 0]: Controller starting up
14:52:29.032 [main] INFO  k.s.ZookeeperLeaderElector - 0 successfully elected as leader
14:52:29.032 [main] INFO  k.c.KafkaController - [Controller 0]: Broker 0 starting become controller state transition
14:52:29.033 [main] INFO  k.c.KafkaController - [Controller 0]: Controller 0 incremented epoch to 1
14:52:29.036 [main] INFO  k.c.KafkaController - [Controller 0]: Partitions undergoing preferred replica election: 
14:52:29.036 [main] INFO  k.c.KafkaController - [Controller 0]: Partitions that completed preferred replica election: 
14:52:29.036 [main] INFO  k.c.KafkaController - [Controller 0]: Resuming preferred replica election for partitions: 
14:52:29.036 [main] INFO  k.c.KafkaController - [Controller 0]: Partitions being reassigned: Map()
14:52:29.036 [main] INFO  k.c.KafkaController - [Controller 0]: Partitions already reassigned: List()
14:52:29.037 [main] INFO  k.c.KafkaController - [Controller 0]: Resuming reassignment of partitions: Map()
14:52:29.037 [main] INFO  k.c.KafkaController - [Controller 0]: List of topics to be deleted: 
14:52:29.037 [main] INFO  k.c.KafkaController - [Controller 0]: List of topics ineligible for deletion: 
14:52:29.037 [main] INFO  k.c.KafkaController - [Controller 0]: Currently active brokers in the cluster: Set()
14:52:29.037 [main] INFO  k.c.KafkaController - [Controller 0]: Currently shutting brokers in the cluster: Set()
14:52:29.037 [main] INFO  k.c.KafkaController - [Controller 0]: Current list of topics in the cluster: Set()
14:52:29.037 [main] INFO  k.c.ReplicaStateMachine - [Replica state machine on controller 0]: Started replica state machine with initial state -> Map()
14:52:29.037 [main] INFO  k.c.PartitionStateMachine - [Partition state machine on Controller 0]: Started partition state machine with initial state -> Map()
14:52:29.037 [main] INFO  k.c.KafkaController - [Controller 0]: Broker 0 is ready to serve as the new controller with epoch 1
14:52:29.037 [main] INFO  k.c.KafkaController - [Controller 0]: Starting preferred replica leader election for partitions 
14:52:29.037 [main] INFO  k.c.PartitionStateMachine - [Partition state machine on Controller 0]: Invoking state change to OnlinePartition for partitions 
14:52:29.038 [main] INFO  k.c.KafkaController - [Controller 0]: starting the partition rebalance scheduler
14:52:29.039 [main] INFO  k.c.KafkaController - [Controller 0]: Controller startup complete
14:52:29.039 [ZkClient-EventThread-939-127.0.0.1:46619] INFO  k.s.ZookeeperLeaderElector$LeaderChangeListener - New leader is 0
14:52:29.043 [main] INFO  k.u.ZkUtils$ - Registered broker 0 at path /brokers/ids/0 with address testing-worker-linux-docker-cb11978e-3394-linux-9.prod.travis-ci.org:35529.
14:52:29.043 [main] INFO  k.s.KafkaServer - [Kafka Server 0], started
14:52:29.043 [main] INFO  o.a.k.c.p.ProducerConfig - ProducerConfig values: 
	compression.type = none
	metric.reporters = []
	metadata.max.age.ms = 300000
	metadata.fetch.timeout.ms = 1000
	acks = 1
	batch.size = 16384
	reconnect.backoff.ms = 10
	bootstrap.servers = [localhost:35529]
	receive.buffer.bytes = 32768
	retry.backoff.ms = 100
	buffer.memory = 33554432
	timeout.ms = 30000
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	retries = 0
	max.request.size = 1048576
	block.on.buffer.full = true
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
	metrics.sample.window.ms = 30000
	send.buffer.bytes = 131072
	max.in.flight.requests.per.connection = 5
	metrics.num.samples = 2
	linger.ms = 0
	client.id = 

14:52:29.045 [ZkClient-EventThread-939-127.0.0.1:46619] INFO  k.c.ReplicaStateMachine$BrokerChangeListener - [BrokerChangeListener on Controller 0]: Broker change listener fired for path /brokers/ids with children 0
14:52:29.047 [ZkClient-EventThread-939-127.0.0.1:46619] INFO  k.c.ReplicaStateMachine$BrokerChangeListener - [BrokerChangeListener on Controller 0]: Newly added brokers: 0, deleted brokers: , all live brokers: 0
14:52:29.048 [ZkClient-EventThread-939-127.0.0.1:46619] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Controller 0 connected to id:0,host:testing-worker-linux-docker-cb11978e-3394-linux-9.prod.travis-ci.org,port:35529 for sending state change requests
14:52:29.048 [ZkClient-EventThread-939-127.0.0.1:46619] INFO  k.c.KafkaController - [Controller 0]: New broker startup callback for 0
14:52:29.048 [Controller-0-to-broker-0-send-thread] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Starting 
14:52:29.049 [main] INFO  o.a.k.c.p.ProducerConfig - ProducerConfig values: 
	compression.type = none
	metric.reporters = []
	metadata.max.age.ms = 300000
	metadata.fetch.timeout.ms = 1000
	acks = 1
	batch.size = 16384
	reconnect.backoff.ms = 10
	bootstrap.servers = [localhost:35529]
	receive.buffer.bytes = 32768
	retry.backoff.ms = 100
	buffer.memory = 33554432
	timeout.ms = 30000
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	retries = 0
	max.request.size = 1048576
	block.on.buffer.full = true
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	metrics.sample.window.ms = 30000
	send.buffer.bytes = 131072
	max.in.flight.requests.per.connection = 5
	metrics.num.samples = 2
	linger.ms = 0
	client.id = 

14:52:29.051 [main] INFO  k.s.KafkaServer - [Kafka Server 0], shutting down
14:52:29.051 [main] INFO  k.s.KafkaServer - [Kafka Server 0], Starting controlled shutdown
14:52:29.053 [kafka-request-handler-0] INFO  k.c.KafkaController - [Controller 0]: Shutting down broker 0
14:52:29.053 [main] INFO  k.s.KafkaServer - [Kafka Server 0], Controlled shutdown succeeded
14:52:29.054 [main] INFO  k.n.SocketServer - [Socket Server on Broker 0], Shutting down
14:52:29.054 [kafka-network-thread-35529-1] INFO  k.n.Processor - Closing socket connection to /172.17.9.115.
14:52:29.054 [main] INFO  k.n.SocketServer - [Socket Server on Broker 0], Shutdown completed
14:52:29.054 [main] INFO  k.s.KafkaRequestHandlerPool - [Kafka Request Handler on Broker 0], shutting down
14:52:29.055 [main] INFO  k.s.KafkaRequestHandlerPool - [Kafka Request Handler on Broker 0], shut down completely
14:52:29.415 [Curator-Framework-0-SendThread(127.0.0.1:38865)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
14:52:29.415 [Curator-Framework-0-SendThread(127.0.0.1:56833)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
14:52:29.423 [main] INFO  k.s.ReplicaManager - [Replica Manager on Broker 0]: Shut down
14:52:29.423 [main] INFO  k.s.ReplicaFetcherManager - [ReplicaFetcherManager on broker 0] shutting down
14:52:29.423 [main] INFO  k.s.ReplicaFetcherManager - [ReplicaFetcherManager on broker 0] shutdown completed
14:52:29.423 [main] INFO  k.s.ReplicaManager - [Replica Manager on Broker 0]: Shut down completely
14:52:29.423 [main] INFO  k.l.LogManager - Shutting down.
14:52:29.423 [main] INFO  k.l.LogManager - Shutdown complete.
14:52:29.423 [main] INFO  k.c.PartitionStateMachine - [Partition state machine on Controller 0]: Stopped partition state machine
14:52:29.423 [main] INFO  k.c.ReplicaStateMachine - [Replica state machine on controller 0]: Stopped replica state machine
14:52:29.423 [main] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Shutting down
14:52:29.423 [Controller-0-to-broker-0-send-thread] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Stopped 
14:52:29.425 [main] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Shutdown completed
14:52:29.425 [ZkClient-EventThread-939-127.0.0.1:46619] INFO  o.I.z.ZkEventThread - Terminate ZkClient event thread.
14:52:29.428 [main] INFO  k.s.KafkaServer - [Kafka Server 0], shut down completed
14:52:29.429 [Curator-Framework-0] INFO  o.a.c.f.i.CuratorFrameworkImpl - backgroundOperationsLoop exiting

--------------------------------------------------
--------------------------------------------------
classname: org.apache.storm.kafka.bolt.KafkaBoltTest / testname: executeWithByteArrayKeyAndMessageSync
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.kafka.bolt.KafkaBoltTest.generateTestTuple(KafkaBoltTest.java:290)
	at org.apache.storm.kafka.bolt.KafkaBoltTest.executeWithByteArrayKeyAndMessageSync(KafkaBoltTest.java:131)

-------------------- system-out --------------------
14:52:29.495 [Curator-Framework-0-SendThread(127.0.0.1:51624)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
14:52:29.515 [Curator-Framework-0-SendThread(127.0.0.1:33293)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
14:52:29.616 [Curator-Framework-0-SendThread(127.0.0.1:35858)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
14:52:29.712 [Curator-Framework-0-SendThread(127.0.0.1:45954)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
14:52:30.516 [Curator-Framework-0-SendThread(127.0.0.1:56833)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
14:52:30.516 [Curator-Framework-0-SendThread(127.0.0.1:38865)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
14:52:30.517 [main] INFO  o.a.c.f.i.CuratorFrameworkImpl - Starting
14:52:30.519 [main] INFO  k.u.VerifiableProperties - Verifying properties
14:52:30.519 [main] INFO  k.u.VerifiableProperties - Property broker.id is overridden to 0
14:52:30.519 [main] INFO  k.u.VerifiableProperties - Property log.dirs is overridden to /tmp/kafka/logs/kafka-test-49175
14:52:30.523 [main] INFO  k.u.VerifiableProperties - Property port is overridden to 49175
14:52:30.523 [main-EventThread] INFO  o.a.c.f.s.ConnectionStateManager - State change: CONNECTED
14:52:30.523 [main] INFO  k.u.VerifiableProperties - Property zookeeper.connect is overridden to 127.0.0.1:48002
14:52:30.523 [main] INFO  k.s.KafkaServer - [Kafka Server 0], starting
14:52:30.523 [main] INFO  k.s.KafkaServer - [Kafka Server 0], Connecting to zookeeper on 127.0.0.1:48002
14:52:30.524 [ZkClient-EventThread-976-127.0.0.1:48002] INFO  o.I.z.ZkEventThread - Starting ZkClient event thread.
14:52:30.586 [main-EventThread] INFO  o.I.z.ZkClient - zookeeper state changed (SyncConnected)
14:52:30.596 [Curator-Framework-0-SendThread(127.0.0.1:51624)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
14:52:30.597 [main] INFO  k.l.LogManager - Log directory '/tmp/kafka/logs/kafka-test-49175' not found, creating it.
14:52:30.598 [main] INFO  k.l.LogManager - Loading logs.
14:52:30.598 [main] INFO  k.l.LogManager - Logs loading complete.
14:52:30.598 [main] INFO  k.l.LogManager - Starting log cleanup with a period of 300000 ms.
14:52:30.598 [main] INFO  k.l.LogManager - Starting log flusher with a default period of 9223372036854775807 ms.
14:52:30.602 [main] INFO  k.n.Acceptor - Awaiting socket connections on 0.0.0.0:49175.
14:52:30.602 [main] INFO  k.n.SocketServer - [Socket Server on Broker 0], Started
14:52:30.615 [main] INFO  k.u.Mx4jLoader$ - Will not load MX4J, mx4j-tools.jar is not in the classpath
14:52:30.615 [main] INFO  k.c.KafkaController - [Controller 0]: Controller starting up
14:52:30.616 [Curator-Framework-0-SendThread(127.0.0.1:33293)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
14:52:30.618 [main] INFO  k.s.ZookeeperLeaderElector - 0 successfully elected as leader
14:52:30.618 [main] INFO  k.c.KafkaController - [Controller 0]: Broker 0 starting become controller state transition
14:52:30.633 [main] INFO  k.c.KafkaController - [Controller 0]: Controller 0 incremented epoch to 1
14:52:30.646 [main] INFO  k.c.KafkaController - [Controller 0]: Partitions undergoing preferred replica election: 
14:52:30.646 [main] INFO  k.c.KafkaController - [Controller 0]: Partitions that completed preferred replica election: 
14:52:30.646 [main] INFO  k.c.KafkaController - [Controller 0]: Resuming preferred replica election for partitions: 
14:52:30.646 [main] INFO  k.c.KafkaController - [Controller 0]: Partitions being reassigned: Map()
14:52:30.646 [main] INFO  k.c.KafkaController - [Controller 0]: Partitions already reassigned: List()
14:52:30.646 [main] INFO  k.c.KafkaController - [Controller 0]: Resuming reassignment of partitions: Map()
14:52:30.647 [main] INFO  k.c.KafkaController - [Controller 0]: List of topics to be deleted: 
14:52:30.647 [main] INFO  k.c.KafkaController - [Controller 0]: List of topics ineligible for deletion: 
14:52:30.647 [main] INFO  k.c.KafkaController - [Controller 0]: Currently active brokers in the cluster: Set()
14:52:30.647 [main] INFO  k.c.KafkaController - [Controller 0]: Currently shutting brokers in the cluster: Set()
14:52:30.647 [main] INFO  k.c.KafkaController - [Controller 0]: Current list of topics in the cluster: Set()
14:52:30.647 [main] INFO  k.c.ReplicaStateMachine - [Replica state machine on controller 0]: Started replica state machine with initial state -> Map()
14:52:30.647 [main] INFO  k.c.PartitionStateMachine - [Partition state machine on Controller 0]: Started partition state machine with initial state -> Map()
14:52:30.647 [main] INFO  k.c.KafkaController - [Controller 0]: Broker 0 is ready to serve as the new controller with epoch 1
14:52:30.647 [main] INFO  k.c.KafkaController - [Controller 0]: Starting preferred replica leader election for partitions 
14:52:30.647 [main] INFO  k.c.PartitionStateMachine - [Partition state machine on Controller 0]: Invoking state change to OnlinePartition for partitions 
14:52:30.648 [main] INFO  k.c.KafkaController - [Controller 0]: starting the partition rebalance scheduler
14:52:30.666 [main] INFO  k.c.KafkaController - [Controller 0]: Controller startup complete
14:52:30.667 [ZkClient-EventThread-976-127.0.0.1:48002] INFO  k.s.ZookeeperLeaderElector$LeaderChangeListener - New leader is 0
14:52:30.673 [main] INFO  k.u.ZkUtils$ - Registered broker 0 at path /brokers/ids/0 with address testing-worker-linux-docker-cb11978e-3394-linux-9.prod.travis-ci.org:49175.
14:52:30.673 [main] INFO  k.s.KafkaServer - [Kafka Server 0], started
14:52:30.673 [ZkClient-EventThread-976-127.0.0.1:48002] INFO  k.c.ReplicaStateMachine$BrokerChangeListener - [BrokerChangeListener on Controller 0]: Broker change listener fired for path /brokers/ids with children 0
14:52:30.673 [main] INFO  o.a.k.c.p.ProducerConfig - ProducerConfig values: 
	compression.type = none
	metric.reporters = []
	metadata.max.age.ms = 300000
	metadata.fetch.timeout.ms = 1000
	acks = 1
	batch.size = 16384
	reconnect.backoff.ms = 10
	bootstrap.servers = [localhost:49175]
	receive.buffer.bytes = 32768
	retry.backoff.ms = 100
	buffer.memory = 33554432
	timeout.ms = 30000
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	retries = 0
	max.request.size = 1048576
	block.on.buffer.full = true
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
	metrics.sample.window.ms = 30000
	send.buffer.bytes = 131072
	max.in.flight.requests.per.connection = 5
	metrics.num.samples = 2
	linger.ms = 0
	client.id = 

14:52:30.677 [ZkClient-EventThread-976-127.0.0.1:48002] INFO  k.c.ReplicaStateMachine$BrokerChangeListener - [BrokerChangeListener on Controller 0]: Newly added brokers: 0, deleted brokers: , all live brokers: 0
14:52:30.690 [main] INFO  o.a.k.c.p.ProducerConfig - ProducerConfig values: 
	compression.type = none
	metric.reporters = []
	metadata.max.age.ms = 300000
	metadata.fetch.timeout.ms = 1000
	acks = 1
	batch.size = 16384
	reconnect.backoff.ms = 10
	bootstrap.servers = [localhost:49175]
	receive.buffer.bytes = 32768
	retry.backoff.ms = 100
	buffer.memory = 33554432
	timeout.ms = 30000
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	retries = 0
	max.request.size = 1048576
	block.on.buffer.full = true
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	metrics.sample.window.ms = 30000
	send.buffer.bytes = 131072
	max.in.flight.requests.per.connection = 5
	metrics.num.samples = 2
	linger.ms = 0
	client.id = 

14:52:30.690 [ZkClient-EventThread-976-127.0.0.1:48002] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Controller 0 connected to id:0,host:testing-worker-linux-docker-cb11978e-3394-linux-9.prod.travis-ci.org,port:49175 for sending state change requests
14:52:30.706 [ZkClient-EventThread-976-127.0.0.1:48002] INFO  k.c.KafkaController - [Controller 0]: New broker startup callback for 0
14:52:30.708 [Controller-0-to-broker-0-send-thread] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Starting 
14:52:30.708 [main] INFO  k.s.KafkaServer - [Kafka Server 0], shutting down
14:52:30.708 [main] INFO  k.s.KafkaServer - [Kafka Server 0], Starting controlled shutdown
14:52:30.712 [kafka-request-handler-0] INFO  k.c.KafkaController - [Controller 0]: Shutting down broker 0
14:52:30.712 [main] INFO  k.s.KafkaServer - [Kafka Server 0], Controlled shutdown succeeded
14:52:30.713 [main] INFO  k.n.SocketServer - [Socket Server on Broker 0], Shutting down
14:52:30.713 [kafka-network-thread-49175-1] INFO  k.n.Processor - Closing socket connection to /172.17.9.115.
14:52:30.715 [main] INFO  k.n.SocketServer - [Socket Server on Broker 0], Shutdown completed
14:52:30.715 [main] INFO  k.s.KafkaRequestHandlerPool - [Kafka Request Handler on Broker 0], shutting down
14:52:30.716 [main] INFO  k.s.KafkaRequestHandlerPool - [Kafka Request Handler on Broker 0], shut down completely
14:52:30.717 [Curator-Framework-0-SendThread(127.0.0.1:35858)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
14:52:30.813 [Curator-Framework-0-SendThread(127.0.0.1:45954)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
14:52:31.014 [main] INFO  k.s.ReplicaManager - [Replica Manager on Broker 0]: Shut down
14:52:31.014 [main] INFO  k.s.ReplicaFetcherManager - [ReplicaFetcherManager on broker 0] shutting down
14:52:31.014 [main] INFO  k.s.ReplicaFetcherManager - [ReplicaFetcherManager on broker 0] shutdown completed
14:52:31.014 [main] INFO  k.s.ReplicaManager - [Replica Manager on Broker 0]: Shut down completely
14:52:31.014 [main] INFO  k.l.LogManager - Shutting down.
14:52:31.014 [main] INFO  k.l.LogManager - Shutdown complete.
14:52:31.014 [main] INFO  k.c.PartitionStateMachine - [Partition state machine on Controller 0]: Stopped partition state machine
14:52:31.014 [main] INFO  k.c.ReplicaStateMachine - [Replica state machine on controller 0]: Stopped replica state machine
14:52:31.015 [main] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Shutting down
14:52:31.015 [Controller-0-to-broker-0-send-thread] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Stopped 
14:52:31.016 [main] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Shutdown completed
14:52:31.016 [ZkClient-EventThread-976-127.0.0.1:48002] INFO  o.I.z.ZkEventThread - Terminate ZkClient event thread.
14:52:31.017 [main] INFO  k.s.KafkaServer - [Kafka Server 0], shut down completed
14:52:31.018 [Curator-Framework-0] INFO  o.a.c.f.i.CuratorFrameworkImpl - backgroundOperationsLoop exiting

--------------------------------------------------
--------------------------------------------------
classname: org.apache.storm.kafka.bolt.KafkaBoltTest / testname: executeWithByteArrayKeyAndMessageAsync
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.kafka.bolt.KafkaBoltTest.generateTestTuple(KafkaBoltTest.java:290)
	at org.apache.storm.kafka.bolt.KafkaBoltTest.executeWithByteArrayKeyAndMessageAsync(KafkaBoltTest.java:146)

-------------------- system-out --------------------
14:52:31.617 [Curator-Framework-0-SendThread(127.0.0.1:38865)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
14:52:31.617 [Curator-Framework-0-SendThread(127.0.0.1:56833)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
14:52:31.697 [Curator-Framework-0-SendThread(127.0.0.1:51624)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
14:52:31.717 [Curator-Framework-0-SendThread(127.0.0.1:33293)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
14:52:31.818 [Curator-Framework-0-SendThread(127.0.0.1:35858)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
14:52:31.914 [Curator-Framework-0-SendThread(127.0.0.1:45954)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
14:52:32.031 [main] INFO  o.a.c.f.i.CuratorFrameworkImpl - Starting
14:52:32.033 [main] INFO  k.u.VerifiableProperties - Verifying properties
14:52:32.033 [main] INFO  k.u.VerifiableProperties - Property broker.id is overridden to 0
14:52:32.033 [main] INFO  k.u.VerifiableProperties - Property log.dirs is overridden to /tmp/kafka/logs/kafka-test-33412
14:52:32.033 [main] INFO  k.u.VerifiableProperties - Property port is overridden to 33412
14:52:32.033 [main] INFO  k.u.VerifiableProperties - Property zookeeper.connect is overridden to 127.0.0.1:54210
14:52:32.035 [main] INFO  k.s.KafkaServer - [Kafka Server 0], starting
14:52:32.035 [main] INFO  k.s.KafkaServer - [Kafka Server 0], Connecting to zookeeper on 127.0.0.1:54210
14:52:32.035 [main-EventThread] INFO  o.a.c.f.s.ConnectionStateManager - State change: CONNECTED
14:52:32.035 [ZkClient-EventThread-1013-127.0.0.1:54210] INFO  o.I.z.ZkEventThread - Starting ZkClient event thread.
14:52:32.037 [main-EventThread] INFO  o.I.z.ZkClient - zookeeper state changed (SyncConnected)
14:52:32.051 [main] INFO  k.l.LogManager - Log directory '/tmp/kafka/logs/kafka-test-33412' not found, creating it.
14:52:32.051 [main] INFO  k.l.LogManager - Loading logs.
14:52:32.052 [main] INFO  k.l.LogManager - Logs loading complete.
14:52:32.052 [main] INFO  k.l.LogManager - Starting log cleanup with a period of 300000 ms.
14:52:32.052 [main] INFO  k.l.LogManager - Starting log flusher with a default period of 9223372036854775807 ms.
14:52:32.053 [main] INFO  k.n.Acceptor - Awaiting socket connections on 0.0.0.0:33412.
14:52:32.054 [main] INFO  k.n.SocketServer - [Socket Server on Broker 0], Started
14:52:32.056 [main] INFO  k.u.Mx4jLoader$ - Will not load MX4J, mx4j-tools.jar is not in the classpath
14:52:32.056 [main] INFO  k.c.KafkaController - [Controller 0]: Controller starting up
14:52:32.058 [main] INFO  k.s.ZookeeperLeaderElector - 0 successfully elected as leader
14:52:32.058 [main] INFO  k.c.KafkaController - [Controller 0]: Broker 0 starting become controller state transition
14:52:32.060 [main] INFO  k.c.KafkaController - [Controller 0]: Controller 0 incremented epoch to 1
14:52:32.062 [main] INFO  k.c.KafkaController - [Controller 0]: Partitions undergoing preferred replica election: 
14:52:32.062 [main] INFO  k.c.KafkaController - [Controller 0]: Partitions that completed preferred replica election: 
14:52:32.062 [main] INFO  k.c.KafkaController - [Controller 0]: Resuming preferred replica election for partitions: 
14:52:32.062 [main] INFO  k.c.KafkaController - [Controller 0]: Partitions being reassigned: Map()
14:52:32.062 [main] INFO  k.c.KafkaController - [Controller 0]: Partitions already reassigned: List()
14:52:32.062 [main] INFO  k.c.KafkaController - [Controller 0]: Resuming reassignment of partitions: Map()
14:52:32.065 [main] INFO  k.c.KafkaController - [Controller 0]: List of topics to be deleted: 
14:52:32.065 [main] INFO  k.c.KafkaController - [Controller 0]: List of topics ineligible for deletion: 
14:52:32.065 [main] INFO  k.c.KafkaController - [Controller 0]: Currently active brokers in the cluster: Set()
14:52:32.065 [main] INFO  k.c.KafkaController - [Controller 0]: Currently shutting brokers in the cluster: Set()
14:52:32.065 [main] INFO  k.c.KafkaController - [Controller 0]: Current list of topics in the cluster: Set()
14:52:32.065 [main] INFO  k.c.ReplicaStateMachine - [Replica state machine on controller 0]: Started replica state machine with initial state -> Map()
14:52:32.065 [main] INFO  k.c.PartitionStateMachine - [Partition state machine on Controller 0]: Started partition state machine with initial state -> Map()
14:52:32.065 [main] INFO  k.c.KafkaController - [Controller 0]: Broker 0 is ready to serve as the new controller with epoch 1
14:52:32.065 [main] INFO  k.c.KafkaController - [Controller 0]: Starting preferred replica leader election for partitions 
14:52:32.065 [main] INFO  k.c.PartitionStateMachine - [Partition state machine on Controller 0]: Invoking state change to OnlinePartition for partitions 
14:52:32.066 [main] INFO  k.c.KafkaController - [Controller 0]: starting the partition rebalance scheduler
14:52:32.067 [main] INFO  k.c.KafkaController - [Controller 0]: Controller startup complete
14:52:32.067 [ZkClient-EventThread-1013-127.0.0.1:54210] INFO  k.s.ZookeeperLeaderElector$LeaderChangeListener - New leader is 0
14:52:32.070 [main] INFO  k.u.ZkUtils$ - Registered broker 0 at path /brokers/ids/0 with address testing-worker-linux-docker-cb11978e-3394-linux-9.prod.travis-ci.org:33412.
14:52:32.070 [main] INFO  k.s.KafkaServer - [Kafka Server 0], started
14:52:32.071 [main] INFO  o.a.k.c.p.ProducerConfig - ProducerConfig values: 
	compression.type = none
	metric.reporters = []
	metadata.max.age.ms = 300000
	metadata.fetch.timeout.ms = 1000
	acks = 1
	batch.size = 16384
	reconnect.backoff.ms = 10
	bootstrap.servers = [localhost:33412]
	receive.buffer.bytes = 32768
	retry.backoff.ms = 100
	buffer.memory = 33554432
	timeout.ms = 30000
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	retries = 0
	max.request.size = 1048576
	block.on.buffer.full = true
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
	metrics.sample.window.ms = 30000
	send.buffer.bytes = 131072
	max.in.flight.requests.per.connection = 5
	metrics.num.samples = 2
	linger.ms = 0
	client.id = 

14:52:32.071 [ZkClient-EventThread-1013-127.0.0.1:54210] INFO  k.c.ReplicaStateMachine$BrokerChangeListener - [BrokerChangeListener on Controller 0]: Broker change listener fired for path /brokers/ids with children 0
14:52:32.072 [main] INFO  k.s.KafkaServer - [Kafka Server 0], shutting down
14:52:32.072 [main] INFO  k.s.KafkaServer - [Kafka Server 0], Starting controlled shutdown
14:52:32.079 [ZkClient-EventThread-1013-127.0.0.1:54210] INFO  k.c.ReplicaStateMachine$BrokerChangeListener - [BrokerChangeListener on Controller 0]: Newly added brokers: 0, deleted brokers: , all live brokers: 0
14:52:32.079 [ZkClient-EventThread-1013-127.0.0.1:54210] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Controller 0 connected to id:0,host:testing-worker-linux-docker-cb11978e-3394-linux-9.prod.travis-ci.org,port:33412 for sending state change requests
14:52:32.079 [ZkClient-EventThread-1013-127.0.0.1:54210] INFO  k.c.KafkaController - [Controller 0]: New broker startup callback for 0
14:52:32.079 [Controller-0-to-broker-0-send-thread] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Starting 
14:52:32.080 [kafka-request-handler-0] INFO  k.c.KafkaController - [Controller 0]: Shutting down broker 0
14:52:32.081 [main] INFO  k.s.KafkaServer - [Kafka Server 0], Controlled shutdown succeeded
14:52:32.081 [main] INFO  k.n.SocketServer - [Socket Server on Broker 0], Shutting down
14:52:32.081 [kafka-network-thread-33412-1] INFO  k.n.Processor - Closing socket connection to /172.17.9.115.
14:52:32.082 [main] INFO  k.n.SocketServer - [Socket Server on Broker 0], Shutdown completed
14:52:32.082 [main] INFO  k.s.KafkaRequestHandlerPool - [Kafka Request Handler on Broker 0], shutting down
14:52:32.082 [main] INFO  k.s.KafkaRequestHandlerPool - [Kafka Request Handler on Broker 0], shut down completely
14:52:32.455 [main] INFO  k.s.ReplicaManager - [Replica Manager on Broker 0]: Shut down
14:52:32.455 [main] INFO  k.s.ReplicaFetcherManager - [ReplicaFetcherManager on broker 0] shutting down
14:52:32.455 [main] INFO  k.s.ReplicaFetcherManager - [ReplicaFetcherManager on broker 0] shutdown completed
14:52:32.455 [main] INFO  k.s.ReplicaManager - [Replica Manager on Broker 0]: Shut down completely
14:52:32.455 [main] INFO  k.l.LogManager - Shutting down.
14:52:32.455 [main] INFO  k.l.LogManager - Shutdown complete.
14:52:32.456 [main] INFO  k.c.PartitionStateMachine - [Partition state machine on Controller 0]: Stopped partition state machine
14:52:32.456 [main] INFO  k.c.ReplicaStateMachine - [Replica state machine on controller 0]: Stopped replica state machine
14:52:32.456 [main] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Shutting down
14:52:32.456 [main] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Shutdown completed
14:52:32.456 [ZkClient-EventThread-1013-127.0.0.1:54210] INFO  o.I.z.ZkEventThread - Terminate ZkClient event thread.
14:52:32.456 [Controller-0-to-broker-0-send-thread] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Stopped 
14:52:32.458 [main] INFO  k.s.KafkaServer - [Kafka Server 0], shut down completed
14:52:32.459 [Curator-Framework-0] INFO  o.a.c.f.i.CuratorFrameworkImpl - backgroundOperationsLoop exiting

--------------------------------------------------
--------------------------------------------------
classname: org.apache.storm.kafka.bolt.KafkaBoltTest / testname: executeWithKey
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.kafka.bolt.KafkaBoltTest.generateTestTuple(KafkaBoltTest.java:290)
	at org.apache.storm.kafka.bolt.KafkaBoltTest.executeWithKey(KafkaBoltTest.java:115)

-------------------- system-out --------------------
14:52:32.718 [Curator-Framework-0-SendThread(127.0.0.1:38865)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
14:52:32.719 [Curator-Framework-0-SendThread(127.0.0.1:56833)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
14:52:32.798 [Curator-Framework-0-SendThread(127.0.0.1:51624)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
14:52:32.818 [Curator-Framework-0-SendThread(127.0.0.1:33293)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
14:52:32.918 [Curator-Framework-0-SendThread(127.0.0.1:35858)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
14:52:33.014 [Curator-Framework-0-SendThread(127.0.0.1:45954)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
14:52:33.487 [main] INFO  o.a.c.f.i.CuratorFrameworkImpl - Starting
14:52:33.491 [main] INFO  k.u.VerifiableProperties - Verifying properties
14:52:33.492 [main] INFO  k.u.VerifiableProperties - Property broker.id is overridden to 0
14:52:33.492 [main] INFO  k.u.VerifiableProperties - Property log.dirs is overridden to /tmp/kafka/logs/kafka-test-39211
14:52:33.492 [main] INFO  k.u.VerifiableProperties - Property port is overridden to 39211
14:52:33.492 [main] INFO  k.u.VerifiableProperties - Property zookeeper.connect is overridden to 127.0.0.1:53646
14:52:33.492 [main] INFO  k.s.KafkaServer - [Kafka Server 0], starting
14:52:33.492 [main] INFO  k.s.KafkaServer - [Kafka Server 0], Connecting to zookeeper on 127.0.0.1:53646
14:52:33.492 [ZkClient-EventThread-1049-127.0.0.1:53646] INFO  o.I.z.ZkEventThread - Starting ZkClient event thread.
14:52:33.506 [main-EventThread] INFO  o.a.c.f.s.ConnectionStateManager - State change: CONNECTED
14:52:33.507 [main-EventThread] INFO  o.I.z.ZkClient - zookeeper state changed (SyncConnected)
14:52:33.519 [main] INFO  k.l.LogManager - Log directory '/tmp/kafka/logs/kafka-test-39211' not found, creating it.
14:52:33.519 [main] INFO  k.l.LogManager - Loading logs.
14:52:33.519 [main] INFO  k.l.LogManager - Logs loading complete.
14:52:33.519 [main] INFO  k.l.LogManager - Starting log cleanup with a period of 300000 ms.
14:52:33.527 [main] INFO  k.l.LogManager - Starting log flusher with a default period of 9223372036854775807 ms.
14:52:33.529 [main] INFO  k.n.Acceptor - Awaiting socket connections on 0.0.0.0:39211.
14:52:33.529 [main] INFO  k.n.SocketServer - [Socket Server on Broker 0], Started
14:52:33.531 [main] INFO  k.u.Mx4jLoader$ - Will not load MX4J, mx4j-tools.jar is not in the classpath
14:52:33.532 [main] INFO  k.c.KafkaController - [Controller 0]: Controller starting up
14:52:33.538 [main] INFO  k.s.ZookeeperLeaderElector - 0 successfully elected as leader
14:52:33.538 [main] INFO  k.c.KafkaController - [Controller 0]: Broker 0 starting become controller state transition
14:52:33.541 [main] INFO  k.c.KafkaController - [Controller 0]: Controller 0 incremented epoch to 1
14:52:33.543 [main] INFO  k.c.KafkaController - [Controller 0]: Partitions undergoing preferred replica election: 
14:52:33.543 [main] INFO  k.c.KafkaController - [Controller 0]: Partitions that completed preferred replica election: 
14:52:33.543 [main] INFO  k.c.KafkaController - [Controller 0]: Resuming preferred replica election for partitions: 
14:52:33.544 [main] INFO  k.c.KafkaController - [Controller 0]: Partitions being reassigned: Map()
14:52:33.544 [main] INFO  k.c.KafkaController - [Controller 0]: Partitions already reassigned: List()
14:52:33.544 [main] INFO  k.c.KafkaController - [Controller 0]: Resuming reassignment of partitions: Map()
14:52:33.544 [main] INFO  k.c.KafkaController - [Controller 0]: List of topics to be deleted: 
14:52:33.544 [main] INFO  k.c.KafkaController - [Controller 0]: List of topics ineligible for deletion: 
14:52:33.544 [main] INFO  k.c.KafkaController - [Controller 0]: Currently active brokers in the cluster: Set()
14:52:33.544 [main] INFO  k.c.KafkaController - [Controller 0]: Currently shutting brokers in the cluster: Set()
14:52:33.544 [main] INFO  k.c.KafkaController - [Controller 0]: Current list of topics in the cluster: Set()
14:52:33.545 [main] INFO  k.c.ReplicaStateMachine - [Replica state machine on controller 0]: Started replica state machine with initial state -> Map()
14:52:33.545 [main] INFO  k.c.PartitionStateMachine - [Partition state machine on Controller 0]: Started partition state machine with initial state -> Map()
14:52:33.545 [main] INFO  k.c.KafkaController - [Controller 0]: Broker 0 is ready to serve as the new controller with epoch 1
14:52:33.545 [main] INFO  k.c.KafkaController - [Controller 0]: Starting preferred replica leader election for partitions 
14:52:33.545 [main] INFO  k.c.PartitionStateMachine - [Partition state machine on Controller 0]: Invoking state change to OnlinePartition for partitions 
14:52:33.546 [main] INFO  k.c.KafkaController - [Controller 0]: starting the partition rebalance scheduler
14:52:33.546 [main] INFO  k.c.KafkaController - [Controller 0]: Controller startup complete
14:52:33.547 [ZkClient-EventThread-1049-127.0.0.1:53646] INFO  k.s.ZookeeperLeaderElector$LeaderChangeListener - New leader is 0
14:52:33.549 [main] INFO  k.u.ZkUtils$ - Registered broker 0 at path /brokers/ids/0 with address testing-worker-linux-docker-cb11978e-3394-linux-9.prod.travis-ci.org:39211.
14:52:33.549 [main] INFO  k.s.KafkaServer - [Kafka Server 0], started
14:52:33.550 [main] INFO  o.a.k.c.p.ProducerConfig - ProducerConfig values: 
	compression.type = none
	metric.reporters = []
	metadata.max.age.ms = 300000
	metadata.fetch.timeout.ms = 1000
	acks = 1
	batch.size = 16384
	reconnect.backoff.ms = 10
	bootstrap.servers = [localhost:39211]
	receive.buffer.bytes = 32768
	retry.backoff.ms = 100
	buffer.memory = 33554432
	timeout.ms = 30000
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	retries = 0
	max.request.size = 1048576
	block.on.buffer.full = true
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
	metrics.sample.window.ms = 30000
	send.buffer.bytes = 131072
	max.in.flight.requests.per.connection = 5
	metrics.num.samples = 2
	linger.ms = 0
	client.id = 

14:52:33.551 [main] INFO  k.s.KafkaServer - [Kafka Server 0], shutting down
14:52:33.551 [main] INFO  k.s.KafkaServer - [Kafka Server 0], Starting controlled shutdown
14:52:33.553 [ZkClient-EventThread-1049-127.0.0.1:53646] INFO  k.c.ReplicaStateMachine$BrokerChangeListener - [BrokerChangeListener on Controller 0]: Broker change listener fired for path /brokers/ids with children 0
14:52:33.559 [ZkClient-EventThread-1049-127.0.0.1:53646] INFO  k.c.ReplicaStateMachine$BrokerChangeListener - [BrokerChangeListener on Controller 0]: Newly added brokers: 0, deleted brokers: , all live brokers: 0
14:52:33.559 [ZkClient-EventThread-1049-127.0.0.1:53646] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Controller 0 connected to id:0,host:testing-worker-linux-docker-cb11978e-3394-linux-9.prod.travis-ci.org,port:39211 for sending state change requests
14:52:33.559 [ZkClient-EventThread-1049-127.0.0.1:53646] INFO  k.c.KafkaController - [Controller 0]: New broker startup callback for 0
14:52:33.559 [Controller-0-to-broker-0-send-thread] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Starting 
14:52:33.560 [kafka-request-handler-0] INFO  k.c.KafkaController - [Controller 0]: Shutting down broker 0
14:52:33.560 [main] INFO  k.s.KafkaServer - [Kafka Server 0], Controlled shutdown succeeded
14:52:33.560 [main] INFO  k.n.SocketServer - [Socket Server on Broker 0], Shutting down
14:52:33.560 [kafka-network-thread-39211-0] INFO  k.n.Processor - Closing socket connection to /172.17.9.115.
14:52:33.562 [main] INFO  k.n.SocketServer - [Socket Server on Broker 0], Shutdown completed
14:52:33.562 [main] INFO  k.s.KafkaRequestHandlerPool - [Kafka Request Handler on Broker 0], shutting down
14:52:33.563 [main] INFO  k.s.KafkaRequestHandlerPool - [Kafka Request Handler on Broker 0], shut down completely
14:52:33.819 [Curator-Framework-0-SendThread(127.0.0.1:56833)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
14:52:33.819 [Curator-Framework-0-SendThread(127.0.0.1:38865)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
14:52:33.899 [Curator-Framework-0-SendThread(127.0.0.1:51624)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
14:52:33.919 [Curator-Framework-0-SendThread(127.0.0.1:33293)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
14:52:33.931 [main] INFO  k.s.ReplicaManager - [Replica Manager on Broker 0]: Shut down
14:52:33.931 [main] INFO  k.s.ReplicaFetcherManager - [ReplicaFetcherManager on broker 0] shutting down
14:52:33.931 [main] INFO  k.s.ReplicaFetcherManager - [ReplicaFetcherManager on broker 0] shutdown completed
14:52:33.931 [main] INFO  k.s.ReplicaManager - [Replica Manager on Broker 0]: Shut down completely
14:52:33.931 [main] INFO  k.l.LogManager - Shutting down.
14:52:33.931 [main] INFO  k.l.LogManager - Shutdown complete.
14:52:33.931 [main] INFO  k.c.PartitionStateMachine - [Partition state machine on Controller 0]: Stopped partition state machine
14:52:33.931 [main] INFO  k.c.ReplicaStateMachine - [Replica state machine on controller 0]: Stopped replica state machine
14:52:33.932 [main] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Shutting down
14:52:33.932 [Controller-0-to-broker-0-send-thread] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Stopped 
14:52:33.932 [main] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Shutdown completed
14:52:33.932 [ZkClient-EventThread-1049-127.0.0.1:53646] INFO  o.I.z.ZkEventThread - Terminate ZkClient event thread.
14:52:33.934 [main] INFO  k.s.KafkaServer - [Kafka Server 0], shut down completed
14:52:33.934 [Curator-Framework-0] INFO  o.a.c.f.i.CuratorFrameworkImpl - backgroundOperationsLoop exiting

--------------------------------------------------
--------------------------------------------------
classname: org.apache.storm.kafka.bolt.KafkaBoltTest / testname: executeWithBoltSpecifiedProperties
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.kafka.bolt.KafkaBoltTest.generateTestTuple(KafkaBoltTest.java:290)
	at org.apache.storm.kafka.bolt.KafkaBoltTest.executeWithBoltSpecifiedProperties(KafkaBoltTest.java:199)

-------------------- system-out --------------------
14:52:34.019 [Curator-Framework-0-SendThread(127.0.0.1:35858)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
14:52:34.115 [Curator-Framework-0-SendThread(127.0.0.1:45954)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
14:52:34.920 [Curator-Framework-0-SendThread(127.0.0.1:38865)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
14:52:34.920 [Curator-Framework-0-SendThread(127.0.0.1:56833)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
14:52:34.962 [main] INFO  o.a.c.f.i.CuratorFrameworkImpl - Starting
14:52:34.963 [main] INFO  k.u.VerifiableProperties - Verifying properties
14:52:34.964 [main] INFO  k.u.VerifiableProperties - Property broker.id is overridden to 0
14:52:34.964 [main] INFO  k.u.VerifiableProperties - Property log.dirs is overridden to /tmp/kafka/logs/kafka-test-37601
14:52:34.964 [main] INFO  k.u.VerifiableProperties - Property port is overridden to 37601
14:52:34.964 [main] INFO  k.u.VerifiableProperties - Property zookeeper.connect is overridden to 127.0.0.1:49744
14:52:34.964 [main] INFO  k.s.KafkaServer - [Kafka Server 0], starting
14:52:34.964 [main] INFO  k.s.KafkaServer - [Kafka Server 0], Connecting to zookeeper on 127.0.0.1:49744
14:52:34.965 [ZkClient-EventThread-1085-127.0.0.1:49744] INFO  o.I.z.ZkEventThread - Starting ZkClient event thread.
14:52:34.967 [main-EventThread] INFO  o.a.c.f.s.ConnectionStateManager - State change: CONNECTED
14:52:34.967 [main-EventThread] INFO  o.I.z.ZkClient - zookeeper state changed (SyncConnected)
14:52:34.979 [main] INFO  k.l.LogManager - Log directory '/tmp/kafka/logs/kafka-test-37601' not found, creating it.
14:52:34.979 [main] INFO  k.l.LogManager - Loading logs.
14:52:34.979 [main] INFO  k.l.LogManager - Logs loading complete.
14:52:34.980 [main] INFO  k.l.LogManager - Starting log cleanup with a period of 300000 ms.
14:52:34.980 [main] INFO  k.l.LogManager - Starting log flusher with a default period of 9223372036854775807 ms.
14:52:34.981 [main] INFO  k.n.Acceptor - Awaiting socket connections on 0.0.0.0:37601.
14:52:34.982 [main] INFO  k.n.SocketServer - [Socket Server on Broker 0], Started
14:52:34.988 [main] INFO  k.u.Mx4jLoader$ - Will not load MX4J, mx4j-tools.jar is not in the classpath
14:52:34.988 [main] INFO  k.c.KafkaController - [Controller 0]: Controller starting up
14:52:34.991 [main] INFO  k.s.ZookeeperLeaderElector - 0 successfully elected as leader
14:52:34.991 [main] INFO  k.c.KafkaController - [Controller 0]: Broker 0 starting become controller state transition
14:52:34.993 [main] INFO  k.c.KafkaController - [Controller 0]: Controller 0 incremented epoch to 1
14:52:34.995 [main] INFO  k.c.KafkaController - [Controller 0]: Partitions undergoing preferred replica election: 
14:52:34.995 [main] INFO  k.c.KafkaController - [Controller 0]: Partitions that completed preferred replica election: 
14:52:34.995 [main] INFO  k.c.KafkaController - [Controller 0]: Resuming preferred replica election for partitions: 
14:52:34.996 [main] INFO  k.c.KafkaController - [Controller 0]: Partitions being reassigned: Map()
14:52:34.996 [main] INFO  k.c.KafkaController - [Controller 0]: Partitions already reassigned: List()
14:52:34.996 [main] INFO  k.c.KafkaController - [Controller 0]: Resuming reassignment of partitions: Map()
14:52:34.996 [main] INFO  k.c.KafkaController - [Controller 0]: List of topics to be deleted: 
14:52:34.996 [main] INFO  k.c.KafkaController - [Controller 0]: List of topics ineligible for deletion: 
14:52:34.996 [main] INFO  k.c.KafkaController - [Controller 0]: Currently active brokers in the cluster: Set()
14:52:34.996 [main] INFO  k.c.KafkaController - [Controller 0]: Currently shutting brokers in the cluster: Set()
14:52:34.996 [main] INFO  k.c.KafkaController - [Controller 0]: Current list of topics in the cluster: Set()
14:52:34.996 [main] INFO  k.c.ReplicaStateMachine - [Replica state machine on controller 0]: Started replica state machine with initial state -> Map()
14:52:34.996 [main] INFO  k.c.PartitionStateMachine - [Partition state machine on Controller 0]: Started partition state machine with initial state -> Map()
14:52:34.996 [main] INFO  k.c.KafkaController - [Controller 0]: Broker 0 is ready to serve as the new controller with epoch 1
14:52:34.996 [main] INFO  k.c.KafkaController - [Controller 0]: Starting preferred replica leader election for partitions 
14:52:34.996 [main] INFO  k.c.PartitionStateMachine - [Partition state machine on Controller 0]: Invoking state change to OnlinePartition for partitions 
14:52:34.997 [main] INFO  k.c.KafkaController - [Controller 0]: starting the partition rebalance scheduler
14:52:34.998 [main] INFO  k.c.KafkaController - [Controller 0]: Controller startup complete
14:52:34.999 [ZkClient-EventThread-1085-127.0.0.1:49744] INFO  k.s.ZookeeperLeaderElector$LeaderChangeListener - New leader is 0
14:52:34.999 [Curator-Framework-0-SendThread(127.0.0.1:51624)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
14:52:35.003 [main] INFO  k.u.ZkUtils$ - Registered broker 0 at path /brokers/ids/0 with address testing-worker-linux-docker-cb11978e-3394-linux-9.prod.travis-ci.org:37601.
14:52:35.003 [main] INFO  k.s.KafkaServer - [Kafka Server 0], started
14:52:35.003 [main] INFO  o.a.k.c.p.ProducerConfig - ProducerConfig values: 
	compression.type = none
	metric.reporters = []
	metadata.max.age.ms = 300000
	metadata.fetch.timeout.ms = 1000
	acks = 1
	batch.size = 16384
	reconnect.backoff.ms = 10
	bootstrap.servers = [localhost:37601]
	receive.buffer.bytes = 32768
	retry.backoff.ms = 100
	buffer.memory = 33554432
	timeout.ms = 30000
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	retries = 0
	max.request.size = 1048576
	block.on.buffer.full = true
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
	metrics.sample.window.ms = 30000
	send.buffer.bytes = 131072
	max.in.flight.requests.per.connection = 5
	metrics.num.samples = 2
	linger.ms = 0
	client.id = 

14:52:35.004 [ZkClient-EventThread-1085-127.0.0.1:49744] INFO  k.c.ReplicaStateMachine$BrokerChangeListener - [BrokerChangeListener on Controller 0]: Broker change listener fired for path /brokers/ids with children 0
14:52:35.007 [main] INFO  o.a.k.c.p.ProducerConfig - ProducerConfig values: 
	compression.type = none
	metric.reporters = []
	metadata.max.age.ms = 300000
	metadata.fetch.timeout.ms = 1000
	acks = 1
	batch.size = 16384
	reconnect.backoff.ms = 10
	bootstrap.servers = [localhost:37601]
	receive.buffer.bytes = 32768
	retry.backoff.ms = 100
	buffer.memory = 33554432
	timeout.ms = 30000
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	retries = 0
	max.request.size = 1048576
	block.on.buffer.full = true
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	metrics.sample.window.ms = 30000
	send.buffer.bytes = 131072
	max.in.flight.requests.per.connection = 5
	metrics.num.samples = 2
	linger.ms = 0
	client.id = 

14:52:35.009 [ZkClient-EventThread-1085-127.0.0.1:49744] INFO  k.c.ReplicaStateMachine$BrokerChangeListener - [BrokerChangeListener on Controller 0]: Newly added brokers: 0, deleted brokers: , all live brokers: 0
14:52:35.010 [main] INFO  k.s.KafkaServer - [Kafka Server 0], shutting down
14:52:35.010 [main] INFO  k.s.KafkaServer - [Kafka Server 0], Starting controlled shutdown
14:52:35.010 [ZkClient-EventThread-1085-127.0.0.1:49744] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Controller 0 connected to id:0,host:testing-worker-linux-docker-cb11978e-3394-linux-9.prod.travis-ci.org,port:37601 for sending state change requests
14:52:35.011 [ZkClient-EventThread-1085-127.0.0.1:49744] INFO  k.c.KafkaController - [Controller 0]: New broker startup callback for 0
14:52:35.011 [Controller-0-to-broker-0-send-thread] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Starting 
14:52:35.014 [kafka-request-handler-1] INFO  k.c.KafkaController - [Controller 0]: Shutting down broker 0
14:52:35.015 [main] INFO  k.s.KafkaServer - [Kafka Server 0], Controlled shutdown succeeded
14:52:35.015 [kafka-network-thread-37601-1] INFO  k.n.Processor - Closing socket connection to /172.17.9.115.
14:52:35.015 [main] INFO  k.n.SocketServer - [Socket Server on Broker 0], Shutting down
14:52:35.016 [main] INFO  k.n.SocketServer - [Socket Server on Broker 0], Shutdown completed
14:52:35.016 [main] INFO  k.s.KafkaRequestHandlerPool - [Kafka Request Handler on Broker 0], shutting down
14:52:35.017 [main] INFO  k.s.KafkaRequestHandlerPool - [Kafka Request Handler on Broker 0], shut down completely
14:52:35.020 [Curator-Framework-0-SendThread(127.0.0.1:33293)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
14:52:35.120 [Curator-Framework-0-SendThread(127.0.0.1:35858)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
14:52:35.216 [Curator-Framework-0-SendThread(127.0.0.1:45954)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
14:52:35.384 [main] INFO  k.s.ReplicaManager - [Replica Manager on Broker 0]: Shut down
14:52:35.384 [main] INFO  k.s.ReplicaFetcherManager - [ReplicaFetcherManager on broker 0] shutting down
14:52:35.384 [main] INFO  k.s.ReplicaFetcherManager - [ReplicaFetcherManager on broker 0] shutdown completed
14:52:35.384 [main] INFO  k.s.ReplicaManager - [Replica Manager on Broker 0]: Shut down completely
14:52:35.384 [main] INFO  k.l.LogManager - Shutting down.
14:52:35.384 [main] INFO  k.l.LogManager - Shutdown complete.
14:52:35.385 [main] INFO  k.c.PartitionStateMachine - [Partition state machine on Controller 0]: Stopped partition state machine
14:52:35.385 [main] INFO  k.c.ReplicaStateMachine - [Replica state machine on controller 0]: Stopped replica state machine
14:52:35.385 [main] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Shutting down
14:52:35.386 [main] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Shutdown completed
14:52:35.386 [ZkClient-EventThread-1085-127.0.0.1:49744] INFO  o.I.z.ZkEventThread - Terminate ZkClient event thread.
14:52:35.386 [Controller-0-to-broker-0-send-thread] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Stopped 
14:52:35.388 [main] INFO  k.s.KafkaServer - [Kafka Server 0], shut down completed
14:52:35.388 [Curator-Framework-0] INFO  o.a.c.f.i.CuratorFrameworkImpl - backgroundOperationsLoop exiting

--------------------------------------------------
Checking ./external/storm-kafka/target/surefire-reports/TEST-org.apache.storm.kafka.ExponentialBackoffMsgRetryManagerTest.xml
Checking ./external/storm-kafka/target/surefire-reports/TEST-org.apache.storm.kafka.ZkCoordinatorTest.xml
Checking ./external/storm-kafka/target/surefire-reports/TEST-org.apache.storm.kafka.TridentKafkaTest.xml
Looking for errors in ./external/storm-redis/target/surefire-reports
Checking ./external/storm-redis/target/surefire-reports/TEST-org.apache.storm.redis.state.DefaultStateSerializerTest.xml
Checking ./external/storm-redis/target/surefire-reports/TEST-org.apache.storm.redis.state.RedisKeyValueStateTest.xml
Checking ./external/storm-redis/target/surefire-reports/TEST-org.apache.storm.redis.state.RedisKeyValueStateProviderTest.xml

travis_time:end:2d4d2080:start=1466779867521625168,finish=1466780232787477428,duration=365265852260[0K
[31;1mThe command "/bin/bash ./dev-tools/travis/travis-script.sh `pwd` $MODULES" exited with 1.[0m
travis_fold:start:cache.2[0Kstore build cache
travis_time:start:04c61172[0K
travis_time:end:04c61172:start=1466780232793865159,finish=1466780232798437532,duration=4572373[0Ktravis_time:start:17dbe090[0K[32;1mchange detected (content changed, file is created, or file is deleted):
/home/travis/.m2/repository/eigenbase/eigenbase-properties/1.1.4/eigenbase-properties-1.1.4.pom.lastUpdated
/home/travis/.m2/repository/io/confluent/kafka-avro-serializer/1.0/kafka-avro-serializer-1.0.pom.lastUpdated
/home/travis/.m2/repository/io/confluent/kafka-schema-registry-client/1.0/kafka-schema-registry-client-1.0.pom.lastUpdated
/home/travis/.m2/repository/net/hydromatic/linq4j/0.4/linq4j-0.4.pom.lastUpdated
/home/travis/.m2/repository/net/hydromatic/quidem/0.1.1/quidem-0.1.1.pom.lastUpdated
/home/travis/.m2/repository/org/apache/storm/flux/2.0.0-SNAPSHOT/maven-metadata-local.xml
/home/travis/.m2/repository/org/apache/storm/flux/2.0.0-SNAPSHOT/_remote.repositories
/home/travis/.m2/repository/org/apache/storm/flux-core/2.0.0-SNAPSHOT/flux-core-2.0.0-SNAPSHOT.jar
/home/travis/.m2/repository/org/apache/storm/flux-core/2.0.0-SNAPSHOT/maven-metadata-local.xml
/home/travis/.m2/repository/org/apache/storm/flux-core/2.0.0-SNAPSHOT/_remote.repositories
/home/travis/.m2/repository/org/ap
[0m
[32;1m...
[0m
[32;1mchanges detected, packing new archive[0m
.
.
.
.
.
.
.
.
[32;1muploading archive[0m

travis_time:end:17dbe090:start=1466780232803618722,finish=1466780306695358026,duration=73891739304[0Ktravis_fold:end:cache.2[0K
Done. Your build exited with 1.
