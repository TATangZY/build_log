Using worker: worker-linux-docker-a83abded.prod.travis-ci.org:travis-linux-4

travis_fold:start:system_info[0K[33;1mBuild system information[0m
Build language: java
Build group: stable
Build dist: precise
[34m[1mBuild image provisioning date and time[0m
Thu Feb  5 15:09:33 UTC 2015
[34m[1mOperating System Details[0m
Distributor ID:	Ubuntu
Description:	Ubuntu 12.04.5 LTS
Release:	12.04
Codename:	precise
[34m[1mLinux Version[0m
3.13.0-29-generic
[34m[1mCookbooks Version[0m
a68419e https://github.com/travis-ci/travis-cookbooks/tree/a68419e
[34m[1mGCC version[0m
gcc (Ubuntu/Linaro 4.6.3-1ubuntu5) 4.6.3
Copyright (C) 2011 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.

[34m[1mLLVM version[0m
clang version 3.4 (tags/RELEASE_34/final)
Target: x86_64-unknown-linux-gnu
Thread model: posix
[34m[1mPre-installed Ruby versions[0m
ruby-1.9.3-p551
[34m[1mPre-installed Node.js versions[0m
v0.10.36
[34m[1mPre-installed Go versions[0m
1.4.1
[34m[1mRedis version[0m
redis-server 2.8.19
[34m[1mriak version[0m
2.0.2
[34m[1mMongoDB version[0m
MongoDB 2.4.12
[34m[1mCouchDB version[0m
couchdb 1.6.1
[34m[1mNeo4j version[0m
1.9.4
[34m[1mRabbitMQ Version[0m
3.4.3
[34m[1mElasticSearch version[0m
1.4.0
[34m[1mInstalled Sphinx versions[0m
2.0.10
2.1.9
2.2.6
[34m[1mDefault Sphinx version[0m
2.2.6
[34m[1mInstalled Firefox version[0m
firefox 31.0esr
[34m[1mPhantomJS version[0m
1.9.8
[34m[1mant -version[0m
Apache Ant(TM) version 1.8.2 compiled on December 3 2011
[34m[1mmvn -version[0m
Apache Maven 3.2.5 (12a6b3acb947671f09b81f49094c53f426d8cea1; 2014-12-14T17:29:23+00:00)
Maven home: /usr/local/maven
Java version: 1.7.0_76, vendor: Oracle Corporation
Java home: /usr/lib/jvm/java-7-oracle/jre
Default locale: en_US, platform encoding: ANSI_X3.4-1968
OS name: "linux", version: "3.13.0-29-generic", arch: "amd64", family: "unix"
travis_fold:end:system_info[0K
travis_fold:start:fix.CVE-2015-7547[0K$ export DEBIAN_FRONTEND=noninteractive
W: Size of file /var/lib/apt/lists/us.archive.ubuntu.com_ubuntu_dists_precise-backports_multiverse_source_Sources.gz is not what the server reported 5886 5888
W: Failed to fetch http://ppa.launchpad.net/couchdb/stable/ubuntu/dists/precise/Release.gpg  Could not connect to ppa.launchpad.net:80 (91.189.95.83), connection timed out

W: Failed to fetch http://ppa.launchpad.net/git-core/v1.8/ubuntu/dists/precise/Release.gpg  Unable to connect to ppa.launchpad.net:http:

W: Failed to fetch http://ppa.launchpad.net/rwky/redis/ubuntu/dists/precise/Release.gpg  Unable to connect to ppa.launchpad.net:http:

W: Failed to fetch http://ppa.launchpad.net/travis-ci/zero-mq/ubuntu/dists/precise/Release.gpg  Unable to connect to ppa.launchpad.net:http:

W: Failed to fetch http://ppa.launchpad.net/ubuntugis/ppa/ubuntu/dists/precise/Release.gpg  Unable to connect to ppa.launchpad.net:http:

W: Failed to fetch http://ppa.launchpad.net/webupd8team/java/ubuntu/dists/precise/Release.gpg  Unable to connect to ppa.launchpad.net:http:

W: Some index files failed to download. They have been ignored, or old ones used instead.
Reading package lists...
Building dependency tree...
Reading state information...
The following extra packages will be installed:
  libc-bin libc-dev-bin libc6-dev
Suggested packages:
  glibc-doc
The following packages will be upgraded:
  libc-bin libc-dev-bin libc6 libc6-dev
4 upgraded, 0 newly installed, 0 to remove and 244 not upgraded.
Need to get 8,840 kB of archives.
After this operation, 14.3 kB disk space will be freed.
Get:1 http://us.archive.ubuntu.com/ubuntu/ precise-updates/main libc6-dev amd64 2.15-0ubuntu10.15 [2,943 kB]
Get:2 http://us.archive.ubuntu.com/ubuntu/ precise-updates/main libc-dev-bin amd64 2.15-0ubuntu10.15 [84.7 kB]
Get:3 http://us.archive.ubuntu.com/ubuntu/ precise-updates/main libc-bin amd64 2.15-0ubuntu10.15 [1,177 kB]
Get:4 http://us.archive.ubuntu.com/ubuntu/ precise-updates/main libc6 amd64 2.15-0ubuntu10.15 [4,636 kB]
Fetched 8,840 kB in 0s (15.1 MB/s)
Preconfiguring packages ...
(Reading database ... (Reading database ... 5%(Reading database ... 10%(Reading database ... 15%(Reading database ... 20%(Reading database ... 25%(Reading database ... 30%(Reading database ... 35%(Reading database ... 40%(Reading database ... 45%(Reading database ... 50%(Reading database ... 55%(Reading database ... 60%(Reading database ... 65%(Reading database ... 70%(Reading database ... 75%(Reading database ... 80%(Reading database ... 85%(Reading database ... 90%(Reading database ... 95%(Reading database ... 100%(Reading database ... 72019 files and directories currently installed.)
Preparing to replace libc6-dev 2.15-0ubuntu10.10 (using .../libc6-dev_2.15-0ubuntu10.15_amd64.deb) ...
Unpacking replacement libc6-dev ...
Preparing to replace libc-dev-bin 2.15-0ubuntu10.10 (using .../libc-dev-bin_2.15-0ubuntu10.15_amd64.deb) ...
Unpacking replacement libc-dev-bin ...
Preparing to replace libc-bin 2.15-0ubuntu10.10 (using .../libc-bin_2.15-0ubuntu10.15_amd64.deb) ...
Unpacking replacement libc-bin ...
Processing triggers for man-db ...
Setting up libc-bin (2.15-0ubuntu10.15) ...
(Reading database ... (Reading database ... 5%(Reading database ... 10%(Reading database ... 15%(Reading database ... 20%(Reading database ... 25%(Reading database ... 30%(Reading database ... 35%(Reading database ... 40%(Reading database ... 45%(Reading database ... 50%(Reading database ... 55%(Reading database ... 60%(Reading database ... 65%(Reading database ... 70%(Reading database ... 75%(Reading database ... 80%(Reading database ... 85%(Reading database ... 90%(Reading database ... 95%(Reading database ... 100%(Reading database ... 72018 files and directories currently installed.)
Preparing to replace libc6 2.15-0ubuntu10.10 (using .../libc6_2.15-0ubuntu10.15_amd64.deb) ...
Unpacking replacement libc6 ...
Setting up libc6 (2.15-0ubuntu10.15) ...
Setting up libc-dev-bin (2.15-0ubuntu10.15) ...
Setting up libc6-dev (2.15-0ubuntu10.15) ...
Processing triggers for libc-bin ...
ldconfig deferred processing now taking place
travis_fold:end:fix.CVE-2015-7547[0Ktravis_fold:start:git.checkout[0Ktravis_time:start:0de74180[0K$ git clone --depth=50 https://github.com/apache/storm.git apache/storm
Cloning into 'apache/storm'...
remote: Counting objects: 17967, done.[K
remote: Compressing objects:   0% (1/7268)   [Kremote: Compressing objects:   1% (73/7268)   [Kremote: Compressing objects:   2% (146/7268)   [Kremote: Compressing objects:   3% (219/7268)   [Kremote: Compressing objects:   4% (291/7268)   [Kremote: Compressing objects:   5% (364/7268)   [Kremote: Compressing objects:   6% (437/7268)   [Kremote: Compressing objects:   7% (509/7268)   [Kremote: Compressing objects:   8% (582/7268)   [Kremote: Compressing objects:   9% (655/7268)   [Kremote: Compressing objects:  10% (727/7268)   [Kremote: Compressing objects:  11% (800/7268)   [Kremote: Compressing objects:  12% (873/7268)   [Kremote: Compressing objects:  13% (945/7268)   [Kremote: Compressing objects:  14% (1018/7268)   [Kremote: Compressing objects:  15% (1091/7268)   [Kremote: Compressing objects:  16% (1163/7268)   [Kremote: Compressing objects:  17% (1236/7268)   [Kremote: Compressing objects:  18% (1309/7268)   [Kremote: Compressing objects:  19% (1381/7268)   [Kremote: Compressing objects:  20% (1454/7268)   [Kremote: Compressing objects:  21% (1527/7268)   [Kremote: Compressing objects:  22% (1599/7268)   [Kremote: Compressing objects:  23% (1672/7268)   [Kremote: Compressing objects:  24% (1745/7268)   [Kremote: Compressing objects:  25% (1817/7268)   [Kremote: Compressing objects:  26% (1890/7268)   [Kremote: Compressing objects:  27% (1963/7268)   [Kremote: Compressing objects:  28% (2036/7268)   [Kremote: Compressing objects:  29% (2108/7268)   [Kremote: Compressing objects:  30% (2181/7268)   [Kremote: Compressing objects:  31% (2254/7268)   [Kremote: Compressing objects:  32% (2326/7268)   [Kremote: Compressing objects:  33% (2399/7268)   [Kremote: Compressing objects:  34% (2472/7268)   [Kremote: Compressing objects:  35% (2544/7268)   [Kremote: Compressing objects:  36% (2617/7268)   [Kremote: Compressing objects:  37% (2690/7268)   [Kremote: Compressing objects:  38% (2762/7268)   [Kremote: Compressing objects:  39% (2835/7268)   [Kremote: Compressing objects:  40% (2908/7268)   [Kremote: Compressing objects:  41% (2980/7268)   [Kremote: Compressing objects:  42% (3053/7268)   [Kremote: Compressing objects:  43% (3126/7268)   [Kremote: Compressing objects:  44% (3198/7268)   [Kremote: Compressing objects:  45% (3271/7268)   [Kremote: Compressing objects:  46% (3344/7268)   [Kremote: Compressing objects:  47% (3416/7268)   [Kremote: Compressing objects:  48% (3489/7268)   [Kremote: Compressing objects:  49% (3562/7268)   [Kremote: Compressing objects:  50% (3634/7268)   [Kremote: Compressing objects:  51% (3707/7268)   [Kremote: Compressing objects:  52% (3780/7268)   [Kremote: Compressing objects:  53% (3853/7268)   [Kremote: Compressing objects:  54% (3925/7268)   [Kremote: Compressing objects:  55% (3998/7268)   [Kremote: Compressing objects:  56% (4071/7268)   [Kremote: Compressing objects:  57% (4143/7268)   [Kremote: Compressing objects:  58% (4216/7268)   [Kremote: Compressing objects:  59% (4289/7268)   [Kremote: Compressing objects:  60% (4361/7268)   [Kremote: Compressing objects:  61% (4434/7268)   [Kremote: Compressing objects:  62% (4507/7268)   [Kremote: Compressing objects:  63% (4579/7268)   [Kremote: Compressing objects:  64% (4652/7268)   [Kremote: Compressing objects:  65% (4725/7268)   [Kremote: Compressing objects:  66% (4797/7268)   [Kremote: Compressing objects:  67% (4870/7268)   [Kremote: Compressing objects:  68% (4943/7268)   [Kremote: Compressing objects:  69% (5015/7268)   [Kremote: Compressing objects:  70% (5088/7268)   [Kremote: Compressing objects:  71% (5161/7268)   [Kremote: Compressing objects:  72% (5233/7268)   [Kremote: Compressing objects:  73% (5306/7268)   [Kremote: Compressing objects:  74% (5379/7268)   [Kremote: Compressing objects:  75% (5451/7268)   [Kremote: Compressing objects:  76% (5524/7268)   [Kremote: Compressing objects:  77% (5597/7268)   [Kremote: Compressing objects:  78% (5670/7268)   [Kremote: Compressing objects:  79% (5742/7268)   [Kremote: Compressing objects:  80% (5815/7268)   [Kremote: Compressing objects:  81% (5888/7268)   [Kremote: Compressing objects:  82% (5960/7268)   [Kremote: Compressing objects:  83% (6033/7268)   [Kremote: Compressing objects:  84% (6106/7268)   [Kremote: Compressing objects:  85% (6178/7268)   [Kremote: Compressing objects:  86% (6251/7268)   [Kremote: Compressing objects:  87% (6324/7268)   [Kremote: Compressing objects:  88% (6396/7268)   [Kremote: Compressing objects:  89% (6469/7268)   [Kremote: Compressing objects:  90% (6542/7268)   [Kremote: Compressing objects:  91% (6614/7268)   [Kremote: Compressing objects:  92% (6687/7268)   [Kremote: Compressing objects:  93% (6760/7268)   [Kremote: Compressing objects:  94% (6832/7268)   [Kremote: Compressing objects:  95% (6905/7268)   [Kremote: Compressing objects:  96% (6978/7268)   [Kremote: Compressing objects:  97% (7050/7268)   [Kremote: Compressing objects:  98% (7123/7268)   [Kremote: Compressing objects:  99% (7196/7268)   [Kremote: Compressing objects: 100% (7268/7268)   [Kremote: Compressing objects: 100% (7268/7268), done.[K
Receiving objects:   0% (1/17967)   Receiving objects:   1% (180/17967)   Receiving objects:   2% (360/17967)   Receiving objects:   3% (540/17967)   Receiving objects:   4% (719/17967)   Receiving objects:   5% (899/17967)   Receiving objects:   6% (1079/17967)   Receiving objects:   7% (1258/17967)   Receiving objects:   8% (1438/17967)   Receiving objects:   9% (1618/17967)   Receiving objects:  10% (1797/17967)   Receiving objects:  11% (1977/17967)   Receiving objects:  12% (2157/17967)   Receiving objects:  13% (2336/17967)   Receiving objects:  14% (2516/17967)   Receiving objects:  15% (2696/17967)   Receiving objects:  16% (2875/17967)   Receiving objects:  17% (3055/17967)   Receiving objects:  18% (3235/17967)   Receiving objects:  19% (3414/17967)   Receiving objects:  20% (3594/17967)   Receiving objects:  21% (3774/17967)   Receiving objects:  22% (3953/17967)   Receiving objects:  23% (4133/17967)   Receiving objects:  24% (4313/17967)   Receiving objects:  25% (4492/17967)   Receiving objects:  26% (4672/17967)   Receiving objects:  27% (4852/17967)   Receiving objects:  28% (5031/17967)   Receiving objects:  29% (5211/17967)   Receiving objects:  30% (5391/17967)   Receiving objects:  31% (5570/17967)   Receiving objects:  32% (5750/17967)   Receiving objects:  33% (5930/17967)   Receiving objects:  34% (6109/17967)   Receiving objects:  35% (6289/17967)   Receiving objects:  36% (6469/17967)   Receiving objects:  37% (6648/17967)   Receiving objects:  38% (6828/17967)   Receiving objects:  39% (7008/17967)   Receiving objects:  40% (7187/17967)   Receiving objects:  41% (7367/17967)   Receiving objects:  42% (7547/17967)   Receiving objects:  43% (7726/17967)   Receiving objects:  44% (7906/17967)   Receiving objects:  45% (8086/17967)   Receiving objects:  46% (8265/17967)   Receiving objects:  47% (8445/17967)   Receiving objects:  48% (8625/17967)   Receiving objects:  49% (8804/17967)   Receiving objects:  50% (8984/17967)   Receiving objects:  51% (9164/17967)   Receiving objects:  52% (9343/17967)   Receiving objects:  53% (9523/17967)   Receiving objects:  54% (9703/17967)   Receiving objects:  55% (9882/17967)   Receiving objects:  56% (10062/17967)   Receiving objects:  57% (10242/17967)   Receiving objects:  58% (10421/17967)   Receiving objects:  59% (10601/17967)   Receiving objects:  60% (10781/17967)   Receiving objects:  61% (10960/17967)   Receiving objects:  62% (11140/17967)   Receiving objects:  63% (11320/17967)   Receiving objects:  64% (11499/17967)   Receiving objects:  65% (11679/17967)   Receiving objects:  66% (11859/17967)   Receiving objects:  67% (12038/17967)   Receiving objects:  68% (12218/17967)   Receiving objects:  69% (12398/17967)   Receiving objects:  70% (12577/17967)   Receiving objects:  71% (12757/17967)   Receiving objects:  72% (12937/17967)   Receiving objects:  73% (13116/17967)   Receiving objects:  74% (13296/17967)   Receiving objects:  75% (13476/17967)   Receiving objects:  76% (13655/17967)   Receiving objects:  77% (13835/17967)   Receiving objects:  78% (14015/17967)   Receiving objects:  79% (14194/17967)   Receiving objects:  80% (14374/17967), 12.25 MiB | 24.44 MiB/s   Receiving objects:  81% (14554/17967), 12.25 MiB | 24.44 MiB/s   Receiving objects:  82% (14733/17967), 12.25 MiB | 24.44 MiB/s   Receiving objects:  83% (14913/17967), 12.25 MiB | 24.44 MiB/s   Receiving objects:  84% (15093/17967), 12.25 MiB | 24.44 MiB/s   Receiving objects:  85% (15272/17967), 12.25 MiB | 24.44 MiB/s   Receiving objects:  86% (15452/17967), 12.25 MiB | 24.44 MiB/s   Receiving objects:  87% (15632/17967), 12.25 MiB | 24.44 MiB/s   Receiving objects:  88% (15811/17967), 12.25 MiB | 24.44 MiB/s   Receiving objects:  89% (15991/17967), 12.25 MiB | 24.44 MiB/s   Receiving objects:  90% (16171/17967), 12.25 MiB | 24.44 MiB/s   Receiving objects:  91% (16350/17967), 12.25 MiB | 24.44 MiB/s   Receiving objects:  92% (16530/17967), 12.25 MiB | 24.44 MiB/s   Receiving objects:  93% (16710/17967), 12.25 MiB | 24.44 MiB/s   Receiving objects:  94% (16889/17967), 12.25 MiB | 24.44 MiB/s   Receiving objects:  95% (17069/17967), 12.25 MiB | 24.44 MiB/s   Receiving objects:  96% (17249/17967), 12.25 MiB | 24.44 MiB/s   Receiving objects:  97% (17428/17967), 12.25 MiB | 24.44 MiB/s   remote: Total 17967 (delta 8445), reused 15465 (delta 6589), pack-reused 0[K
Receiving objects:  98% (17608/17967), 12.25 MiB | 24.44 MiB/s   Receiving objects:  99% (17788/17967), 12.25 MiB | 24.44 MiB/s   Receiving objects: 100% (17967/17967), 12.25 MiB | 24.44 MiB/s   Receiving objects: 100% (17967/17967), 14.42 MiB | 24.44 MiB/s, done.
Resolving deltas:   0% (0/8445)   Resolving deltas:   1% (85/8445)   Resolving deltas:   2% (173/8445)   Resolving deltas:   3% (255/8445)   Resolving deltas:   4% (340/8445)   Resolving deltas:   5% (441/8445)   Resolving deltas:   6% (519/8445)   Resolving deltas:   7% (597/8445)   Resolving deltas:   8% (677/8445)   Resolving deltas:   9% (761/8445)   Resolving deltas:  10% (848/8445)   Resolving deltas:  11% (929/8445)   Resolving deltas:  12% (1020/8445)   Resolving deltas:  13% (1103/8445)   Resolving deltas:  14% (1213/8445)   Resolving deltas:  15% (1268/8445)   Resolving deltas:  16% (1352/8445)   Resolving deltas:  17% (1441/8445)   Resolving deltas:  18% (1522/8445)   Resolving deltas:  19% (1607/8445)   Resolving deltas:  20% (1689/8445)   Resolving deltas:  21% (1775/8445)   Resolving deltas:  22% (1859/8445)   Resolving deltas:  23% (1946/8445)   Resolving deltas:  24% (2027/8445)   Resolving deltas:  25% (2114/8445)   Resolving deltas:  27% (2291/8445)   Resolving deltas:  28% (2366/8445)   Resolving deltas:  29% (2450/8445)   Resolving deltas:  30% (2536/8445)   Resolving deltas:  31% (2620/8445)   Resolving deltas:  32% (2707/8445)   Resolving deltas:  33% (2792/8445)   Resolving deltas:  34% (2877/8445)   Resolving deltas:  35% (2957/8445)   Resolving deltas:  36% (3041/8445)   Resolving deltas:  37% (3131/8445)   Resolving deltas:  38% (3212/8445)   Resolving deltas:  39% (3297/8445)   Resolving deltas:  40% (3381/8445)   Resolving deltas:  41% (3465/8445)   Resolving deltas:  42% (3548/8445)   Resolving deltas:  43% (3632/8445)   Resolving deltas:  44% (3718/8445)   Resolving deltas:  45% (3804/8445)   Resolving deltas:  46% (3890/8445)   Resolving deltas:  48% (4073/8445)   Resolving deltas:  49% (4139/8445)   Resolving deltas:  50% (4233/8445)   Resolving deltas:  51% (4328/8445)   Resolving deltas:  52% (4393/8445)   Resolving deltas:  53% (4477/8445)   Resolving deltas:  54% (4562/8445)   Resolving deltas:  55% (4647/8445)   Resolving deltas:  59% (4999/8445)   Resolving deltas:  60% (5078/8445)   Resolving deltas:  61% (5193/8445)   Resolving deltas:  62% (5269/8445)   Resolving deltas:  63% (5403/8445)   Resolving deltas:  64% (5405/8445)   Resolving deltas:  66% (5606/8445)   Resolving deltas:  67% (5687/8445)   Resolving deltas:  68% (5818/8445)   Resolving deltas:  69% (5909/8445)   Resolving deltas:  71% (5996/8445)   Resolving deltas:  72% (6139/8445)   Resolving deltas:  73% (6191/8445)   Resolving deltas:  74% (6266/8445)   Resolving deltas:  75% (6388/8445)   Resolving deltas:  76% (6463/8445)   Resolving deltas:  77% (6519/8445)   Resolving deltas:  78% (6591/8445)   Resolving deltas:  79% (6678/8445)   Resolving deltas:  80% (6770/8445)   Resolving deltas:  81% (6848/8445)   Resolving deltas:  82% (6928/8445)   Resolving deltas:  83% (7011/8445)   Resolving deltas:  84% (7101/8445)   Resolving deltas:  85% (7182/8445)   Resolving deltas:  86% (7268/8445)   Resolving deltas:  87% (7404/8445)   Resolving deltas:  88% (7482/8445)   Resolving deltas:  91% (7721/8445)   Resolving deltas:  92% (7774/8445)   Resolving deltas:  93% (7882/8445)   Resolving deltas:  94% (7940/8445)   Resolving deltas:  95% (8024/8445)   Resolving deltas:  96% (8108/8445)   Resolving deltas:  97% (8192/8445)   Resolving deltas:  98% (8278/8445)   Resolving deltas:  99% (8375/8445)   Resolving deltas: 100% (8445/8445)   Resolving deltas: 100% (8445/8445), done.
Checking connectivity... done.

travis_time:end:0de74180:start=1466699311219348616,finish=1466699315959712327,duration=4740363711[0K$ cd apache/storm
travis_time:start:0b3e0e57[0K$ git fetch origin +refs/pull/1515/merge:
remote: Counting objects: 11, done.[K
remote: Compressing objects:  20% (1/5)   [Kremote: Compressing objects:  40% (2/5)   [Kremote: Compressing objects:  60% (3/5)   [Kremote: Compressing objects:  80% (4/5)   [Kremote: Compressing objects: 100% (5/5)   [Kremote: Compressing objects: 100% (5/5), done.[K
remote: Total 11 (delta 5), reused 9 (delta 3), pack-reused 0[K
Unpacking objects:   9% (1/11)   Unpacking objects:  18% (2/11)   Unpacking objects:  27% (3/11)   Unpacking objects:  36% (4/11)   Unpacking objects:  45% (5/11)   Unpacking objects:  54% (6/11)   Unpacking objects:  63% (7/11)   Unpacking objects:  72% (8/11)   Unpacking objects:  81% (9/11)   Unpacking objects:  90% (10/11)   Unpacking objects: 100% (11/11)   Unpacking objects: 100% (11/11), done.
From https://github.com/apache/storm
 * branch            refs/pull/1515/merge -> FETCH_HEAD

travis_time:end:0b3e0e57:start=1466699315963288376,finish=1466699316291103510,duration=327815134[0K$ git checkout -qf FETCH_HEAD
travis_fold:end:git.checkout[0K
[33;1mThis job is running on container-based infrastructure, which does not allow use of 'sudo', setuid and setguid executables.[0m
[33;1mIf you require sudo, add 'sudo: required' to your .travis.yml[0m
[33;1mSee https://docs.travis-ci.com/user/workers/container-based-infrastructure/ for details.[0m

[33;1mSetting environment variables from .travis.yml[0m
$ export MODULES='!storm-core'

$ jdk_switcher use oraclejdk8
Switching to Oracle JDK8 (java-8-oracle), JAVA_HOME will be set to /usr/lib/jvm/java-8-oracle
travis_fold:start:cache.1[0KSetting up build cache
$ export CASHER_DIR=$HOME/.casher
travis_time:start:0f982c77[0K$ Installing caching utilities

travis_time:end:0f982c77:start=1466699321964064073,finish=1466699322065544248,duration=101480175[0Ktravis_time:start:00c4cc8f[0K
travis_time:end:00c4cc8f:start=1466699322069957691,finish=1466699322072971515,duration=3013824[0Ktravis_time:start:157b0457[0K[32;1mattempting to download cache archive[0m
[32;1mfetching PR.1515/cache-linux-precise-0b3559b5dae2b3e32156c6d840f23972d650aa066922e3318f7e5db8246681fa--jdk-oraclejdk8.tgz[0m
[32;1mfetching PR.1515/cache--jdk-oraclejdk8.tgz[0m
[32;1mfetching master/cache-linux-precise-0b3559b5dae2b3e32156c6d840f23972d650aa066922e3318f7e5db8246681fa--jdk-oraclejdk8.tgz[0m
[32;1mfound cache[0m

travis_time:end:157b0457:start=1466699322076487027,finish=1466699332979280973,duration=10902793946[0Ktravis_time:start:00696969[0K
travis_time:end:00696969:start=1466699332982827251,finish=1466699332985909722,duration=3082471[0Ktravis_time:start:154f3182[0K[32;1madding /home/travis/.m2/repository to cache[0m
[32;1madding /home/travis/.rvm to cache[0m
[32;1madding /home/travis/.nvm to cache[0m

travis_time:end:154f3182:start=1466699332989395317,finish=1466699346222690309,duration=13233294992[0Ktravis_fold:end:cache.1[0K$ java -Xmx32m -version
java version "1.8.0_31"
Java(TM) SE Runtime Environment (build 1.8.0_31-b13)
Java HotSpot(TM) 64-Bit Server VM (build 25.31-b07, mixed mode)
$ javac -J-Xmx32m -version
javac 1.8.0_31
travis_fold:start:before_install.1[0Ktravis_time:start:1248e4f3[0K$ rvm use 2.1.5 --install
[32mUsing /home/travis/.rvm/gems/ruby-2.1.5[0m

travis_time:end:1248e4f3:start=1466699346818726743,finish=1466699347088178263,duration=269451520[0Ktravis_fold:end:before_install.1[0Ktravis_fold:start:before_install.2[0Ktravis_time:start:02f9cfb0[0K$ nvm install 0.12.2
v0.12.2 is already installed.
Now using node v0.12.2

travis_time:end:02f9cfb0:start=1466699347092391581,finish=1466699347402467111,duration=310075530[0Ktravis_fold:end:before_install.2[0Ktravis_fold:start:before_install.3[0Ktravis_time:start:00789922[0K$ nvm use 0.12.2
Now using node v0.12.2

travis_time:end:00789922:start=1466699347406765406,finish=1466699347466992501,duration=60227095[0Ktravis_fold:end:before_install.3[0Ktravis_fold:start:install[0Ktravis_time:start:29361734[0K$ /bin/bash ./dev-tools/travis/travis-install.sh `pwd`
Python version :   Python 2.7.3
Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=192m; support was removed in 8.0
Maven version  :   Apache Maven 3.2.5 (12a6b3acb947671f09b81f49094c53f426d8cea1; 2014-12-14T17:29:23+00:00) Maven home: /usr/local/maven Java version: 1.8.0_31, vendor: Oracle Corporation Java home: /usr/lib/jvm/java-8-oracle/jre Default locale: en_US, platform encoding: UTF-8 OS name: "linux", version: "3.13.0-40-generic", arch: "amd64", family: "unix"
['mvn', 'clean', 'install', '-DskipTests', '-Pnative', '--batch-mode'] writing to install.txt
2 seconds 2 log lines12 seconds 75 log lines23 seconds 238 log lines64 seconds 324 log lines76 seconds 331 log lines86 seconds 407 log lines98 seconds 573 log lines113 seconds 653 log lines123 seconds 827 log lines134 seconds 866 log lines145 seconds 884 log lines155 seconds 964 log lines165 seconds 1223 log lines185 seconds 1478 log lines195 seconds 1701 log lines206 seconds 2116 log lines217 seconds 2237 log lines227 seconds 2498 log lines240 seconds 2684 log lines240 seconds 2685 log lines
['mvn', 'clean', 'install', '-DskipTests', '-Pnative', '--batch-mode'] done 0

travis_time:end:29361734:start=1466699347471460928,finish=1466699591532512977,duration=244061052049[0Ktravis_fold:end:install[0Ktravis_time:start:14930bdd[0K$ /bin/bash ./dev-tools/travis/travis-script.sh `pwd` $MODULES
Python version :   Python 2.7.3
Ruby version   :   ruby 2.1.5p273 (2014-11-13 revision 48405) [x86_64-linux]
NodeJs version :   v0.12.2
Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=192m; support was removed in 8.0
Maven version  :   Apache Maven 3.2.5 (12a6b3acb947671f09b81f49094c53f426d8cea1; 2014-12-14T17:29:23+00:00) Maven home: /usr/local/maven Java version: 1.8.0_31, vendor: Oracle Corporation Java home: /usr/lib/jvm/java-8-oracle/jre Default locale: en_US, platform encoding: UTF-8 OS name: "linux", version: "3.13.0-40-generic", arch: "amd64", family: "unix"
Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=192m; support was removed in 8.0
[INFO] Scanning for projects...
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Build Order:
[INFO] 
[INFO] Storm
[INFO] multilang-javascript
[INFO] multilang-python
[INFO] multilang-ruby
[INFO] maven-shade-clojure-transformer
[INFO] storm-maven-plugins
[INFO] storm-rename-hack
[INFO] storm-kafka
[INFO] storm-hdfs
[INFO] storm-hbase
[INFO] storm-hive
[INFO] storm-jdbc
[INFO] storm-redis
[INFO] storm-eventhubs
[INFO] flux
[INFO] flux-wrappers
[INFO] flux-core
[INFO] flux-examples
[INFO] storm-sql-runtime
[INFO] storm-sql-core
[INFO] storm-sql-kafka
[INFO] sql
[INFO] storm-elasticsearch
[INFO] storm-solr
[INFO] storm-metrics
[INFO] storm-cassandra
[INFO] storm-mqtt-parent
[INFO] storm-mqtt
[INFO] storm-mqtt-examples
[INFO] storm-mongodb
[INFO] storm-clojure
[INFO] storm-starter
[INFO] storm-kafka-client
[INFO] storm-opentsdb
[INFO] storm-kafka-monitor
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Storm 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ storm ---
[INFO] 
[INFO] --- apache-rat-plugin:0.11:check (default) @ storm ---
[INFO] 51 implicit excludes (use -debug for more details).
[INFO] Exclude: **/target/**
[INFO] Exclude: **/*.iml
[INFO] Exclude: **/.idea/**
[INFO] Exclude: **/metastore_db/**
[INFO] Exclude: **/build/**
[INFO] Exclude: **/logs/**
[INFO] Exclude: **/CHANGELOG.md
[INFO] Exclude: **/README.md
[INFO] Exclude: **/README.markdown
[INFO] Exclude: **/DEVELOPER.md
[INFO] Exclude: **/BYLAWS.md
[INFO] Exclude: **/STORM-UI-REST-API.md
[INFO] Exclude: SECURITY.md
[INFO] Exclude: VERSION
[INFO] Exclude: TODO
[INFO] Exclude: **/src/py/**
[INFO] Exclude: **/src/ui/public/js/jquery.dataTables.1.10.4.min.js
[INFO] Exclude: **/src/ui/public/css/jquery.dataTables.1.10.4.min.css
[INFO] Exclude: **/src/ui/public/images/*
[INFO] Exclude: **/src/ui/public/js/bootstrap-3.3.1.min.js
[INFO] Exclude: **/src/ui/public/css/bootstrap-3.3.1.min.css
[INFO] Exclude: **/src/ui/public/js/dataTables.bootstrap.min.js
[INFO] Exclude: **/src/ui/public/css/dataTables.bootstrap.css
[INFO] Exclude: **/src/ui/public/js/jsonFormatter.min.js
[INFO] Exclude: **/src/ui/public/css/jsonFormatter.min.css
[INFO] Exclude: **/src/ui/public/js/jquery-1.11.1.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.cookies.2.2.0.min.js
[INFO] Exclude: **/src/ui/public/js/moment.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.blockUI.min.js
[INFO] Exclude: **/src/ui/public/js/url.min.js
[INFO] Exclude: **/src/ui/public/js/arbor.js
[INFO] Exclude: **/src/ui/public/js/arbor-graphics.js
[INFO] Exclude: **/src/ui/public/js/arbor-tween.js
[INFO] Exclude: **/src/ui/public/js/jquery.mustache.js
[INFO] Exclude: **/src/ui/public/js/typeahead.jquery.min.js
[INFO] Exclude: **/dependency-reduced-pom.xml
[INFO] Exclude: **/docs/**
[INFO] Exclude: **/.git/**
[INFO] Exclude: **/derby.log
[INFO] Exclude: **/src/dev/**
[INFO] Exclude: **/src/codegen/config.fmpp
[INFO] Exclude: **/src/codegen/data/Parser.tdd
[INFO] Exclude: **/src/test/resources/FixedAvroSerializer.config
[INFO] 1850 resources included (use -debug for more details)
Warning:  org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
Compiler warnings:
  WARNING:  'org.apache.xerces.jaxp.SAXParserImpl: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.'
Warning:  org.apache.xerces.parsers.SAXParser: Feature 'http://javax.xml.XMLConstants/feature/secure-processing' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
[INFO] Rat check: Summary of files. Unapproved: 0 unknown: 0 generated: 0 approved: 1837 licence.
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building multilang-javascript 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ multilang-javascript ---
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ multilang-javascript ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 1 resource
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ multilang-javascript ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ multilang-javascript ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/storm-multilang/javascript/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ multilang-javascript ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-surefire-plugin:2.18.1:test (default-test) @ multilang-javascript ---
[INFO] 
[INFO] --- apache-rat-plugin:0.11:check (default) @ multilang-javascript ---
[INFO] 51 implicit excludes (use -debug for more details).
[INFO] Exclude: **/target/**
[INFO] Exclude: **/*.iml
[INFO] Exclude: **/.idea/**
[INFO] Exclude: **/metastore_db/**
[INFO] Exclude: **/build/**
[INFO] Exclude: **/logs/**
[INFO] Exclude: **/CHANGELOG.md
[INFO] Exclude: **/README.md
[INFO] Exclude: **/README.markdown
[INFO] Exclude: **/DEVELOPER.md
[INFO] Exclude: **/BYLAWS.md
[INFO] Exclude: **/STORM-UI-REST-API.md
[INFO] Exclude: SECURITY.md
[INFO] Exclude: VERSION
[INFO] Exclude: TODO
[INFO] Exclude: **/src/py/**
[INFO] Exclude: **/src/ui/public/js/jquery.dataTables.1.10.4.min.js
[INFO] Exclude: **/src/ui/public/css/jquery.dataTables.1.10.4.min.css
[INFO] Exclude: **/src/ui/public/images/*
[INFO] Exclude: **/src/ui/public/js/bootstrap-3.3.1.min.js
[INFO] Exclude: **/src/ui/public/css/bootstrap-3.3.1.min.css
[INFO] Exclude: **/src/ui/public/js/dataTables.bootstrap.min.js
[INFO] Exclude: **/src/ui/public/css/dataTables.bootstrap.css
[INFO] Exclude: **/src/ui/public/js/jsonFormatter.min.js
[INFO] Exclude: **/src/ui/public/css/jsonFormatter.min.css
[INFO] Exclude: **/src/ui/public/js/jquery-1.11.1.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.cookies.2.2.0.min.js
[INFO] Exclude: **/src/ui/public/js/moment.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.blockUI.min.js
[INFO] Exclude: **/src/ui/public/js/url.min.js
[INFO] Exclude: **/src/ui/public/js/arbor.js
[INFO] Exclude: **/src/ui/public/js/arbor-graphics.js
[INFO] Exclude: **/src/ui/public/js/arbor-tween.js
[INFO] Exclude: **/src/ui/public/js/jquery.mustache.js
[INFO] Exclude: **/src/ui/public/js/typeahead.jquery.min.js
[INFO] Exclude: **/dependency-reduced-pom.xml
[INFO] Exclude: **/docs/**
[INFO] Exclude: **/.git/**
[INFO] Exclude: **/derby.log
[INFO] Exclude: **/src/dev/**
[INFO] Exclude: **/src/codegen/config.fmpp
[INFO] Exclude: **/src/codegen/data/Parser.tdd
[INFO] Exclude: **/src/test/resources/FixedAvroSerializer.config
[INFO] 2 resources included (use -debug for more details)
Warning:  org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
Compiler warnings:
  WARNING:  'org.apache.xerces.jaxp.SAXParserImpl: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.'
Warning:  org.apache.xerces.parsers.SAXParser: Feature 'http://javax.xml.XMLConstants/feature/secure-processing' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
[INFO] Rat check: Summary of files. Unapproved: 0 unknown: 0 generated: 0 approved: 2 licence.
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building multilang-python 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ multilang-python ---
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ multilang-python ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 1 resource
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ multilang-python ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ multilang-python ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/storm-multilang/python/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ multilang-python ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-surefire-plugin:2.18.1:test (default-test) @ multilang-python ---
[INFO] 
[INFO] --- apache-rat-plugin:0.11:check (default) @ multilang-python ---
[INFO] 51 implicit excludes (use -debug for more details).
[INFO] Exclude: **/target/**
[INFO] Exclude: **/*.iml
[INFO] Exclude: **/.idea/**
[INFO] Exclude: **/metastore_db/**
[INFO] Exclude: **/build/**
[INFO] Exclude: **/logs/**
[INFO] Exclude: **/CHANGELOG.md
[INFO] Exclude: **/README.md
[INFO] Exclude: **/README.markdown
[INFO] Exclude: **/DEVELOPER.md
[INFO] Exclude: **/BYLAWS.md
[INFO] Exclude: **/STORM-UI-REST-API.md
[INFO] Exclude: SECURITY.md
[INFO] Exclude: VERSION
[INFO] Exclude: TODO
[INFO] Exclude: **/src/py/**
[INFO] Exclude: **/src/ui/public/js/jquery.dataTables.1.10.4.min.js
[INFO] Exclude: **/src/ui/public/css/jquery.dataTables.1.10.4.min.css
[INFO] Exclude: **/src/ui/public/images/*
[INFO] Exclude: **/src/ui/public/js/bootstrap-3.3.1.min.js
[INFO] Exclude: **/src/ui/public/css/bootstrap-3.3.1.min.css
[INFO] Exclude: **/src/ui/public/js/dataTables.bootstrap.min.js
[INFO] Exclude: **/src/ui/public/css/dataTables.bootstrap.css
[INFO] Exclude: **/src/ui/public/js/jsonFormatter.min.js
[INFO] Exclude: **/src/ui/public/css/jsonFormatter.min.css
[INFO] Exclude: **/src/ui/public/js/jquery-1.11.1.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.cookies.2.2.0.min.js
[INFO] Exclude: **/src/ui/public/js/moment.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.blockUI.min.js
[INFO] Exclude: **/src/ui/public/js/url.min.js
[INFO] Exclude: **/src/ui/public/js/arbor.js
[INFO] Exclude: **/src/ui/public/js/arbor-graphics.js
[INFO] Exclude: **/src/ui/public/js/arbor-tween.js
[INFO] Exclude: **/src/ui/public/js/jquery.mustache.js
[INFO] Exclude: **/src/ui/public/js/typeahead.jquery.min.js
[INFO] Exclude: **/dependency-reduced-pom.xml
[INFO] Exclude: **/docs/**
[INFO] Exclude: **/.git/**
[INFO] Exclude: **/derby.log
[INFO] Exclude: **/src/dev/**
[INFO] Exclude: **/src/codegen/config.fmpp
[INFO] Exclude: **/src/codegen/data/Parser.tdd
[INFO] Exclude: **/src/test/resources/FixedAvroSerializer.config
[INFO] 2 resources included (use -debug for more details)
Warning:  org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
Compiler warnings:
  WARNING:  'org.apache.xerces.jaxp.SAXParserImpl: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.'
Warning:  org.apache.xerces.parsers.SAXParser: Feature 'http://javax.xml.XMLConstants/feature/secure-processing' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
[INFO] Rat check: Summary of files. Unapproved: 0 unknown: 0 generated: 0 approved: 2 licence.
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building multilang-ruby 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ multilang-ruby ---
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ multilang-ruby ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 1 resource
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ multilang-ruby ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ multilang-ruby ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/storm-multilang/ruby/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ multilang-ruby ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-surefire-plugin:2.18.1:test (default-test) @ multilang-ruby ---
[INFO] 
[INFO] --- apache-rat-plugin:0.11:check (default) @ multilang-ruby ---
[INFO] 51 implicit excludes (use -debug for more details).
[INFO] Exclude: **/target/**
[INFO] Exclude: **/*.iml
[INFO] Exclude: **/.idea/**
[INFO] Exclude: **/metastore_db/**
[INFO] Exclude: **/build/**
[INFO] Exclude: **/logs/**
[INFO] Exclude: **/CHANGELOG.md
[INFO] Exclude: **/README.md
[INFO] Exclude: **/README.markdown
[INFO] Exclude: **/DEVELOPER.md
[INFO] Exclude: **/BYLAWS.md
[INFO] Exclude: **/STORM-UI-REST-API.md
[INFO] Exclude: SECURITY.md
[INFO] Exclude: VERSION
[INFO] Exclude: TODO
[INFO] Exclude: **/src/py/**
[INFO] Exclude: **/src/ui/public/js/jquery.dataTables.1.10.4.min.js
[INFO] Exclude: **/src/ui/public/css/jquery.dataTables.1.10.4.min.css
[INFO] Exclude: **/src/ui/public/images/*
[INFO] Exclude: **/src/ui/public/js/bootstrap-3.3.1.min.js
[INFO] Exclude: **/src/ui/public/css/bootstrap-3.3.1.min.css
[INFO] Exclude: **/src/ui/public/js/dataTables.bootstrap.min.js
[INFO] Exclude: **/src/ui/public/css/dataTables.bootstrap.css
[INFO] Exclude: **/src/ui/public/js/jsonFormatter.min.js
[INFO] Exclude: **/src/ui/public/css/jsonFormatter.min.css
[INFO] Exclude: **/src/ui/public/js/jquery-1.11.1.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.cookies.2.2.0.min.js
[INFO] Exclude: **/src/ui/public/js/moment.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.blockUI.min.js
[INFO] Exclude: **/src/ui/public/js/url.min.js
[INFO] Exclude: **/src/ui/public/js/arbor.js
[INFO] Exclude: **/src/ui/public/js/arbor-graphics.js
[INFO] Exclude: **/src/ui/public/js/arbor-tween.js
[INFO] Exclude: **/src/ui/public/js/jquery.mustache.js
[INFO] Exclude: **/src/ui/public/js/typeahead.jquery.min.js
[INFO] Exclude: **/dependency-reduced-pom.xml
[INFO] Exclude: **/docs/**
[INFO] Exclude: **/.git/**
[INFO] Exclude: **/derby.log
[INFO] Exclude: **/src/dev/**
[INFO] Exclude: **/src/codegen/config.fmpp
[INFO] Exclude: **/src/codegen/data/Parser.tdd
[INFO] Exclude: **/src/test/resources/FixedAvroSerializer.config
[INFO] 2 resources included (use -debug for more details)
Warning:  org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
Compiler warnings:
  WARNING:  'org.apache.xerces.jaxp.SAXParserImpl: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.'
Warning:  org.apache.xerces.parsers.SAXParser: Feature 'http://javax.xml.XMLConstants/feature/secure-processing' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
[INFO] Rat check: Summary of files. Unapproved: 0 unknown: 0 generated: 0 approved: 2 licence.
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building maven-shade-clojure-transformer 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ maven-shade-clojure-transformer ---
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ maven-shade-clojure-transformer ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/storm-buildtools/maven-shade-clojure-transformer/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ maven-shade-clojure-transformer ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ maven-shade-clojure-transformer ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/storm-buildtools/maven-shade-clojure-transformer/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ maven-shade-clojure-transformer ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-surefire-plugin:2.18.1:test (default-test) @ maven-shade-clojure-transformer ---
[INFO] 
[INFO] --- apache-rat-plugin:0.11:check (default) @ maven-shade-clojure-transformer ---
[INFO] 51 implicit excludes (use -debug for more details).
[INFO] Exclude: **/target/**
[INFO] Exclude: **/*.iml
[INFO] Exclude: **/.idea/**
[INFO] Exclude: **/metastore_db/**
[INFO] Exclude: **/build/**
[INFO] Exclude: **/logs/**
[INFO] Exclude: **/CHANGELOG.md
[INFO] Exclude: **/README.md
[INFO] Exclude: **/README.markdown
[INFO] Exclude: **/DEVELOPER.md
[INFO] Exclude: **/BYLAWS.md
[INFO] Exclude: **/STORM-UI-REST-API.md
[INFO] Exclude: SECURITY.md
[INFO] Exclude: VERSION
[INFO] Exclude: TODO
[INFO] Exclude: **/src/py/**
[INFO] Exclude: **/src/ui/public/js/jquery.dataTables.1.10.4.min.js
[INFO] Exclude: **/src/ui/public/css/jquery.dataTables.1.10.4.min.css
[INFO] Exclude: **/src/ui/public/images/*
[INFO] Exclude: **/src/ui/public/js/bootstrap-3.3.1.min.js
[INFO] Exclude: **/src/ui/public/css/bootstrap-3.3.1.min.css
[INFO] Exclude: **/src/ui/public/js/dataTables.bootstrap.min.js
[INFO] Exclude: **/src/ui/public/css/dataTables.bootstrap.css
[INFO] Exclude: **/src/ui/public/js/jsonFormatter.min.js
[INFO] Exclude: **/src/ui/public/css/jsonFormatter.min.css
[INFO] Exclude: **/src/ui/public/js/jquery-1.11.1.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.cookies.2.2.0.min.js
[INFO] Exclude: **/src/ui/public/js/moment.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.blockUI.min.js
[INFO] Exclude: **/src/ui/public/js/url.min.js
[INFO] Exclude: **/src/ui/public/js/arbor.js
[INFO] Exclude: **/src/ui/public/js/arbor-graphics.js
[INFO] Exclude: **/src/ui/public/js/arbor-tween.js
[INFO] Exclude: **/src/ui/public/js/jquery.mustache.js
[INFO] Exclude: **/src/ui/public/js/typeahead.jquery.min.js
[INFO] Exclude: **/dependency-reduced-pom.xml
[INFO] Exclude: **/docs/**
[INFO] Exclude: **/.git/**
[INFO] Exclude: **/derby.log
[INFO] Exclude: **/src/dev/**
[INFO] Exclude: **/src/codegen/config.fmpp
[INFO] Exclude: **/src/codegen/data/Parser.tdd
[INFO] Exclude: **/src/test/resources/FixedAvroSerializer.config
[INFO] 2 resources included (use -debug for more details)
Warning:  org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
Compiler warnings:
  WARNING:  'org.apache.xerces.jaxp.SAXParserImpl: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.'
Warning:  org.apache.xerces.parsers.SAXParser: Feature 'http://javax.xml.XMLConstants/feature/secure-processing' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
[INFO] Rat check: Summary of files. Unapproved: 0 unknown: 0 generated: 0 approved: 2 licence.
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building storm-maven-plugins 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ storm-maven-plugins ---
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ storm-maven-plugins ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/storm-buildtools/storm-maven-plugins/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ storm-maven-plugins ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-plugin-plugin:3.0:descriptor (default-descriptor) @ storm-maven-plugins ---
[INFO] Using 'UTF-8' encoding to read mojo metadata.
[INFO] Applying mojo extractor for language: java
[INFO] Mojo extractor for language: java found 0 mojo descriptors.
[INFO] Applying mojo extractor for language: bsh
[INFO] Mojo extractor for language: bsh found 0 mojo descriptors.
[INFO] Applying mojo extractor for language: java-annotations
[INFO] Mojo extractor for language: java-annotations found 1 mojo descriptors.
[INFO] 
[INFO] --- maven-plugin-plugin:3.0:descriptor (mojo-descriptor) @ storm-maven-plugins ---
[INFO] Using 'UTF-8' encoding to read mojo metadata.
[INFO] Applying mojo extractor for language: java
[INFO] Mojo extractor for language: java found 0 mojo descriptors.
[INFO] Applying mojo extractor for language: bsh
[INFO] Mojo extractor for language: bsh found 0 mojo descriptors.
[INFO] Applying mojo extractor for language: java-annotations
[INFO] Mojo extractor for language: java-annotations found 1 mojo descriptors.
[INFO] 
[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ storm-maven-plugins ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/storm-buildtools/storm-maven-plugins/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ storm-maven-plugins ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-surefire-plugin:2.18.1:test (default-test) @ storm-maven-plugins ---
[INFO] 
[INFO] --- apache-rat-plugin:0.11:check (default) @ storm-maven-plugins ---
[INFO] 51 implicit excludes (use -debug for more details).
[INFO] Exclude: **/target/**
[INFO] Exclude: **/*.iml
[INFO] Exclude: **/.idea/**
[INFO] Exclude: **/metastore_db/**
[INFO] Exclude: **/build/**
[INFO] Exclude: **/logs/**
[INFO] Exclude: **/CHANGELOG.md
[INFO] Exclude: **/README.md
[INFO] Exclude: **/README.markdown
[INFO] Exclude: **/DEVELOPER.md
[INFO] Exclude: **/BYLAWS.md
[INFO] Exclude: **/STORM-UI-REST-API.md
[INFO] Exclude: SECURITY.md
[INFO] Exclude: VERSION
[INFO] Exclude: TODO
[INFO] Exclude: **/src/py/**
[INFO] Exclude: **/src/ui/public/js/jquery.dataTables.1.10.4.min.js
[INFO] Exclude: **/src/ui/public/css/jquery.dataTables.1.10.4.min.css
[INFO] Exclude: **/src/ui/public/images/*
[INFO] Exclude: **/src/ui/public/js/bootstrap-3.3.1.min.js
[INFO] Exclude: **/src/ui/public/css/bootstrap-3.3.1.min.css
[INFO] Exclude: **/src/ui/public/js/dataTables.bootstrap.min.js
[INFO] Exclude: **/src/ui/public/css/dataTables.bootstrap.css
[INFO] Exclude: **/src/ui/public/js/jsonFormatter.min.js
[INFO] Exclude: **/src/ui/public/css/jsonFormatter.min.css
[INFO] Exclude: **/src/ui/public/js/jquery-1.11.1.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.cookies.2.2.0.min.js
[INFO] Exclude: **/src/ui/public/js/moment.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.blockUI.min.js
[INFO] Exclude: **/src/ui/public/js/url.min.js
[INFO] Exclude: **/src/ui/public/js/arbor.js
[INFO] Exclude: **/src/ui/public/js/arbor-graphics.js
[INFO] Exclude: **/src/ui/public/js/arbor-tween.js
[INFO] Exclude: **/src/ui/public/js/jquery.mustache.js
[INFO] Exclude: **/src/ui/public/js/typeahead.jquery.min.js
[INFO] Exclude: **/dependency-reduced-pom.xml
[INFO] Exclude: **/docs/**
[INFO] Exclude: **/.git/**
[INFO] Exclude: **/derby.log
[INFO] Exclude: **/src/dev/**
[INFO] Exclude: **/src/codegen/config.fmpp
[INFO] Exclude: **/src/codegen/data/Parser.tdd
[INFO] Exclude: **/src/test/resources/FixedAvroSerializer.config
[INFO] 3 resources included (use -debug for more details)
Warning:  org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
Compiler warnings:
  WARNING:  'org.apache.xerces.jaxp.SAXParserImpl: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.'
Warning:  org.apache.xerces.parsers.SAXParser: Feature 'http://javax.xml.XMLConstants/feature/secure-processing' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
[INFO] Rat check: Summary of files. Unapproved: 0 unknown: 0 generated: 0 approved: 3 licence.
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building storm-rename-hack 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] Downloading: https://repository.apache.org/snapshots/org/apache/storm/storm-core/2.0.0-SNAPSHOT/maven-metadata.xml
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ storm-rename-hack ---
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ storm-rename-hack ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/storm-rename-hack/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ storm-rename-hack ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ storm-rename-hack ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/storm-rename-hack/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ storm-rename-hack ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-surefire-plugin:2.18.1:test (default-test) @ storm-rename-hack ---
[INFO] 
[INFO] --- apache-rat-plugin:0.11:check (default) @ storm-rename-hack ---
[INFO] 51 implicit excludes (use -debug for more details).
[INFO] Exclude: **/target/**
[INFO] Exclude: **/*.iml
[INFO] Exclude: **/.idea/**
[INFO] Exclude: **/metastore_db/**
[INFO] Exclude: **/build/**
[INFO] Exclude: **/logs/**
[INFO] Exclude: **/CHANGELOG.md
[INFO] Exclude: **/README.md
[INFO] Exclude: **/README.markdown
[INFO] Exclude: **/DEVELOPER.md
[INFO] Exclude: **/BYLAWS.md
[INFO] Exclude: **/STORM-UI-REST-API.md
[INFO] Exclude: SECURITY.md
[INFO] Exclude: VERSION
[INFO] Exclude: TODO
[INFO] Exclude: **/src/py/**
[INFO] Exclude: **/src/ui/public/js/jquery.dataTables.1.10.4.min.js
[INFO] Exclude: **/src/ui/public/css/jquery.dataTables.1.10.4.min.css
[INFO] Exclude: **/src/ui/public/images/*
[INFO] Exclude: **/src/ui/public/js/bootstrap-3.3.1.min.js
[INFO] Exclude: **/src/ui/public/css/bootstrap-3.3.1.min.css
[INFO] Exclude: **/src/ui/public/js/dataTables.bootstrap.min.js
[INFO] Exclude: **/src/ui/public/css/dataTables.bootstrap.css
[INFO] Exclude: **/src/ui/public/js/jsonFormatter.min.js
[INFO] Exclude: **/src/ui/public/css/jsonFormatter.min.css
[INFO] Exclude: **/src/ui/public/js/jquery-1.11.1.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.cookies.2.2.0.min.js
[INFO] Exclude: **/src/ui/public/js/moment.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.blockUI.min.js
[INFO] Exclude: **/src/ui/public/js/url.min.js
[INFO] Exclude: **/src/ui/public/js/arbor.js
[INFO] Exclude: **/src/ui/public/js/arbor-graphics.js
[INFO] Exclude: **/src/ui/public/js/arbor-tween.js
[INFO] Exclude: **/src/ui/public/js/jquery.mustache.js
[INFO] Exclude: **/src/ui/public/js/typeahead.jquery.min.js
[INFO] Exclude: **/dependency-reduced-pom.xml
[INFO] Exclude: **/docs/**
[INFO] Exclude: **/.git/**
[INFO] Exclude: **/derby.log
[INFO] Exclude: **/src/dev/**
[INFO] Exclude: **/src/codegen/config.fmpp
[INFO] Exclude: **/src/codegen/data/Parser.tdd
[INFO] Exclude: **/src/test/resources/FixedAvroSerializer.config
[INFO] 10 resources included (use -debug for more details)
Warning:  org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
Compiler warnings:
  WARNING:  'org.apache.xerces.jaxp.SAXParserImpl: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.'
Warning:  org.apache.xerces.parsers.SAXParser: Feature 'http://javax.xml.XMLConstants/feature/secure-processing' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
[INFO] Rat check: Summary of files. Unapproved: 0 unknown: 0 generated: 0 approved: 10 licence.
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building storm-kafka 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.2.201409121644:prepare-agent (jacoco-initialize) @ storm-kafka ---
[INFO] argLine set to -javaagent:/home/travis/.m2/repository/org/jacoco/org.jacoco.agent/0.7.2.201409121644/org.jacoco.agent-0.7.2.201409121644-runtime.jar=destfile=/home/travis/build/apache/storm/external/storm-kafka/target/jacoco.exec
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ storm-kafka ---
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ storm-kafka ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/storm-kafka/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ storm-kafka ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ storm-kafka ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/storm-kafka/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ storm-kafka ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:2.18.1:test (default-test) @ storm-kafka ---
[INFO] Surefire report directory: /home/travis/build/apache/storm/external/storm-kafka/target/surefire-reports
[INFO] parallel='none', perCoreThreadCount=true, threadCount=0, useUnlimitedThreads=false, threadCountSuites=0, threadCountClasses=0, threadCountMethods=0, parallelOptimized=true

-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running org.apache.storm.kafka.TestStringScheme
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.864 sec - in org.apache.storm.kafka.TestStringScheme
Running org.apache.storm.kafka.StringKeyValueSchemeTest
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.116 sec - in org.apache.storm.kafka.StringKeyValueSchemeTest
Running org.apache.storm.kafka.DynamicBrokersReaderTest
Tests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 9.302 sec - in org.apache.storm.kafka.DynamicBrokersReaderTest
Running org.apache.storm.kafka.KafkaUtilsTest
Tests run: 18, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 32.727 sec - in org.apache.storm.kafka.KafkaUtilsTest
Running org.apache.storm.kafka.KafkaErrorTest
Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0 sec - in org.apache.storm.kafka.KafkaErrorTest
Running org.apache.storm.kafka.bolt.KafkaBoltTest
Tests run: 8, Failures: 0, Errors: 7, Skipped: 0, Time elapsed: 20.505 sec <<< FAILURE! - in org.apache.storm.kafka.bolt.KafkaBoltTest
executeWithBrokerDown(org.apache.storm.kafka.bolt.KafkaBoltTest)  Time elapsed: 3.006 sec  <<< ERROR!
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.kafka.bolt.KafkaBoltTest.generateTestTuple(KafkaBoltTest.java:301)
	at org.apache.storm.kafka.bolt.KafkaBoltTest.executeWithBrokerDown(KafkaBoltTest.java:266)

executeWithoutKey(org.apache.storm.kafka.bolt.KafkaBoltTest)  Time elapsed: 1.546 sec  <<< ERROR!
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.kafka.bolt.KafkaBoltTest.generateTestTuple(KafkaBoltTest.java:301)
	at org.apache.storm.kafka.bolt.KafkaBoltTest.executeWithoutKey(KafkaBoltTest.java:255)

executeWithByteArrayKeyAndMessageFire(org.apache.storm.kafka.bolt.KafkaBoltTest)  Time elapsed: 2.979 sec  <<< ERROR!
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.kafka.bolt.KafkaBoltTest.generateTestTuple(KafkaBoltTest.java:290)
	at org.apache.storm.kafka.bolt.KafkaBoltTest.executeWithByteArrayKeyAndMessageFire(KafkaBoltTest.java:176)

executeWithByteArrayKeyAndMessageSync(org.apache.storm.kafka.bolt.KafkaBoltTest)  Time elapsed: 2.8 sec  <<< ERROR!
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.kafka.bolt.KafkaBoltTest.generateTestTuple(KafkaBoltTest.java:290)
	at org.apache.storm.kafka.bolt.KafkaBoltTest.executeWithByteArrayKeyAndMessageSync(KafkaBoltTest.java:131)

executeWithByteArrayKeyAndMessageAsync(org.apache.storm.kafka.bolt.KafkaBoltTest)  Time elapsed: 5.626 sec  <<< ERROR!
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.kafka.bolt.KafkaBoltTest.generateTestTuple(KafkaBoltTest.java:290)
	at org.apache.storm.kafka.bolt.KafkaBoltTest.executeWithByteArrayKeyAndMessageAsync(KafkaBoltTest.java:146)

executeWithKey(org.apache.storm.kafka.bolt.KafkaBoltTest)  Time elapsed: 1.461 sec  <<< ERROR!
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.kafka.bolt.KafkaBoltTest.generateTestTuple(KafkaBoltTest.java:290)
	at org.apache.storm.kafka.bolt.KafkaBoltTest.executeWithKey(KafkaBoltTest.java:115)

executeWithBoltSpecifiedProperties(org.apache.storm.kafka.bolt.KafkaBoltTest)  Time elapsed: 1.279 sec  <<< ERROR!
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.kafka.bolt.KafkaBoltTest.generateTestTuple(KafkaBoltTest.java:290)
	at org.apache.storm.kafka.bolt.KafkaBoltTest.executeWithBoltSpecifiedProperties(KafkaBoltTest.java:199)

Running org.apache.storm.kafka.ExponentialBackoffMsgRetryManagerTest
Tests run: 12, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 6.541 sec - in org.apache.storm.kafka.ExponentialBackoffMsgRetryManagerTest
Running org.apache.storm.kafka.ZkCoordinatorTest
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 7.747 sec - in org.apache.storm.kafka.ZkCoordinatorTest
Running org.apache.storm.kafka.TridentKafkaTest
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.669 sec - in org.apache.storm.kafka.TridentKafkaTest

Results :

Tests in error: 
  KafkaBoltTest.executeWithBoltSpecifiedProperties:199->generateTestTuple:290 » IllegalArgument
  KafkaBoltTest.executeWithBrokerDown:266->generateTestTuple:301 » IllegalArgument
  KafkaBoltTest.executeWithByteArrayKeyAndMessageAsync:146->generateTestTuple:290 » IllegalArgument
  KafkaBoltTest.executeWithByteArrayKeyAndMessageFire:176->generateTestTuple:290 » IllegalArgument
  KafkaBoltTest.executeWithByteArrayKeyAndMessageSync:131->generateTestTuple:290 » IllegalArgument
  KafkaBoltTest.executeWithKey:115->generateTestTuple:290 » IllegalArgument Spou...
  KafkaBoltTest.executeWithoutKey:255->generateTestTuple:301 » IllegalArgument S...

Tests run: 57, Failures: 0, Errors: 7, Skipped: 0

[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building storm-hdfs 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ storm-hdfs ---
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ storm-hdfs ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/storm-hdfs/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:2.5.1:compile (default-compile) @ storm-hdfs ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ storm-hdfs ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 2 resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:2.5.1:testCompile (default-testCompile) @ storm-hdfs ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:2.18.1:test (default-test) @ storm-hdfs ---
[INFO] Surefire report directory: /home/travis/build/apache/storm/external/storm-hdfs/target/surefire-reports
[INFO] parallel='none', perCoreThreadCount=true, threadCount=0, useUnlimitedThreads=false, threadCountSuites=0, threadCountClasses=0, threadCountMethods=0, parallelOptimized=true

-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running org.apache.storm.hdfs.trident.HdfsStateTest
Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.696 sec - in org.apache.storm.hdfs.trident.HdfsStateTest
Running org.apache.storm.hdfs.bolt.TestWritersMap
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0 sec - in org.apache.storm.hdfs.bolt.TestWritersMap
Running org.apache.storm.hdfs.bolt.TestHdfsBolt
Tests run: 6, Failures: 0, Errors: 6, Skipped: 0, Time elapsed: 0.17 sec <<< FAILURE! - in org.apache.storm.hdfs.bolt.TestHdfsBolt
testTwoTuplesTwoFiles(org.apache.storm.hdfs.bolt.TestHdfsBolt)  Time elapsed: 0 sec  <<< ERROR!
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hdfs.bolt.TestHdfsBolt.generateTestTuple(TestHdfsBolt.java:243)
	at org.apache.storm.hdfs.bolt.TestHdfsBolt.<init>(TestHdfsBolt.java:69)

testPartitionedOutput(org.apache.storm.hdfs.bolt.TestHdfsBolt)  Time elapsed: 0 sec  <<< ERROR!
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hdfs.bolt.TestHdfsBolt.generateTestTuple(TestHdfsBolt.java:243)
	at org.apache.storm.hdfs.bolt.TestHdfsBolt.<init>(TestHdfsBolt.java:69)

testTwoTuplesOneFile(org.apache.storm.hdfs.bolt.TestHdfsBolt)  Time elapsed: 0 sec  <<< ERROR!
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hdfs.bolt.TestHdfsBolt.generateTestTuple(TestHdfsBolt.java:243)
	at org.apache.storm.hdfs.bolt.TestHdfsBolt.<init>(TestHdfsBolt.java:69)

testFailedSync(org.apache.storm.hdfs.bolt.TestHdfsBolt)  Time elapsed: 0 sec  <<< ERROR!
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hdfs.bolt.TestHdfsBolt.generateTestTuple(TestHdfsBolt.java:243)
	at org.apache.storm.hdfs.bolt.TestHdfsBolt.<init>(TestHdfsBolt.java:69)

testTickTuples(org.apache.storm.hdfs.bolt.TestHdfsBolt)  Time elapsed: 0 sec  <<< ERROR!
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hdfs.bolt.TestHdfsBolt.generateTestTuple(TestHdfsBolt.java:243)
	at org.apache.storm.hdfs.bolt.TestHdfsBolt.<init>(TestHdfsBolt.java:69)

testFailureFilecount(org.apache.storm.hdfs.bolt.TestHdfsBolt)  Time elapsed: 0 sec  <<< ERROR!
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hdfs.bolt.TestHdfsBolt.generateTestTuple(TestHdfsBolt.java:243)
	at org.apache.storm.hdfs.bolt.TestHdfsBolt.<init>(TestHdfsBolt.java:69)

Running org.apache.storm.hdfs.bolt.AvroGenericRecordBoltTest
Tests run: 5, Failures: 0, Errors: 5, Skipped: 0, Time elapsed: 0.005 sec <<< FAILURE! - in org.apache.storm.hdfs.bolt.AvroGenericRecordBoltTest
schemaThrashing(org.apache.storm.hdfs.bolt.AvroGenericRecordBoltTest)  Time elapsed: 0.001 sec  <<< ERROR!
java.lang.ExceptionInInitializerError: null
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hdfs.bolt.AvroGenericRecordBoltTest.generateTestTuple(AvroGenericRecordBoltTest.java:220)
	at org.apache.storm.hdfs.bolt.AvroGenericRecordBoltTest.<clinit>(AvroGenericRecordBoltTest.java:95)

forwardSchemaChangeWorks(org.apache.storm.hdfs.bolt.AvroGenericRecordBoltTest)  Time elapsed: 0.001 sec  <<< ERROR!
java.lang.NoClassDefFoundError: Could not initialize class org.apache.storm.hdfs.bolt.AvroGenericRecordBoltTest
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:408)
	at org.junit.runners.BlockJUnit4ClassRunner.createTest(BlockJUnit4ClassRunner.java:195)
	at org.junit.runners.BlockJUnit4ClassRunner$1.runReflectiveCall(BlockJUnit4ClassRunner.java:244)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.BlockJUnit4ClassRunner.methodBlock(BlockJUnit4ClassRunner.java:241)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:108)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:78)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:54)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:144)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:203)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:155)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:103)

multipleTuplesOneFile(org.apache.storm.hdfs.bolt.AvroGenericRecordBoltTest)  Time elapsed: 0 sec  <<< ERROR!
java.lang.NoClassDefFoundError: Could not initialize class org.apache.storm.hdfs.bolt.AvroGenericRecordBoltTest
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:408)
	at org.junit.runners.BlockJUnit4ClassRunner.createTest(BlockJUnit4ClassRunner.java:195)
	at org.junit.runners.BlockJUnit4ClassRunner$1.runReflectiveCall(BlockJUnit4ClassRunner.java:244)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.BlockJUnit4ClassRunner.methodBlock(BlockJUnit4ClassRunner.java:241)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:108)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:78)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:54)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:144)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:203)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:155)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:103)

multipleTuplesMutliplesFiles(org.apache.storm.hdfs.bolt.AvroGenericRecordBoltTest)  Time elapsed: 0.001 sec  <<< ERROR!
java.lang.NoClassDefFoundError: Could not initialize class org.apache.storm.hdfs.bolt.AvroGenericRecordBoltTest
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:408)
	at org.junit.runners.BlockJUnit4ClassRunner.createTest(BlockJUnit4ClassRunner.java:195)
	at org.junit.runners.BlockJUnit4ClassRunner$1.runReflectiveCall(BlockJUnit4ClassRunner.java:244)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.BlockJUnit4ClassRunner.methodBlock(BlockJUnit4ClassRunner.java:241)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:108)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:78)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:54)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:144)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:203)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:155)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:103)

backwardSchemaChangeWorks(org.apache.storm.hdfs.bolt.AvroGenericRecordBoltTest)  Time elapsed: 0.002 sec  <<< ERROR!
java.lang.NoClassDefFoundError: Could not initialize class org.apache.storm.hdfs.bolt.AvroGenericRecordBoltTest
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:408)
	at org.junit.runners.BlockJUnit4ClassRunner.createTest(BlockJUnit4ClassRunner.java:195)
	at org.junit.runners.BlockJUnit4ClassRunner$1.runReflectiveCall(BlockJUnit4ClassRunner.java:244)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.BlockJUnit4ClassRunner.methodBlock(BlockJUnit4ClassRunner.java:241)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:108)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:78)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:54)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:144)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:203)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:155)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:103)

Running org.apache.storm.hdfs.bolt.TestSequenceFileBolt
Tests run: 3, Failures: 0, Errors: 3, Skipped: 0, Time elapsed: 5.448 sec <<< FAILURE! - in org.apache.storm.hdfs.bolt.TestSequenceFileBolt
testTwoTuplesTwoFiles(org.apache.storm.hdfs.bolt.TestSequenceFileBolt)  Time elapsed: 0 sec  <<< ERROR!
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hdfs.bolt.TestSequenceFileBolt.generateTestTuple(TestSequenceFileBolt.java:161)
	at org.apache.storm.hdfs.bolt.TestSequenceFileBolt.<init>(TestSequenceFileBolt.java:68)

testTwoTuplesOneFile(org.apache.storm.hdfs.bolt.TestSequenceFileBolt)  Time elapsed: 0.001 sec  <<< ERROR!
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hdfs.bolt.TestSequenceFileBolt.generateTestTuple(TestSequenceFileBolt.java:161)
	at org.apache.storm.hdfs.bolt.TestSequenceFileBolt.<init>(TestSequenceFileBolt.java:68)

testFailedSync(org.apache.storm.hdfs.bolt.TestSequenceFileBolt)  Time elapsed: 0.005 sec  <<< ERROR!
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hdfs.bolt.TestSequenceFileBolt.generateTestTuple(TestSequenceFileBolt.java:161)
	at org.apache.storm.hdfs.bolt.TestSequenceFileBolt.<init>(TestSequenceFileBolt.java:68)

Running org.apache.storm.hdfs.spout.TestDirLock
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 20.637 sec - in org.apache.storm.hdfs.spout.TestDirLock
Running org.apache.storm.hdfs.spout.TestHdfsSemantics
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.497 sec - in org.apache.storm.hdfs.spout.TestHdfsSemantics
Running org.apache.storm.hdfs.spout.TestProgressTracker
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.843 sec - in org.apache.storm.hdfs.spout.TestProgressTracker
Running org.apache.storm.hdfs.spout.TestFileLock
Tests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 31.593 sec - in org.apache.storm.hdfs.spout.TestFileLock
Running org.apache.storm.hdfs.spout.TestHdfsSpout
Tests run: 10, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 10.095 sec - in org.apache.storm.hdfs.spout.TestHdfsSpout
Running org.apache.storm.hdfs.blobstore.HdfsBlobStoreImplTest
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.058 sec - in org.apache.storm.hdfs.blobstore.HdfsBlobStoreImplTest
Running org.apache.storm.hdfs.blobstore.BlobStoreTest
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 10.625 sec - in org.apache.storm.hdfs.blobstore.BlobStoreTest
Running org.apache.storm.hdfs.avro.TestFixedAvroSerializer
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.001 sec - in org.apache.storm.hdfs.avro.TestFixedAvroSerializer
Running org.apache.storm.hdfs.avro.TestGenericAvroSerializer
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.001 sec - in org.apache.storm.hdfs.avro.TestGenericAvroSerializer

Results :

Tests in error: 
  AvroGenericRecordBoltTest.backwardSchemaChangeWorks » NoClassDefFound Could no...
  AvroGenericRecordBoltTest.forwardSchemaChangeWorks » NoClassDefFound Could not...
  AvroGenericRecordBoltTest.multipleTuplesMutliplesFiles » NoClassDefFound Could...
  AvroGenericRecordBoltTest.multipleTuplesOneFile » NoClassDefFound Could not in...
  AvroGenericRecordBoltTest.schemaThrashing » ExceptionInInitializer
  TestHdfsBolt.<init>:69->generateTestTuple:243 » IllegalArgument Spouts is not ...
  TestHdfsBolt.<init>:69->generateTestTuple:243 » IllegalArgument Spouts is not ...
  TestHdfsBolt.<init>:69->generateTestTuple:243 » IllegalArgument Spouts is not ...
  TestHdfsBolt.<init>:69->generateTestTuple:243 » IllegalArgument Spouts is not ...
  TestHdfsBolt.<init>:69->generateTestTuple:243 » IllegalArgument Spouts is not ...
  TestHdfsBolt.<init>:69->generateTestTuple:243 » IllegalArgument Spouts is not ...
  TestSequenceFileBolt.<init>:68->generateTestTuple:161 » IllegalArgument Spouts...
  TestSequenceFileBolt.<init>:68->generateTestTuple:161 » IllegalArgument Spouts...
  TestSequenceFileBolt.<init>:68->generateTestTuple:161 » IllegalArgument Spouts...

Tests run: 54, Failures: 0, Errors: 14, Skipped: 0

[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building storm-hbase 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ storm-hbase ---
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ storm-hbase ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/storm-hbase/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ storm-hbase ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ storm-hbase ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/storm-hbase/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ storm-hbase ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:2.18.1:test (default-test) @ storm-hbase ---
[INFO] 
[INFO] --- apache-rat-plugin:0.11:check (default) @ storm-hbase ---
[INFO] 51 implicit excludes (use -debug for more details).
[INFO] Exclude: **/target/**
[INFO] Exclude: **/*.iml
[INFO] Exclude: **/.idea/**
[INFO] Exclude: **/metastore_db/**
[INFO] Exclude: **/build/**
[INFO] Exclude: **/logs/**
[INFO] Exclude: **/CHANGELOG.md
[INFO] Exclude: **/README.md
[INFO] Exclude: **/README.markdown
[INFO] Exclude: **/DEVELOPER.md
[INFO] Exclude: **/BYLAWS.md
[INFO] Exclude: **/STORM-UI-REST-API.md
[INFO] Exclude: SECURITY.md
[INFO] Exclude: VERSION
[INFO] Exclude: TODO
[INFO] Exclude: **/src/py/**
[INFO] Exclude: **/src/ui/public/js/jquery.dataTables.1.10.4.min.js
[INFO] Exclude: **/src/ui/public/css/jquery.dataTables.1.10.4.min.css
[INFO] Exclude: **/src/ui/public/images/*
[INFO] Exclude: **/src/ui/public/js/bootstrap-3.3.1.min.js
[INFO] Exclude: **/src/ui/public/css/bootstrap-3.3.1.min.css
[INFO] Exclude: **/src/ui/public/js/dataTables.bootstrap.min.js
[INFO] Exclude: **/src/ui/public/css/dataTables.bootstrap.css
[INFO] Exclude: **/src/ui/public/js/jsonFormatter.min.js
[INFO] Exclude: **/src/ui/public/css/jsonFormatter.min.css
[INFO] Exclude: **/src/ui/public/js/jquery-1.11.1.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.cookies.2.2.0.min.js
[INFO] Exclude: **/src/ui/public/js/moment.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.blockUI.min.js
[INFO] Exclude: **/src/ui/public/js/url.min.js
[INFO] Exclude: **/src/ui/public/js/arbor.js
[INFO] Exclude: **/src/ui/public/js/arbor-graphics.js
[INFO] Exclude: **/src/ui/public/js/arbor-tween.js
[INFO] Exclude: **/src/ui/public/js/jquery.mustache.js
[INFO] Exclude: **/src/ui/public/js/typeahead.jquery.min.js
[INFO] Exclude: **/dependency-reduced-pom.xml
[INFO] Exclude: **/docs/**
[INFO] Exclude: **/.git/**
[INFO] Exclude: **/derby.log
[INFO] Exclude: **/src/dev/**
[INFO] Exclude: **/src/codegen/config.fmpp
[INFO] Exclude: **/src/codegen/data/Parser.tdd
[INFO] Exclude: **/src/test/resources/FixedAvroSerializer.config
[INFO] 35 resources included (use -debug for more details)
Warning:  org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
Compiler warnings:
  WARNING:  'org.apache.xerces.jaxp.SAXParserImpl: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.'
Warning:  org.apache.xerces.parsers.SAXParser: Feature 'http://javax.xml.XMLConstants/feature/secure-processing' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
[INFO] Rat check: Summary of files. Unapproved: 0 unknown: 0 generated: 0 approved: 35 licence.
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building storm-hive 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ storm-hive ---
[INFO] Downloading: https://oss.sonatype.org/content/repositories/releases/org/pentaho/pentaho-aggdesigner-algorithm/5.1.3-jhyde/pentaho-aggdesigner-algorithm-5.1.3-jhyde.pom
[INFO] Downloading: https://repository.apache.org/releases/org/pentaho/pentaho-aggdesigner-algorithm/5.1.3-jhyde/pentaho-aggdesigner-algorithm-5.1.3-jhyde.pom
[INFO] Downloading: https://oss.sonatype.org/content/repositories/releases/net/hydromatic/linq4j/0.4/linq4j-0.4.pom
[INFO] Downloading: https://repository.apache.org/releases/net/hydromatic/linq4j/0.4/linq4j-0.4.pom
[INFO] Downloading: https://oss.sonatype.org/content/repositories/releases/net/hydromatic/quidem/0.1.1/quidem-0.1.1.pom
[INFO] Downloading: https://repository.apache.org/releases/net/hydromatic/quidem/0.1.1/quidem-0.1.1.pom
[INFO] Downloading: https://oss.sonatype.org/content/repositories/releases/eigenbase/eigenbase-properties/1.1.4/eigenbase-properties-1.1.4.pom
[INFO] Downloading: https://repository.apache.org/releases/eigenbase/eigenbase-properties/1.1.4/eigenbase-properties-1.1.4.pom
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ storm-hive ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/storm-hive/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ storm-hive ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ storm-hive ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/storm-hive/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ storm-hive ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (cleanup) @ storm-hive ---
[INFO] 
[INFO] --- maven-surefire-plugin:2.18.1:test (default-test) @ storm-hive ---
[INFO] Surefire report directory: /home/travis/build/apache/storm/external/storm-hive/target/surefire-reports
[INFO] parallel='none', perCoreThreadCount=true, threadCount=0, useUnlimitedThreads=false, threadCountSuites=0, threadCountClasses=0, threadCountMethods=0, parallelOptimized=true

-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running org.apache.storm.hive.bolt.TestHiveBolt
Tests run: 11, Failures: 0, Errors: 9, Skipped: 0, Time elapsed: 17.094 sec <<< FAILURE! - in org.apache.storm.hive.bolt.TestHiveBolt
testMultiPartitionTuples(org.apache.storm.hive.bolt.TestHiveBolt)  Time elapsed: 1.569 sec  <<< ERROR!
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hive.bolt.TestHiveBolt.generateTestTuple(TestHiveBolt.java:447)
	at org.apache.storm.hive.bolt.TestHiveBolt.testMultiPartitionTuples(TestHiveBolt.java:411)

testNoAcksUntilFlushed(org.apache.storm.hive.bolt.TestHiveBolt)  Time elapsed: 2.875 sec  <<< ERROR!
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hive.bolt.TestHiveBolt.generateTestTuple(TestHiveBolt.java:447)
	at org.apache.storm.hive.bolt.TestHiveBolt.testNoAcksUntilFlushed(TestHiveBolt.java:297)

testData(org.apache.storm.hive.bolt.TestHiveBolt)  Time elapsed: 1.576 sec  <<< ERROR!
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hive.bolt.TestHiveBolt.generateTestTuple(TestHiveBolt.java:447)
	at org.apache.storm.hive.bolt.TestHiveBolt.testData(TestHiveBolt.java:251)

testWithoutPartitions(org.apache.storm.hive.bolt.TestHiveBolt)  Time elapsed: 1.217 sec  <<< ERROR!
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hive.bolt.TestHiveBolt.generateTestTuple(TestHiveBolt.java:447)
	at org.apache.storm.hive.bolt.TestHiveBolt.testWithoutPartitions(TestHiveBolt.java:194)

testJsonWriter(org.apache.storm.hive.bolt.TestHiveBolt)  Time elapsed: 0.566 sec  <<< ERROR!
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hive.bolt.TestHiveBolt.generateTestTuple(TestHiveBolt.java:447)
	at org.apache.storm.hive.bolt.TestHiveBolt.testJsonWriter(TestHiveBolt.java:274)

testTickTuple(org.apache.storm.hive.bolt.TestHiveBolt)  Time elapsed: 0.454 sec  <<< ERROR!
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hive.bolt.TestHiveBolt.generateTestTuple(TestHiveBolt.java:447)
	at org.apache.storm.hive.bolt.TestHiveBolt.testTickTuple(TestHiveBolt.java:352)

testWithTimeformat(org.apache.storm.hive.bolt.TestHiveBolt)  Time elapsed: 0.572 sec  <<< ERROR!
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hive.bolt.TestHiveBolt.generateTestTuple(TestHiveBolt.java:447)
	at org.apache.storm.hive.bolt.TestHiveBolt.testWithTimeformat(TestHiveBolt.java:230)

testWithByteArrayIdandMessage(org.apache.storm.hive.bolt.TestHiveBolt)  Time elapsed: 0.404 sec  <<< ERROR!
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hive.bolt.TestHiveBolt.generateTestTuple(TestHiveBolt.java:447)
	at org.apache.storm.hive.bolt.TestHiveBolt.testWithByteArrayIdandMessage(TestHiveBolt.java:161)

testNoAcksIfFlushFails(org.apache.storm.hive.bolt.TestHiveBolt)  Time elapsed: 0.596 sec  <<< ERROR!
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hive.bolt.TestHiveBolt.generateTestTuple(TestHiveBolt.java:447)
	at org.apache.storm.hive.bolt.TestHiveBolt.testNoAcksIfFlushFails(TestHiveBolt.java:327)

Running org.apache.storm.hive.common.TestHiveWriter
Tests run: 3, Failures: 0, Errors: 2, Skipped: 0, Time elapsed: 7.041 sec <<< FAILURE! - in org.apache.storm.hive.common.TestHiveWriter
testWriteBasic(org.apache.storm.hive.common.TestHiveWriter)  Time elapsed: 0.862 sec  <<< ERROR!
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hive.common.TestHiveWriter.generateTestTuple(TestHiveWriter.java:164)
	at org.apache.storm.hive.common.TestHiveWriter.writeTuples(TestHiveWriter.java:179)
	at org.apache.storm.hive.common.TestHiveWriter.testWriteBasic(TestHiveWriter.java:127)

testWriteMultiFlush(org.apache.storm.hive.common.TestHiveWriter)  Time elapsed: 3.189 sec  <<< ERROR!
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hive.common.TestHiveWriter.generateTestTuple(TestHiveWriter.java:164)
	at org.apache.storm.hive.common.TestHiveWriter.testWriteMultiFlush(TestHiveWriter.java:142)


Results :

Tests in error: 
  TestHiveBolt.testData:251->generateTestTuple:447 » IllegalArgument Spouts is n...
  TestHiveBolt.testJsonWriter:274->generateTestTuple:447 » IllegalArgument Spout...
  TestHiveBolt.testMultiPartitionTuples:411->generateTestTuple:447 » IllegalArgument
  TestHiveBolt.testNoAcksIfFlushFails:327->generateTestTuple:447 » IllegalArgument
  TestHiveBolt.testNoAcksUntilFlushed:297->generateTestTuple:447 » IllegalArgument
  TestHiveBolt.testTickTuple:352->generateTestTuple:447 » IllegalArgument Spouts...
  TestHiveBolt.testWithByteArrayIdandMessage:161->generateTestTuple:447 » IllegalArgument
  TestHiveBolt.testWithTimeformat:230->generateTestTuple:447 » IllegalArgument S...
  TestHiveBolt.testWithoutPartitions:194->generateTestTuple:447 » IllegalArgument
  TestHiveWriter.testWriteBasic:127->writeTuples:179->generateTestTuple:164 » IllegalArgument
  TestHiveWriter.testWriteMultiFlush:142->generateTestTuple:164 » IllegalArgument

Tests run: 14, Failures: 0, Errors: 11, Skipped: 0

[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building storm-jdbc 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ storm-jdbc ---
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ storm-jdbc ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/storm-jdbc/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ storm-jdbc ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ storm-jdbc ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/storm-jdbc/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- sql-maven-plugin:1.5:execute (create-db) @ storm-jdbc ---
[INFO] Executing file: /tmp/test.387261775sql
[INFO] 1 of 1 SQL statements executed successfully
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ storm-jdbc ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:2.18.1:test (default-test) @ storm-jdbc ---
[INFO] Surefire report directory: /home/travis/build/apache/storm/external/storm-jdbc/target/surefire-reports
[INFO] parallel='none', perCoreThreadCount=true, threadCount=0, useUnlimitedThreads=false, threadCountSuites=0, threadCountClasses=0, threadCountMethods=0, parallelOptimized=true

-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running org.apache.storm.jdbc.common.UtilTest
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.007 sec - in org.apache.storm.jdbc.common.UtilTest
Running org.apache.storm.jdbc.common.JdbcClientTest
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.999 sec - in org.apache.storm.jdbc.common.JdbcClientTest
Running org.apache.storm.jdbc.bolt.JdbcLookupBoltTest
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.005 sec - in org.apache.storm.jdbc.bolt.JdbcLookupBoltTest
Running org.apache.storm.jdbc.bolt.JdbcInsertBoltTest
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.002 sec - in org.apache.storm.jdbc.bolt.JdbcInsertBoltTest

Results :

Tests run: 5, Failures: 0, Errors: 0, Skipped: 0

[INFO] 
[INFO] --- apache-rat-plugin:0.11:check (default) @ storm-jdbc ---
[INFO] 51 implicit excludes (use -debug for more details).
[INFO] Exclude: **/target/**
[INFO] Exclude: **/*.iml
[INFO] Exclude: **/.idea/**
[INFO] Exclude: **/metastore_db/**
[INFO] Exclude: **/build/**
[INFO] Exclude: **/logs/**
[INFO] Exclude: **/CHANGELOG.md
[INFO] Exclude: **/README.md
[INFO] Exclude: **/README.markdown
[INFO] Exclude: **/DEVELOPER.md
[INFO] Exclude: **/BYLAWS.md
[INFO] Exclude: **/STORM-UI-REST-API.md
[INFO] Exclude: SECURITY.md
[INFO] Exclude: VERSION
[INFO] Exclude: TODO
[INFO] Exclude: **/src/py/**
[INFO] Exclude: **/src/ui/public/js/jquery.dataTables.1.10.4.min.js
[INFO] Exclude: **/src/ui/public/css/jquery.dataTables.1.10.4.min.css
[INFO] Exclude: **/src/ui/public/images/*
[INFO] Exclude: **/src/ui/public/js/bootstrap-3.3.1.min.js
[INFO] Exclude: **/src/ui/public/css/bootstrap-3.3.1.min.css
[INFO] Exclude: **/src/ui/public/js/dataTables.bootstrap.min.js
[INFO] Exclude: **/src/ui/public/css/dataTables.bootstrap.css
[INFO] Exclude: **/src/ui/public/js/jsonFormatter.min.js
[INFO] Exclude: **/src/ui/public/css/jsonFormatter.min.css
[INFO] Exclude: **/src/ui/public/js/jquery-1.11.1.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.cookies.2.2.0.min.js
[INFO] Exclude: **/src/ui/public/js/moment.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.blockUI.min.js
[INFO] Exclude: **/src/ui/public/js/url.min.js
[INFO] Exclude: **/src/ui/public/js/arbor.js
[INFO] Exclude: **/src/ui/public/js/arbor-graphics.js
[INFO] Exclude: **/src/ui/public/js/arbor-tween.js
[INFO] Exclude: **/src/ui/public/js/jquery.mustache.js
[INFO] Exclude: **/src/ui/public/js/typeahead.jquery.min.js
[INFO] Exclude: **/dependency-reduced-pom.xml
[INFO] Exclude: **/docs/**
[INFO] Exclude: **/.git/**
[INFO] Exclude: **/derby.log
[INFO] Exclude: **/src/dev/**
[INFO] Exclude: **/src/codegen/config.fmpp
[INFO] Exclude: **/src/codegen/data/Parser.tdd
[INFO] Exclude: **/src/test/resources/FixedAvroSerializer.config
[INFO] 26 resources included (use -debug for more details)
Warning:  org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
Compiler warnings:
  WARNING:  'org.apache.xerces.jaxp.SAXParserImpl: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.'
Warning:  org.apache.xerces.parsers.SAXParser: Feature 'http://javax.xml.XMLConstants/feature/secure-processing' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
[INFO] Rat check: Summary of files. Unapproved: 0 unknown: 0 generated: 0 approved: 26 licence.
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building storm-redis 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ storm-redis ---
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ storm-redis ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/storm-redis/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ storm-redis ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ storm-redis ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/storm-redis/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ storm-redis ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:2.18.1:test (default-test) @ storm-redis ---
[INFO] Surefire report directory: /home/travis/build/apache/storm/external/storm-redis/target/surefire-reports
[INFO] parallel='none', perCoreThreadCount=true, threadCount=0, useUnlimitedThreads=false, threadCountSuites=0, threadCountClasses=0, threadCountMethods=0, parallelOptimized=true

-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running org.apache.storm.redis.state.DefaultStateSerializerTest
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.134 sec - in org.apache.storm.redis.state.DefaultStateSerializerTest
Running org.apache.storm.redis.state.RedisKeyValueStateTest
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.112 sec - in org.apache.storm.redis.state.RedisKeyValueStateTest
Running org.apache.storm.redis.state.RedisKeyValueStateProviderTest
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.267 sec - in org.apache.storm.redis.state.RedisKeyValueStateProviderTest

Results :

Tests run: 5, Failures: 0, Errors: 0, Skipped: 0

[INFO] 
[INFO] --- apache-rat-plugin:0.11:check (default) @ storm-redis ---
[INFO] 51 implicit excludes (use -debug for more details).
[INFO] Exclude: **/target/**
[INFO] Exclude: **/*.iml
[INFO] Exclude: **/.idea/**
[INFO] Exclude: **/metastore_db/**
[INFO] Exclude: **/build/**
[INFO] Exclude: **/logs/**
[INFO] Exclude: **/CHANGELOG.md
[INFO] Exclude: **/README.md
[INFO] Exclude: **/README.markdown
[INFO] Exclude: **/DEVELOPER.md
[INFO] Exclude: **/BYLAWS.md
[INFO] Exclude: **/STORM-UI-REST-API.md
[INFO] Exclude: SECURITY.md
[INFO] Exclude: VERSION
[INFO] Exclude: TODO
[INFO] Exclude: **/src/py/**
[INFO] Exclude: **/src/ui/public/js/jquery.dataTables.1.10.4.min.js
[INFO] Exclude: **/src/ui/public/css/jquery.dataTables.1.10.4.min.css
[INFO] Exclude: **/src/ui/public/images/*
[INFO] Exclude: **/src/ui/public/js/bootstrap-3.3.1.min.js
[INFO] Exclude: **/src/ui/public/css/bootstrap-3.3.1.min.css
[INFO] Exclude: **/src/ui/public/js/dataTables.bootstrap.min.js
[INFO] Exclude: **/src/ui/public/css/dataTables.bootstrap.css
[INFO] Exclude: **/src/ui/public/js/jsonFormatter.min.js
[INFO] Exclude: **/src/ui/public/css/jsonFormatter.min.css
[INFO] Exclude: **/src/ui/public/js/jquery-1.11.1.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.cookies.2.2.0.min.js
[INFO] Exclude: **/src/ui/public/js/moment.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.blockUI.min.js
[INFO] Exclude: **/src/ui/public/js/url.min.js
[INFO] Exclude: **/src/ui/public/js/arbor.js
[INFO] Exclude: **/src/ui/public/js/arbor-graphics.js
[INFO] Exclude: **/src/ui/public/js/arbor-tween.js
[INFO] Exclude: **/src/ui/public/js/jquery.mustache.js
[INFO] Exclude: **/src/ui/public/js/typeahead.jquery.min.js
[INFO] Exclude: **/dependency-reduced-pom.xml
[INFO] Exclude: **/docs/**
[INFO] Exclude: **/.git/**
[INFO] Exclude: **/derby.log
[INFO] Exclude: **/src/dev/**
[INFO] Exclude: **/src/codegen/config.fmpp
[INFO] Exclude: **/src/codegen/data/Parser.tdd
[INFO] Exclude: **/src/test/resources/FixedAvroSerializer.config
[INFO] 44 resources included (use -debug for more details)
Warning:  org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
Compiler warnings:
  WARNING:  'org.apache.xerces.jaxp.SAXParserImpl: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.'
Warning:  org.apache.xerces.parsers.SAXParser: Feature 'http://javax.xml.XMLConstants/feature/secure-processing' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
[INFO] Rat check: Summary of files. Unapproved: 0 unknown: 0 generated: 0 approved: 44 licence.
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building storm-eventhubs 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ storm-eventhubs ---
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ storm-eventhubs ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 1 resource
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ storm-eventhubs ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ storm-eventhubs ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/storm-eventhubs/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ storm-eventhubs ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:2.18.1:test (default-test) @ storm-eventhubs ---
[INFO] Surefire report directory: /home/travis/build/apache/storm/external/storm-eventhubs/target/surefire-reports
[INFO] parallel='none', perCoreThreadCount=true, threadCount=0, useUnlimitedThreads=false, threadCountSuites=0, threadCountClasses=0, threadCountMethods=0, parallelOptimized=true

-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running org.apache.storm.eventhubs.trident.TestTransactionalTridentEmitter
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.59 sec - in org.apache.storm.eventhubs.trident.TestTransactionalTridentEmitter
Running org.apache.storm.eventhubs.spout.TestPartitionManager
Tests run: 8, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.039 sec - in org.apache.storm.eventhubs.spout.TestPartitionManager
Running org.apache.storm.eventhubs.spout.TestEventData
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0 sec - in org.apache.storm.eventhubs.spout.TestEventData
Running org.apache.storm.eventhubs.spout.TestEventHubSpout
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 2.02 sec - in org.apache.storm.eventhubs.spout.TestEventHubSpout

Results :

Tests run: 14, Failures: 0, Errors: 0, Skipped: 0

[INFO] 
[INFO] --- apache-rat-plugin:0.11:check (default) @ storm-eventhubs ---
[INFO] 51 implicit excludes (use -debug for more details).
[INFO] Exclude: **/target/**
[INFO] Exclude: **/*.iml
[INFO] Exclude: **/.idea/**
[INFO] Exclude: **/metastore_db/**
[INFO] Exclude: **/build/**
[INFO] Exclude: **/logs/**
[INFO] Exclude: **/CHANGELOG.md
[INFO] Exclude: **/README.md
[INFO] Exclude: **/README.markdown
[INFO] Exclude: **/DEVELOPER.md
[INFO] Exclude: **/BYLAWS.md
[INFO] Exclude: **/STORM-UI-REST-API.md
[INFO] Exclude: SECURITY.md
[INFO] Exclude: VERSION
[INFO] Exclude: TODO
[INFO] Exclude: **/src/py/**
[INFO] Exclude: **/src/ui/public/js/jquery.dataTables.1.10.4.min.js
[INFO] Exclude: **/src/ui/public/css/jquery.dataTables.1.10.4.min.css
[INFO] Exclude: **/src/ui/public/images/*
[INFO] Exclude: **/src/ui/public/js/bootstrap-3.3.1.min.js
[INFO] Exclude: **/src/ui/public/css/bootstrap-3.3.1.min.css
[INFO] Exclude: **/src/ui/public/js/dataTables.bootstrap.min.js
[INFO] Exclude: **/src/ui/public/css/dataTables.bootstrap.css
[INFO] Exclude: **/src/ui/public/js/jsonFormatter.min.js
[INFO] Exclude: **/src/ui/public/css/jsonFormatter.min.css
[INFO] Exclude: **/src/ui/public/js/jquery-1.11.1.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.cookies.2.2.0.min.js
[INFO] Exclude: **/src/ui/public/js/moment.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.blockUI.min.js
[INFO] Exclude: **/src/ui/public/js/url.min.js
[INFO] Exclude: **/src/ui/public/js/arbor.js
[INFO] Exclude: **/src/ui/public/js/arbor-graphics.js
[INFO] Exclude: **/src/ui/public/js/arbor-tween.js
[INFO] Exclude: **/src/ui/public/js/jquery.mustache.js
[INFO] Exclude: **/src/ui/public/js/typeahead.jquery.min.js
[INFO] Exclude: **/dependency-reduced-pom.xml
[INFO] Exclude: **/docs/**
[INFO] Exclude: **/.git/**
[INFO] Exclude: **/derby.log
[INFO] Exclude: **/src/dev/**
[INFO] Exclude: **/src/codegen/config.fmpp
[INFO] Exclude: **/src/codegen/data/Parser.tdd
[INFO] Exclude: **/src/test/resources/FixedAvroSerializer.config
[INFO] 52 resources included (use -debug for more details)
Warning:  org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
Compiler warnings:
  WARNING:  'org.apache.xerces.jaxp.SAXParserImpl: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.'
Warning:  org.apache.xerces.parsers.SAXParser: Feature 'http://javax.xml.XMLConstants/feature/secure-processing' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
[INFO] Rat check: Summary of files. Unapproved: 0 unknown: 0 generated: 0 approved: 52 licence.
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building flux 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ flux ---
[INFO] 
[INFO] --- apache-rat-plugin:0.11:check (default) @ flux ---
[INFO] 51 implicit excludes (use -debug for more details).
[INFO] Exclude: **/target/**
[INFO] Exclude: **/*.iml
[INFO] Exclude: **/.idea/**
[INFO] Exclude: **/metastore_db/**
[INFO] Exclude: **/build/**
[INFO] Exclude: **/logs/**
[INFO] Exclude: **/CHANGELOG.md
[INFO] Exclude: **/README.md
[INFO] Exclude: **/README.markdown
[INFO] Exclude: **/DEVELOPER.md
[INFO] Exclude: **/BYLAWS.md
[INFO] Exclude: **/STORM-UI-REST-API.md
[INFO] Exclude: SECURITY.md
[INFO] Exclude: VERSION
[INFO] Exclude: TODO
[INFO] Exclude: **/src/py/**
[INFO] Exclude: **/src/ui/public/js/jquery.dataTables.1.10.4.min.js
[INFO] Exclude: **/src/ui/public/css/jquery.dataTables.1.10.4.min.css
[INFO] Exclude: **/src/ui/public/images/*
[INFO] Exclude: **/src/ui/public/js/bootstrap-3.3.1.min.js
[INFO] Exclude: **/src/ui/public/css/bootstrap-3.3.1.min.css
[INFO] Exclude: **/src/ui/public/js/dataTables.bootstrap.min.js
[INFO] Exclude: **/src/ui/public/css/dataTables.bootstrap.css
[INFO] Exclude: **/src/ui/public/js/jsonFormatter.min.js
[INFO] Exclude: **/src/ui/public/css/jsonFormatter.min.css
[INFO] Exclude: **/src/ui/public/js/jquery-1.11.1.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.cookies.2.2.0.min.js
[INFO] Exclude: **/src/ui/public/js/moment.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.blockUI.min.js
[INFO] Exclude: **/src/ui/public/js/url.min.js
[INFO] Exclude: **/src/ui/public/js/arbor.js
[INFO] Exclude: **/src/ui/public/js/arbor-graphics.js
[INFO] Exclude: **/src/ui/public/js/arbor-tween.js
[INFO] Exclude: **/src/ui/public/js/jquery.mustache.js
[INFO] Exclude: **/src/ui/public/js/typeahead.jquery.min.js
[INFO] Exclude: **/dependency-reduced-pom.xml
[INFO] Exclude: **/docs/**
[INFO] Exclude: **/.git/**
[INFO] Exclude: **/derby.log
[INFO] Exclude: **/src/dev/**
[INFO] Exclude: **/src/codegen/config.fmpp
[INFO] Exclude: **/src/codegen/data/Parser.tdd
[INFO] Exclude: **/src/test/resources/FixedAvroSerializer.config
[INFO] 69 resources included (use -debug for more details)
Warning:  org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
Compiler warnings:
  WARNING:  'org.apache.xerces.jaxp.SAXParserImpl: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.'
Warning:  org.apache.xerces.parsers.SAXParser: Feature 'http://javax.xml.XMLConstants/feature/secure-processing' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
[INFO] Rat check: Summary of files. Unapproved: 0 unknown: 0 generated: 0 approved: 68 licence.
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building flux-wrappers 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ flux-wrappers ---
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ flux-wrappers ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 2 resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.3:compile (default-compile) @ flux-wrappers ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ flux-wrappers ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/flux/flux-wrappers/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.3:testCompile (default-testCompile) @ flux-wrappers ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-surefire-plugin:2.18.1:test (default-test) @ flux-wrappers ---
[INFO] 
[INFO] --- apache-rat-plugin:0.11:check (default) @ flux-wrappers ---
[INFO] 51 implicit excludes (use -debug for more details).
[INFO] Exclude: **/target/**
[INFO] Exclude: **/*.iml
[INFO] Exclude: **/.idea/**
[INFO] Exclude: **/metastore_db/**
[INFO] Exclude: **/build/**
[INFO] Exclude: **/logs/**
[INFO] Exclude: **/CHANGELOG.md
[INFO] Exclude: **/README.md
[INFO] Exclude: **/README.markdown
[INFO] Exclude: **/DEVELOPER.md
[INFO] Exclude: **/BYLAWS.md
[INFO] Exclude: **/STORM-UI-REST-API.md
[INFO] Exclude: SECURITY.md
[INFO] Exclude: VERSION
[INFO] Exclude: TODO
[INFO] Exclude: **/src/py/**
[INFO] Exclude: **/src/ui/public/js/jquery.dataTables.1.10.4.min.js
[INFO] Exclude: **/src/ui/public/css/jquery.dataTables.1.10.4.min.css
[INFO] Exclude: **/src/ui/public/images/*
[INFO] Exclude: **/src/ui/public/js/bootstrap-3.3.1.min.js
[INFO] Exclude: **/src/ui/public/css/bootstrap-3.3.1.min.css
[INFO] Exclude: **/src/ui/public/js/dataTables.bootstrap.min.js
[INFO] Exclude: **/src/ui/public/css/dataTables.bootstrap.css
[INFO] Exclude: **/src/ui/public/js/jsonFormatter.min.js
[INFO] Exclude: **/src/ui/public/css/jsonFormatter.min.css
[INFO] Exclude: **/src/ui/public/js/jquery-1.11.1.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.cookies.2.2.0.min.js
[INFO] Exclude: **/src/ui/public/js/moment.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.blockUI.min.js
[INFO] Exclude: **/src/ui/public/js/url.min.js
[INFO] Exclude: **/src/ui/public/js/arbor.js
[INFO] Exclude: **/src/ui/public/js/arbor-graphics.js
[INFO] Exclude: **/src/ui/public/js/arbor-tween.js
[INFO] Exclude: **/src/ui/public/js/jquery.mustache.js
[INFO] Exclude: **/src/ui/public/js/typeahead.jquery.min.js
[INFO] Exclude: **/dependency-reduced-pom.xml
[INFO] Exclude: **/docs/**
[INFO] Exclude: **/.git/**
[INFO] Exclude: **/derby.log
[INFO] Exclude: **/src/dev/**
[INFO] Exclude: **/src/codegen/config.fmpp
[INFO] Exclude: **/src/codegen/data/Parser.tdd
[INFO] Exclude: **/src/test/resources/FixedAvroSerializer.config
[INFO] 6 resources included (use -debug for more details)
Warning:  org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
Compiler warnings:
  WARNING:  'org.apache.xerces.jaxp.SAXParserImpl: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.'
Warning:  org.apache.xerces.parsers.SAXParser: Feature 'http://javax.xml.XMLConstants/feature/secure-processing' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
[INFO] Rat check: Summary of files. Unapproved: 0 unknown: 0 generated: 0 approved: 6 licence.
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Skipping flux-core
[INFO] This project has been banned from the build due to previous failures.
[INFO] ------------------------------------------------------------------------
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Skipping flux-examples
[INFO] This project has been banned from the build due to previous failures.
[INFO] ------------------------------------------------------------------------
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building storm-sql-runtime 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ storm-sql-runtime ---
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ storm-sql-runtime ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/sql/storm-sql-runtime/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ storm-sql-runtime ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ storm-sql-runtime ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/sql/storm-sql-runtime/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ storm-sql-runtime ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:2.18.1:test (default-test) @ storm-sql-runtime ---
[INFO] Surefire report directory: /home/travis/build/apache/storm/external/sql/storm-sql-runtime/target/surefire-reports
[INFO] parallel='none', perCoreThreadCount=true, threadCount=0, useUnlimitedThreads=false, threadCountSuites=0, threadCountClasses=0, threadCountMethods=0, parallelOptimized=true

-------------------------------------------------------
 T E S T S
-------------------------------------------------------

Results :

Tests run: 0, Failures: 0, Errors: 0, Skipped: 0

[INFO] 
[INFO] --- apache-rat-plugin:0.11:check (default) @ storm-sql-runtime ---
[INFO] 51 implicit excludes (use -debug for more details).
[INFO] Exclude: **/target/**
[INFO] Exclude: **/*.iml
[INFO] Exclude: **/.idea/**
[INFO] Exclude: **/metastore_db/**
[INFO] Exclude: **/build/**
[INFO] Exclude: **/logs/**
[INFO] Exclude: **/CHANGELOG.md
[INFO] Exclude: **/README.md
[INFO] Exclude: **/README.markdown
[INFO] Exclude: **/DEVELOPER.md
[INFO] Exclude: **/BYLAWS.md
[INFO] Exclude: **/STORM-UI-REST-API.md
[INFO] Exclude: SECURITY.md
[INFO] Exclude: VERSION
[INFO] Exclude: TODO
[INFO] Exclude: **/src/py/**
[INFO] Exclude: **/src/ui/public/js/jquery.dataTables.1.10.4.min.js
[INFO] Exclude: **/src/ui/public/css/jquery.dataTables.1.10.4.min.css
[INFO] Exclude: **/src/ui/public/images/*
[INFO] Exclude: **/src/ui/public/js/bootstrap-3.3.1.min.js
[INFO] Exclude: **/src/ui/public/css/bootstrap-3.3.1.min.css
[INFO] Exclude: **/src/ui/public/js/dataTables.bootstrap.min.js
[INFO] Exclude: **/src/ui/public/css/dataTables.bootstrap.css
[INFO] Exclude: **/src/ui/public/js/jsonFormatter.min.js
[INFO] Exclude: **/src/ui/public/css/jsonFormatter.min.css
[INFO] Exclude: **/src/ui/public/js/jquery-1.11.1.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.cookies.2.2.0.min.js
[INFO] Exclude: **/src/ui/public/js/moment.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.blockUI.min.js
[INFO] Exclude: **/src/ui/public/js/url.min.js
[INFO] Exclude: **/src/ui/public/js/arbor.js
[INFO] Exclude: **/src/ui/public/js/arbor-graphics.js
[INFO] Exclude: **/src/ui/public/js/arbor-tween.js
[INFO] Exclude: **/src/ui/public/js/jquery.mustache.js
[INFO] Exclude: **/src/ui/public/js/typeahead.jquery.min.js
[INFO] Exclude: **/dependency-reduced-pom.xml
[INFO] Exclude: **/docs/**
[INFO] Exclude: **/.git/**
[INFO] Exclude: **/derby.log
[INFO] Exclude: **/src/dev/**
[INFO] Exclude: **/src/codegen/config.fmpp
[INFO] Exclude: **/src/codegen/data/Parser.tdd
[INFO] Exclude: **/src/test/resources/FixedAvroSerializer.config
[INFO] 15 resources included (use -debug for more details)
Warning:  org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
Compiler warnings:
  WARNING:  'org.apache.xerces.jaxp.SAXParserImpl: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.'
Warning:  org.apache.xerces.parsers.SAXParser: Feature 'http://javax.xml.XMLConstants/feature/secure-processing' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
[INFO] Rat check: Summary of files. Unapproved: 0 unknown: 0 generated: 0 approved: 15 licence.
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building storm-sql-core 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-resources-plugin:2.5:copy-resources (copy-fmpp-resources) @ storm-sql-core ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 4 resources
[INFO] 
[INFO] --- maven-dependency-plugin:2.8:unpack (unpack-parser-template) @ storm-sql-core ---
[INFO] Configured Artifact: org.apache.calcite:calcite-core:?:jar
[INFO] Unpacking /home/travis/.m2/repository/org/apache/calcite/calcite-core/1.4.0-incubating/calcite-core-1.4.0-incubating.jar to /home/travis/build/apache/storm/external/sql/storm-sql-core/target with includes "**/Parser.jj" and excludes ""
[INFO] 
[INFO] --- fmpp-maven-plugin:1.0:generate (generate-fmpp-sources) @ storm-sql-core ---
- Executing: Parser.jj
log4j:WARN No appenders could be found for logger (freemarker.cache).
log4j:WARN Please initialize the log4j system properly.
[INFO] Done
[INFO] 
[INFO] --- javacc-maven-plugin:2.4:javacc (javacc) @ storm-sql-core ---
Java Compiler Compiler Version 4.0 (Parser Generator)
(type "javacc" with no arguments for help)
Reading from file /home/travis/build/apache/storm/external/sql/storm-sql-core/target/generated-sources/javacc/Parser.jj . . .
Note: UNICODE_INPUT option is specified. Please make sure you create the parser/lexer using a Reader with the correct character encoding.
Warning: Lookahead adequacy checking not being performed since option LOOKAHEAD is more than 1.  Set option FORCE_LA_CHECK to true to force checking.
Parser generated with 0 errors and 1 warnings.
[INFO] Processed 1 grammar
[INFO] 
[INFO] --- maven-resources-plugin:2.5:copy-resources (copy-java-sources) @ storm-sql-core ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 19 resources
[INFO] Copying 8 resources
[INFO] Copying 8 resources
[INFO] 
[INFO] --- build-helper-maven-plugin:1.5:add-source (add-generated-sources) @ storm-sql-core ---
[INFO] Source directory: /home/travis/build/apache/storm/external/sql/storm-sql-core/target/generated-sources added.
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ storm-sql-core ---
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ storm-sql-core ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/sql/storm-sql-core/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ storm-sql-core ---
[INFO] Changes detected - recompiling the module!
[INFO] Compiling 26 source files to /home/travis/build/apache/storm/external/sql/storm-sql-core/target/classes
[WARNING] /home/travis/build/apache/storm/external/sql/storm-sql-core/target/generated-sources/org/apache/storm/sql/parser/impl/StormParserImpl.java: Some input files use or override a deprecated API.
[WARNING] /home/travis/build/apache/storm/external/sql/storm-sql-core/target/generated-sources/org/apache/storm/sql/parser/impl/StormParserImpl.java: Recompile with -Xlint:deprecation for details.
[WARNING] /home/travis/build/apache/storm/external/sql/storm-sql-core/target/generated-sources/org/apache/storm/sql/parser/impl/StormParserImpl.java: /home/travis/build/apache/storm/external/sql/storm-sql-core/target/generated-sources/org/apache/storm/sql/parser/impl/StormParserImpl.java uses unchecked or unsafe operations.
[WARNING] /home/travis/build/apache/storm/external/sql/storm-sql-core/target/generated-sources/org/apache/storm/sql/parser/impl/StormParserImpl.java: Recompile with -Xlint:unchecked for details.
[INFO] 
[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ storm-sql-core ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/sql/storm-sql-core/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ storm-sql-core ---
[INFO] Changes detected - recompiling the module!
[INFO] Compiling 15 source files to /home/travis/build/apache/storm/external/sql/storm-sql-core/target/test-classes
[WARNING] /home/travis/build/apache/storm/external/sql/storm-sql-core/target/generated-sources/org/apache/storm/sql/parser/impl/StormParserImpl.java: /home/travis/build/apache/storm/external/sql/storm-sql-core/target/generated-sources/org/apache/storm/sql/parser/impl/StormParserImpl.java uses or overrides a deprecated API.
[WARNING] /home/travis/build/apache/storm/external/sql/storm-sql-core/target/generated-sources/org/apache/storm/sql/parser/impl/StormParserImpl.java: Recompile with -Xlint:deprecation for details.
[WARNING] /home/travis/build/apache/storm/external/sql/storm-sql-core/target/generated-sources/org/apache/storm/sql/parser/impl/StormParserImpl.java: /home/travis/build/apache/storm/external/sql/storm-sql-core/target/generated-sources/org/apache/storm/sql/parser/impl/StormParserImpl.java uses unchecked or unsafe operations.
[WARNING] /home/travis/build/apache/storm/external/sql/storm-sql-core/target/generated-sources/org/apache/storm/sql/parser/impl/StormParserImpl.java: Recompile with -Xlint:unchecked for details.
[INFO] 
[INFO] --- maven-surefire-plugin:2.18.1:test (default-test) @ storm-sql-core ---
[INFO] Surefire report directory: /home/travis/build/apache/storm/external/sql/storm-sql-core/target/surefire-reports
[INFO] parallel='none', perCoreThreadCount=true, threadCount=0, useUnlimitedThreads=false, threadCountSuites=0, threadCountClasses=0, threadCountMethods=0, parallelOptimized=true

-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running org.apache.storm.sql.parser.TestSqlParser
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.234 sec - in org.apache.storm.sql.parser.TestSqlParser
Running org.apache.storm.sql.compiler.backends.standalone.TestRelNodeCompiler
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.705 sec - in org.apache.storm.sql.compiler.backends.standalone.TestRelNodeCompiler
Running org.apache.storm.sql.compiler.backends.standalone.TestPlanCompiler
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 2.938 sec - in org.apache.storm.sql.compiler.backends.standalone.TestPlanCompiler
Running org.apache.storm.sql.compiler.backends.trident.TestPlanCompiler
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 33.394 sec - in org.apache.storm.sql.compiler.backends.trident.TestPlanCompiler
Running org.apache.storm.sql.compiler.TestExprCompiler
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.046 sec - in org.apache.storm.sql.compiler.TestExprCompiler
Running org.apache.storm.sql.compiler.TestExprSemantic
Tests run: 10, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 2.209 sec - in org.apache.storm.sql.compiler.TestExprSemantic
Running org.apache.storm.sql.TestStormSql
Tests run: 15, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 4.27 sec - in org.apache.storm.sql.TestStormSql

Results :

Tests run: 39, Failures: 0, Errors: 0, Skipped: 0

[INFO] 
[INFO] --- apache-rat-plugin:0.11:check (default) @ storm-sql-core ---
[INFO] 51 implicit excludes (use -debug for more details).
[INFO] Exclude: **/target/**
[INFO] Exclude: **/*.iml
[INFO] Exclude: **/.idea/**
[INFO] Exclude: **/metastore_db/**
[INFO] Exclude: **/build/**
[INFO] Exclude: **/logs/**
[INFO] Exclude: **/CHANGELOG.md
[INFO] Exclude: **/README.md
[INFO] Exclude: **/README.markdown
[INFO] Exclude: **/DEVELOPER.md
[INFO] Exclude: **/BYLAWS.md
[INFO] Exclude: **/STORM-UI-REST-API.md
[INFO] Exclude: SECURITY.md
[INFO] Exclude: VERSION
[INFO] Exclude: TODO
[INFO] Exclude: **/src/py/**
[INFO] Exclude: **/src/ui/public/js/jquery.dataTables.1.10.4.min.js
[INFO] Exclude: **/src/ui/public/css/jquery.dataTables.1.10.4.min.css
[INFO] Exclude: **/src/ui/public/images/*
[INFO] Exclude: **/src/ui/public/js/bootstrap-3.3.1.min.js
[INFO] Exclude: **/src/ui/public/css/bootstrap-3.3.1.min.css
[INFO] Exclude: **/src/ui/public/js/dataTables.bootstrap.min.js
[INFO] Exclude: **/src/ui/public/css/dataTables.bootstrap.css
[INFO] Exclude: **/src/ui/public/js/jsonFormatter.min.js
[INFO] Exclude: **/src/ui/public/css/jsonFormatter.min.css
[INFO] Exclude: **/src/ui/public/js/jquery-1.11.1.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.cookies.2.2.0.min.js
[INFO] Exclude: **/src/ui/public/js/moment.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.blockUI.min.js
[INFO] Exclude: **/src/ui/public/js/url.min.js
[INFO] Exclude: **/src/ui/public/js/arbor.js
[INFO] Exclude: **/src/ui/public/js/arbor-graphics.js
[INFO] Exclude: **/src/ui/public/js/arbor-tween.js
[INFO] Exclude: **/src/ui/public/js/jquery.mustache.js
[INFO] Exclude: **/src/ui/public/js/typeahead.jquery.min.js
[INFO] Exclude: **/dependency-reduced-pom.xml
[INFO] Exclude: **/docs/**
[INFO] Exclude: **/.git/**
[INFO] Exclude: **/derby.log
[INFO] Exclude: **/src/dev/**
[INFO] Exclude: **/src/codegen/config.fmpp
[INFO] Exclude: **/src/codegen/data/Parser.tdd
[INFO] Exclude: **/src/test/resources/FixedAvroSerializer.config
[INFO] 30 resources included (use -debug for more details)
Warning:  org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
Compiler warnings:
  WARNING:  'org.apache.xerces.jaxp.SAXParserImpl: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.'
Warning:  org.apache.xerces.parsers.SAXParser: Feature 'http://javax.xml.XMLConstants/feature/secure-processing' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
[INFO] Rat check: Summary of files. Unapproved: 0 unknown: 0 generated: 0 approved: 30 licence.
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Skipping storm-sql-kafka
[INFO] This project has been banned from the build due to previous failures.
[INFO] ------------------------------------------------------------------------
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building sql 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ sql ---
[INFO] 
[INFO] --- apache-rat-plugin:0.11:check (default) @ sql ---
[INFO] 51 implicit excludes (use -debug for more details).
[INFO] Exclude: **/target/**
[INFO] Exclude: **/*.iml
[INFO] Exclude: **/.idea/**
[INFO] Exclude: **/metastore_db/**
[INFO] Exclude: **/build/**
[INFO] Exclude: **/logs/**
[INFO] Exclude: **/CHANGELOG.md
[INFO] Exclude: **/README.md
[INFO] Exclude: **/README.markdown
[INFO] Exclude: **/DEVELOPER.md
[INFO] Exclude: **/BYLAWS.md
[INFO] Exclude: **/STORM-UI-REST-API.md
[INFO] Exclude: SECURITY.md
[INFO] Exclude: VERSION
[INFO] Exclude: TODO
[INFO] Exclude: **/src/py/**
[INFO] Exclude: **/src/ui/public/js/jquery.dataTables.1.10.4.min.js
[INFO] Exclude: **/src/ui/public/css/jquery.dataTables.1.10.4.min.css
[INFO] Exclude: **/src/ui/public/images/*
[INFO] Exclude: **/src/ui/public/js/bootstrap-3.3.1.min.js
[INFO] Exclude: **/src/ui/public/css/bootstrap-3.3.1.min.css
[INFO] Exclude: **/src/ui/public/js/dataTables.bootstrap.min.js
[INFO] Exclude: **/src/ui/public/css/dataTables.bootstrap.css
[INFO] Exclude: **/src/ui/public/js/jsonFormatter.min.js
[INFO] Exclude: **/src/ui/public/css/jsonFormatter.min.css
[INFO] Exclude: **/src/ui/public/js/jquery-1.11.1.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.cookies.2.2.0.min.js
[INFO] Exclude: **/src/ui/public/js/moment.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.blockUI.min.js
[INFO] Exclude: **/src/ui/public/js/url.min.js
[INFO] Exclude: **/src/ui/public/js/arbor.js
[INFO] Exclude: **/src/ui/public/js/arbor-graphics.js
[INFO] Exclude: **/src/ui/public/js/arbor-tween.js
[INFO] Exclude: **/src/ui/public/js/jquery.mustache.js
[INFO] Exclude: **/src/ui/public/js/typeahead.jquery.min.js
[INFO] Exclude: **/dependency-reduced-pom.xml
[INFO] Exclude: **/docs/**
[INFO] Exclude: **/.git/**
[INFO] Exclude: **/derby.log
[INFO] Exclude: **/src/dev/**
[INFO] Exclude: **/src/codegen/config.fmpp
[INFO] Exclude: **/src/codegen/data/Parser.tdd
[INFO] Exclude: **/src/test/resources/FixedAvroSerializer.config
[INFO] 53 resources included (use -debug for more details)
Warning:  org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
Compiler warnings:
  WARNING:  'org.apache.xerces.jaxp.SAXParserImpl: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.'
Warning:  org.apache.xerces.parsers.SAXParser: Feature 'http://javax.xml.XMLConstants/feature/secure-processing' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
[INFO] Rat check: Summary of files. Unapproved: 0 unknown: 0 generated: 0 approved: 53 licence.
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building storm-elasticsearch 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ storm-elasticsearch ---
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ storm-elasticsearch ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/storm-elasticsearch/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ storm-elasticsearch ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ storm-elasticsearch ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/storm-elasticsearch/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ storm-elasticsearch ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (cleanup) @ storm-elasticsearch ---
[INFO] 
[INFO] --- maven-surefire-plugin:2.18.1:test (default-test) @ storm-elasticsearch ---
[INFO] Surefire report directory: /home/travis/build/apache/storm/external/storm-elasticsearch/target/surefire-reports
[INFO] parallel='none', perCoreThreadCount=true, threadCount=0, useUnlimitedThreads=false, threadCountSuites=0, threadCountClasses=0, threadCountMethods=0, parallelOptimized=true

-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running org.apache.storm.elasticsearch.bolt.EsLookupBoltTest
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.614 sec - in org.apache.storm.elasticsearch.bolt.EsLookupBoltTest
Running org.apache.storm.elasticsearch.trident.EsStateFactoryTest
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.005 sec - in org.apache.storm.elasticsearch.trident.EsStateFactoryTest
Running org.apache.storm.elasticsearch.common.EsConfigTest
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.041 sec - in org.apache.storm.elasticsearch.common.EsConfigTest
Running org.apache.storm.elasticsearch.common.TransportAddressesTest
Tests run: 9, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.042 sec - in org.apache.storm.elasticsearch.common.TransportAddressesTest

Results :

Tests run: 18, Failures: 0, Errors: 0, Skipped: 0

[INFO] 
[INFO] --- apache-rat-plugin:0.11:check (default) @ storm-elasticsearch ---
[INFO] 51 implicit excludes (use -debug for more details).
[INFO] Exclude: **/target/**
[INFO] Exclude: **/*.iml
[INFO] Exclude: **/.idea/**
[INFO] Exclude: **/metastore_db/**
[INFO] Exclude: **/build/**
[INFO] Exclude: **/logs/**
[INFO] Exclude: **/CHANGELOG.md
[INFO] Exclude: **/README.md
[INFO] Exclude: **/README.markdown
[INFO] Exclude: **/DEVELOPER.md
[INFO] Exclude: **/BYLAWS.md
[INFO] Exclude: **/STORM-UI-REST-API.md
[INFO] Exclude: SECURITY.md
[INFO] Exclude: VERSION
[INFO] Exclude: TODO
[INFO] Exclude: **/src/py/**
[INFO] Exclude: **/src/ui/public/js/jquery.dataTables.1.10.4.min.js
[INFO] Exclude: **/src/ui/public/css/jquery.dataTables.1.10.4.min.css
[INFO] Exclude: **/src/ui/public/images/*
[INFO] Exclude: **/src/ui/public/js/bootstrap-3.3.1.min.js
[INFO] Exclude: **/src/ui/public/css/bootstrap-3.3.1.min.css
[INFO] Exclude: **/src/ui/public/js/dataTables.bootstrap.min.js
[INFO] Exclude: **/src/ui/public/css/dataTables.bootstrap.css
[INFO] Exclude: **/src/ui/public/js/jsonFormatter.min.js
[INFO] Exclude: **/src/ui/public/css/jsonFormatter.min.css
[INFO] Exclude: **/src/ui/public/js/jquery-1.11.1.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.cookies.2.2.0.min.js
[INFO] Exclude: **/src/ui/public/js/moment.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.blockUI.min.js
[INFO] Exclude: **/src/ui/public/js/url.min.js
[INFO] Exclude: **/src/ui/public/js/arbor.js
[INFO] Exclude: **/src/ui/public/js/arbor-graphics.js
[INFO] Exclude: **/src/ui/public/js/arbor-tween.js
[INFO] Exclude: **/src/ui/public/js/jquery.mustache.js
[INFO] Exclude: **/src/ui/public/js/typeahead.jquery.min.js
[INFO] Exclude: **/dependency-reduced-pom.xml
[INFO] Exclude: **/docs/**
[INFO] Exclude: **/.git/**
[INFO] Exclude: **/derby.log
[INFO] Exclude: **/src/dev/**
[INFO] Exclude: **/src/codegen/config.fmpp
[INFO] Exclude: **/src/codegen/data/Parser.tdd
[INFO] Exclude: **/src/test/resources/FixedAvroSerializer.config
[INFO] 28 resources included (use -debug for more details)
Warning:  org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
Compiler warnings:
  WARNING:  'org.apache.xerces.jaxp.SAXParserImpl: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.'
Warning:  org.apache.xerces.parsers.SAXParser: Feature 'http://javax.xml.XMLConstants/feature/secure-processing' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
[INFO] Rat check: Summary of files. Unapproved: 0 unknown: 0 generated: 0 approved: 28 licence.
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building storm-solr 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ storm-solr ---
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ storm-solr ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/storm-solr/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ storm-solr ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ storm-solr ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/storm-solr/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ storm-solr ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:2.18.1:test (default-test) @ storm-solr ---
[INFO] Surefire report directory: /home/travis/build/apache/storm/external/storm-solr/target/surefire-reports
[INFO] parallel='none', perCoreThreadCount=true, threadCount=0, useUnlimitedThreads=false, threadCountSuites=0, threadCountClasses=0, threadCountMethods=0, parallelOptimized=true

-------------------------------------------------------
 T E S T S
-------------------------------------------------------

Results :

Tests run: 0, Failures: 0, Errors: 0, Skipped: 0

[INFO] 
[INFO] --- apache-rat-plugin:0.11:check (default) @ storm-solr ---
[INFO] 51 implicit excludes (use -debug for more details).
[INFO] Exclude: **/target/**
[INFO] Exclude: **/*.iml
[INFO] Exclude: **/.idea/**
[INFO] Exclude: **/metastore_db/**
[INFO] Exclude: **/build/**
[INFO] Exclude: **/logs/**
[INFO] Exclude: **/CHANGELOG.md
[INFO] Exclude: **/README.md
[INFO] Exclude: **/README.markdown
[INFO] Exclude: **/DEVELOPER.md
[INFO] Exclude: **/BYLAWS.md
[INFO] Exclude: **/STORM-UI-REST-API.md
[INFO] Exclude: SECURITY.md
[INFO] Exclude: VERSION
[INFO] Exclude: TODO
[INFO] Exclude: **/src/py/**
[INFO] Exclude: **/src/ui/public/js/jquery.dataTables.1.10.4.min.js
[INFO] Exclude: **/src/ui/public/css/jquery.dataTables.1.10.4.min.css
[INFO] Exclude: **/src/ui/public/images/*
[INFO] Exclude: **/src/ui/public/js/bootstrap-3.3.1.min.js
[INFO] Exclude: **/src/ui/public/css/bootstrap-3.3.1.min.css
[INFO] Exclude: **/src/ui/public/js/dataTables.bootstrap.min.js
[INFO] Exclude: **/src/ui/public/css/dataTables.bootstrap.css
[INFO] Exclude: **/src/ui/public/js/jsonFormatter.min.js
[INFO] Exclude: **/src/ui/public/css/jsonFormatter.min.css
[INFO] Exclude: **/src/ui/public/js/jquery-1.11.1.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.cookies.2.2.0.min.js
[INFO] Exclude: **/src/ui/public/js/moment.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.blockUI.min.js
[INFO] Exclude: **/src/ui/public/js/url.min.js
[INFO] Exclude: **/src/ui/public/js/arbor.js
[INFO] Exclude: **/src/ui/public/js/arbor-graphics.js
[INFO] Exclude: **/src/ui/public/js/arbor-tween.js
[INFO] Exclude: **/src/ui/public/js/jquery.mustache.js
[INFO] Exclude: **/src/ui/public/js/typeahead.jquery.min.js
[INFO] Exclude: **/dependency-reduced-pom.xml
[INFO] Exclude: **/docs/**
[INFO] Exclude: **/.git/**
[INFO] Exclude: **/derby.log
[INFO] Exclude: **/src/dev/**
[INFO] Exclude: **/src/codegen/config.fmpp
[INFO] Exclude: **/src/codegen/data/Parser.tdd
[INFO] Exclude: **/src/test/resources/FixedAvroSerializer.config
[INFO] 27 resources included (use -debug for more details)
Warning:  org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
Compiler warnings:
  WARNING:  'org.apache.xerces.jaxp.SAXParserImpl: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.'
Warning:  org.apache.xerces.parsers.SAXParser: Feature 'http://javax.xml.XMLConstants/feature/secure-processing' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
[INFO] Rat check: Summary of files. Unapproved: 0 unknown: 0 generated: 0 approved: 27 licence.
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building storm-metrics 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-antrun-plugin:1.6:run (prepare) @ storm-metrics ---
[WARNING] Parameter tasks is deprecated, use target instead
[INFO] Executing tasks

main:
     [echo] Downloading sigar native binaries...
      [get] Destination already exists (skipping): /home/travis/.m2/repository/org/fusesource/sigar/1.6.4/hyperic-sigar-1.6.4.zip
    [unzip] Expanding: /home/travis/.m2/repository/org/fusesource/sigar/1.6.4/hyperic-sigar-1.6.4.zip into /home/travis/build/apache/storm/external/storm-metrics/target/classes/resources
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ storm-metrics ---
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ storm-metrics ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/storm-metrics/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ storm-metrics ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ storm-metrics ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/storm-metrics/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ storm-metrics ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-surefire-plugin:2.18.1:test (default-test) @ storm-metrics ---
[INFO] 
[INFO] --- apache-rat-plugin:0.11:check (default) @ storm-metrics ---
[INFO] 51 implicit excludes (use -debug for more details).
[INFO] Exclude: **/target/**
[INFO] Exclude: **/*.iml
[INFO] Exclude: **/.idea/**
[INFO] Exclude: **/metastore_db/**
[INFO] Exclude: **/build/**
[INFO] Exclude: **/logs/**
[INFO] Exclude: **/CHANGELOG.md
[INFO] Exclude: **/README.md
[INFO] Exclude: **/README.markdown
[INFO] Exclude: **/DEVELOPER.md
[INFO] Exclude: **/BYLAWS.md
[INFO] Exclude: **/STORM-UI-REST-API.md
[INFO] Exclude: SECURITY.md
[INFO] Exclude: VERSION
[INFO] Exclude: TODO
[INFO] Exclude: **/src/py/**
[INFO] Exclude: **/src/ui/public/js/jquery.dataTables.1.10.4.min.js
[INFO] Exclude: **/src/ui/public/css/jquery.dataTables.1.10.4.min.css
[INFO] Exclude: **/src/ui/public/images/*
[INFO] Exclude: **/src/ui/public/js/bootstrap-3.3.1.min.js
[INFO] Exclude: **/src/ui/public/css/bootstrap-3.3.1.min.css
[INFO] Exclude: **/src/ui/public/js/dataTables.bootstrap.min.js
[INFO] Exclude: **/src/ui/public/css/dataTables.bootstrap.css
[INFO] Exclude: **/src/ui/public/js/jsonFormatter.min.js
[INFO] Exclude: **/src/ui/public/css/jsonFormatter.min.css
[INFO] Exclude: **/src/ui/public/js/jquery-1.11.1.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.cookies.2.2.0.min.js
[INFO] Exclude: **/src/ui/public/js/moment.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.blockUI.min.js
[INFO] Exclude: **/src/ui/public/js/url.min.js
[INFO] Exclude: **/src/ui/public/js/arbor.js
[INFO] Exclude: **/src/ui/public/js/arbor-graphics.js
[INFO] Exclude: **/src/ui/public/js/arbor-tween.js
[INFO] Exclude: **/src/ui/public/js/jquery.mustache.js
[INFO] Exclude: **/src/ui/public/js/typeahead.jquery.min.js
[INFO] Exclude: **/dependency-reduced-pom.xml
[INFO] Exclude: **/docs/**
[INFO] Exclude: **/.git/**
[INFO] Exclude: **/derby.log
[INFO] Exclude: **/src/dev/**
[INFO] Exclude: **/src/codegen/config.fmpp
[INFO] Exclude: **/src/codegen/data/Parser.tdd
[INFO] Exclude: **/src/test/resources/FixedAvroSerializer.config
[INFO] 3 resources included (use -debug for more details)
Warning:  org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
Compiler warnings:
  WARNING:  'org.apache.xerces.jaxp.SAXParserImpl: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.'
Warning:  org.apache.xerces.parsers.SAXParser: Feature 'http://javax.xml.XMLConstants/feature/secure-processing' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
[INFO] Rat check: Summary of files. Unapproved: 0 unknown: 0 generated: 0 approved: 3 licence.
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building storm-cassandra 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ storm-cassandra ---
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ storm-cassandra ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/storm-cassandra/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ storm-cassandra ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ storm-cassandra ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 1 resource
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ storm-cassandra ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:2.18.1:test (default-test) @ storm-cassandra ---
[INFO] 
[INFO] --- apache-rat-plugin:0.11:check (default) @ storm-cassandra ---
[INFO] 51 implicit excludes (use -debug for more details).
[INFO] Exclude: **/target/**
[INFO] Exclude: **/*.iml
[INFO] Exclude: **/.idea/**
[INFO] Exclude: **/metastore_db/**
[INFO] Exclude: **/build/**
[INFO] Exclude: **/logs/**
[INFO] Exclude: **/CHANGELOG.md
[INFO] Exclude: **/README.md
[INFO] Exclude: **/README.markdown
[INFO] Exclude: **/DEVELOPER.md
[INFO] Exclude: **/BYLAWS.md
[INFO] Exclude: **/STORM-UI-REST-API.md
[INFO] Exclude: SECURITY.md
[INFO] Exclude: VERSION
[INFO] Exclude: TODO
[INFO] Exclude: **/src/py/**
[INFO] Exclude: **/src/ui/public/js/jquery.dataTables.1.10.4.min.js
[INFO] Exclude: **/src/ui/public/css/jquery.dataTables.1.10.4.min.css
[INFO] Exclude: **/src/ui/public/images/*
[INFO] Exclude: **/src/ui/public/js/bootstrap-3.3.1.min.js
[INFO] Exclude: **/src/ui/public/css/bootstrap-3.3.1.min.css
[INFO] Exclude: **/src/ui/public/js/dataTables.bootstrap.min.js
[INFO] Exclude: **/src/ui/public/css/dataTables.bootstrap.css
[INFO] Exclude: **/src/ui/public/js/jsonFormatter.min.js
[INFO] Exclude: **/src/ui/public/css/jsonFormatter.min.css
[INFO] Exclude: **/src/ui/public/js/jquery-1.11.1.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.cookies.2.2.0.min.js
[INFO] Exclude: **/src/ui/public/js/moment.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.blockUI.min.js
[INFO] Exclude: **/src/ui/public/js/url.min.js
[INFO] Exclude: **/src/ui/public/js/arbor.js
[INFO] Exclude: **/src/ui/public/js/arbor-graphics.js
[INFO] Exclude: **/src/ui/public/js/arbor-tween.js
[INFO] Exclude: **/src/ui/public/js/jquery.mustache.js
[INFO] Exclude: **/src/ui/public/js/typeahead.jquery.min.js
[INFO] Exclude: **/dependency-reduced-pom.xml
[INFO] Exclude: **/docs/**
[INFO] Exclude: **/.git/**
[INFO] Exclude: **/derby.log
[INFO] Exclude: **/src/dev/**
[INFO] Exclude: **/src/codegen/config.fmpp
[INFO] Exclude: **/src/codegen/data/Parser.tdd
[INFO] Exclude: **/src/test/resources/FixedAvroSerializer.config
[INFO] 50 resources included (use -debug for more details)
Warning:  org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
Compiler warnings:
  WARNING:  'org.apache.xerces.jaxp.SAXParserImpl: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.'
Warning:  org.apache.xerces.parsers.SAXParser: Feature 'http://javax.xml.XMLConstants/feature/secure-processing' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
[INFO] Rat check: Summary of files. Unapproved: 0 unknown: 0 generated: 0 approved: 50 licence.
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building storm-mqtt-parent 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ storm-mqtt-parent ---
[INFO] 
[INFO] --- apache-rat-plugin:0.11:check (default) @ storm-mqtt-parent ---
[INFO] 51 implicit excludes (use -debug for more details).
[INFO] Exclude: **/target/**
[INFO] Exclude: **/*.iml
[INFO] Exclude: **/.idea/**
[INFO] Exclude: **/metastore_db/**
[INFO] Exclude: **/build/**
[INFO] Exclude: **/logs/**
[INFO] Exclude: **/CHANGELOG.md
[INFO] Exclude: **/README.md
[INFO] Exclude: **/README.markdown
[INFO] Exclude: **/DEVELOPER.md
[INFO] Exclude: **/BYLAWS.md
[INFO] Exclude: **/STORM-UI-REST-API.md
[INFO] Exclude: SECURITY.md
[INFO] Exclude: VERSION
[INFO] Exclude: TODO
[INFO] Exclude: **/src/py/**
[INFO] Exclude: **/src/ui/public/js/jquery.dataTables.1.10.4.min.js
[INFO] Exclude: **/src/ui/public/css/jquery.dataTables.1.10.4.min.css
[INFO] Exclude: **/src/ui/public/images/*
[INFO] Exclude: **/src/ui/public/js/bootstrap-3.3.1.min.js
[INFO] Exclude: **/src/ui/public/css/bootstrap-3.3.1.min.css
[INFO] Exclude: **/src/ui/public/js/dataTables.bootstrap.min.js
[INFO] Exclude: **/src/ui/public/css/dataTables.bootstrap.css
[INFO] Exclude: **/src/ui/public/js/jsonFormatter.min.js
[INFO] Exclude: **/src/ui/public/css/jsonFormatter.min.css
[INFO] Exclude: **/src/ui/public/js/jquery-1.11.1.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.cookies.2.2.0.min.js
[INFO] Exclude: **/src/ui/public/js/moment.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.blockUI.min.js
[INFO] Exclude: **/src/ui/public/js/url.min.js
[INFO] Exclude: **/src/ui/public/js/arbor.js
[INFO] Exclude: **/src/ui/public/js/arbor-graphics.js
[INFO] Exclude: **/src/ui/public/js/arbor-tween.js
[INFO] Exclude: **/src/ui/public/js/jquery.mustache.js
[INFO] Exclude: **/src/ui/public/js/typeahead.jquery.min.js
[INFO] Exclude: **/dependency-reduced-pom.xml
[INFO] Exclude: **/docs/**
[INFO] Exclude: **/.git/**
[INFO] Exclude: **/derby.log
[INFO] Exclude: **/src/dev/**
[INFO] Exclude: **/src/codegen/config.fmpp
[INFO] Exclude: **/src/codegen/data/Parser.tdd
[INFO] Exclude: **/src/test/resources/FixedAvroSerializer.config
[INFO] 25 resources included (use -debug for more details)
Warning:  org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
Compiler warnings:
  WARNING:  'org.apache.xerces.jaxp.SAXParserImpl: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.'
Warning:  org.apache.xerces.parsers.SAXParser: Feature 'http://javax.xml.XMLConstants/feature/secure-processing' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
[INFO] Rat check: Summary of files. Unapproved: 0 unknown: 0 generated: 0 approved: 25 licence.
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building storm-mqtt 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ storm-mqtt ---
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ storm-mqtt ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/storm-mqtt/core/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ storm-mqtt ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ storm-mqtt ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/storm-mqtt/core/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ storm-mqtt ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:2.18.1:test (default-test) @ storm-mqtt ---
[WARNING] The parameter forkMode is deprecated since version 2.14. Use forkCount and reuseForks instead.
[INFO] Surefire report directory: /home/travis/build/apache/storm/external/storm-mqtt/core/target/surefire-reports
[INFO] parallel='none', perCoreThreadCount=true, threadCount=0, useUnlimitedThreads=false, threadCountSuites=0, threadCountClasses=0, threadCountMethods=0, parallelOptimized=true

-------------------------------------------------------
 T E S T S
-------------------------------------------------------

Results :

Tests run: 0, Failures: 0, Errors: 0, Skipped: 0

[INFO] 
[INFO] --- apache-rat-plugin:0.11:check (default) @ storm-mqtt ---
[INFO] 51 implicit excludes (use -debug for more details).
[INFO] Exclude: **/target/**
[INFO] Exclude: **/*.iml
[INFO] Exclude: **/.idea/**
[INFO] Exclude: **/metastore_db/**
[INFO] Exclude: **/build/**
[INFO] Exclude: **/logs/**
[INFO] Exclude: **/CHANGELOG.md
[INFO] Exclude: **/README.md
[INFO] Exclude: **/README.markdown
[INFO] Exclude: **/DEVELOPER.md
[INFO] Exclude: **/BYLAWS.md
[INFO] Exclude: **/STORM-UI-REST-API.md
[INFO] Exclude: SECURITY.md
[INFO] Exclude: VERSION
[INFO] Exclude: TODO
[INFO] Exclude: **/src/py/**
[INFO] Exclude: **/src/ui/public/js/jquery.dataTables.1.10.4.min.js
[INFO] Exclude: **/src/ui/public/css/jquery.dataTables.1.10.4.min.css
[INFO] Exclude: **/src/ui/public/images/*
[INFO] Exclude: **/src/ui/public/js/bootstrap-3.3.1.min.js
[INFO] Exclude: **/src/ui/public/css/bootstrap-3.3.1.min.css
[INFO] Exclude: **/src/ui/public/js/dataTables.bootstrap.min.js
[INFO] Exclude: **/src/ui/public/css/dataTables.bootstrap.css
[INFO] Exclude: **/src/ui/public/js/jsonFormatter.min.js
[INFO] Exclude: **/src/ui/public/css/jsonFormatter.min.css
[INFO] Exclude: **/src/ui/public/js/jquery-1.11.1.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.cookies.2.2.0.min.js
[INFO] Exclude: **/src/ui/public/js/moment.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.blockUI.min.js
[INFO] Exclude: **/src/ui/public/js/url.min.js
[INFO] Exclude: **/src/ui/public/js/arbor.js
[INFO] Exclude: **/src/ui/public/js/arbor-graphics.js
[INFO] Exclude: **/src/ui/public/js/arbor-tween.js
[INFO] Exclude: **/src/ui/public/js/jquery.mustache.js
[INFO] Exclude: **/src/ui/public/js/typeahead.jquery.min.js
[INFO] Exclude: **/dependency-reduced-pom.xml
[INFO] Exclude: **/docs/**
[INFO] Exclude: **/.git/**
[INFO] Exclude: **/derby.log
[INFO] Exclude: **/src/dev/**
[INFO] Exclude: **/src/codegen/config.fmpp
[INFO] Exclude: **/src/codegen/data/Parser.tdd
[INFO] Exclude: **/src/test/resources/FixedAvroSerializer.config
[INFO] 18 resources included (use -debug for more details)
Warning:  org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
Compiler warnings:
  WARNING:  'org.apache.xerces.jaxp.SAXParserImpl: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.'
Warning:  org.apache.xerces.parsers.SAXParser: Feature 'http://javax.xml.XMLConstants/feature/secure-processing' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
[INFO] Rat check: Summary of files. Unapproved: 0 unknown: 0 generated: 0 approved: 18 licence.
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Skipping storm-mqtt-examples
[INFO] This project has been banned from the build due to previous failures.
[INFO] ------------------------------------------------------------------------
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building storm-mongodb 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ storm-mongodb ---
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ storm-mongodb ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/storm-mongodb/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ storm-mongodb ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ storm-mongodb ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/storm-mongodb/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ storm-mongodb ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:2.18.1:test (default-test) @ storm-mongodb ---
[INFO] 
[INFO] --- apache-rat-plugin:0.11:check (default) @ storm-mongodb ---
[INFO] 51 implicit excludes (use -debug for more details).
[INFO] Exclude: **/target/**
[INFO] Exclude: **/*.iml
[INFO] Exclude: **/.idea/**
[INFO] Exclude: **/metastore_db/**
[INFO] Exclude: **/build/**
[INFO] Exclude: **/logs/**
[INFO] Exclude: **/CHANGELOG.md
[INFO] Exclude: **/README.md
[INFO] Exclude: **/README.markdown
[INFO] Exclude: **/DEVELOPER.md
[INFO] Exclude: **/BYLAWS.md
[INFO] Exclude: **/STORM-UI-REST-API.md
[INFO] Exclude: SECURITY.md
[INFO] Exclude: VERSION
[INFO] Exclude: TODO
[INFO] Exclude: **/src/py/**
[INFO] Exclude: **/src/ui/public/js/jquery.dataTables.1.10.4.min.js
[INFO] Exclude: **/src/ui/public/css/jquery.dataTables.1.10.4.min.css
[INFO] Exclude: **/src/ui/public/images/*
[INFO] Exclude: **/src/ui/public/js/bootstrap-3.3.1.min.js
[INFO] Exclude: **/src/ui/public/css/bootstrap-3.3.1.min.css
[INFO] Exclude: **/src/ui/public/js/dataTables.bootstrap.min.js
[INFO] Exclude: **/src/ui/public/css/dataTables.bootstrap.css
[INFO] Exclude: **/src/ui/public/js/jsonFormatter.min.js
[INFO] Exclude: **/src/ui/public/css/jsonFormatter.min.css
[INFO] Exclude: **/src/ui/public/js/jquery-1.11.1.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.cookies.2.2.0.min.js
[INFO] Exclude: **/src/ui/public/js/moment.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.blockUI.min.js
[INFO] Exclude: **/src/ui/public/js/url.min.js
[INFO] Exclude: **/src/ui/public/js/arbor.js
[INFO] Exclude: **/src/ui/public/js/arbor-graphics.js
[INFO] Exclude: **/src/ui/public/js/arbor-tween.js
[INFO] Exclude: **/src/ui/public/js/jquery.mustache.js
[INFO] Exclude: **/src/ui/public/js/typeahead.jquery.min.js
[INFO] Exclude: **/dependency-reduced-pom.xml
[INFO] Exclude: **/docs/**
[INFO] Exclude: **/.git/**
[INFO] Exclude: **/derby.log
[INFO] Exclude: **/src/dev/**
[INFO] Exclude: **/src/codegen/config.fmpp
[INFO] Exclude: **/src/codegen/data/Parser.tdd
[INFO] Exclude: **/src/test/resources/FixedAvroSerializer.config
[INFO] 18 resources included (use -debug for more details)
Warning:  org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
Compiler warnings:
  WARNING:  'org.apache.xerces.jaxp.SAXParserImpl: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.'
Warning:  org.apache.xerces.parsers.SAXParser: Feature 'http://javax.xml.XMLConstants/feature/secure-processing' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
[INFO] Rat check: Summary of files. Unapproved: 0 unknown: 0 generated: 0 approved: 18 licence.
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building storm-clojure 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ storm-clojure ---
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ storm-clojure ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/storm-clojure/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ storm-clojure ---
[INFO] No sources to compile
[INFO] 
[INFO] --- clojure-maven-plugin:1.7.1:compile (compile) @ storm-clojure ---
[INFO] 
[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ storm-clojure ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/storm-clojure/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ storm-clojure ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-surefire-plugin:2.18.1:test (default-test) @ storm-clojure ---
[INFO] 
[INFO] --- apache-rat-plugin:0.11:check (default) @ storm-clojure ---
[INFO] 51 implicit excludes (use -debug for more details).
[INFO] Exclude: **/target/**
[INFO] Exclude: **/*.iml
[INFO] Exclude: **/.idea/**
[INFO] Exclude: **/metastore_db/**
[INFO] Exclude: **/build/**
[INFO] Exclude: **/logs/**
[INFO] Exclude: **/CHANGELOG.md
[INFO] Exclude: **/README.md
[INFO] Exclude: **/README.markdown
[INFO] Exclude: **/DEVELOPER.md
[INFO] Exclude: **/BYLAWS.md
[INFO] Exclude: **/STORM-UI-REST-API.md
[INFO] Exclude: SECURITY.md
[INFO] Exclude: VERSION
[INFO] Exclude: TODO
[INFO] Exclude: **/src/py/**
[INFO] Exclude: **/src/ui/public/js/jquery.dataTables.1.10.4.min.js
[INFO] Exclude: **/src/ui/public/css/jquery.dataTables.1.10.4.min.css
[INFO] Exclude: **/src/ui/public/images/*
[INFO] Exclude: **/src/ui/public/js/bootstrap-3.3.1.min.js
[INFO] Exclude: **/src/ui/public/css/bootstrap-3.3.1.min.css
[INFO] Exclude: **/src/ui/public/js/dataTables.bootstrap.min.js
[INFO] Exclude: **/src/ui/public/css/dataTables.bootstrap.css
[INFO] Exclude: **/src/ui/public/js/jsonFormatter.min.js
[INFO] Exclude: **/src/ui/public/css/jsonFormatter.min.css
[INFO] Exclude: **/src/ui/public/js/jquery-1.11.1.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.cookies.2.2.0.min.js
[INFO] Exclude: **/src/ui/public/js/moment.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.blockUI.min.js
[INFO] Exclude: **/src/ui/public/js/url.min.js
[INFO] Exclude: **/src/ui/public/js/arbor.js
[INFO] Exclude: **/src/ui/public/js/arbor-graphics.js
[INFO] Exclude: **/src/ui/public/js/arbor-tween.js
[INFO] Exclude: **/src/ui/public/js/jquery.mustache.js
[INFO] Exclude: **/src/ui/public/js/typeahead.jquery.min.js
[INFO] Exclude: **/dependency-reduced-pom.xml
[INFO] Exclude: **/docs/**
[INFO] Exclude: **/.git/**
[INFO] Exclude: **/derby.log
[INFO] Exclude: **/src/dev/**
[INFO] Exclude: **/src/codegen/config.fmpp
[INFO] Exclude: **/src/codegen/data/Parser.tdd
[INFO] Exclude: **/src/test/resources/FixedAvroSerializer.config
[INFO] 4 resources included (use -debug for more details)
Warning:  org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
Compiler warnings:
  WARNING:  'org.apache.xerces.jaxp.SAXParserImpl: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.'
Warning:  org.apache.xerces.parsers.SAXParser: Feature 'http://javax.xml.XMLConstants/feature/secure-processing' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
[INFO] Rat check: Summary of files. Unapproved: 0 unknown: 0 generated: 0 approved: 4 licence.
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Skipping storm-starter
[INFO] This project has been banned from the build due to previous failures.
[INFO] ------------------------------------------------------------------------
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building storm-kafka-client 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ storm-kafka-client ---
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ storm-kafka-client ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/storm-kafka-client/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ storm-kafka-client ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ storm-kafka-client ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/storm-kafka-client/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ storm-kafka-client ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:2.18.1:test (default-test) @ storm-kafka-client ---
[INFO] 
[INFO] --- apache-rat-plugin:0.11:check (default) @ storm-kafka-client ---
[INFO] 51 implicit excludes (use -debug for more details).
[INFO] Exclude: **/target/**
[INFO] Exclude: **/*.iml
[INFO] Exclude: **/.idea/**
[INFO] Exclude: **/metastore_db/**
[INFO] Exclude: **/build/**
[INFO] Exclude: **/logs/**
[INFO] Exclude: **/CHANGELOG.md
[INFO] Exclude: **/README.md
[INFO] Exclude: **/README.markdown
[INFO] Exclude: **/DEVELOPER.md
[INFO] Exclude: **/BYLAWS.md
[INFO] Exclude: **/STORM-UI-REST-API.md
[INFO] Exclude: SECURITY.md
[INFO] Exclude: VERSION
[INFO] Exclude: TODO
[INFO] Exclude: **/src/py/**
[INFO] Exclude: **/src/ui/public/js/jquery.dataTables.1.10.4.min.js
[INFO] Exclude: **/src/ui/public/css/jquery.dataTables.1.10.4.min.css
[INFO] Exclude: **/src/ui/public/images/*
[INFO] Exclude: **/src/ui/public/js/bootstrap-3.3.1.min.js
[INFO] Exclude: **/src/ui/public/css/bootstrap-3.3.1.min.css
[INFO] Exclude: **/src/ui/public/js/dataTables.bootstrap.min.js
[INFO] Exclude: **/src/ui/public/css/dataTables.bootstrap.css
[INFO] Exclude: **/src/ui/public/js/jsonFormatter.min.js
[INFO] Exclude: **/src/ui/public/css/jsonFormatter.min.css
[INFO] Exclude: **/src/ui/public/js/jquery-1.11.1.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.cookies.2.2.0.min.js
[INFO] Exclude: **/src/ui/public/js/moment.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.blockUI.min.js
[INFO] Exclude: **/src/ui/public/js/url.min.js
[INFO] Exclude: **/src/ui/public/js/arbor.js
[INFO] Exclude: **/src/ui/public/js/arbor-graphics.js
[INFO] Exclude: **/src/ui/public/js/arbor-tween.js
[INFO] Exclude: **/src/ui/public/js/jquery.mustache.js
[INFO] Exclude: **/src/ui/public/js/typeahead.jquery.min.js
[INFO] Exclude: **/dependency-reduced-pom.xml
[INFO] Exclude: **/docs/**
[INFO] Exclude: **/.git/**
[INFO] Exclude: **/derby.log
[INFO] Exclude: **/src/dev/**
[INFO] Exclude: **/src/codegen/config.fmpp
[INFO] Exclude: **/src/codegen/data/Parser.tdd
[INFO] Exclude: **/src/test/resources/FixedAvroSerializer.config
[INFO] 14 resources included (use -debug for more details)
Warning:  org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
Compiler warnings:
  WARNING:  'org.apache.xerces.jaxp.SAXParserImpl: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.'
Warning:  org.apache.xerces.parsers.SAXParser: Feature 'http://javax.xml.XMLConstants/feature/secure-processing' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
[INFO] Rat check: Summary of files. Unapproved: 0 unknown: 0 generated: 0 approved: 14 licence.
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building storm-opentsdb 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ storm-opentsdb ---
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ storm-opentsdb ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/storm-opentsdb/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ storm-opentsdb ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ storm-opentsdb ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/storm-opentsdb/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ storm-opentsdb ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:2.18.1:test (default-test) @ storm-opentsdb ---
[INFO] 
[INFO] --- apache-rat-plugin:0.11:check (default) @ storm-opentsdb ---
[INFO] 51 implicit excludes (use -debug for more details).
[INFO] Exclude: **/target/**
[INFO] Exclude: **/*.iml
[INFO] Exclude: **/.idea/**
[INFO] Exclude: **/metastore_db/**
[INFO] Exclude: **/build/**
[INFO] Exclude: **/logs/**
[INFO] Exclude: **/CHANGELOG.md
[INFO] Exclude: **/README.md
[INFO] Exclude: **/README.markdown
[INFO] Exclude: **/DEVELOPER.md
[INFO] Exclude: **/BYLAWS.md
[INFO] Exclude: **/STORM-UI-REST-API.md
[INFO] Exclude: SECURITY.md
[INFO] Exclude: VERSION
[INFO] Exclude: TODO
[INFO] Exclude: **/src/py/**
[INFO] Exclude: **/src/ui/public/js/jquery.dataTables.1.10.4.min.js
[INFO] Exclude: **/src/ui/public/css/jquery.dataTables.1.10.4.min.css
[INFO] Exclude: **/src/ui/public/images/*
[INFO] Exclude: **/src/ui/public/js/bootstrap-3.3.1.min.js
[INFO] Exclude: **/src/ui/public/css/bootstrap-3.3.1.min.css
[INFO] Exclude: **/src/ui/public/js/dataTables.bootstrap.min.js
[INFO] Exclude: **/src/ui/public/css/dataTables.bootstrap.css
[INFO] Exclude: **/src/ui/public/js/jsonFormatter.min.js
[INFO] Exclude: **/src/ui/public/css/jsonFormatter.min.css
[INFO] Exclude: **/src/ui/public/js/jquery-1.11.1.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.cookies.2.2.0.min.js
[INFO] Exclude: **/src/ui/public/js/moment.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.blockUI.min.js
[INFO] Exclude: **/src/ui/public/js/url.min.js
[INFO] Exclude: **/src/ui/public/js/arbor.js
[INFO] Exclude: **/src/ui/public/js/arbor-graphics.js
[INFO] Exclude: **/src/ui/public/js/arbor-tween.js
[INFO] Exclude: **/src/ui/public/js/jquery.mustache.js
[INFO] Exclude: **/src/ui/public/js/typeahead.jquery.min.js
[INFO] Exclude: **/dependency-reduced-pom.xml
[INFO] Exclude: **/docs/**
[INFO] Exclude: **/.git/**
[INFO] Exclude: **/derby.log
[INFO] Exclude: **/src/dev/**
[INFO] Exclude: **/src/codegen/config.fmpp
[INFO] Exclude: **/src/codegen/data/Parser.tdd
[INFO] Exclude: **/src/test/resources/FixedAvroSerializer.config
[INFO] 14 resources included (use -debug for more details)
Warning:  org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
Compiler warnings:
  WARNING:  'org.apache.xerces.jaxp.SAXParserImpl: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.'
Warning:  org.apache.xerces.parsers.SAXParser: Feature 'http://javax.xml.XMLConstants/feature/secure-processing' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
[INFO] Rat check: Summary of files. Unapproved: 0 unknown: 0 generated: 0 approved: 14 licence.
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building storm-kafka-monitor 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ storm-kafka-monitor ---
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ storm-kafka-monitor ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/storm-kafka-monitor/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ storm-kafka-monitor ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ storm-kafka-monitor ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/storm-kafka-monitor/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ storm-kafka-monitor ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-surefire-plugin:2.18.1:test (default-test) @ storm-kafka-monitor ---
[INFO] 
[INFO] --- apache-rat-plugin:0.11:check (default) @ storm-kafka-monitor ---
[INFO] 51 implicit excludes (use -debug for more details).
[INFO] Exclude: **/target/**
[INFO] Exclude: **/*.iml
[INFO] Exclude: **/.idea/**
[INFO] Exclude: **/metastore_db/**
[INFO] Exclude: **/build/**
[INFO] Exclude: **/logs/**
[INFO] Exclude: **/CHANGELOG.md
[INFO] Exclude: **/README.md
[INFO] Exclude: **/README.markdown
[INFO] Exclude: **/DEVELOPER.md
[INFO] Exclude: **/BYLAWS.md
[INFO] Exclude: **/STORM-UI-REST-API.md
[INFO] Exclude: SECURITY.md
[INFO] Exclude: VERSION
[INFO] Exclude: TODO
[INFO] Exclude: **/src/py/**
[INFO] Exclude: **/src/ui/public/js/jquery.dataTables.1.10.4.min.js
[INFO] Exclude: **/src/ui/public/css/jquery.dataTables.1.10.4.min.css
[INFO] Exclude: **/src/ui/public/images/*
[INFO] Exclude: **/src/ui/public/js/bootstrap-3.3.1.min.js
[INFO] Exclude: **/src/ui/public/css/bootstrap-3.3.1.min.css
[INFO] Exclude: **/src/ui/public/js/dataTables.bootstrap.min.js
[INFO] Exclude: **/src/ui/public/css/dataTables.bootstrap.css
[INFO] Exclude: **/src/ui/public/js/jsonFormatter.min.js
[INFO] Exclude: **/src/ui/public/css/jsonFormatter.min.css
[INFO] Exclude: **/src/ui/public/js/jquery-1.11.1.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.cookies.2.2.0.min.js
[INFO] Exclude: **/src/ui/public/js/moment.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.blockUI.min.js
[INFO] Exclude: **/src/ui/public/js/url.min.js
[INFO] Exclude: **/src/ui/public/js/arbor.js
[INFO] Exclude: **/src/ui/public/js/arbor-graphics.js
[INFO] Exclude: **/src/ui/public/js/arbor-tween.js
[INFO] Exclude: **/src/ui/public/js/jquery.mustache.js
[INFO] Exclude: **/src/ui/public/js/typeahead.jquery.min.js
[INFO] Exclude: **/dependency-reduced-pom.xml
[INFO] Exclude: **/docs/**
[INFO] Exclude: **/.git/**
[INFO] Exclude: **/derby.log
[INFO] Exclude: **/src/dev/**
[INFO] Exclude: **/src/codegen/config.fmpp
[INFO] Exclude: **/src/codegen/data/Parser.tdd
[INFO] Exclude: **/src/test/resources/FixedAvroSerializer.config
[INFO] 5 resources included (use -debug for more details)
Warning:  org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
Compiler warnings:
  WARNING:  'org.apache.xerces.jaxp.SAXParserImpl: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.'
Warning:  org.apache.xerces.parsers.SAXParser: Feature 'http://javax.xml.XMLConstants/feature/secure-processing' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
[INFO] Rat check: Summary of files. Unapproved: 0 unknown: 0 generated: 0 approved: 5 licence.
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO] 
[INFO] Storm .............................................. SUCCESS [  6.495 s]
[INFO] multilang-javascript ............................... SUCCESS [  1.248 s]
[INFO] multilang-python ................................... SUCCESS [  0.176 s]
[INFO] multilang-ruby ..................................... SUCCESS [  0.183 s]
[INFO] maven-shade-clojure-transformer .................... SUCCESS [  2.157 s]
[INFO] storm-maven-plugins ................................ SUCCESS [  5.403 s]
[INFO] storm-rename-hack .................................. SUCCESS [  4.537 s]
[INFO] storm-kafka ........................................ FAILURE [01:23 min]
[INFO] storm-hdfs ......................................... FAILURE [01:29 min]
[INFO] storm-hbase ........................................ SUCCESS [  8.443 s]
[INFO] storm-hive ......................................... FAILURE [ 43.999 s]
[INFO] storm-jdbc ......................................... SUCCESS [  2.373 s]
[INFO] storm-redis ........................................ SUCCESS [  2.344 s]
[INFO] storm-eventhubs .................................... SUCCESS [  4.012 s]
[INFO] flux ............................................... SUCCESS [  0.156 s]
[INFO] flux-wrappers ...................................... SUCCESS [  0.349 s]
[INFO] flux-core .......................................... SKIPPED
[INFO] flux-examples ...................................... SKIPPED
[INFO] storm-sql-runtime .................................. SUCCESS [  0.480 s]
[INFO] storm-sql-core ..................................... SUCCESS [01:04 min]
[INFO] storm-sql-kafka .................................... SKIPPED
[INFO] sql ................................................ SUCCESS [  0.139 s]
[INFO] storm-elasticsearch ................................ SUCCESS [  6.513 s]
[INFO] storm-solr ......................................... SUCCESS [  3.710 s]
[INFO] storm-metrics ...................................... SUCCESS [  1.233 s]
[INFO] storm-cassandra .................................... SUCCESS [  0.661 s]
[INFO] storm-mqtt-parent .................................. SUCCESS [  0.151 s]
[INFO] storm-mqtt ......................................... SUCCESS [  2.365 s]
[INFO] storm-mqtt-examples ................................ SKIPPED
[INFO] storm-mongodb ...................................... SUCCESS [  0.172 s]
[INFO] storm-clojure ...................................... SUCCESS [  1.603 s]
[INFO] storm-starter ...................................... SKIPPED
[INFO] storm-kafka-client ................................. SUCCESS [  0.087 s]
[INFO] storm-opentsdb ..................................... SUCCESS [  0.929 s]
[INFO] storm-kafka-monitor ................................ SUCCESS [  0.257 s]
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 05:40 min
[INFO] Finished at: 2016-06-23T16:38:53+00:00
[INFO] Final Memory: 83M/483M
[INFO] ------------------------------------------------------------------------
[WARNING] The requested profile "native" could not be activated because it does not exist.
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:2.18.1:test (default-test) on project storm-kafka: There are test failures.
[ERROR] 
[ERROR] Please refer to /home/travis/build/apache/storm/external/storm-kafka/target/surefire-reports for the individual test results.
[ERROR] -> [Help 1]
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:2.18.1:test (default-test) on project storm-hdfs: There are test failures.
[ERROR] 
[ERROR] Please refer to /home/travis/build/apache/storm/external/storm-hdfs/target/surefire-reports for the individual test results.
[ERROR] -> [Help 1]
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:2.18.1:test (default-test) on project storm-hive: There are test failures.
[ERROR] 
[ERROR] Please refer to /home/travis/build/apache/storm/external/storm-hive/target/surefire-reports for the individual test results.
[ERROR] -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException
[ERROR] 
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn <goals> -rf :storm-kafka
Looking for errors in ./external/sql/storm-sql-core/target/surefire-reports
Checking ./external/sql/storm-sql-core/target/surefire-reports/TEST-org.apache.storm.sql.parser.TestSqlParser.xml
Checking ./external/sql/storm-sql-core/target/surefire-reports/TEST-org.apache.storm.sql.compiler.backends.standalone.TestRelNodeCompiler.xml
Checking ./external/sql/storm-sql-core/target/surefire-reports/TEST-org.apache.storm.sql.compiler.backends.standalone.TestPlanCompiler.xml
Checking ./external/sql/storm-sql-core/target/surefire-reports/TEST-org.apache.storm.sql.compiler.backends.trident.TestPlanCompiler.xml
Checking ./external/sql/storm-sql-core/target/surefire-reports/TEST-org.apache.storm.sql.compiler.TestExprCompiler.xml
Checking ./external/sql/storm-sql-core/target/surefire-reports/TEST-org.apache.storm.sql.compiler.TestExprSemantic.xml
Checking ./external/sql/storm-sql-core/target/surefire-reports/TEST-org.apache.storm.sql.TestStormSql.xml
Looking for errors in ./external/storm-elasticsearch/target/surefire-reports
Checking ./external/storm-elasticsearch/target/surefire-reports/TEST-org.apache.storm.elasticsearch.bolt.EsLookupBoltTest.xml
Checking ./external/storm-elasticsearch/target/surefire-reports/TEST-org.apache.storm.elasticsearch.trident.EsStateFactoryTest.xml
Checking ./external/storm-elasticsearch/target/surefire-reports/TEST-org.apache.storm.elasticsearch.common.EsConfigTest.xml
Checking ./external/storm-elasticsearch/target/surefire-reports/TEST-org.apache.storm.elasticsearch.common.TransportAddressesTest.xml
Looking for errors in ./external/storm-eventhubs/target/surefire-reports
Checking ./external/storm-eventhubs/target/surefire-reports/TEST-org.apache.storm.eventhubs.trident.TestTransactionalTridentEmitter.xml
Checking ./external/storm-eventhubs/target/surefire-reports/TEST-org.apache.storm.eventhubs.spout.TestPartitionManager.xml
Checking ./external/storm-eventhubs/target/surefire-reports/TEST-org.apache.storm.eventhubs.spout.TestEventData.xml
Checking ./external/storm-eventhubs/target/surefire-reports/TEST-org.apache.storm.eventhubs.spout.TestEventHubSpout.xml
Looking for errors in ./external/storm-hdfs/target/surefire-reports
Checking ./external/storm-hdfs/target/surefire-reports/TEST-org.apache.storm.hdfs.trident.HdfsStateTest.xml
Checking ./external/storm-hdfs/target/surefire-reports/TEST-org.apache.storm.hdfs.bolt.TestWritersMap.xml
Checking ./external/storm-hdfs/target/surefire-reports/TEST-org.apache.storm.hdfs.bolt.TestHdfsBolt.xml
--------------------------------------------------
classname: org.apache.storm.hdfs.bolt.TestHdfsBolt / testname: testTwoTuplesTwoFiles
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hdfs.bolt.TestHdfsBolt.generateTestTuple(TestHdfsBolt.java:243)
	at org.apache.storm.hdfs.bolt.TestHdfsBolt.<init>(TestHdfsBolt.java:69)

--------------------------------------------------
--------------------------------------------------
classname: org.apache.storm.hdfs.bolt.TestHdfsBolt / testname: testPartitionedOutput
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hdfs.bolt.TestHdfsBolt.generateTestTuple(TestHdfsBolt.java:243)
	at org.apache.storm.hdfs.bolt.TestHdfsBolt.<init>(TestHdfsBolt.java:69)

--------------------------------------------------
--------------------------------------------------
classname: org.apache.storm.hdfs.bolt.TestHdfsBolt / testname: testTwoTuplesOneFile
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hdfs.bolt.TestHdfsBolt.generateTestTuple(TestHdfsBolt.java:243)
	at org.apache.storm.hdfs.bolt.TestHdfsBolt.<init>(TestHdfsBolt.java:69)

--------------------------------------------------
--------------------------------------------------
classname: org.apache.storm.hdfs.bolt.TestHdfsBolt / testname: testFailedSync
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hdfs.bolt.TestHdfsBolt.generateTestTuple(TestHdfsBolt.java:243)
	at org.apache.storm.hdfs.bolt.TestHdfsBolt.<init>(TestHdfsBolt.java:69)

--------------------------------------------------
--------------------------------------------------
classname: org.apache.storm.hdfs.bolt.TestHdfsBolt / testname: testTickTuples
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hdfs.bolt.TestHdfsBolt.generateTestTuple(TestHdfsBolt.java:243)
	at org.apache.storm.hdfs.bolt.TestHdfsBolt.<init>(TestHdfsBolt.java:69)

--------------------------------------------------
--------------------------------------------------
classname: org.apache.storm.hdfs.bolt.TestHdfsBolt / testname: testFailureFilecount
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hdfs.bolt.TestHdfsBolt.generateTestTuple(TestHdfsBolt.java:243)
	at org.apache.storm.hdfs.bolt.TestHdfsBolt.<init>(TestHdfsBolt.java:69)

--------------------------------------------------
Checking ./external/storm-hdfs/target/surefire-reports/TEST-org.apache.storm.hdfs.bolt.AvroGenericRecordBoltTest.xml
--------------------------------------------------
classname: org.apache.storm.hdfs.bolt.AvroGenericRecordBoltTest / testname: schemaThrashing
java.lang.ExceptionInInitializerError: null
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hdfs.bolt.AvroGenericRecordBoltTest.generateTestTuple(AvroGenericRecordBoltTest.java:220)
	at org.apache.storm.hdfs.bolt.AvroGenericRecordBoltTest.<clinit>(AvroGenericRecordBoltTest.java:95)

--------------------------------------------------
--------------------------------------------------
classname: org.apache.storm.hdfs.bolt.AvroGenericRecordBoltTest / testname: forwardSchemaChangeWorks
java.lang.NoClassDefFoundError: Could not initialize class org.apache.storm.hdfs.bolt.AvroGenericRecordBoltTest
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:408)
	at org.junit.runners.BlockJUnit4ClassRunner.createTest(BlockJUnit4ClassRunner.java:195)
	at org.junit.runners.BlockJUnit4ClassRunner$1.runReflectiveCall(BlockJUnit4ClassRunner.java:244)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.BlockJUnit4ClassRunner.methodBlock(BlockJUnit4ClassRunner.java:241)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:108)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:78)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:54)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:144)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:203)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:155)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:103)

--------------------------------------------------
--------------------------------------------------
classname: org.apache.storm.hdfs.bolt.AvroGenericRecordBoltTest / testname: multipleTuplesOneFile
java.lang.NoClassDefFoundError: Could not initialize class org.apache.storm.hdfs.bolt.AvroGenericRecordBoltTest
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:408)
	at org.junit.runners.BlockJUnit4ClassRunner.createTest(BlockJUnit4ClassRunner.java:195)
	at org.junit.runners.BlockJUnit4ClassRunner$1.runReflectiveCall(BlockJUnit4ClassRunner.java:244)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.BlockJUnit4ClassRunner.methodBlock(BlockJUnit4ClassRunner.java:241)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:108)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:78)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:54)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:144)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:203)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:155)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:103)

--------------------------------------------------
--------------------------------------------------
classname: org.apache.storm.hdfs.bolt.AvroGenericRecordBoltTest / testname: multipleTuplesMutliplesFiles
java.lang.NoClassDefFoundError: Could not initialize class org.apache.storm.hdfs.bolt.AvroGenericRecordBoltTest
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:408)
	at org.junit.runners.BlockJUnit4ClassRunner.createTest(BlockJUnit4ClassRunner.java:195)
	at org.junit.runners.BlockJUnit4ClassRunner$1.runReflectiveCall(BlockJUnit4ClassRunner.java:244)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.BlockJUnit4ClassRunner.methodBlock(BlockJUnit4ClassRunner.java:241)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:108)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:78)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:54)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:144)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:203)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:155)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:103)

--------------------------------------------------
--------------------------------------------------
classname: org.apache.storm.hdfs.bolt.AvroGenericRecordBoltTest / testname: backwardSchemaChangeWorks
java.lang.NoClassDefFoundError: Could not initialize class org.apache.storm.hdfs.bolt.AvroGenericRecordBoltTest
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:408)
	at org.junit.runners.BlockJUnit4ClassRunner.createTest(BlockJUnit4ClassRunner.java:195)
	at org.junit.runners.BlockJUnit4ClassRunner$1.runReflectiveCall(BlockJUnit4ClassRunner.java:244)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.BlockJUnit4ClassRunner.methodBlock(BlockJUnit4ClassRunner.java:241)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:108)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:78)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:54)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:144)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:203)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:155)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:103)

--------------------------------------------------
Checking ./external/storm-hdfs/target/surefire-reports/TEST-org.apache.storm.hdfs.bolt.TestSequenceFileBolt.xml
--------------------------------------------------
classname: org.apache.storm.hdfs.bolt.TestSequenceFileBolt / testname: testTwoTuplesTwoFiles
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hdfs.bolt.TestSequenceFileBolt.generateTestTuple(TestSequenceFileBolt.java:161)
	at org.apache.storm.hdfs.bolt.TestSequenceFileBolt.<init>(TestSequenceFileBolt.java:68)

--------------------------------------------------
--------------------------------------------------
classname: org.apache.storm.hdfs.bolt.TestSequenceFileBolt / testname: testTwoTuplesOneFile
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hdfs.bolt.TestSequenceFileBolt.generateTestTuple(TestSequenceFileBolt.java:161)
	at org.apache.storm.hdfs.bolt.TestSequenceFileBolt.<init>(TestSequenceFileBolt.java:68)

--------------------------------------------------
--------------------------------------------------
classname: org.apache.storm.hdfs.bolt.TestSequenceFileBolt / testname: testFailedSync
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hdfs.bolt.TestSequenceFileBolt.generateTestTuple(TestSequenceFileBolt.java:161)
	at org.apache.storm.hdfs.bolt.TestSequenceFileBolt.<init>(TestSequenceFileBolt.java:68)

--------------------------------------------------
Checking ./external/storm-hdfs/target/surefire-reports/TEST-org.apache.storm.hdfs.spout.TestDirLock.xml
Checking ./external/storm-hdfs/target/surefire-reports/TEST-org.apache.storm.hdfs.spout.TestHdfsSemantics.xml
Checking ./external/storm-hdfs/target/surefire-reports/TEST-org.apache.storm.hdfs.spout.TestProgressTracker.xml
Checking ./external/storm-hdfs/target/surefire-reports/TEST-org.apache.storm.hdfs.spout.TestFileLock.xml
Checking ./external/storm-hdfs/target/surefire-reports/TEST-org.apache.storm.hdfs.spout.TestHdfsSpout.xml
Checking ./external/storm-hdfs/target/surefire-reports/TEST-org.apache.storm.hdfs.blobstore.HdfsBlobStoreImplTest.xml
Checking ./external/storm-hdfs/target/surefire-reports/TEST-org.apache.storm.hdfs.blobstore.BlobStoreTest.xml
Checking ./external/storm-hdfs/target/surefire-reports/TEST-org.apache.storm.hdfs.avro.TestFixedAvroSerializer.xml
Checking ./external/storm-hdfs/target/surefire-reports/TEST-org.apache.storm.hdfs.avro.TestGenericAvroSerializer.xml
Looking for errors in ./external/storm-hive/target/surefire-reports
Checking ./external/storm-hive/target/surefire-reports/TEST-org.apache.storm.hive.bolt.TestHiveBolt.xml
--------------------------------------------------
classname: org.apache.storm.hive.bolt.TestHiveBolt / testname: testMultiPartitionTuples
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hive.bolt.TestHiveBolt.generateTestTuple(TestHiveBolt.java:447)
	at org.apache.storm.hive.bolt.TestHiveBolt.testMultiPartitionTuples(TestHiveBolt.java:411)

-------------------- system-out --------------------
6261 [main] WARN  o.a.h.u.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
6369 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
6408 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
6658 [main] INFO  D.Persistence - Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
6658 [main] INFO  D.Persistence - Property datanucleus.cache.level2 unknown - will be ignored
8214 [main] INFO  o.a.h.h.m.ObjectStore - Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
8349 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
9442 [main] INFO  D.Datastore - The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
9443 [main] INFO  D.Datastore - The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
11807 [main] INFO  D.Datastore - The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
11807 [main] INFO  D.Datastore - The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
12175 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
12338 [main] WARN  o.a.h.h.m.ObjectStore - Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 0.14.0
12550 [main] WARN  o.a.h.h.m.ObjectStore - Failed to get database default, returning NoSuchObjectException
12917 [main] INFO  o.a.h.h.m.HiveMetaStore - Added admin role in metastore
12921 [main] INFO  o.a.h.h.m.HiveMetaStore - Added public role in metastore
13072 [main] INFO  o.a.h.h.m.HiveMetaStore - No user is added in admin role, since config is empty
13321 [main] INFO  o.a.h.h.q.s.SessionState - Created HDFS directory: /tmp/hive/travis
13323 [main] INFO  o.a.h.h.q.s.SessionState - Created local directory: /tmp/travis
13327 [main] INFO  o.a.h.h.q.s.SessionState - Created local directory: /tmp/08b7e818-74c2-4bbe-8f1e-d5d9426e8875_resources
13331 [main] INFO  o.a.h.h.q.s.SessionState - Created HDFS directory: /tmp/hive/travis/08b7e818-74c2-4bbe-8f1e-d5d9426e8875
13334 [main] INFO  o.a.h.h.q.s.SessionState - Created local directory: /tmp/travis/08b7e818-74c2-4bbe-8f1e-d5d9426e8875
13337 [main] INFO  o.a.h.h.q.s.SessionState - Created HDFS directory: /tmp/hive/travis/08b7e818-74c2-4bbe-8f1e-d5d9426e8875/_tmp_space.db
13337 [main] INFO  o.a.h.h.q.s.SessionState - No Tez session required at this point. hive.execution.engine=mr.
13460 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
13461 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
13467 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
13467 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
13468 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_table_names_by_filter: db = testdb, filter = 
13468 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table_names_by_filter: db = testdb, filter = 	
13486 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_database: testdb
13486 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_database: testdb	
13487 [main] WARN  o.a.h.h.m.ObjectStore - Failed to get database testdb, returning NoSuchObjectException
13489 [main] ERROR o.a.h.h.m.RetryingHMSHandler - NoSuchObjectException(message:testdb)
	at org.apache.hadoop.hive.metastore.ObjectStore.getDatabase(ObjectStore.java:539)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:483)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:98)
	at com.sun.proxy.$Proxy24.getDatabase(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_database_core(HiveMetaStore.java:914)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_database(HiveMetaStore.java:888)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:483)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:102)
	at com.sun.proxy.$Proxy26.get_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getDatabase(HiveMetaStoreClient.java:1043)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:657)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:644)
	at org.apache.storm.hive.bolt.HiveSetupUtil.dropDB(HiveSetupUtil.java:166)
	at org.apache.storm.hive.bolt.TestHiveBolt.setup(TestHiveBolt.java:119)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:483)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:108)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:78)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:54)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:144)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:203)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:155)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:103)

13489 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Shutting down the object store...
13490 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=Shutting down the object store...	
13490 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Metastore shutdown complete.
13490 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=Metastore shutdown complete.	
13491 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: create_database: Database(name:testdb, description:null, locationUri:raw:///tmp/junit5355118619987390763/testdb.db, parameters:null)
13491 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=create_database: Database(name:testdb, description:null, locationUri:raw:///tmp/junit5355118619987390763/testdb.db, parameters:null)	
13491 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
13492 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
13493 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
13495 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
13495 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
13497 [main] WARN  o.a.h.h.m.ObjectStore - Failed to get database testdb, returning NoSuchObjectException
13512 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: create_table: Table(tableName:test_table, dbName:testdb, owner:null, createTime:0, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:), FieldSchema(name:msg, type:string, comment:)], location:raw:///tmp/junit5355118619987390763/testdb.db/test_table, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:1, serdeInfo:SerDeInfo(name:test_table, serializationLib:org.apache.hadoop.hive.ql.io.orc.OrcSerde, parameters:{serialization.format=1}), bucketCols:[], sortCols:null, parameters:null), partitionKeys:[FieldSchema(name:city, type:string, comment:), FieldSchema(name:state, type:string, comment:)], parameters:{}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)
13512 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=create_table: Table(tableName:test_table, dbName:testdb, owner:null, createTime:0, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:), FieldSchema(name:msg, type:string, comment:)], location:raw:///tmp/junit5355118619987390763/testdb.db/test_table, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:1, serdeInfo:SerDeInfo(name:test_table, serializationLib:org.apache.hadoop.hive.ql.io.orc.OrcSerde, parameters:{serialization.format=1}), bucketCols:[], sortCols:null, parameters:null), partitionKeys:[FieldSchema(name:city, type:string, comment:), FieldSchema(name:state, type:string, comment:)], parameters:{}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
13531 [main] WARN  o.a.h.h.m.HiveMetaStore - Location: raw:///tmp/junit5355118619987390763/testdb.db/test_table specified for non-external table:test_table
13535 [main] INFO  o.a.h.h.c.FileUtils - Creating directory if it doesn't exist: raw:/tmp/junit5355118619987390763/testdb.db/test_table
13807 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: add_partition : db=testdb tbl=test_table
13809 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=add_partition : db=testdb tbl=test_table	
14021 [main] INFO  o.a.h.h.c.FileUtils - Creating directory if it doesn't exist: raw:/tmp/junit5355118619987390763/testdb.db/test_table/city=sunnyvale/state=ca
14102 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Shutting down the object store...
14102 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=Shutting down the object store...	
14103 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Metastore shutdown complete.
14103 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=Metastore shutdown complete.	
done
14109 [main] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=compile from=org.apache.hadoop.hive.ql.Driver>
14132 [main] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=parse from=org.apache.hadoop.hive.ql.Driver>
14135 [main] INFO  h.q.p.ParseDriver - Parsing command: select * from testdb.test_table
14444 [main] INFO  h.q.p.ParseDriver - Parse Completed
14445 [main] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=parse start=1466699818306 end=1466699818619 duration=313 from=org.apache.hadoop.hive.ql.Driver>
14482 [main] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=semanticAnalyze from=org.apache.hadoop.hive.ql.Driver>
14524 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Starting Semantic Analysis
14525 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Completed phase 1 of Semantic Analysis
14525 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Get metadata for source tables
14525 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_table : db=testdb tbl=test_table
14525 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table : db=testdb tbl=test_table	
14525 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
14526 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
14527 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
14529 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
14530 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
14555 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Get metadata for subqueries
14561 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Get metadata for destination tables
14567 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Completed getting MetaData in Semantic Analysis
14687 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Set stats collection dir : file:/tmp/travis/08b7e818-74c2-4bbe-8f1e-d5d9426e8875/hive_2016-06-23_16-36-58_306_464515256968076867-1/-ext-10002
14766 [main] INFO  o.a.h.h.q.p.OpProcFactory - Processing for FS(2)
14766 [main] INFO  o.a.h.h.q.p.OpProcFactory - Processing for SEL(1)
14766 [main] INFO  o.a.h.h.q.p.OpProcFactory - Processing for TS(0)
14789 [main] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=partition-retrieving from=org.apache.hadoop.hive.ql.optimizer.ppr.PartitionPruner>
14789 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_partitions : db=testdb tbl=test_table
14790 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_partitions : db=testdb tbl=test_table	
14845 [main] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=partition-retrieving start=1466699818963 end=1466699819019 duration=56 from=org.apache.hadoop.hive.ql.optimizer.ppr.PartitionPruner>
14872 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Completed plan generation
14872 [main] INFO  o.a.h.h.q.Driver - Semantic Analysis Completed
14872 [main] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=semanticAnalyze start=1466699818656 end=1466699819046 duration=390 from=org.apache.hadoop.hive.ql.Driver>
14885 [main] INFO  o.a.h.h.q.e.TableScanOperator - Initializing Self TS[0]
14886 [main] INFO  o.a.h.h.q.e.TableScanOperator - Operator 0 TS initialized
14886 [main] INFO  o.a.h.h.q.e.TableScanOperator - Initializing children of 0 TS
14886 [main] INFO  o.a.h.h.q.e.SelectOperator - Initializing child 1 SEL
14886 [main] INFO  o.a.h.h.q.e.SelectOperator - Initializing Self SEL[1]
14888 [main] INFO  o.a.h.h.q.e.SelectOperator - SELECT struct<id:int,msg:string,city:string,state:string>
14888 [main] INFO  o.a.h.h.q.e.SelectOperator - Operator 1 SEL initialized
14889 [main] INFO  o.a.h.h.q.e.SelectOperator - Initializing children of 1 SEL
14889 [main] INFO  o.a.h.h.q.e.ListSinkOperator - Initializing child 3 OP
14889 [main] INFO  o.a.h.h.q.e.ListSinkOperator - Initializing Self OP[3]
14890 [main] INFO  o.a.h.h.q.e.ListSinkOperator - Operator 3 OP initialized
14890 [main] INFO  o.a.h.h.q.e.ListSinkOperator - Initialization Done 3 OP
14890 [main] INFO  o.a.h.h.q.e.SelectOperator - Initialization Done 1 SEL
14890 [main] INFO  o.a.h.h.q.e.TableScanOperator - Initialization Done 0 TS
14892 [main] INFO  o.a.h.h.q.Driver - Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:test_table.id, type:int, comment:null), FieldSchema(name:test_table.msg, type:string, comment:null), FieldSchema(name:test_table.city, type:string, comment:null), FieldSchema(name:test_table.state, type:string, comment:null)], properties:null)
14893 [main] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=compile start=1466699818283 end=1466699819067 duration=784 from=org.apache.hadoop.hive.ql.Driver>

-------------------- system-err --------------------
Unable to drop index HL_TXNID_INDEX Index 'HL_TXNID_INDEX' does not exist.
Unable to drop table TXN_COMPONENTS: 'DROP TABLE' cannot be performed on 'TXN_COMPONENTS' because it does not exist.
Unable to drop table COMPLETED_TXN_COMPONENTS: 'DROP TABLE' cannot be performed on 'COMPLETED_TXN_COMPONENTS' because it does not exist.
Unable to drop table TXNS: 'DROP TABLE' cannot be performed on 'TXNS' because it does not exist.
Unable to drop table NEXT_TXN_ID: 'DROP TABLE' cannot be performed on 'NEXT_TXN_ID' because it does not exist.
Unable to drop table HIVE_LOCKS: 'DROP TABLE' cannot be performed on 'HIVE_LOCKS' because it does not exist.
Unable to drop table NEXT_LOCK_ID: 'DROP TABLE' cannot be performed on 'NEXT_LOCK_ID' because it does not exist.
Unable to drop table COMPACTION_QUEUE: 'DROP TABLE' cannot be performed on 'COMPACTION_QUEUE' because it does not exist.
Unable to drop table NEXT_COMPACTION_QUEUE_ID: 'DROP TABLE' cannot be performed on 'NEXT_COMPACTION_QUEUE_ID' because it does not exist.

--------------------------------------------------
--------------------------------------------------
classname: org.apache.storm.hive.bolt.TestHiveBolt / testname: testNoAcksUntilFlushed
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hive.bolt.TestHiveBolt.generateTestTuple(TestHiveBolt.java:447)
	at org.apache.storm.hive.bolt.TestHiveBolt.testNoAcksUntilFlushed(TestHiveBolt.java:297)

-------------------- system-out --------------------
15502 [main] INFO  o.a.h.h.q.s.SessionState - Created local directory: /tmp/8bd0da95-3109-43e1-80d2-9d3b3da3a85e_resources
15534 [main] INFO  o.a.h.h.q.s.SessionState - Created HDFS directory: /tmp/hive/travis/8bd0da95-3109-43e1-80d2-9d3b3da3a85e
15570 [main] INFO  o.a.h.h.q.s.SessionState - Created local directory: /tmp/travis/8bd0da95-3109-43e1-80d2-9d3b3da3a85e
15602 [main] INFO  o.a.h.h.q.s.SessionState - Created HDFS directory: /tmp/hive/travis/8bd0da95-3109-43e1-80d2-9d3b3da3a85e/_tmp_space.db
15602 [main] INFO  o.a.h.h.q.s.SessionState - No Tez session required at this point. hive.execution.engine=mr.
15605 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
15607 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
15609 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
15610 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
15610 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_table_names_by_filter: db = testdb, filter = 
15610 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table_names_by_filter: db = testdb, filter = 	
15614 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_table : db=testdb tbl=test_table
15614 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table : db=testdb tbl=test_table	
15664 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: drop_table : db=testdb tbl=test_table
15664 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=drop_table : db=testdb tbl=test_table	
15681 [main] INFO  D.Datastore - The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
15681 [main] INFO  D.Datastore - The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
15975 [main] INFO  D.Datastore - The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
15976 [main] INFO  D.Datastore - The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
16135 [main] INFO  D.Datastore - The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
16142 [main] INFO  D.Datastore - The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
16561 [main] INFO  D.Datastore - The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
16561 [main] INFO  D.Datastore - The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
16646 [main] INFO  D.Datastore - The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
16646 [main] INFO  D.Datastore - The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
17215 [main] INFO  h.m.hivemetastoressimpl - deleting  raw:/tmp/junit5355118619987390763/testdb.db/test_table
17216 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_database: testdb
17216 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_database: testdb	
17241 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: drop_database: testdb
17253 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=drop_database: testdb	
17254 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_all_tables: db=testdb
17254 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_all_tables: db=testdb	
17302 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_functions: db=testdb pat=*
17302 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_functions: db=testdb pat=*	
17309 [main] INFO  D.Datastore - The class "org.apache.hadoop.hive.metastore.model.MResourceUri" is tagged as "embedded-only" so does not have its own datastore table.
17928 [main] INFO  o.a.h.h.m.ObjectStore - Dropping database testdb along with all tables
18094 [main] INFO  h.m.hivemetastoressimpl - deleting  raw:/tmp/junit5355118619987390763/testdb.db
18095 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
18096 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
18119 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
18120 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
18120 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: create_database: Database(name:testdb, description:null, locationUri:raw:///tmp/junit5448887660750266488/testdb.db, parameters:null)
18120 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=create_database: Database(name:testdb, description:null, locationUri:raw:///tmp/junit5448887660750266488/testdb.db, parameters:null)	
18132 [main] WARN  o.a.h.h.m.ObjectStore - Failed to get database testdb, returning NoSuchObjectException
18152 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: create_table: Table(tableName:test_table, dbName:testdb, owner:null, createTime:0, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:), FieldSchema(name:msg, type:string, comment:)], location:raw:///tmp/junit5448887660750266488/testdb.db/test_table, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:1, serdeInfo:SerDeInfo(name:test_table, serializationLib:org.apache.hadoop.hive.ql.io.orc.OrcSerde, parameters:{serialization.format=1}), bucketCols:[], sortCols:null, parameters:null), partitionKeys:[FieldSchema(name:city, type:string, comment:), FieldSchema(name:state, type:string, comment:)], parameters:{}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)
18152 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=create_table: Table(tableName:test_table, dbName:testdb, owner:null, createTime:0, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:), FieldSchema(name:msg, type:string, comment:)], location:raw:///tmp/junit5448887660750266488/testdb.db/test_table, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:1, serdeInfo:SerDeInfo(name:test_table, serializationLib:org.apache.hadoop.hive.ql.io.orc.OrcSerde, parameters:{serialization.format=1}), bucketCols:[], sortCols:null, parameters:null), partitionKeys:[FieldSchema(name:city, type:string, comment:), FieldSchema(name:state, type:string, comment:)], parameters:{}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
18166 [main] WARN  o.a.h.h.m.HiveMetaStore - Location: raw:///tmp/junit5448887660750266488/testdb.db/test_table specified for non-external table:test_table
18166 [main] INFO  o.a.h.h.c.FileUtils - Creating directory if it doesn't exist: raw:/tmp/junit5448887660750266488/testdb.db/test_table
18317 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: add_partition : db=testdb tbl=test_table
18317 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=add_partition : db=testdb tbl=test_table	
18445 [main] INFO  o.a.h.h.c.FileUtils - Creating directory if it doesn't exist: raw:/tmp/junit5448887660750266488/testdb.db/test_table/city=sunnyvale/state=ca
18466 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Shutting down the object store...
18466 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=Shutting down the object store...	
18466 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Metastore shutdown complete.
18466 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=Metastore shutdown complete.	
done

--------------------------------------------------
--------------------------------------------------
classname: org.apache.storm.hive.bolt.TestHiveBolt / testname: testData
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hive.bolt.TestHiveBolt.generateTestTuple(TestHiveBolt.java:447)
	at org.apache.storm.hive.bolt.TestHiveBolt.testData(TestHiveBolt.java:251)

-------------------- system-out --------------------
18979 [main] INFO  o.a.h.h.q.s.SessionState - Created local directory: /tmp/f5632083-284d-41a6-ad50-e76be0aaeefd_resources
18989 [main] INFO  o.a.h.h.q.s.SessionState - Created HDFS directory: /tmp/hive/travis/f5632083-284d-41a6-ad50-e76be0aaeefd
19001 [main] INFO  o.a.h.h.q.s.SessionState - Created local directory: /tmp/travis/f5632083-284d-41a6-ad50-e76be0aaeefd
19033 [main] INFO  o.a.h.h.q.s.SessionState - Created HDFS directory: /tmp/hive/travis/f5632083-284d-41a6-ad50-e76be0aaeefd/_tmp_space.db
19033 [main] INFO  o.a.h.h.q.s.SessionState - No Tez session required at this point. hive.execution.engine=mr.
19035 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_table_names_by_filter: db = testdb, filter = 
19035 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table_names_by_filter: db = testdb, filter = 	
19035 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
19036 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
19036 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
19060 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
19060 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
19070 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_table : db=testdb tbl=test_table
19070 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table : db=testdb tbl=test_table	
19087 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: drop_table : db=testdb tbl=test_table
19087 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=drop_table : db=testdb tbl=test_table	
19674 [main] INFO  h.m.hivemetastoressimpl - deleting  raw:/tmp/junit5448887660750266488/testdb.db/test_table
19674 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_database: testdb
19674 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_database: testdb	
19686 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: drop_database: testdb
19686 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=drop_database: testdb	
19687 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_all_tables: db=testdb
19687 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_all_tables: db=testdb	
19693 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_functions: db=testdb pat=*
19693 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_functions: db=testdb pat=*	
20222 [main] INFO  o.a.h.h.m.ObjectStore - Dropping database testdb along with all tables
20319 [main] INFO  h.m.hivemetastoressimpl - deleting  raw:/tmp/junit5448887660750266488/testdb.db
20321 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
20322 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
20324 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
20324 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
20324 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: create_database: Database(name:testdb, description:null, locationUri:raw:///tmp/junit1602902864450792706/testdb.db, parameters:null)
20324 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=create_database: Database(name:testdb, description:null, locationUri:raw:///tmp/junit1602902864450792706/testdb.db, parameters:null)	
20325 [main] WARN  o.a.h.h.m.ObjectStore - Failed to get database testdb, returning NoSuchObjectException
20330 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: create_table: Table(tableName:test_table, dbName:testdb, owner:null, createTime:0, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:), FieldSchema(name:msg, type:string, comment:)], location:raw:///tmp/junit1602902864450792706/testdb.db/test_table, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:1, serdeInfo:SerDeInfo(name:test_table, serializationLib:org.apache.hadoop.hive.ql.io.orc.OrcSerde, parameters:{serialization.format=1}), bucketCols:[], sortCols:null, parameters:null), partitionKeys:[FieldSchema(name:city, type:string, comment:), FieldSchema(name:state, type:string, comment:)], parameters:{}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)
20330 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=create_table: Table(tableName:test_table, dbName:testdb, owner:null, createTime:0, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:), FieldSchema(name:msg, type:string, comment:)], location:raw:///tmp/junit1602902864450792706/testdb.db/test_table, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:1, serdeInfo:SerDeInfo(name:test_table, serializationLib:org.apache.hadoop.hive.ql.io.orc.OrcSerde, parameters:{serialization.format=1}), bucketCols:[], sortCols:null, parameters:null), partitionKeys:[FieldSchema(name:city, type:string, comment:), FieldSchema(name:state, type:string, comment:)], parameters:{}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
20332 [main] WARN  o.a.h.h.m.HiveMetaStore - Location: raw:///tmp/junit1602902864450792706/testdb.db/test_table specified for non-external table:test_table
20332 [main] INFO  o.a.h.h.c.FileUtils - Creating directory if it doesn't exist: raw:/tmp/junit1602902864450792706/testdb.db/test_table
20416 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: add_partition : db=testdb tbl=test_table
20420 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=add_partition : db=testdb tbl=test_table	
20543 [main] INFO  o.a.h.h.c.FileUtils - Creating directory if it doesn't exist: raw:/tmp/junit1602902864450792706/testdb.db/test_table/city=sunnyvale/state=ca
20610 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Shutting down the object store...
20610 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=Shutting down the object store...	
20610 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Metastore shutdown complete.
20611 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=Metastore shutdown complete.	
done

--------------------------------------------------
--------------------------------------------------
classname: org.apache.storm.hive.bolt.TestHiveBolt / testname: testWithoutPartitions
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hive.bolt.TestHiveBolt.generateTestTuple(TestHiveBolt.java:447)
	at org.apache.storm.hive.bolt.TestHiveBolt.testWithoutPartitions(TestHiveBolt.java:194)

-------------------- system-out --------------------
21123 [main] INFO  o.a.h.h.q.s.SessionState - Created local directory: /tmp/0039c459-8d2f-4839-a939-5ec5731e1ea6_resources
21145 [main] INFO  o.a.h.h.q.s.SessionState - Created HDFS directory: /tmp/hive/travis/0039c459-8d2f-4839-a939-5ec5731e1ea6
21175 [main] INFO  o.a.h.h.q.s.SessionState - Created local directory: /tmp/travis/0039c459-8d2f-4839-a939-5ec5731e1ea6
21193 [main] INFO  o.a.h.h.q.s.SessionState - Created HDFS directory: /tmp/hive/travis/0039c459-8d2f-4839-a939-5ec5731e1ea6/_tmp_space.db
21194 [main] INFO  o.a.h.h.q.s.SessionState - No Tez session required at this point. hive.execution.engine=mr.
21199 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_table_names_by_filter: db = testdb, filter = 
21199 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table_names_by_filter: db = testdb, filter = 	
21199 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
21201 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
21202 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
21204 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
21204 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
21257 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_table : db=testdb tbl=test_table
21257 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table : db=testdb tbl=test_table	
21300 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: drop_table : db=testdb tbl=test_table
21300 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=drop_table : db=testdb tbl=test_table	
21832 [main] INFO  h.m.hivemetastoressimpl - deleting  raw:/tmp/junit1602902864450792706/testdb.db/test_table
21833 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_database: testdb
21833 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_database: testdb	
21843 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: drop_database: testdb
21844 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=drop_database: testdb	
21845 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_all_tables: db=testdb
21846 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_all_tables: db=testdb	
21863 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_functions: db=testdb pat=*
21863 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_functions: db=testdb pat=*	
21876 [main] INFO  o.a.h.h.m.ObjectStore - Dropping database testdb along with all tables
21923 [main] INFO  h.m.hivemetastoressimpl - deleting  raw:/tmp/junit1602902864450792706/testdb.db
21924 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
21925 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
21926 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
21926 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
21927 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: create_database: Database(name:testdb, description:null, locationUri:raw:///tmp/junit4143590812424338288/testdb.db, parameters:null)
21927 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=create_database: Database(name:testdb, description:null, locationUri:raw:///tmp/junit4143590812424338288/testdb.db, parameters:null)	
21927 [main] WARN  o.a.h.h.m.ObjectStore - Failed to get database testdb, returning NoSuchObjectException
21950 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: create_table: Table(tableName:test_table, dbName:testdb, owner:null, createTime:0, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:), FieldSchema(name:msg, type:string, comment:)], location:raw:///tmp/junit4143590812424338288/testdb.db/test_table, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:1, serdeInfo:SerDeInfo(name:test_table, serializationLib:org.apache.hadoop.hive.ql.io.orc.OrcSerde, parameters:{serialization.format=1}), bucketCols:[], sortCols:null, parameters:null), partitionKeys:[FieldSchema(name:city, type:string, comment:), FieldSchema(name:state, type:string, comment:)], parameters:{}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)
21950 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=create_table: Table(tableName:test_table, dbName:testdb, owner:null, createTime:0, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:), FieldSchema(name:msg, type:string, comment:)], location:raw:///tmp/junit4143590812424338288/testdb.db/test_table, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:1, serdeInfo:SerDeInfo(name:test_table, serializationLib:org.apache.hadoop.hive.ql.io.orc.OrcSerde, parameters:{serialization.format=1}), bucketCols:[], sortCols:null, parameters:null), partitionKeys:[FieldSchema(name:city, type:string, comment:), FieldSchema(name:state, type:string, comment:)], parameters:{}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
21952 [main] WARN  o.a.h.h.m.HiveMetaStore - Location: raw:///tmp/junit4143590812424338288/testdb.db/test_table specified for non-external table:test_table
21952 [main] INFO  o.a.h.h.c.FileUtils - Creating directory if it doesn't exist: raw:/tmp/junit4143590812424338288/testdb.db/test_table
22103 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: add_partition : db=testdb tbl=test_table
22104 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=add_partition : db=testdb tbl=test_table	
22215 [main] INFO  o.a.h.h.c.FileUtils - Creating directory if it doesn't exist: raw:/tmp/junit4143590812424338288/testdb.db/test_table/city=sunnyvale/state=ca
22258 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Shutting down the object store...
22258 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=Shutting down the object store...	
22258 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Metastore shutdown complete.
22258 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=Metastore shutdown complete.	
done
22259 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_table_names_by_filter: db = testdb1, filter = 
22259 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table_names_by_filter: db = testdb1, filter = 	
22259 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
22260 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
22261 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
22264 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
22264 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
22278 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_database: testdb1
22278 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_database: testdb1	
22279 [main] WARN  o.a.h.h.m.ObjectStore - Failed to get database testdb1, returning NoSuchObjectException
22280 [main] ERROR o.a.h.h.m.RetryingHMSHandler - NoSuchObjectException(message:testdb1)
	at org.apache.hadoop.hive.metastore.ObjectStore.getDatabase(ObjectStore.java:539)
	at sun.reflect.GeneratedMethodAccessor10.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:483)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:98)
	at com.sun.proxy.$Proxy24.getDatabase(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_database_core(HiveMetaStore.java:914)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_database(HiveMetaStore.java:888)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:483)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:102)
	at com.sun.proxy.$Proxy26.get_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getDatabase(HiveMetaStoreClient.java:1043)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:657)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:644)
	at org.apache.storm.hive.bolt.HiveSetupUtil.dropDB(HiveSetupUtil.java:166)
	at org.apache.storm.hive.bolt.TestHiveBolt.testWithoutPartitions(TestHiveBolt.java:175)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:483)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:108)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:78)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:54)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:144)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:203)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:155)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:103)

22280 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Shutting down the object store...
22280 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=Shutting down the object store...	
22280 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Metastore shutdown complete.
22280 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=Metastore shutdown complete.	
22281 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: create_database: Database(name:testdb1, description:null, locationUri:raw:///tmp/junit4143590812424338288/testdb.db, parameters:null)
22281 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=create_database: Database(name:testdb1, description:null, locationUri:raw:///tmp/junit4143590812424338288/testdb.db, parameters:null)	
22281 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
22282 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
22282 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
22284 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
22284 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
22285 [main] WARN  o.a.h.h.m.ObjectStore - Failed to get database testdb1, returning NoSuchObjectException
22299 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: create_table: Table(tableName:test_table1, dbName:testdb1, owner:null, createTime:0, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:), FieldSchema(name:msg, type:string, comment:)], location:raw:///tmp/junit4143590812424338288/testdb.db/test_table1, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:1, serdeInfo:SerDeInfo(name:test_table1, serializationLib:org.apache.hadoop.hive.ql.io.orc.OrcSerde, parameters:{serialization.format=1}), bucketCols:[], sortCols:null, parameters:null), partitionKeys:null, parameters:{}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)
22299 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=create_table: Table(tableName:test_table1, dbName:testdb1, owner:null, createTime:0, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:), FieldSchema(name:msg, type:string, comment:)], location:raw:///tmp/junit4143590812424338288/testdb.db/test_table1, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:1, serdeInfo:SerDeInfo(name:test_table1, serializationLib:org.apache.hadoop.hive.ql.io.orc.OrcSerde, parameters:{serialization.format=1}), bucketCols:[], sortCols:null, parameters:null), partitionKeys:null, parameters:{}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
22300 [main] WARN  o.a.h.h.m.HiveMetaStore - Location: raw:///tmp/junit4143590812424338288/testdb.db/test_table1 specified for non-external table:test_table1
22301 [main] INFO  o.a.h.h.c.FileUtils - Creating directory if it doesn't exist: raw:/tmp/junit4143590812424338288/testdb.db/test_table1
22312 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Shutting down the object store...
22312 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=Shutting down the object store...	
22312 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Metastore shutdown complete.
22312 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=Metastore shutdown complete.	
22312 [main] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=compile from=org.apache.hadoop.hive.ql.Driver>
22321 [main] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=parse from=org.apache.hadoop.hive.ql.Driver>
22321 [main] INFO  h.q.p.ParseDriver - Parsing command: select * from testdb1.test_table1
22321 [main] INFO  h.q.p.ParseDriver - Parse Completed
22321 [main] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=parse start=1466699826495 end=1466699826495 duration=0 from=org.apache.hadoop.hive.ql.Driver>
22336 [main] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=semanticAnalyze from=org.apache.hadoop.hive.ql.Driver>
22345 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Starting Semantic Analysis
22345 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Completed phase 1 of Semantic Analysis
22345 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Get metadata for source tables
22345 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_table : db=testdb1 tbl=test_table1
22345 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table : db=testdb1 tbl=test_table1	
22346 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
22346 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
22347 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
22353 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
22353 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
22401 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Get metadata for subqueries
22401 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Get metadata for destination tables
22404 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Completed getting MetaData in Semantic Analysis
22405 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Set stats collection dir : file:/tmp/travis/0039c459-8d2f-4839-a939-5ec5731e1ea6/hive_2016-06-23_16-37-06_487_1784194405276069685-1/-ext-10002
22406 [main] INFO  o.a.h.h.q.p.OpProcFactory - Processing for FS(6)
22406 [main] INFO  o.a.h.h.q.p.OpProcFactory - Processing for SEL(5)
22406 [main] INFO  o.a.h.h.q.p.OpProcFactory - Processing for TS(4)
22408 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Completed plan generation
22408 [main] INFO  o.a.h.h.q.Driver - Semantic Analysis Completed
22408 [main] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=semanticAnalyze start=1466699826510 end=1466699826582 duration=72 from=org.apache.hadoop.hive.ql.Driver>
22408 [main] INFO  o.a.h.h.q.e.TableScanOperator - Initializing Self TS[4]
22408 [main] INFO  o.a.h.h.q.e.TableScanOperator - Operator 4 TS initialized
22408 [main] INFO  o.a.h.h.q.e.TableScanOperator - Initializing children of 4 TS
22408 [main] INFO  o.a.h.h.q.e.SelectOperator - Initializing child 5 SEL
22408 [main] INFO  o.a.h.h.q.e.SelectOperator - Initializing Self SEL[5]
22409 [main] INFO  o.a.h.h.q.e.SelectOperator - SELECT struct<id:int,msg:string>
22409 [main] INFO  o.a.h.h.q.e.SelectOperator - Operator 5 SEL initialized
22409 [main] INFO  o.a.h.h.q.e.SelectOperator - Initializing children of 5 SEL
22409 [main] INFO  o.a.h.h.q.e.ListSinkOperator - Initializing child 7 OP
22409 [main] INFO  o.a.h.h.q.e.ListSinkOperator - Initializing Self OP[7]
22409 [main] INFO  o.a.h.h.q.e.ListSinkOperator - Operator 7 OP initialized
22409 [main] INFO  o.a.h.h.q.e.ListSinkOperator - Initialization Done 7 OP
22409 [main] INFO  o.a.h.h.q.e.SelectOperator - Initialization Done 5 SEL
22409 [main] INFO  o.a.h.h.q.e.TableScanOperator - Initialization Done 4 TS
22409 [main] INFO  o.a.h.h.q.Driver - Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:test_table1.id, type:int, comment:null), FieldSchema(name:test_table1.msg, type:string, comment:null)], properties:null)
22409 [main] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=compile start=1466699826486 end=1466699826583 duration=97 from=org.apache.hadoop.hive.ql.Driver>

--------------------------------------------------
--------------------------------------------------
classname: org.apache.storm.hive.bolt.TestHiveBolt / testname: testJsonWriter
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hive.bolt.TestHiveBolt.generateTestTuple(TestHiveBolt.java:447)
	at org.apache.storm.hive.bolt.TestHiveBolt.testJsonWriter(TestHiveBolt.java:274)

-------------------- system-out --------------------
24181 [main] INFO  o.a.h.h.q.s.SessionState - Created local directory: /tmp/ecc07f26-225e-4758-b305-e320ab452028_resources
24182 [main] INFO  o.a.h.h.q.s.SessionState - Created HDFS directory: /tmp/hive/travis/ecc07f26-225e-4758-b305-e320ab452028
24185 [main] INFO  o.a.h.h.q.s.SessionState - Created local directory: /tmp/travis/ecc07f26-225e-4758-b305-e320ab452028
24187 [main] INFO  o.a.h.h.q.s.SessionState - Created HDFS directory: /tmp/hive/travis/ecc07f26-225e-4758-b305-e320ab452028/_tmp_space.db
24187 [main] INFO  o.a.h.h.q.s.SessionState - No Tez session required at this point. hive.execution.engine=mr.
24188 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_table_names_by_filter: db = testdb, filter = 
24188 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table_names_by_filter: db = testdb, filter = 	
24188 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
24189 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
24190 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
24191 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
24192 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
24207 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_table : db=testdb tbl=test_table
24207 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table : db=testdb tbl=test_table	
24219 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: drop_table : db=testdb tbl=test_table
24219 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=drop_table : db=testdb tbl=test_table	
24448 [main] INFO  h.m.hivemetastoressimpl - deleting  raw:/tmp/junit810914867095943667/testdb.db/test_table
24448 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_database: testdb
24459 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_database: testdb	
24468 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: drop_database: testdb
24468 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=drop_database: testdb	
24469 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_all_tables: db=testdb
24469 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_all_tables: db=testdb	
24473 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_functions: db=testdb pat=*
24474 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_functions: db=testdb pat=*	
24478 [main] INFO  o.a.h.h.m.ObjectStore - Dropping database testdb along with all tables
24497 [main] INFO  h.m.hivemetastoressimpl - deleting  raw:/tmp/junit810914867095943667/testdb.db
24499 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
24499 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
24501 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
24502 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
24502 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: create_database: Database(name:testdb, description:null, locationUri:raw:///tmp/junit4345208362864671909/testdb.db, parameters:null)
24502 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=create_database: Database(name:testdb, description:null, locationUri:raw:///tmp/junit4345208362864671909/testdb.db, parameters:null)	
24503 [main] WARN  o.a.h.h.m.ObjectStore - Failed to get database testdb, returning NoSuchObjectException
24508 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: create_table: Table(tableName:test_table, dbName:testdb, owner:null, createTime:0, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:), FieldSchema(name:msg, type:string, comment:)], location:raw:///tmp/junit4345208362864671909/testdb.db/test_table, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:1, serdeInfo:SerDeInfo(name:test_table, serializationLib:org.apache.hadoop.hive.ql.io.orc.OrcSerde, parameters:{serialization.format=1}), bucketCols:[], sortCols:null, parameters:null), partitionKeys:[FieldSchema(name:city, type:string, comment:), FieldSchema(name:state, type:string, comment:)], parameters:{}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)
24508 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=create_table: Table(tableName:test_table, dbName:testdb, owner:null, createTime:0, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:), FieldSchema(name:msg, type:string, comment:)], location:raw:///tmp/junit4345208362864671909/testdb.db/test_table, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:1, serdeInfo:SerDeInfo(name:test_table, serializationLib:org.apache.hadoop.hive.ql.io.orc.OrcSerde, parameters:{serialization.format=1}), bucketCols:[], sortCols:null, parameters:null), partitionKeys:[FieldSchema(name:city, type:string, comment:), FieldSchema(name:state, type:string, comment:)], parameters:{}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
24509 [main] WARN  o.a.h.h.m.HiveMetaStore - Location: raw:///tmp/junit4345208362864671909/testdb.db/test_table specified for non-external table:test_table
24510 [main] INFO  o.a.h.h.c.FileUtils - Creating directory if it doesn't exist: raw:/tmp/junit4345208362864671909/testdb.db/test_table
24560 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: add_partition : db=testdb tbl=test_table
24561 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=add_partition : db=testdb tbl=test_table	
24675 [main] INFO  o.a.h.h.c.FileUtils - Creating directory if it doesn't exist: raw:/tmp/junit4345208362864671909/testdb.db/test_table/city=sunnyvale/state=ca
24743 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Shutting down the object store...
24743 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=Shutting down the object store...	
24744 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Metastore shutdown complete.
24744 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=Metastore shutdown complete.	
done

--------------------------------------------------
--------------------------------------------------
classname: org.apache.storm.hive.bolt.TestHiveBolt / testname: testTickTuple
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hive.bolt.TestHiveBolt.generateTestTuple(TestHiveBolt.java:447)
	at org.apache.storm.hive.bolt.TestHiveBolt.testTickTuple(TestHiveBolt.java:352)

-------------------- system-out --------------------
26174 [main] INFO  o.a.h.h.q.s.SessionState - Created local directory: /tmp/15bcb091-dbbb-4767-b32e-93699626bd13_resources
26175 [main] INFO  o.a.h.h.q.s.SessionState - Created HDFS directory: /tmp/hive/travis/15bcb091-dbbb-4767-b32e-93699626bd13
26177 [main] INFO  o.a.h.h.q.s.SessionState - Created local directory: /tmp/travis/15bcb091-dbbb-4767-b32e-93699626bd13
26179 [main] INFO  o.a.h.h.q.s.SessionState - Created HDFS directory: /tmp/hive/travis/15bcb091-dbbb-4767-b32e-93699626bd13/_tmp_space.db
26179 [main] INFO  o.a.h.h.q.s.SessionState - No Tez session required at this point. hive.execution.engine=mr.
26180 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_table_names_by_filter: db = testdb, filter = 
26180 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table_names_by_filter: db = testdb, filter = 	
26180 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
26181 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
26182 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
26183 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
26183 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
26189 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_table : db=testdb tbl=test_table
26189 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table : db=testdb tbl=test_table	
26219 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: drop_table : db=testdb tbl=test_table
26220 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=drop_table : db=testdb tbl=test_table	
26429 [main] INFO  h.m.hivemetastoressimpl - deleting  raw:/tmp/junit1981073102343418524/testdb.db/test_table
26429 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_database: testdb
26429 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_database: testdb	
26436 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: drop_database: testdb
26436 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=drop_database: testdb	
26437 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_all_tables: db=testdb
26437 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_all_tables: db=testdb	
26441 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_functions: db=testdb pat=*
26441 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_functions: db=testdb pat=*	
26447 [main] INFO  o.a.h.h.m.ObjectStore - Dropping database testdb along with all tables
26466 [main] INFO  h.m.hivemetastoressimpl - deleting  raw:/tmp/junit1981073102343418524/testdb.db
26467 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
26468 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
26470 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
26470 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
26470 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: create_database: Database(name:testdb, description:null, locationUri:raw:///tmp/junit3168743599929306685/testdb.db, parameters:null)
26470 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=create_database: Database(name:testdb, description:null, locationUri:raw:///tmp/junit3168743599929306685/testdb.db, parameters:null)	
26471 [main] WARN  o.a.h.h.m.ObjectStore - Failed to get database testdb, returning NoSuchObjectException
26479 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: create_table: Table(tableName:test_table, dbName:testdb, owner:null, createTime:0, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:), FieldSchema(name:msg, type:string, comment:)], location:raw:///tmp/junit3168743599929306685/testdb.db/test_table, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:1, serdeInfo:SerDeInfo(name:test_table, serializationLib:org.apache.hadoop.hive.ql.io.orc.OrcSerde, parameters:{serialization.format=1}), bucketCols:[], sortCols:null, parameters:null), partitionKeys:[FieldSchema(name:city, type:string, comment:), FieldSchema(name:state, type:string, comment:)], parameters:{}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)
26479 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=create_table: Table(tableName:test_table, dbName:testdb, owner:null, createTime:0, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:), FieldSchema(name:msg, type:string, comment:)], location:raw:///tmp/junit3168743599929306685/testdb.db/test_table, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:1, serdeInfo:SerDeInfo(name:test_table, serializationLib:org.apache.hadoop.hive.ql.io.orc.OrcSerde, parameters:{serialization.format=1}), bucketCols:[], sortCols:null, parameters:null), partitionKeys:[FieldSchema(name:city, type:string, comment:), FieldSchema(name:state, type:string, comment:)], parameters:{}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
26481 [main] WARN  o.a.h.h.m.HiveMetaStore - Location: raw:///tmp/junit3168743599929306685/testdb.db/test_table specified for non-external table:test_table
26481 [main] INFO  o.a.h.h.c.FileUtils - Creating directory if it doesn't exist: raw:/tmp/junit3168743599929306685/testdb.db/test_table
26553 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: add_partition : db=testdb tbl=test_table
26553 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=add_partition : db=testdb tbl=test_table	
26607 [main] INFO  o.a.h.h.c.FileUtils - Creating directory if it doesn't exist: raw:/tmp/junit3168743599929306685/testdb.db/test_table/city=sunnyvale/state=ca
26639 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Shutting down the object store...
26639 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=Shutting down the object store...	
26639 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Metastore shutdown complete.
26639 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=Metastore shutdown complete.	
done

--------------------------------------------------
--------------------------------------------------
classname: org.apache.storm.hive.bolt.TestHiveBolt / testname: testWithTimeformat
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hive.bolt.TestHiveBolt.generateTestTuple(TestHiveBolt.java:447)
	at org.apache.storm.hive.bolt.TestHiveBolt.testWithTimeformat(TestHiveBolt.java:230)

-------------------- system-out --------------------
27241 [main] INFO  o.a.h.h.q.s.SessionState - Created local directory: /tmp/0768ae16-ecb5-4bc8-888b-056a05e02726_resources
27243 [main] INFO  o.a.h.h.q.s.SessionState - Created HDFS directory: /tmp/hive/travis/0768ae16-ecb5-4bc8-888b-056a05e02726
27244 [main] INFO  o.a.h.h.q.s.SessionState - Created local directory: /tmp/travis/0768ae16-ecb5-4bc8-888b-056a05e02726
27261 [main] INFO  o.a.h.h.q.s.SessionState - Created HDFS directory: /tmp/hive/travis/0768ae16-ecb5-4bc8-888b-056a05e02726/_tmp_space.db
27262 [main] INFO  o.a.h.h.q.s.SessionState - No Tez session required at this point. hive.execution.engine=mr.
27265 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_table_names_by_filter: db = testdb, filter = 
27265 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table_names_by_filter: db = testdb, filter = 	
27265 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
27266 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
27266 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
27268 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
27268 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
27277 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_table : db=testdb tbl=test_table
27278 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table : db=testdb tbl=test_table	
27286 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: drop_table : db=testdb tbl=test_table
27286 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=drop_table : db=testdb tbl=test_table	
27444 [main] INFO  h.m.hivemetastoressimpl - deleting  raw:/tmp/junit3168743599929306685/testdb.db/test_table
27444 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_database: testdb
27444 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_database: testdb	
27449 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: drop_database: testdb
27449 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=drop_database: testdb	
27450 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_all_tables: db=testdb
27450 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_all_tables: db=testdb	
27453 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_functions: db=testdb pat=*
27453 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_functions: db=testdb pat=*	
27456 [main] INFO  o.a.h.h.m.ObjectStore - Dropping database testdb along with all tables
27469 [main] INFO  h.m.hivemetastoressimpl - deleting  raw:/tmp/junit3168743599929306685/testdb.db
27470 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
27470 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
27477 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
27477 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
27477 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: create_database: Database(name:testdb, description:null, locationUri:raw:///tmp/junit7433668900009835085/testdb.db, parameters:null)
27477 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=create_database: Database(name:testdb, description:null, locationUri:raw:///tmp/junit7433668900009835085/testdb.db, parameters:null)	
27478 [main] WARN  o.a.h.h.m.ObjectStore - Failed to get database testdb, returning NoSuchObjectException
27495 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: create_table: Table(tableName:test_table, dbName:testdb, owner:null, createTime:0, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:), FieldSchema(name:msg, type:string, comment:)], location:raw:///tmp/junit7433668900009835085/testdb.db/test_table, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:1, serdeInfo:SerDeInfo(name:test_table, serializationLib:org.apache.hadoop.hive.ql.io.orc.OrcSerde, parameters:{serialization.format=1}), bucketCols:[], sortCols:null, parameters:null), partitionKeys:[FieldSchema(name:city, type:string, comment:), FieldSchema(name:state, type:string, comment:)], parameters:{}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)
27495 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=create_table: Table(tableName:test_table, dbName:testdb, owner:null, createTime:0, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:), FieldSchema(name:msg, type:string, comment:)], location:raw:///tmp/junit7433668900009835085/testdb.db/test_table, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:1, serdeInfo:SerDeInfo(name:test_table, serializationLib:org.apache.hadoop.hive.ql.io.orc.OrcSerde, parameters:{serialization.format=1}), bucketCols:[], sortCols:null, parameters:null), partitionKeys:[FieldSchema(name:city, type:string, comment:), FieldSchema(name:state, type:string, comment:)], parameters:{}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
27497 [main] WARN  o.a.h.h.m.HiveMetaStore - Location: raw:///tmp/junit7433668900009835085/testdb.db/test_table specified for non-external table:test_table
27498 [main] INFO  o.a.h.h.c.FileUtils - Creating directory if it doesn't exist: raw:/tmp/junit7433668900009835085/testdb.db/test_table
27558 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: add_partition : db=testdb tbl=test_table
27559 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=add_partition : db=testdb tbl=test_table	
27607 [main] INFO  o.a.h.h.c.FileUtils - Creating directory if it doesn't exist: raw:/tmp/junit7433668900009835085/testdb.db/test_table/city=sunnyvale/state=ca
27622 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Shutting down the object store...
27622 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=Shutting down the object store...	
27622 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Metastore shutdown complete.
27622 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=Metastore shutdown complete.	
done
27622 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_table_names_by_filter: db = testdb1, filter = 
27623 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table_names_by_filter: db = testdb1, filter = 	
27623 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
27623 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
27624 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
27625 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
27625 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
27630 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_table : db=testdb1 tbl=test_table1
27630 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table : db=testdb1 tbl=test_table1	
27638 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: drop_table : db=testdb1 tbl=test_table1
27638 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=drop_table : db=testdb1 tbl=test_table1	
27675 [main] INFO  h.m.hivemetastoressimpl - deleting  raw:/tmp/junit4143590812424338288/testdb.db/test_table1
27676 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_database: testdb1
27676 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_database: testdb1	
27680 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: drop_database: testdb1
27681 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=drop_database: testdb1	
27681 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_all_tables: db=testdb1
27681 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_all_tables: db=testdb1	
27682 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_functions: db=testdb1 pat=*
27682 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_functions: db=testdb1 pat=*	
27682 [main] INFO  o.a.h.h.m.ObjectStore - Dropping database testdb1 along with all tables
27697 [main] INFO  h.m.hivemetastoressimpl - deleting  raw:/tmp/junit4143590812424338288/testdb.db
27698 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
27698 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
27700 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
27700 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
27700 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: create_database: Database(name:testdb1, description:null, locationUri:raw:///tmp/junit7433668900009835085/testdb.db, parameters:null)
27700 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=create_database: Database(name:testdb1, description:null, locationUri:raw:///tmp/junit7433668900009835085/testdb.db, parameters:null)	
27701 [main] WARN  o.a.h.h.m.ObjectStore - Failed to get database testdb1, returning NoSuchObjectException
27707 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: create_table: Table(tableName:test_table1, dbName:testdb1, owner:null, createTime:0, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:), FieldSchema(name:msg, type:string, comment:)], location:raw:///tmp/junit7433668900009835085/testdb.db/test_table1, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:1, serdeInfo:SerDeInfo(name:test_table1, serializationLib:org.apache.hadoop.hive.ql.io.orc.OrcSerde, parameters:{serialization.format=1}), bucketCols:[], sortCols:null, parameters:null), partitionKeys:[FieldSchema(name:dt, type:string, comment:)], parameters:{}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)
27707 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=create_table: Table(tableName:test_table1, dbName:testdb1, owner:null, createTime:0, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:), FieldSchema(name:msg, type:string, comment:)], location:raw:///tmp/junit7433668900009835085/testdb.db/test_table1, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:1, serdeInfo:SerDeInfo(name:test_table1, serializationLib:org.apache.hadoop.hive.ql.io.orc.OrcSerde, parameters:{serialization.format=1}), bucketCols:[], sortCols:null, parameters:null), partitionKeys:[FieldSchema(name:dt, type:string, comment:)], parameters:{}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
27708 [main] WARN  o.a.h.h.m.HiveMetaStore - Location: raw:///tmp/junit7433668900009835085/testdb.db/test_table1 specified for non-external table:test_table1
27708 [main] INFO  o.a.h.h.c.FileUtils - Creating directory if it doesn't exist: raw:/tmp/junit7433668900009835085/testdb.db/test_table1
27718 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Shutting down the object store...
27718 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=Shutting down the object store...	
27719 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Metastore shutdown complete.
27719 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=Metastore shutdown complete.	
27728 [main] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=compile from=org.apache.hadoop.hive.ql.Driver>
27735 [main] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=parse from=org.apache.hadoop.hive.ql.Driver>
27735 [main] INFO  h.q.p.ParseDriver - Parsing command: select * from testdb1.test_table1
27736 [main] INFO  h.q.p.ParseDriver - Parse Completed
27736 [main] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=parse start=1466699831909 end=1466699831910 duration=1 from=org.apache.hadoop.hive.ql.Driver>
27743 [main] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=semanticAnalyze from=org.apache.hadoop.hive.ql.Driver>
27743 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Starting Semantic Analysis
27743 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Completed phase 1 of Semantic Analysis
27743 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Get metadata for source tables
27743 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_table : db=testdb1 tbl=test_table1
27744 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table : db=testdb1 tbl=test_table1	
27744 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
27744 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
27745 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
27746 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
27746 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
27762 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Get metadata for subqueries
27762 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Get metadata for destination tables
27805 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Completed getting MetaData in Semantic Analysis
27807 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Set stats collection dir : file:/tmp/travis/0768ae16-ecb5-4bc8-888b-056a05e02726/hive_2016-06-23_16-37-11_908_3255888837766123967-1/-ext-10002
27810 [main] INFO  o.a.h.h.q.p.OpProcFactory - Processing for FS(10)
27810 [main] INFO  o.a.h.h.q.p.OpProcFactory - Processing for SEL(9)
27810 [main] INFO  o.a.h.h.q.p.OpProcFactory - Processing for TS(8)
27824 [main] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=partition-retrieving from=org.apache.hadoop.hive.ql.optimizer.ppr.PartitionPruner>
27824 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_partitions : db=testdb1 tbl=test_table1
27824 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_partitions : db=testdb1 tbl=test_table1	
27830 [main] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=partition-retrieving start=1466699831998 end=1466699832004 duration=6 from=org.apache.hadoop.hive.ql.optimizer.ppr.PartitionPruner>
27831 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Completed plan generation
27831 [main] INFO  o.a.h.h.q.Driver - Semantic Analysis Completed
27831 [main] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=semanticAnalyze start=1466699831917 end=1466699832005 duration=88 from=org.apache.hadoop.hive.ql.Driver>
27831 [main] INFO  o.a.h.h.q.e.TableScanOperator - Initializing Self TS[8]
27831 [main] INFO  o.a.h.h.q.e.TableScanOperator - Operator 8 TS initialized
27831 [main] INFO  o.a.h.h.q.e.TableScanOperator - Initializing children of 8 TS
27831 [main] INFO  o.a.h.h.q.e.SelectOperator - Initializing child 9 SEL
27832 [main] INFO  o.a.h.h.q.e.SelectOperator - Initializing Self SEL[9]
27832 [main] INFO  o.a.h.h.q.e.SelectOperator - SELECT struct<id:int,msg:string,dt:string>
27832 [main] INFO  o.a.h.h.q.e.SelectOperator - Operator 9 SEL initialized
27832 [main] INFO  o.a.h.h.q.e.SelectOperator - Initializing children of 9 SEL
27832 [main] INFO  o.a.h.h.q.e.ListSinkOperator - Initializing child 11 OP
27832 [main] INFO  o.a.h.h.q.e.ListSinkOperator - Initializing Self OP[11]
27832 [main] INFO  o.a.h.h.q.e.ListSinkOperator - Operator 11 OP initialized
27832 [main] INFO  o.a.h.h.q.e.ListSinkOperator - Initialization Done 11 OP
27832 [main] INFO  o.a.h.h.q.e.SelectOperator - Initialization Done 9 SEL
27832 [main] INFO  o.a.h.h.q.e.TableScanOperator - Initialization Done 8 TS
27832 [main] INFO  o.a.h.h.q.Driver - Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:test_table1.id, type:int, comment:null), FieldSchema(name:test_table1.msg, type:string, comment:null), FieldSchema(name:test_table1.dt, type:string, comment:null)], properties:null)
27832 [main] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=compile start=1466699831902 end=1466699832006 duration=104 from=org.apache.hadoop.hive.ql.Driver>

--------------------------------------------------
--------------------------------------------------
classname: org.apache.storm.hive.bolt.TestHiveBolt / testname: testWithByteArrayIdandMessage
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hive.bolt.TestHiveBolt.generateTestTuple(TestHiveBolt.java:447)
	at org.apache.storm.hive.bolt.TestHiveBolt.testWithByteArrayIdandMessage(TestHiveBolt.java:161)

-------------------- system-out --------------------
28170 [main] INFO  o.a.h.h.q.s.SessionState - Created local directory: /tmp/5074ba4b-ab3d-41f0-b539-72453f47c985_resources
28173 [main] INFO  o.a.h.h.q.s.SessionState - Created HDFS directory: /tmp/hive/travis/5074ba4b-ab3d-41f0-b539-72453f47c985
28190 [main] INFO  o.a.h.h.q.s.SessionState - Created local directory: /tmp/travis/5074ba4b-ab3d-41f0-b539-72453f47c985
28194 [main] INFO  o.a.h.h.q.s.SessionState - Created HDFS directory: /tmp/hive/travis/5074ba4b-ab3d-41f0-b539-72453f47c985/_tmp_space.db
28195 [main] INFO  o.a.h.h.q.s.SessionState - No Tez session required at this point. hive.execution.engine=mr.
28197 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
28197 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
28199 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
28199 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
28200 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_table_names_by_filter: db = testdb, filter = 
28200 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table_names_by_filter: db = testdb, filter = 	
28201 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_table : db=testdb tbl=test_table
28201 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table : db=testdb tbl=test_table	
28211 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: drop_table : db=testdb tbl=test_table
28211 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=drop_table : db=testdb tbl=test_table	
28372 [main] INFO  h.m.hivemetastoressimpl - deleting  raw:/tmp/junit7433668900009835085/testdb.db/test_table
28372 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_database: testdb
28373 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_database: testdb	
28377 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: drop_database: testdb
28377 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=drop_database: testdb	
28378 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_all_tables: db=testdb
28378 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_all_tables: db=testdb	
28380 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_functions: db=testdb pat=*
28381 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_functions: db=testdb pat=*	
28384 [main] INFO  o.a.h.h.m.ObjectStore - Dropping database testdb along with all tables
28419 [main] INFO  h.m.hivemetastoressimpl - deleting  raw:/tmp/junit7433668900009835085/testdb.db
28420 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
28421 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
28422 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
28423 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
28423 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: create_database: Database(name:testdb, description:null, locationUri:raw:///tmp/junit1403602809806318750/testdb.db, parameters:null)
28423 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=create_database: Database(name:testdb, description:null, locationUri:raw:///tmp/junit1403602809806318750/testdb.db, parameters:null)	
28424 [main] WARN  o.a.h.h.m.ObjectStore - Failed to get database testdb, returning NoSuchObjectException
28430 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: create_table: Table(tableName:test_table, dbName:testdb, owner:null, createTime:0, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:), FieldSchema(name:msg, type:string, comment:)], location:raw:///tmp/junit1403602809806318750/testdb.db/test_table, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:1, serdeInfo:SerDeInfo(name:test_table, serializationLib:org.apache.hadoop.hive.ql.io.orc.OrcSerde, parameters:{serialization.format=1}), bucketCols:[], sortCols:null, parameters:null), partitionKeys:[FieldSchema(name:city, type:string, comment:), FieldSchema(name:state, type:string, comment:)], parameters:{}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)
28430 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=create_table: Table(tableName:test_table, dbName:testdb, owner:null, createTime:0, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:), FieldSchema(name:msg, type:string, comment:)], location:raw:///tmp/junit1403602809806318750/testdb.db/test_table, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:1, serdeInfo:SerDeInfo(name:test_table, serializationLib:org.apache.hadoop.hive.ql.io.orc.OrcSerde, parameters:{serialization.format=1}), bucketCols:[], sortCols:null, parameters:null), partitionKeys:[FieldSchema(name:city, type:string, comment:), FieldSchema(name:state, type:string, comment:)], parameters:{}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
28432 [main] WARN  o.a.h.h.m.HiveMetaStore - Location: raw:///tmp/junit1403602809806318750/testdb.db/test_table specified for non-external table:test_table
28432 [main] INFO  o.a.h.h.c.FileUtils - Creating directory if it doesn't exist: raw:/tmp/junit1403602809806318750/testdb.db/test_table
28485 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: add_partition : db=testdb tbl=test_table
28485 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=add_partition : db=testdb tbl=test_table	
28523 [main] INFO  o.a.h.h.c.FileUtils - Creating directory if it doesn't exist: raw:/tmp/junit1403602809806318750/testdb.db/test_table/city=sunnyvale/state=ca
28538 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Shutting down the object store...
28538 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=Shutting down the object store...	
28538 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Metastore shutdown complete.
28538 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=Metastore shutdown complete.	
done
28539 [main] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=compile from=org.apache.hadoop.hive.ql.Driver>
28539 [main] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=parse from=org.apache.hadoop.hive.ql.Driver>
28539 [main] INFO  h.q.p.ParseDriver - Parsing command: select * from testdb.test_table
28539 [main] INFO  h.q.p.ParseDriver - Parse Completed
28539 [main] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=parse start=1466699832713 end=1466699832713 duration=0 from=org.apache.hadoop.hive.ql.Driver>
28545 [main] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=semanticAnalyze from=org.apache.hadoop.hive.ql.Driver>
28545 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Starting Semantic Analysis
28545 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Completed phase 1 of Semantic Analysis
28545 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Get metadata for source tables
28545 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_table : db=testdb tbl=test_table
28545 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table : db=testdb tbl=test_table	
28545 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
28546 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
28547 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
28547 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
28548 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
28556 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Get metadata for subqueries
28556 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Get metadata for destination tables
28559 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Completed getting MetaData in Semantic Analysis
28559 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Set stats collection dir : file:/tmp/travis/5074ba4b-ab3d-41f0-b539-72453f47c985/hive_2016-06-23_16-37-12_713_4029045581852653426-1/-ext-10002
28560 [main] INFO  o.a.h.h.q.p.OpProcFactory - Processing for FS(14)
28560 [main] INFO  o.a.h.h.q.p.OpProcFactory - Processing for SEL(13)
28560 [main] INFO  o.a.h.h.q.p.OpProcFactory - Processing for TS(12)
28561 [main] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=partition-retrieving from=org.apache.hadoop.hive.ql.optimizer.ppr.PartitionPruner>
28561 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_partitions : db=testdb tbl=test_table
28561 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_partitions : db=testdb tbl=test_table	
28596 [main] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=partition-retrieving start=1466699832735 end=1466699832770 duration=35 from=org.apache.hadoop.hive.ql.optimizer.ppr.PartitionPruner>
28599 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Completed plan generation
28599 [main] INFO  o.a.h.h.q.Driver - Semantic Analysis Completed
28599 [main] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=semanticAnalyze start=1466699832719 end=1466699832773 duration=54 from=org.apache.hadoop.hive.ql.Driver>
28600 [main] INFO  o.a.h.h.q.e.TableScanOperator - Initializing Self TS[12]
28600 [main] INFO  o.a.h.h.q.e.TableScanOperator - Operator 12 TS initialized
28600 [main] INFO  o.a.h.h.q.e.TableScanOperator - Initializing children of 12 TS
28600 [main] INFO  o.a.h.h.q.e.SelectOperator - Initializing child 13 SEL
28600 [main] INFO  o.a.h.h.q.e.SelectOperator - Initializing Self SEL[13]
28600 [main] INFO  o.a.h.h.q.e.SelectOperator - SELECT struct<id:int,msg:string,city:string,state:string>
28600 [main] INFO  o.a.h.h.q.e.SelectOperator - Operator 13 SEL initialized
28600 [main] INFO  o.a.h.h.q.e.SelectOperator - Initializing children of 13 SEL
28600 [main] INFO  o.a.h.h.q.e.ListSinkOperator - Initializing child 15 OP
28600 [main] INFO  o.a.h.h.q.e.ListSinkOperator - Initializing Self OP[15]
28600 [main] INFO  o.a.h.h.q.e.ListSinkOperator - Operator 15 OP initialized
28600 [main] INFO  o.a.h.h.q.e.ListSinkOperator - Initialization Done 15 OP
28600 [main] INFO  o.a.h.h.q.e.SelectOperator - Initialization Done 13 SEL
28601 [main] INFO  o.a.h.h.q.e.TableScanOperator - Initialization Done 12 TS
28601 [main] INFO  o.a.h.h.q.Driver - Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:test_table.id, type:int, comment:null), FieldSchema(name:test_table.msg, type:string, comment:null), FieldSchema(name:test_table.city, type:string, comment:null), FieldSchema(name:test_table.state, type:string, comment:null)], properties:null)
28601 [main] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=compile start=1466699832713 end=1466699832775 duration=62 from=org.apache.hadoop.hive.ql.Driver>

--------------------------------------------------
--------------------------------------------------
classname: org.apache.storm.hive.bolt.TestHiveBolt / testname: testNoAcksIfFlushFails
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hive.bolt.TestHiveBolt.generateTestTuple(TestHiveBolt.java:447)
	at org.apache.storm.hive.bolt.TestHiveBolt.testNoAcksIfFlushFails(TestHiveBolt.java:327)

-------------------- system-out --------------------
29310 [main] INFO  o.a.h.h.q.s.SessionState - Created local directory: /tmp/0e4e8d57-fd21-4e91-9832-7a4e74ef5027_resources
29312 [main] INFO  o.a.h.h.q.s.SessionState - Created HDFS directory: /tmp/hive/travis/0e4e8d57-fd21-4e91-9832-7a4e74ef5027
29313 [main] INFO  o.a.h.h.q.s.SessionState - Created local directory: /tmp/travis/0e4e8d57-fd21-4e91-9832-7a4e74ef5027
29315 [main] INFO  o.a.h.h.q.s.SessionState - Created HDFS directory: /tmp/hive/travis/0e4e8d57-fd21-4e91-9832-7a4e74ef5027/_tmp_space.db
29315 [main] INFO  o.a.h.h.q.s.SessionState - No Tez session required at this point. hive.execution.engine=mr.
29318 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
29319 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
29320 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
29321 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
29321 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_table_names_by_filter: db = testdb, filter = 
29321 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table_names_by_filter: db = testdb, filter = 	
29327 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_table : db=testdb tbl=test_table
29327 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table : db=testdb tbl=test_table	
29338 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: drop_table : db=testdb tbl=test_table
29338 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=drop_table : db=testdb tbl=test_table	
29669 [main] INFO  h.m.hivemetastoressimpl - deleting  raw:/tmp/junit1403602809806318750/testdb.db/test_table
29671 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_database: testdb
29671 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_database: testdb	
29678 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: drop_database: testdb
29679 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=drop_database: testdb	
29679 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_all_tables: db=testdb
29679 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_all_tables: db=testdb	
29683 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_functions: db=testdb pat=*
29683 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_functions: db=testdb pat=*	
29686 [main] INFO  o.a.h.h.m.ObjectStore - Dropping database testdb along with all tables
29705 [main] INFO  h.m.hivemetastoressimpl - deleting  raw:/tmp/junit1403602809806318750/testdb.db
29706 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
29709 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
29710 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
29711 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
29711 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: create_database: Database(name:testdb, description:null, locationUri:raw:///tmp/junit8438663766622186795/testdb.db, parameters:null)
29711 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=create_database: Database(name:testdb, description:null, locationUri:raw:///tmp/junit8438663766622186795/testdb.db, parameters:null)	
29711 [main] WARN  o.a.h.h.m.ObjectStore - Failed to get database testdb, returning NoSuchObjectException
29723 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: create_table: Table(tableName:test_table, dbName:testdb, owner:null, createTime:0, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:), FieldSchema(name:msg, type:string, comment:)], location:raw:///tmp/junit8438663766622186795/testdb.db/test_table, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:1, serdeInfo:SerDeInfo(name:test_table, serializationLib:org.apache.hadoop.hive.ql.io.orc.OrcSerde, parameters:{serialization.format=1}), bucketCols:[], sortCols:null, parameters:null), partitionKeys:[FieldSchema(name:city, type:string, comment:), FieldSchema(name:state, type:string, comment:)], parameters:{}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)
29723 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=create_table: Table(tableName:test_table, dbName:testdb, owner:null, createTime:0, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:), FieldSchema(name:msg, type:string, comment:)], location:raw:///tmp/junit8438663766622186795/testdb.db/test_table, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:1, serdeInfo:SerDeInfo(name:test_table, serializationLib:org.apache.hadoop.hive.ql.io.orc.OrcSerde, parameters:{serialization.format=1}), bucketCols:[], sortCols:null, parameters:null), partitionKeys:[FieldSchema(name:city, type:string, comment:), FieldSchema(name:state, type:string, comment:)], parameters:{}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
29725 [main] WARN  o.a.h.h.m.HiveMetaStore - Location: raw:///tmp/junit8438663766622186795/testdb.db/test_table specified for non-external table:test_table
29725 [main] INFO  o.a.h.h.c.FileUtils - Creating directory if it doesn't exist: raw:/tmp/junit8438663766622186795/testdb.db/test_table
29777 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: add_partition : db=testdb tbl=test_table
29777 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=add_partition : db=testdb tbl=test_table	
29820 [main] INFO  o.a.h.h.c.FileUtils - Creating directory if it doesn't exist: raw:/tmp/junit8438663766622186795/testdb.db/test_table/city=sunnyvale/state=ca
29873 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Shutting down the object store...
29873 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=Shutting down the object store...	
29873 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Metastore shutdown complete.
29873 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=Metastore shutdown complete.	
done

--------------------------------------------------
Checking ./external/storm-hive/target/surefire-reports/TEST-org.apache.storm.hive.common.TestHiveWriter.xml
--------------------------------------------------
classname: org.apache.storm.hive.common.TestHiveWriter / testname: testWriteBasic
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hive.common.TestHiveWriter.generateTestTuple(TestHiveWriter.java:164)
	at org.apache.storm.hive.common.TestHiveWriter.writeTuples(TestHiveWriter.java:179)
	at org.apache.storm.hive.common.TestHiveWriter.testWriteBasic(TestHiveWriter.java:127)

-------------------- system-out --------------------
30423 [main] INFO  o.a.h.h.q.s.SessionState - Created local directory: /tmp/15db4204-98b2-4616-8234-1f9cd24b1b5a_resources
30425 [main] INFO  o.a.h.h.q.s.SessionState - Created HDFS directory: /tmp/hive/travis/15db4204-98b2-4616-8234-1f9cd24b1b5a
30426 [main] INFO  o.a.h.h.q.s.SessionState - Created local directory: /tmp/travis/15db4204-98b2-4616-8234-1f9cd24b1b5a
30428 [main] INFO  o.a.h.h.q.s.SessionState - Created HDFS directory: /tmp/hive/travis/15db4204-98b2-4616-8234-1f9cd24b1b5a/_tmp_space.db
30428 [main] INFO  o.a.h.h.q.s.SessionState - No Tez session required at this point. hive.execution.engine=mr.
30429 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_table_names_by_filter: db = testdb, filter = 
30429 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table_names_by_filter: db = testdb, filter = 	
30429 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
30430 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
30431 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
30432 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
30432 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
30457 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_table : db=testdb tbl=test_table
30457 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table : db=testdb tbl=test_table	
30468 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: drop_table : db=testdb tbl=test_table
30468 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=drop_table : db=testdb tbl=test_table	
30758 [main] INFO  h.m.hivemetastoressimpl - deleting  raw:/tmp/junit8438663766622186795/testdb.db/test_table
30758 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_database: testdb
30758 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_database: testdb	
30763 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: drop_database: testdb
30763 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=drop_database: testdb	
30764 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_all_tables: db=testdb
30764 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_all_tables: db=testdb	
30767 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_functions: db=testdb pat=*
30767 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_functions: db=testdb pat=*	
30770 [main] INFO  o.a.h.h.m.ObjectStore - Dropping database testdb along with all tables
30781 [main] INFO  h.m.hivemetastoressimpl - deleting  raw:/tmp/junit8438663766622186795/testdb.db
30782 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
30783 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
30784 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
30784 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
30784 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: create_database: Database(name:testdb, description:null, locationUri:/tmp/junit7830132588042215269/testdb.db, parameters:null)
30784 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=create_database: Database(name:testdb, description:null, locationUri:/tmp/junit7830132588042215269/testdb.db, parameters:null)	
30784 [main] WARN  o.a.h.h.m.ObjectStore - Failed to get database testdb, returning NoSuchObjectException
30785 [main] INFO  o.a.h.h.c.FileUtils - Creating directory if it doesn't exist: file:/tmp/junit7830132588042215269/testdb.db
30788 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: create_table: Table(tableName:test_table2, dbName:testdb, owner:null, createTime:0, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:), FieldSchema(name:msg, type:string, comment:)], location:/tmp/junit7830132588042215269/testdb.db/test_table2, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:1, serdeInfo:SerDeInfo(name:test_table2, serializationLib:org.apache.hadoop.hive.ql.io.orc.OrcSerde, parameters:{serialization.format=1}), bucketCols:[], sortCols:null, parameters:null), partitionKeys:[FieldSchema(name:city, type:string, comment:), FieldSchema(name:state, type:string, comment:)], parameters:{}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)
30788 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=create_table: Table(tableName:test_table2, dbName:testdb, owner:null, createTime:0, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:), FieldSchema(name:msg, type:string, comment:)], location:/tmp/junit7830132588042215269/testdb.db/test_table2, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:1, serdeInfo:SerDeInfo(name:test_table2, serializationLib:org.apache.hadoop.hive.ql.io.orc.OrcSerde, parameters:{serialization.format=1}), bucketCols:[], sortCols:null, parameters:null), partitionKeys:[FieldSchema(name:city, type:string, comment:), FieldSchema(name:state, type:string, comment:)], parameters:{}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
30789 [main] WARN  o.a.h.h.m.HiveMetaStore - Location: /tmp/junit7830132588042215269/testdb.db/test_table2 specified for non-external table:test_table2
30789 [main] INFO  o.a.h.h.c.FileUtils - Creating directory if it doesn't exist: file:/tmp/junit7830132588042215269/testdb.db/test_table2
30827 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: add_partition : db=testdb tbl=test_table2
30827 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=add_partition : db=testdb tbl=test_table2	
30856 [main] INFO  o.a.h.h.c.FileUtils - Creating directory if it doesn't exist: file:/tmp/junit7830132588042215269/testdb.db/test_table2/city=sunnyvale/state=ca
30877 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Shutting down the object store...
30877 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=Shutting down the object store...	
30878 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Metastore shutdown complete.
30878 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=Metastore shutdown complete.	
30945 [hiveWriterTest] INFO  o.a.h.h.q.s.SessionState - Created local directory: /tmp/373ee111-c7ef-4816-83ce-cf198fb7e86f_resources
30947 [hiveWriterTest] INFO  o.a.h.h.q.s.SessionState - Created HDFS directory: /tmp/hive/travis/373ee111-c7ef-4816-83ce-cf198fb7e86f
30949 [hiveWriterTest] INFO  o.a.h.h.q.s.SessionState - Created local directory: /tmp/travis/373ee111-c7ef-4816-83ce-cf198fb7e86f
30950 [hiveWriterTest] INFO  o.a.h.h.q.s.SessionState - Created HDFS directory: /tmp/hive/travis/373ee111-c7ef-4816-83ce-cf198fb7e86f/_tmp_space.db
30950 [hiveWriterTest] INFO  o.a.h.h.q.s.SessionState - No Tez session required at this point. hive.execution.engine=mr.
30951 [hiveWriterTest] INFO  o.a.h.h.m.HiveMetaStore - 1: get_table : db=testdb tbl=test_table2
30951 [hiveWriterTest] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table : db=testdb tbl=test_table2	
30951 [hiveWriterTest] INFO  o.a.h.h.m.HiveMetaStore - 1: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
30952 [hiveWriterTest] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
30953 [hiveWriterTest] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
30966 [hiveWriterTest] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
30966 [hiveWriterTest] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
30991 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=Driver.run from=org.apache.hadoop.hive.ql.Driver>
30991 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=TimeToSubmit from=org.apache.hadoop.hive.ql.Driver>
30991 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=compile from=org.apache.hadoop.hive.ql.Driver>
30992 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=parse from=org.apache.hadoop.hive.ql.Driver>
30992 [hiveWriterTest] INFO  h.q.p.ParseDriver - Parsing command: use testdb
30993 [hiveWriterTest] INFO  h.q.p.ParseDriver - Parse Completed
30993 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=parse start=1466699835166 end=1466699835167 duration=1 from=org.apache.hadoop.hive.ql.Driver>
30999 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=semanticAnalyze from=org.apache.hadoop.hive.ql.Driver>
31004 [hiveWriterTest] INFO  o.a.h.h.m.HiveMetaStore - 1: get_database: testdb
31004 [hiveWriterTest] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_database: testdb	
31025 [hiveWriterTest] INFO  o.a.h.h.q.Driver - Semantic Analysis Completed
31025 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=semanticAnalyze start=1466699835173 end=1466699835199 duration=26 from=org.apache.hadoop.hive.ql.Driver>
31025 [hiveWriterTest] INFO  o.a.h.h.q.Driver - Returning Hive schema: Schema(fieldSchemas:null, properties:null)
31025 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=compile start=1466699835165 end=1466699835199 duration=34 from=org.apache.hadoop.hive.ql.Driver>
31026 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=acquireReadWriteLocks from=org.apache.hadoop.hive.ql.Driver>
31028 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=acquireReadWriteLocks start=1466699835200 end=1466699835202 duration=2 from=org.apache.hadoop.hive.ql.Driver>
31028 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=Driver.execute from=org.apache.hadoop.hive.ql.Driver>
31028 [hiveWriterTest] INFO  o.a.h.h.q.Driver - Starting command: use testdb
31030 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=TimeToSubmit start=1466699835165 end=1466699835204 duration=39 from=org.apache.hadoop.hive.ql.Driver>
31030 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=runTasks from=org.apache.hadoop.hive.ql.Driver>
31030 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=task.DDL.Stage-0 from=org.apache.hadoop.hive.ql.Driver>
31034 [hiveWriterTest] INFO  o.a.h.h.q.Driver - Starting task [Stage-0:DDL] in serial mode
31034 [hiveWriterTest] INFO  o.a.h.h.m.HiveMetaStore - 1: get_database: testdb
31034 [hiveWriterTest] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_database: testdb	
31035 [hiveWriterTest] INFO  o.a.h.h.m.HiveMetaStore - 1: get_database: testdb
31035 [hiveWriterTest] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_database: testdb	
31037 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=runTasks start=1466699835204 end=1466699835211 duration=7 from=org.apache.hadoop.hive.ql.Driver>
31037 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=Driver.execute start=1466699835202 end=1466699835211 duration=9 from=org.apache.hadoop.hive.ql.Driver>
31038 [hiveWriterTest] INFO  o.a.h.h.q.Driver - OK
31038 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=releaseLocks from=org.apache.hadoop.hive.ql.Driver>
31038 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=releaseLocks start=1466699835212 end=1466699835212 duration=0 from=org.apache.hadoop.hive.ql.Driver>
31038 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=Driver.run start=1466699835165 end=1466699835212 duration=47 from=org.apache.hadoop.hive.ql.Driver>
31038 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=Driver.run from=org.apache.hadoop.hive.ql.Driver>
31038 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=TimeToSubmit from=org.apache.hadoop.hive.ql.Driver>
31038 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=compile from=org.apache.hadoop.hive.ql.Driver>
31039 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=parse from=org.apache.hadoop.hive.ql.Driver>
31039 [hiveWriterTest] INFO  h.q.p.ParseDriver - Parsing command: alter table test_table2 add if not exists partition  ( city='sunnyvale',state='ca' )
31042 [hiveWriterTest] INFO  h.q.p.ParseDriver - Parse Completed
31042 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=parse start=1466699835213 end=1466699835216 duration=3 from=org.apache.hadoop.hive.ql.Driver>
31044 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=semanticAnalyze from=org.apache.hadoop.hive.ql.Driver>
31045 [hiveWriterTest] INFO  o.a.h.h.m.HiveMetaStore - 1: get_table : db=testdb tbl=test_table2
31045 [hiveWriterTest] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table : db=testdb tbl=test_table2	
31068 [hiveWriterTest] INFO  o.a.h.h.q.Driver - Semantic Analysis Completed
31068 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=semanticAnalyze start=1466699835218 end=1466699835242 duration=24 from=org.apache.hadoop.hive.ql.Driver>
31068 [hiveWriterTest] INFO  o.a.h.h.q.Driver - Returning Hive schema: Schema(fieldSchemas:null, properties:null)
31068 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=compile start=1466699835212 end=1466699835242 duration=30 from=org.apache.hadoop.hive.ql.Driver>
31068 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=acquireReadWriteLocks from=org.apache.hadoop.hive.ql.Driver>
31104 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=acquireReadWriteLocks start=1466699835242 end=1466699835278 duration=36 from=org.apache.hadoop.hive.ql.Driver>
31104 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=Driver.execute from=org.apache.hadoop.hive.ql.Driver>
31104 [hiveWriterTest] INFO  o.a.h.h.q.Driver - Starting command: alter table test_table2 add if not exists partition  ( city='sunnyvale',state='ca' )
31105 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=TimeToSubmit start=1466699835212 end=1466699835279 duration=67 from=org.apache.hadoop.hive.ql.Driver>
31105 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=runTasks from=org.apache.hadoop.hive.ql.Driver>
31105 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=task.DDL.Stage-0 from=org.apache.hadoop.hive.ql.Driver>
31105 [hiveWriterTest] INFO  o.a.h.h.q.Driver - Starting task [Stage-0:DDL] in serial mode
31105 [hiveWriterTest] INFO  o.a.h.h.m.HiveMetaStore - 1: get_table : db=testdb tbl=test_table2
31105 [hiveWriterTest] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table : db=testdb tbl=test_table2	
31117 [hiveWriterTest] INFO  o.a.h.h.m.HiveMetaStore - 1: add_partitions
31118 [hiveWriterTest] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=add_partitions	
31128 [hiveWriterTest] INFO  o.a.h.h.m.HiveMetaStore - Not adding partition Partition(values:[sunnyvale, ca], dbName:testdb, tableName:test_table2, createTime:0, lastAccessTime:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:), FieldSchema(name:msg, type:string, comment:)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:1, serdeInfo:SerDeInfo(name:test_table2, serializationLib:org.apache.hadoop.hive.ql.io.orc.OrcSerde, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), parameters:null) as it already exists
31129 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=runTasks start=1466699835279 end=1466699835303 duration=24 from=org.apache.hadoop.hive.ql.Driver>
31129 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=Driver.execute start=1466699835278 end=1466699835303 duration=25 from=org.apache.hadoop.hive.ql.Driver>
31129 [hiveWriterTest] INFO  o.a.h.h.q.Driver - OK
31129 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=releaseLocks from=org.apache.hadoop.hive.ql.Driver>
31136 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=releaseLocks start=1466699835303 end=1466699835310 duration=7 from=org.apache.hadoop.hive.ql.Driver>
31137 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=Driver.run start=1466699835212 end=1466699835311 duration=99 from=org.apache.hadoop.hive.ql.Driver>
31207 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_table : db=testdb tbl=test_table2
31207 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table : db=testdb tbl=test_table2	
31207 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
31208 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
31208 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
31209 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
31210 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
31219 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_partition : db=testdb tbl=test_table2[sunnyvale,ca]
31219 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_partition : db=testdb tbl=test_table2[sunnyvale,ca]	
31255 [hiveWriterTest] INFO  o.a.h.h.m.HiveMetaStore - 1: get_table : db=testdb tbl=test_table2
31255 [hiveWriterTest] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table : db=testdb tbl=test_table2	

-------------------- system-err --------------------
OK
OK

--------------------------------------------------
--------------------------------------------------
classname: org.apache.storm.hive.common.TestHiveWriter / testname: testWriteMultiFlush
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hive.common.TestHiveWriter.generateTestTuple(TestHiveWriter.java:164)
	at org.apache.storm.hive.common.TestHiveWriter.testWriteMultiFlush(TestHiveWriter.java:142)

-------------------- system-out --------------------
31759 [main] INFO  o.a.h.h.q.s.SessionState - Created local directory: /tmp/fca6544e-719c-467d-b928-d10cafd531d2_resources
31761 [main] INFO  o.a.h.h.q.s.SessionState - Created HDFS directory: /tmp/hive/travis/fca6544e-719c-467d-b928-d10cafd531d2
31763 [main] INFO  o.a.h.h.q.s.SessionState - Created local directory: /tmp/travis/fca6544e-719c-467d-b928-d10cafd531d2
31765 [main] INFO  o.a.h.h.q.s.SessionState - Created HDFS directory: /tmp/hive/travis/fca6544e-719c-467d-b928-d10cafd531d2/_tmp_space.db
31765 [main] INFO  o.a.h.h.q.s.SessionState - No Tez session required at this point. hive.execution.engine=mr.
31766 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
31767 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
31769 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
31769 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
31769 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_table_names_by_filter: db = testdb, filter = 
31769 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table_names_by_filter: db = testdb, filter = 	
31774 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_table : db=testdb tbl=test_table2
31774 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table : db=testdb tbl=test_table2	
31805 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: drop_table : db=testdb tbl=test_table2
31805 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=drop_table : db=testdb tbl=test_table2	
32027 [main] INFO  h.m.hivemetastoressimpl - deleting  file:/tmp/junit7830132588042215269/testdb.db/test_table2
32028 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_database: testdb
32028 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_database: testdb	
32035 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: drop_database: testdb
32035 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=drop_database: testdb	
32036 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_all_tables: db=testdb
32036 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_all_tables: db=testdb	
32040 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_functions: db=testdb pat=*
32041 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_functions: db=testdb pat=*	
32045 [main] INFO  o.a.h.h.m.ObjectStore - Dropping database testdb along with all tables
32074 [main] INFO  h.m.hivemetastoressimpl - deleting  file:/tmp/junit7830132588042215269/testdb.db
32075 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
32075 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
32082 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
32082 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
32083 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: create_database: Database(name:testdb, description:null, locationUri:/tmp/junit8306920894572951850/testdb.db, parameters:null)
32083 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=create_database: Database(name:testdb, description:null, locationUri:/tmp/junit8306920894572951850/testdb.db, parameters:null)	
32083 [main] WARN  o.a.h.h.m.ObjectStore - Failed to get database testdb, returning NoSuchObjectException
32084 [main] INFO  o.a.h.h.c.FileUtils - Creating directory if it doesn't exist: file:/tmp/junit8306920894572951850/testdb.db
32113 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: create_table: Table(tableName:test_table2, dbName:testdb, owner:null, createTime:0, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:), FieldSchema(name:msg, type:string, comment:)], location:/tmp/junit8306920894572951850/testdb.db/test_table2, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:1, serdeInfo:SerDeInfo(name:test_table2, serializationLib:org.apache.hadoop.hive.ql.io.orc.OrcSerde, parameters:{serialization.format=1}), bucketCols:[], sortCols:null, parameters:null), partitionKeys:[FieldSchema(name:city, type:string, comment:), FieldSchema(name:state, type:string, comment:)], parameters:{}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)
32113 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=create_table: Table(tableName:test_table2, dbName:testdb, owner:null, createTime:0, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:), FieldSchema(name:msg, type:string, comment:)], location:/tmp/junit8306920894572951850/testdb.db/test_table2, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:1, serdeInfo:SerDeInfo(name:test_table2, serializationLib:org.apache.hadoop.hive.ql.io.orc.OrcSerde, parameters:{serialization.format=1}), bucketCols:[], sortCols:null, parameters:null), partitionKeys:[FieldSchema(name:city, type:string, comment:), FieldSchema(name:state, type:string, comment:)], parameters:{}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
32114 [main] WARN  o.a.h.h.m.HiveMetaStore - Location: /tmp/junit8306920894572951850/testdb.db/test_table2 specified for non-external table:test_table2
32115 [main] INFO  o.a.h.h.c.FileUtils - Creating directory if it doesn't exist: file:/tmp/junit8306920894572951850/testdb.db/test_table2
32186 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: add_partition : db=testdb tbl=test_table2
32187 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=add_partition : db=testdb tbl=test_table2	
32280 [main] INFO  o.a.h.h.c.FileUtils - Creating directory if it doesn't exist: file:/tmp/junit8306920894572951850/testdb.db/test_table2/city=sunnyvale/state=ca
32324 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Shutting down the object store...
32325 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=Shutting down the object store...	
32325 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Metastore shutdown complete.
32325 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=Metastore shutdown complete.	
32371 [hiveWriterTest] INFO  o.a.h.h.q.s.SessionState - Created local directory: /tmp/b3460929-2873-4770-93a0-ec3eb0386661_resources
32372 [hiveWriterTest] INFO  o.a.h.h.q.s.SessionState - Created HDFS directory: /tmp/hive/travis/b3460929-2873-4770-93a0-ec3eb0386661
32378 [hiveWriterTest] INFO  o.a.h.h.q.s.SessionState - Created local directory: /tmp/travis/b3460929-2873-4770-93a0-ec3eb0386661
32512 [hiveWriterTest] INFO  o.a.h.h.q.s.SessionState - Created HDFS directory: /tmp/hive/travis/b3460929-2873-4770-93a0-ec3eb0386661/_tmp_space.db
32513 [hiveWriterTest] INFO  o.a.h.h.q.s.SessionState - No Tez session required at this point. hive.execution.engine=mr.
32513 [hiveWriterTest] INFO  o.a.h.h.m.HiveMetaStore - 2: get_table : db=testdb tbl=test_table2
32513 [hiveWriterTest] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table : db=testdb tbl=test_table2	
32513 [hiveWriterTest] INFO  o.a.h.h.m.HiveMetaStore - 2: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
32514 [hiveWriterTest] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
32515 [hiveWriterTest] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
32518 [hiveWriterTest] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
32518 [hiveWriterTest] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
32544 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=Driver.run from=org.apache.hadoop.hive.ql.Driver>
32544 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=TimeToSubmit from=org.apache.hadoop.hive.ql.Driver>
32544 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=compile from=org.apache.hadoop.hive.ql.Driver>
32545 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=parse from=org.apache.hadoop.hive.ql.Driver>
32545 [hiveWriterTest] INFO  h.q.p.ParseDriver - Parsing command: use testdb
32545 [hiveWriterTest] INFO  h.q.p.ParseDriver - Parse Completed
32545 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=parse start=1466699836719 end=1466699836719 duration=0 from=org.apache.hadoop.hive.ql.Driver>
32552 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=semanticAnalyze from=org.apache.hadoop.hive.ql.Driver>
32552 [hiveWriterTest] INFO  o.a.h.h.m.HiveMetaStore - 2: get_database: testdb
32552 [hiveWriterTest] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_database: testdb	
32553 [hiveWriterTest] INFO  o.a.h.h.q.Driver - Semantic Analysis Completed
32553 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=semanticAnalyze start=1466699836726 end=1466699836727 duration=1 from=org.apache.hadoop.hive.ql.Driver>
32553 [hiveWriterTest] INFO  o.a.h.h.q.Driver - Returning Hive schema: Schema(fieldSchemas:null, properties:null)
32553 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=compile start=1466699836718 end=1466699836727 duration=9 from=org.apache.hadoop.hive.ql.Driver>
32553 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=acquireReadWriteLocks from=org.apache.hadoop.hive.ql.Driver>
32553 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=acquireReadWriteLocks start=1466699836727 end=1466699836727 duration=0 from=org.apache.hadoop.hive.ql.Driver>
32554 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=Driver.execute from=org.apache.hadoop.hive.ql.Driver>
32554 [hiveWriterTest] INFO  o.a.h.h.q.Driver - Starting command: use testdb
32554 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=TimeToSubmit start=1466699836718 end=1466699836728 duration=10 from=org.apache.hadoop.hive.ql.Driver>
32554 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=runTasks from=org.apache.hadoop.hive.ql.Driver>
32554 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=task.DDL.Stage-0 from=org.apache.hadoop.hive.ql.Driver>
32554 [hiveWriterTest] INFO  o.a.h.h.q.Driver - Starting task [Stage-0:DDL] in serial mode
32554 [hiveWriterTest] INFO  o.a.h.h.m.HiveMetaStore - 2: get_database: testdb
32554 [hiveWriterTest] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_database: testdb	
32555 [hiveWriterTest] INFO  o.a.h.h.m.HiveMetaStore - 2: get_database: testdb
32555 [hiveWriterTest] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_database: testdb	
32556 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=runTasks start=1466699836728 end=1466699836730 duration=2 from=org.apache.hadoop.hive.ql.Driver>
32556 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=Driver.execute start=1466699836728 end=1466699836730 duration=2 from=org.apache.hadoop.hive.ql.Driver>
32556 [hiveWriterTest] INFO  o.a.h.h.q.Driver - OK
32556 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=releaseLocks from=org.apache.hadoop.hive.ql.Driver>
32556 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=releaseLocks start=1466699836730 end=1466699836730 duration=0 from=org.apache.hadoop.hive.ql.Driver>
32556 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=Driver.run start=1466699836718 end=1466699836730 duration=12 from=org.apache.hadoop.hive.ql.Driver>
32556 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=Driver.run from=org.apache.hadoop.hive.ql.Driver>
32556 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=TimeToSubmit from=org.apache.hadoop.hive.ql.Driver>
32556 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=compile from=org.apache.hadoop.hive.ql.Driver>
32557 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=parse from=org.apache.hadoop.hive.ql.Driver>
32557 [hiveWriterTest] INFO  h.q.p.ParseDriver - Parsing command: alter table test_table2 add if not exists partition  ( city='sunnyvale',state='ca' )
32557 [hiveWriterTest] INFO  h.q.p.ParseDriver - Parse Completed
32557 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=parse start=1466699836731 end=1466699836731 duration=0 from=org.apache.hadoop.hive.ql.Driver>
32565 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=semanticAnalyze from=org.apache.hadoop.hive.ql.Driver>
32565 [hiveWriterTest] INFO  o.a.h.h.m.HiveMetaStore - 2: get_table : db=testdb tbl=test_table2
32565 [hiveWriterTest] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table : db=testdb tbl=test_table2	
32590 [hiveWriterTest] INFO  o.a.h.h.q.Driver - Semantic Analysis Completed
32602 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=semanticAnalyze start=1466699836739 end=1466699836776 duration=37 from=org.apache.hadoop.hive.ql.Driver>
32602 [hiveWriterTest] INFO  o.a.h.h.q.Driver - Returning Hive schema: Schema(fieldSchemas:null, properties:null)
32602 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=compile start=1466699836730 end=1466699836776 duration=46 from=org.apache.hadoop.hive.ql.Driver>
32602 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=acquireReadWriteLocks from=org.apache.hadoop.hive.ql.Driver>
33763 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=acquireReadWriteLocks start=1466699836776 end=1466699837937 duration=1161 from=org.apache.hadoop.hive.ql.Driver>
33763 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=Driver.execute from=org.apache.hadoop.hive.ql.Driver>
33763 [hiveWriterTest] INFO  o.a.h.h.q.Driver - Starting command: alter table test_table2 add if not exists partition  ( city='sunnyvale',state='ca' )
33763 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=TimeToSubmit start=1466699836730 end=1466699837937 duration=1207 from=org.apache.hadoop.hive.ql.Driver>
33763 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=runTasks from=org.apache.hadoop.hive.ql.Driver>
33763 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=task.DDL.Stage-0 from=org.apache.hadoop.hive.ql.Driver>
33763 [hiveWriterTest] INFO  o.a.h.h.q.Driver - Starting task [Stage-0:DDL] in serial mode
33763 [hiveWriterTest] INFO  o.a.h.h.m.HiveMetaStore - 2: get_table : db=testdb tbl=test_table2
33763 [hiveWriterTest] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table : db=testdb tbl=test_table2	
33770 [hiveWriterTest] INFO  o.a.h.h.m.HiveMetaStore - 2: add_partitions
33770 [hiveWriterTest] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=add_partitions	
33781 [hiveWriterTest] INFO  o.a.h.h.m.HiveMetaStore - Not adding partition Partition(values:[sunnyvale, ca], dbName:testdb, tableName:test_table2, createTime:0, lastAccessTime:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:), FieldSchema(name:msg, type:string, comment:)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:1, serdeInfo:SerDeInfo(name:test_table2, serializationLib:org.apache.hadoop.hive.ql.io.orc.OrcSerde, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), parameters:null) as it already exists
33782 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=runTasks start=1466699837937 end=1466699837956 duration=19 from=org.apache.hadoop.hive.ql.Driver>
33782 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=Driver.execute start=1466699837937 end=1466699837956 duration=19 from=org.apache.hadoop.hive.ql.Driver>
33783 [hiveWriterTest] INFO  o.a.h.h.q.Driver - OK
33783 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=releaseLocks from=org.apache.hadoop.hive.ql.Driver>
34844 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=releaseLocks start=1466699837957 end=1466699839018 duration=1061 from=org.apache.hadoop.hive.ql.Driver>
34844 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=Driver.run start=1466699836730 end=1466699839018 duration=2288 from=org.apache.hadoop.hive.ql.Driver>
34873 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_table : db=testdb tbl=test_table2
34873 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table : db=testdb tbl=test_table2	
34873 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
34874 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
34874 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
34876 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
34876 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
34889 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_partition : db=testdb tbl=test_table2[sunnyvale,ca]
34889 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_partition : db=testdb tbl=test_table2[sunnyvale,ca]	
34902 [hiveWriterTest] INFO  o.a.h.h.m.HiveMetaStore - 2: get_table : db=testdb tbl=test_table2
34902 [hiveWriterTest] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table : db=testdb tbl=test_table2	

-------------------- system-err --------------------
OK
OK

--------------------------------------------------
Looking for errors in ./external/storm-jdbc/target/surefire-reports
Checking ./external/storm-jdbc/target/surefire-reports/TEST-org.apache.storm.jdbc.common.UtilTest.xml
Checking ./external/storm-jdbc/target/surefire-reports/TEST-org.apache.storm.jdbc.common.JdbcClientTest.xml
Checking ./external/storm-jdbc/target/surefire-reports/TEST-org.apache.storm.jdbc.bolt.JdbcLookupBoltTest.xml
Checking ./external/storm-jdbc/target/surefire-reports/TEST-org.apache.storm.jdbc.bolt.JdbcInsertBoltTest.xml
Looking for errors in ./external/storm-kafka/target/surefire-reports
Checking ./external/storm-kafka/target/surefire-reports/TEST-org.apache.storm.kafka.TestStringScheme.xml
Checking ./external/storm-kafka/target/surefire-reports/TEST-org.apache.storm.kafka.StringKeyValueSchemeTest.xml
Checking ./external/storm-kafka/target/surefire-reports/TEST-org.apache.storm.kafka.DynamicBrokersReaderTest.xml
Checking ./external/storm-kafka/target/surefire-reports/TEST-org.apache.storm.kafka.KafkaUtilsTest.xml
Checking ./external/storm-kafka/target/surefire-reports/TEST-org.apache.storm.kafka.KafkaErrorTest.xml
Checking ./external/storm-kafka/target/surefire-reports/TEST-org.apache.storm.kafka.bolt.KafkaBoltTest.xml
--------------------------------------------------
classname: org.apache.storm.kafka.bolt.KafkaBoltTest / testname: executeWithBrokerDown
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.kafka.bolt.KafkaBoltTest.generateTestTuple(KafkaBoltTest.java:301)
	at org.apache.storm.kafka.bolt.KafkaBoltTest.executeWithBrokerDown(KafkaBoltTest.java:266)

-------------------- system-out --------------------
16:34:23.281 [Curator-Framework-0-SendThread(127.0.0.1:54974)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:34:23.991 [Curator-Framework-0-SendThread(127.0.0.1:34460)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:34:24.014 [Curator-Framework-0-SendThread(127.0.0.1:44710)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:34:24.030 [Curator-Framework-0-SendThread(127.0.0.1:59679)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:34:24.079 [Curator-Framework-0-SendThread(127.0.0.1:58965)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:34:24.159 [Curator-Framework-0-SendThread(127.0.0.1:44186)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:34:24.160 [main] INFO  o.a.c.f.i.CuratorFrameworkImpl - Starting
16:34:24.162 [main] INFO  k.u.VerifiableProperties - Verifying properties
16:34:24.162 [main] INFO  k.u.VerifiableProperties - Property broker.id is overridden to 0
16:34:24.162 [main] INFO  k.u.VerifiableProperties - Property log.dirs is overridden to /tmp/kafka/logs/kafka-test-33897
16:34:24.162 [main] INFO  k.u.VerifiableProperties - Property port is overridden to 33897
16:34:24.162 [main] INFO  k.u.VerifiableProperties - Property zookeeper.connect is overridden to 127.0.0.1:56778
16:34:24.162 [main] INFO  k.s.KafkaServer - [Kafka Server 0], starting
16:34:24.162 [main] INFO  k.s.KafkaServer - [Kafka Server 0], Connecting to zookeeper on 127.0.0.1:56778
16:34:24.163 [ZkClient-EventThread-833-127.0.0.1:56778] INFO  o.I.z.ZkEventThread - Starting ZkClient event thread.
16:34:24.382 [Curator-Framework-0-SendThread(127.0.0.1:54974)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:34:25.092 [Curator-Framework-0-SendThread(127.0.0.1:34460)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:34:25.115 [Curator-Framework-0-SendThread(127.0.0.1:44710)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:34:25.130 [Curator-Framework-0-SendThread(127.0.0.1:59679)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:34:25.180 [Curator-Framework-0-SendThread(127.0.0.1:58965)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:34:25.260 [Curator-Framework-0-SendThread(127.0.0.1:44186)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:34:25.483 [Curator-Framework-0-SendThread(127.0.0.1:54974)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:34:25.541 [SyncThread:0] WARN  o.a.z.s.p.FileTxnLog - fsync-ing the write ahead log in SyncThread:0 took 1376ms which will adversely effect operation latency. See the ZooKeeper troubleshooting guide
16:34:25.541 [main-EventThread] INFO  o.a.c.f.s.ConnectionStateManager - State change: CONNECTED
16:34:25.542 [main-EventThread] INFO  o.I.z.ZkClient - zookeeper state changed (SyncConnected)
16:34:25.649 [main] INFO  k.l.LogManager - Log directory '/tmp/kafka/logs/kafka-test-33897' not found, creating it.
16:34:25.650 [main] INFO  k.l.LogManager - Loading logs.
16:34:25.650 [main] INFO  k.l.LogManager - Logs loading complete.
16:34:25.650 [main] INFO  k.l.LogManager - Starting log cleanup with a period of 300000 ms.
16:34:25.652 [main] INFO  k.l.LogManager - Starting log flusher with a default period of 9223372036854775807 ms.
16:34:25.654 [main] INFO  k.n.Acceptor - Awaiting socket connections on 0.0.0.0:33897.
16:34:25.669 [main] INFO  k.n.SocketServer - [Socket Server on Broker 0], Started
16:34:25.698 [main] INFO  k.u.Mx4jLoader$ - Will not load MX4J, mx4j-tools.jar is not in the classpath
16:34:25.698 [main] INFO  k.c.KafkaController - [Controller 0]: Controller starting up
16:34:25.708 [main] INFO  k.s.ZookeeperLeaderElector - 0 successfully elected as leader
16:34:25.709 [main] INFO  k.c.KafkaController - [Controller 0]: Broker 0 starting become controller state transition
16:34:25.739 [main] INFO  k.c.KafkaController - [Controller 0]: Controller 0 incremented epoch to 1
16:34:25.743 [main] INFO  k.c.KafkaController - [Controller 0]: Partitions undergoing preferred replica election: 
16:34:25.743 [main] INFO  k.c.KafkaController - [Controller 0]: Partitions that completed preferred replica election: 
16:34:25.743 [main] INFO  k.c.KafkaController - [Controller 0]: Resuming preferred replica election for partitions: 
16:34:25.744 [main] INFO  k.c.KafkaController - [Controller 0]: Partitions being reassigned: Map()
16:34:25.744 [main] INFO  k.c.KafkaController - [Controller 0]: Partitions already reassigned: List()
16:34:25.744 [main] INFO  k.c.KafkaController - [Controller 0]: Resuming reassignment of partitions: Map()
16:34:25.744 [main] INFO  k.c.KafkaController - [Controller 0]: List of topics to be deleted: 
16:34:25.744 [main] INFO  k.c.KafkaController - [Controller 0]: List of topics ineligible for deletion: 
16:34:25.744 [main] INFO  k.c.KafkaController - [Controller 0]: Currently active brokers in the cluster: Set()
16:34:25.744 [main] INFO  k.c.KafkaController - [Controller 0]: Currently shutting brokers in the cluster: Set()
16:34:25.744 [main] INFO  k.c.KafkaController - [Controller 0]: Current list of topics in the cluster: Set()
16:34:25.744 [main] INFO  k.c.ReplicaStateMachine - [Replica state machine on controller 0]: Started replica state machine with initial state -> Map()
16:34:25.744 [main] INFO  k.c.PartitionStateMachine - [Partition state machine on Controller 0]: Started partition state machine with initial state -> Map()
16:34:25.744 [main] INFO  k.c.KafkaController - [Controller 0]: Broker 0 is ready to serve as the new controller with epoch 1
16:34:25.745 [main] INFO  k.c.KafkaController - [Controller 0]: Starting preferred replica leader election for partitions 
16:34:25.745 [main] INFO  k.c.PartitionStateMachine - [Partition state machine on Controller 0]: Invoking state change to OnlinePartition for partitions 
16:34:25.749 [main] INFO  k.c.KafkaController - [Controller 0]: starting the partition rebalance scheduler
16:34:25.749 [main] INFO  k.c.KafkaController - [Controller 0]: Controller startup complete
16:34:25.750 [ZkClient-EventThread-833-127.0.0.1:56778] INFO  k.s.ZookeeperLeaderElector$LeaderChangeListener - New leader is 0
16:34:25.753 [main] INFO  k.u.ZkUtils$ - Registered broker 0 at path /brokers/ids/0 with address testing-worker-linux-docker-a83abded-3380-linux-4.prod.travis-ci.org:33897.
16:34:25.753 [main] INFO  k.s.KafkaServer - [Kafka Server 0], started
16:34:25.757 [main] INFO  o.a.k.c.p.ProducerConfig - ProducerConfig values: 
	compression.type = none
	metric.reporters = []
	metadata.max.age.ms = 300000
	metadata.fetch.timeout.ms = 1000
	acks = 1
	batch.size = 16384
	reconnect.backoff.ms = 10
	bootstrap.servers = [localhost:33897]
	receive.buffer.bytes = 32768
	retry.backoff.ms = 100
	buffer.memory = 33554432
	timeout.ms = 30000
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	retries = 0
	max.request.size = 1048576
	block.on.buffer.full = true
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
	metrics.sample.window.ms = 30000
	send.buffer.bytes = 131072
	max.in.flight.requests.per.connection = 5
	metrics.num.samples = 2
	linger.ms = 0
	client.id = 

16:34:25.760 [main] INFO  k.s.KafkaServer - [Kafka Server 0], shutting down
16:34:25.760 [main] INFO  k.s.KafkaServer - [Kafka Server 0], Starting controlled shutdown
16:34:25.760 [ZkClient-EventThread-833-127.0.0.1:56778] INFO  k.c.ReplicaStateMachine$BrokerChangeListener - [BrokerChangeListener on Controller 0]: Broker change listener fired for path /brokers/ids with children 0
16:34:25.765 [ZkClient-EventThread-833-127.0.0.1:56778] INFO  k.c.ReplicaStateMachine$BrokerChangeListener - [BrokerChangeListener on Controller 0]: Newly added brokers: 0, deleted brokers: , all live brokers: 0
16:34:25.765 [ZkClient-EventThread-833-127.0.0.1:56778] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Controller 0 connected to id:0,host:testing-worker-linux-docker-a83abded-3380-linux-4.prod.travis-ci.org,port:33897 for sending state change requests
16:34:25.765 [ZkClient-EventThread-833-127.0.0.1:56778] INFO  k.c.KafkaController - [Controller 0]: New broker startup callback for 0
16:34:25.765 [kafka-request-handler-0] INFO  k.c.KafkaController - [Controller 0]: Shutting down broker 0
16:34:25.765 [Controller-0-to-broker-0-send-thread] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Starting 
16:34:25.766 [main] INFO  k.s.KafkaServer - [Kafka Server 0], Controlled shutdown succeeded
16:34:25.766 [main] INFO  k.n.SocketServer - [Socket Server on Broker 0], Shutting down
16:34:25.766 [kafka-network-thread-33897-0] INFO  k.n.Processor - Closing socket connection to /172.17.1.249.
16:34:25.767 [main] INFO  k.n.SocketServer - [Socket Server on Broker 0], Shutdown completed
16:34:25.767 [main] INFO  k.s.KafkaRequestHandlerPool - [Kafka Request Handler on Broker 0], shutting down
16:34:25.768 [main] INFO  k.s.KafkaRequestHandlerPool - [Kafka Request Handler on Broker 0], shut down completely
16:34:26.078 [main] INFO  k.s.ReplicaManager - [Replica Manager on Broker 0]: Shut down
16:34:26.078 [main] INFO  k.s.ReplicaFetcherManager - [ReplicaFetcherManager on broker 0] shutting down
16:34:26.078 [main] INFO  k.s.ReplicaFetcherManager - [ReplicaFetcherManager on broker 0] shutdown completed
16:34:26.078 [main] INFO  k.s.ReplicaManager - [Replica Manager on Broker 0]: Shut down completely
16:34:26.078 [main] INFO  k.l.LogManager - Shutting down.
16:34:26.078 [main] INFO  k.l.LogManager - Shutdown complete.
16:34:26.079 [main] INFO  k.c.PartitionStateMachine - [Partition state machine on Controller 0]: Stopped partition state machine
16:34:26.079 [main] INFO  k.c.ReplicaStateMachine - [Replica state machine on controller 0]: Stopped replica state machine
16:34:26.079 [main] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Shutting down
16:34:26.079 [Controller-0-to-broker-0-send-thread] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Stopped 
16:34:26.079 [main] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Shutdown completed
16:34:26.080 [ZkClient-EventThread-833-127.0.0.1:56778] INFO  o.I.z.ZkEventThread - Terminate ZkClient event thread.
16:34:26.083 [main] INFO  k.s.KafkaServer - [Kafka Server 0], shut down completed
16:34:26.083 [Curator-Framework-0] INFO  o.a.c.f.i.CuratorFrameworkImpl - backgroundOperationsLoop exiting

--------------------------------------------------
--------------------------------------------------
classname: org.apache.storm.kafka.bolt.KafkaBoltTest / testname: executeWithoutKey
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.kafka.bolt.KafkaBoltTest.generateTestTuple(KafkaBoltTest.java:301)
	at org.apache.storm.kafka.bolt.KafkaBoltTest.executeWithoutKey(KafkaBoltTest.java:255)

-------------------- system-out --------------------
16:34:28.394 [Curator-Framework-0-SendThread(127.0.0.1:34460)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:34:28.418 [Curator-Framework-0-SendThread(127.0.0.1:44710)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:34:28.433 [Curator-Framework-0-SendThread(127.0.0.1:59679)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:34:28.482 [Curator-Framework-0-SendThread(127.0.0.1:58965)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:34:28.563 [Curator-Framework-0-SendThread(127.0.0.1:44186)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:34:28.786 [Curator-Framework-0-SendThread(127.0.0.1:54974)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:34:28.925 [main] INFO  o.a.c.f.i.CuratorFrameworkImpl - Starting
16:34:28.926 [main] INFO  k.u.VerifiableProperties - Verifying properties
16:34:28.926 [main] INFO  k.u.VerifiableProperties - Property broker.id is overridden to 0
16:34:28.926 [main] INFO  k.u.VerifiableProperties - Property log.dirs is overridden to /tmp/kafka/logs/kafka-test-49790
16:34:28.926 [main] INFO  k.u.VerifiableProperties - Property port is overridden to 49790
16:34:28.927 [main] INFO  k.u.VerifiableProperties - Property zookeeper.connect is overridden to 127.0.0.1:41577
16:34:28.927 [main] INFO  k.s.KafkaServer - [Kafka Server 0], starting
16:34:28.927 [main] INFO  k.s.KafkaServer - [Kafka Server 0], Connecting to zookeeper on 127.0.0.1:41577
16:34:28.927 [ZkClient-EventThread-905-127.0.0.1:41577] INFO  o.I.z.ZkEventThread - Starting ZkClient event thread.
16:34:28.993 [main-EventThread] INFO  o.a.c.f.s.ConnectionStateManager - State change: CONNECTED
16:34:29.007 [main-EventThread] INFO  o.I.z.ZkClient - zookeeper state changed (SyncConnected)
16:34:29.034 [main] INFO  k.l.LogManager - Log directory '/tmp/kafka/logs/kafka-test-49790' not found, creating it.
16:34:29.034 [main] INFO  k.l.LogManager - Loading logs.
16:34:29.035 [main] INFO  k.l.LogManager - Logs loading complete.
16:34:29.035 [main] INFO  k.l.LogManager - Starting log cleanup with a period of 300000 ms.
16:34:29.037 [main] INFO  k.l.LogManager - Starting log flusher with a default period of 9223372036854775807 ms.
16:34:29.038 [main] INFO  k.n.Acceptor - Awaiting socket connections on 0.0.0.0:49790.
16:34:29.039 [main] INFO  k.n.SocketServer - [Socket Server on Broker 0], Started
16:34:29.045 [main] INFO  k.u.Mx4jLoader$ - Will not load MX4J, mx4j-tools.jar is not in the classpath
16:34:29.049 [main] INFO  k.c.KafkaController - [Controller 0]: Controller starting up
16:34:29.059 [main] INFO  k.s.ZookeeperLeaderElector - 0 successfully elected as leader
16:34:29.059 [main] INFO  k.c.KafkaController - [Controller 0]: Broker 0 starting become controller state transition
16:34:29.077 [main] INFO  k.c.KafkaController - [Controller 0]: Controller 0 incremented epoch to 1
16:34:29.079 [main] INFO  k.c.KafkaController - [Controller 0]: Partitions undergoing preferred replica election: 
16:34:29.080 [main] INFO  k.c.KafkaController - [Controller 0]: Partitions that completed preferred replica election: 
16:34:29.080 [main] INFO  k.c.KafkaController - [Controller 0]: Resuming preferred replica election for partitions: 
16:34:29.080 [main] INFO  k.c.KafkaController - [Controller 0]: Partitions being reassigned: Map()
16:34:29.080 [main] INFO  k.c.KafkaController - [Controller 0]: Partitions already reassigned: List()
16:34:29.080 [main] INFO  k.c.KafkaController - [Controller 0]: Resuming reassignment of partitions: Map()
16:34:29.080 [main] INFO  k.c.KafkaController - [Controller 0]: List of topics to be deleted: 
16:34:29.080 [main] INFO  k.c.KafkaController - [Controller 0]: List of topics ineligible for deletion: 
16:34:29.080 [main] INFO  k.c.KafkaController - [Controller 0]: Currently active brokers in the cluster: Set()
16:34:29.080 [main] INFO  k.c.KafkaController - [Controller 0]: Currently shutting brokers in the cluster: Set()
16:34:29.080 [main] INFO  k.c.KafkaController - [Controller 0]: Current list of topics in the cluster: Set()
16:34:29.081 [main] INFO  k.c.ReplicaStateMachine - [Replica state machine on controller 0]: Started replica state machine with initial state -> Map()
16:34:29.081 [main] INFO  k.c.PartitionStateMachine - [Partition state machine on Controller 0]: Started partition state machine with initial state -> Map()
16:34:29.081 [main] INFO  k.c.KafkaController - [Controller 0]: Broker 0 is ready to serve as the new controller with epoch 1
16:34:29.081 [main] INFO  k.c.KafkaController - [Controller 0]: Starting preferred replica leader election for partitions 
16:34:29.081 [main] INFO  k.c.PartitionStateMachine - [Partition state machine on Controller 0]: Invoking state change to OnlinePartition for partitions 
16:34:29.082 [main] INFO  k.c.KafkaController - [Controller 0]: starting the partition rebalance scheduler
16:34:29.084 [main] INFO  k.c.KafkaController - [Controller 0]: Controller startup complete
16:34:29.084 [ZkClient-EventThread-905-127.0.0.1:41577] INFO  k.s.ZookeeperLeaderElector$LeaderChangeListener - New leader is 0
16:34:29.104 [main] INFO  k.u.ZkUtils$ - Registered broker 0 at path /brokers/ids/0 with address testing-worker-linux-docker-a83abded-3380-linux-4.prod.travis-ci.org:49790.
16:34:29.104 [main] INFO  k.s.KafkaServer - [Kafka Server 0], started
16:34:29.105 [main] INFO  o.a.k.c.p.ProducerConfig - ProducerConfig values: 
	compression.type = none
	metric.reporters = []
	metadata.max.age.ms = 300000
	metadata.fetch.timeout.ms = 1000
	acks = 1
	batch.size = 16384
	reconnect.backoff.ms = 10
	bootstrap.servers = [localhost:49790]
	receive.buffer.bytes = 32768
	retry.backoff.ms = 100
	buffer.memory = 33554432
	timeout.ms = 30000
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	retries = 0
	max.request.size = 1048576
	block.on.buffer.full = true
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
	metrics.sample.window.ms = 30000
	send.buffer.bytes = 131072
	max.in.flight.requests.per.connection = 5
	metrics.num.samples = 2
	linger.ms = 0
	client.id = 

16:34:29.107 [main] INFO  k.s.KafkaServer - [Kafka Server 0], shutting down
16:34:29.107 [ZkClient-EventThread-905-127.0.0.1:41577] INFO  k.c.ReplicaStateMachine$BrokerChangeListener - [BrokerChangeListener on Controller 0]: Broker change listener fired for path /brokers/ids with children 0
16:34:29.107 [main] INFO  k.s.KafkaServer - [Kafka Server 0], Starting controlled shutdown
16:34:29.110 [ZkClient-EventThread-905-127.0.0.1:41577] INFO  k.c.ReplicaStateMachine$BrokerChangeListener - [BrokerChangeListener on Controller 0]: Newly added brokers: 0, deleted brokers: , all live brokers: 0
16:34:29.110 [ZkClient-EventThread-905-127.0.0.1:41577] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Controller 0 connected to id:0,host:testing-worker-linux-docker-a83abded-3380-linux-4.prod.travis-ci.org,port:49790 for sending state change requests
16:34:29.111 [ZkClient-EventThread-905-127.0.0.1:41577] INFO  k.c.KafkaController - [Controller 0]: New broker startup callback for 0
16:34:29.113 [kafka-request-handler-0] INFO  k.c.KafkaController - [Controller 0]: Shutting down broker 0
16:34:29.114 [main] INFO  k.s.KafkaServer - [Kafka Server 0], Controlled shutdown succeeded
16:34:29.114 [main] INFO  k.n.SocketServer - [Socket Server on Broker 0], Shutting down
16:34:29.115 [kafka-network-thread-49790-1] INFO  k.n.Processor - Closing socket connection to /172.17.1.249.
16:34:29.115 [main] INFO  k.n.SocketServer - [Socket Server on Broker 0], Shutdown completed
16:34:29.115 [main] INFO  k.s.KafkaRequestHandlerPool - [Kafka Request Handler on Broker 0], shutting down
16:34:29.115 [main] INFO  k.s.KafkaRequestHandlerPool - [Kafka Request Handler on Broker 0], shut down completely
16:34:29.116 [Controller-0-to-broker-0-send-thread] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Starting 
16:34:29.445 [main] INFO  k.s.ReplicaManager - [Replica Manager on Broker 0]: Shut down
16:34:29.445 [main] INFO  k.s.ReplicaFetcherManager - [ReplicaFetcherManager on broker 0] shutting down
16:34:29.445 [main] INFO  k.s.ReplicaFetcherManager - [ReplicaFetcherManager on broker 0] shutdown completed
16:34:29.445 [main] INFO  k.s.ReplicaManager - [Replica Manager on Broker 0]: Shut down completely
16:34:29.445 [main] INFO  k.l.LogManager - Shutting down.
16:34:29.445 [main] INFO  k.l.LogManager - Shutdown complete.
16:34:29.446 [main] INFO  k.c.PartitionStateMachine - [Partition state machine on Controller 0]: Stopped partition state machine
16:34:29.446 [main] INFO  k.c.ReplicaStateMachine - [Replica state machine on controller 0]: Stopped replica state machine
16:34:29.446 [main] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Shutting down
16:34:29.446 [main] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Shutdown completed
16:34:29.446 [ZkClient-EventThread-905-127.0.0.1:41577] INFO  o.I.z.ZkEventThread - Terminate ZkClient event thread.
16:34:29.447 [Controller-0-to-broker-0-send-thread] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Stopped 
16:34:29.448 [main] INFO  k.s.KafkaServer - [Kafka Server 0], shut down completed
16:34:29.448 [Curator-Framework-0] INFO  o.a.c.f.i.CuratorFrameworkImpl - backgroundOperationsLoop exiting

--------------------------------------------------
--------------------------------------------------
classname: org.apache.storm.kafka.bolt.KafkaBoltTest / testname: executeWithByteArrayKeyAndMessageFire
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.kafka.bolt.KafkaBoltTest.generateTestTuple(KafkaBoltTest.java:290)
	at org.apache.storm.kafka.bolt.KafkaBoltTest.executeWithByteArrayKeyAndMessageFire(KafkaBoltTest.java:176)

-------------------- system-out --------------------
16:34:29.495 [Curator-Framework-0-SendThread(127.0.0.1:34460)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:34:29.519 [Curator-Framework-0-SendThread(127.0.0.1:44710)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:34:29.534 [Curator-Framework-0-SendThread(127.0.0.1:59679)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:34:29.583 [Curator-Framework-0-SendThread(127.0.0.1:58965)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:34:29.664 [Curator-Framework-0-SendThread(127.0.0.1:44186)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:34:29.886 [Curator-Framework-0-SendThread(127.0.0.1:54974)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:34:30.472 [main] INFO  o.a.c.f.i.CuratorFrameworkImpl - Starting
16:34:30.473 [main] INFO  k.u.VerifiableProperties - Verifying properties
16:34:30.473 [main] INFO  k.u.VerifiableProperties - Property broker.id is overridden to 0
16:34:30.474 [main] INFO  k.u.VerifiableProperties - Property log.dirs is overridden to /tmp/kafka/logs/kafka-test-59917
16:34:30.474 [main] INFO  k.u.VerifiableProperties - Property port is overridden to 59917
16:34:30.474 [main] INFO  k.u.VerifiableProperties - Property zookeeper.connect is overridden to 127.0.0.1:52513
16:34:30.474 [main] INFO  k.s.KafkaServer - [Kafka Server 0], starting
16:34:30.474 [main] INFO  k.s.KafkaServer - [Kafka Server 0], Connecting to zookeeper on 127.0.0.1:52513
16:34:30.474 [ZkClient-EventThread-941-127.0.0.1:52513] INFO  o.I.z.ZkEventThread - Starting ZkClient event thread.
16:34:30.596 [Curator-Framework-0-SendThread(127.0.0.1:34460)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:34:30.620 [Curator-Framework-0-SendThread(127.0.0.1:44710)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:34:30.635 [Curator-Framework-0-SendThread(127.0.0.1:59679)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:34:30.684 [Curator-Framework-0-SendThread(127.0.0.1:58965)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:34:30.765 [Curator-Framework-0-SendThread(127.0.0.1:44186)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:34:30.988 [Curator-Framework-0-SendThread(127.0.0.1:54974)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:34:31.209 [main-EventThread] INFO  o.a.c.f.s.ConnectionStateManager - State change: CONNECTED
16:34:31.238 [main-EventThread] INFO  o.I.z.ZkClient - zookeeper state changed (SyncConnected)
16:34:31.389 [main] INFO  k.l.LogManager - Log directory '/tmp/kafka/logs/kafka-test-59917' not found, creating it.
16:34:31.389 [main] INFO  k.l.LogManager - Loading logs.
16:34:31.389 [main] INFO  k.l.LogManager - Logs loading complete.
16:34:31.389 [main] INFO  k.l.LogManager - Starting log cleanup with a period of 300000 ms.
16:34:31.389 [main] INFO  k.l.LogManager - Starting log flusher with a default period of 9223372036854775807 ms.
16:34:31.392 [main] INFO  k.n.Acceptor - Awaiting socket connections on 0.0.0.0:59917.
16:34:31.392 [main] INFO  k.n.SocketServer - [Socket Server on Broker 0], Started
16:34:31.402 [main] INFO  k.u.Mx4jLoader$ - Will not load MX4J, mx4j-tools.jar is not in the classpath
16:34:31.402 [main] INFO  k.c.KafkaController - [Controller 0]: Controller starting up
16:34:31.427 [main] INFO  k.s.ZookeeperLeaderElector - 0 successfully elected as leader
16:34:31.427 [main] INFO  k.c.KafkaController - [Controller 0]: Broker 0 starting become controller state transition
16:34:31.454 [main] INFO  k.c.KafkaController - [Controller 0]: Controller 0 incremented epoch to 1
16:34:31.457 [main] INFO  k.c.KafkaController - [Controller 0]: Partitions undergoing preferred replica election: 
16:34:31.457 [main] INFO  k.c.KafkaController - [Controller 0]: Partitions that completed preferred replica election: 
16:34:31.457 [main] INFO  k.c.KafkaController - [Controller 0]: Resuming preferred replica election for partitions: 
16:34:31.457 [main] INFO  k.c.KafkaController - [Controller 0]: Partitions being reassigned: Map()
16:34:31.457 [main] INFO  k.c.KafkaController - [Controller 0]: Partitions already reassigned: List()
16:34:31.457 [main] INFO  k.c.KafkaController - [Controller 0]: Resuming reassignment of partitions: Map()
16:34:31.458 [main] INFO  k.c.KafkaController - [Controller 0]: List of topics to be deleted: 
16:34:31.458 [main] INFO  k.c.KafkaController - [Controller 0]: List of topics ineligible for deletion: 
16:34:31.458 [main] INFO  k.c.KafkaController - [Controller 0]: Currently active brokers in the cluster: Set()
16:34:31.458 [main] INFO  k.c.KafkaController - [Controller 0]: Currently shutting brokers in the cluster: Set()
16:34:31.458 [main] INFO  k.c.KafkaController - [Controller 0]: Current list of topics in the cluster: Set()
16:34:31.458 [main] INFO  k.c.ReplicaStateMachine - [Replica state machine on controller 0]: Started replica state machine with initial state -> Map()
16:34:31.458 [main] INFO  k.c.PartitionStateMachine - [Partition state machine on Controller 0]: Started partition state machine with initial state -> Map()
16:34:31.458 [main] INFO  k.c.KafkaController - [Controller 0]: Broker 0 is ready to serve as the new controller with epoch 1
16:34:31.458 [main] INFO  k.c.KafkaController - [Controller 0]: Starting preferred replica leader election for partitions 
16:34:31.458 [main] INFO  k.c.PartitionStateMachine - [Partition state machine on Controller 0]: Invoking state change to OnlinePartition for partitions 
16:34:31.464 [main] INFO  k.c.KafkaController - [Controller 0]: starting the partition rebalance scheduler
16:34:31.464 [main] INFO  k.c.KafkaController - [Controller 0]: Controller startup complete
16:34:31.465 [ZkClient-EventThread-941-127.0.0.1:52513] INFO  k.s.ZookeeperLeaderElector$LeaderChangeListener - New leader is 0
16:34:31.470 [main] INFO  k.u.ZkUtils$ - Registered broker 0 at path /brokers/ids/0 with address testing-worker-linux-docker-a83abded-3380-linux-4.prod.travis-ci.org:59917.
16:34:31.471 [main] INFO  k.s.KafkaServer - [Kafka Server 0], started
16:34:31.471 [main] INFO  o.a.k.c.p.ProducerConfig - ProducerConfig values: 
	compression.type = none
	metric.reporters = []
	metadata.max.age.ms = 300000
	metadata.fetch.timeout.ms = 1000
	acks = 1
	batch.size = 16384
	reconnect.backoff.ms = 10
	bootstrap.servers = [localhost:59917]
	receive.buffer.bytes = 32768
	retry.backoff.ms = 100
	buffer.memory = 33554432
	timeout.ms = 30000
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	retries = 0
	max.request.size = 1048576
	block.on.buffer.full = true
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
	metrics.sample.window.ms = 30000
	send.buffer.bytes = 131072
	max.in.flight.requests.per.connection = 5
	metrics.num.samples = 2
	linger.ms = 0
	client.id = 

16:34:31.474 [ZkClient-EventThread-941-127.0.0.1:52513] INFO  k.c.ReplicaStateMachine$BrokerChangeListener - [BrokerChangeListener on Controller 0]: Broker change listener fired for path /brokers/ids with children 0
16:34:31.490 [ZkClient-EventThread-941-127.0.0.1:52513] INFO  k.c.ReplicaStateMachine$BrokerChangeListener - [BrokerChangeListener on Controller 0]: Newly added brokers: 0, deleted brokers: , all live brokers: 0
16:34:31.499 [ZkClient-EventThread-941-127.0.0.1:52513] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Controller 0 connected to id:0,host:testing-worker-linux-docker-a83abded-3380-linux-4.prod.travis-ci.org,port:59917 for sending state change requests
16:34:31.499 [main] INFO  o.a.k.c.p.ProducerConfig - ProducerConfig values: 
	compression.type = none
	metric.reporters = []
	metadata.max.age.ms = 300000
	metadata.fetch.timeout.ms = 1000
	acks = 1
	batch.size = 16384
	reconnect.backoff.ms = 10
	bootstrap.servers = [localhost:59917]
	receive.buffer.bytes = 32768
	retry.backoff.ms = 100
	buffer.memory = 33554432
	timeout.ms = 30000
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	retries = 0
	max.request.size = 1048576
	block.on.buffer.full = true
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	metrics.sample.window.ms = 30000
	send.buffer.bytes = 131072
	max.in.flight.requests.per.connection = 5
	metrics.num.samples = 2
	linger.ms = 0
	client.id = 

16:34:31.500 [ZkClient-EventThread-941-127.0.0.1:52513] INFO  k.c.KafkaController - [Controller 0]: New broker startup callback for 0
16:34:31.500 [Controller-0-to-broker-0-send-thread] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Starting 
16:34:31.501 [main] INFO  k.s.KafkaServer - [Kafka Server 0], shutting down
16:34:31.501 [main] INFO  k.s.KafkaServer - [Kafka Server 0], Starting controlled shutdown
16:34:31.503 [kafka-request-handler-0] INFO  k.c.KafkaController - [Controller 0]: Shutting down broker 0
16:34:31.503 [main] INFO  k.s.KafkaServer - [Kafka Server 0], Controlled shutdown succeeded
16:34:31.503 [main] INFO  k.n.SocketServer - [Socket Server on Broker 0], Shutting down
16:34:31.506 [main] INFO  k.n.SocketServer - [Socket Server on Broker 0], Shutdown completed
16:34:31.506 [main] INFO  k.s.KafkaRequestHandlerPool - [Kafka Request Handler on Broker 0], shutting down
16:34:31.507 [main] INFO  k.s.KafkaRequestHandlerPool - [Kafka Request Handler on Broker 0], shut down completely
16:34:31.697 [Curator-Framework-0-SendThread(127.0.0.1:34460)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:34:31.720 [Curator-Framework-0-SendThread(127.0.0.1:44710)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:34:31.735 [Curator-Framework-0-SendThread(127.0.0.1:59679)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:34:31.785 [Curator-Framework-0-SendThread(127.0.0.1:58965)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:34:31.794 [main] INFO  k.s.ReplicaManager - [Replica Manager on Broker 0]: Shut down
16:34:31.794 [main] INFO  k.s.ReplicaFetcherManager - [ReplicaFetcherManager on broker 0] shutting down
16:34:31.794 [main] INFO  k.s.ReplicaFetcherManager - [ReplicaFetcherManager on broker 0] shutdown completed
16:34:31.794 [main] INFO  k.s.ReplicaManager - [Replica Manager on Broker 0]: Shut down completely
16:34:31.794 [main] INFO  k.l.LogManager - Shutting down.
16:34:31.795 [main] INFO  k.l.LogManager - Shutdown complete.
16:34:31.796 [main] INFO  k.c.PartitionStateMachine - [Partition state machine on Controller 0]: Stopped partition state machine
16:34:31.796 [main] INFO  k.c.ReplicaStateMachine - [Replica state machine on controller 0]: Stopped replica state machine
16:34:31.797 [main] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Shutting down
16:34:31.797 [Controller-0-to-broker-0-send-thread] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Stopped 
16:34:31.798 [main] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Shutdown completed
16:34:31.798 [ZkClient-EventThread-941-127.0.0.1:52513] INFO  o.I.z.ZkEventThread - Terminate ZkClient event thread.
16:34:31.866 [Curator-Framework-0-SendThread(127.0.0.1:44186)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:34:32.084 [main] INFO  k.s.KafkaServer - [Kafka Server 0], shut down completed
16:34:32.084 [Curator-Framework-0] INFO  o.a.c.f.i.CuratorFrameworkImpl - backgroundOperationsLoop exiting
16:34:32.088 [Curator-Framework-0-SendThread(127.0.0.1:54974)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]

--------------------------------------------------
--------------------------------------------------
classname: org.apache.storm.kafka.bolt.KafkaBoltTest / testname: executeWithByteArrayKeyAndMessageSync
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.kafka.bolt.KafkaBoltTest.generateTestTuple(KafkaBoltTest.java:290)
	at org.apache.storm.kafka.bolt.KafkaBoltTest.executeWithByteArrayKeyAndMessageSync(KafkaBoltTest.java:131)

-------------------- system-out --------------------
16:34:32.798 [Curator-Framework-0-SendThread(127.0.0.1:34460)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:34:32.821 [Curator-Framework-0-SendThread(127.0.0.1:44710)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:34:32.836 [Curator-Framework-0-SendThread(127.0.0.1:59679)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:34:32.886 [Curator-Framework-0-SendThread(127.0.0.1:58965)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:34:32.967 [Curator-Framework-0-SendThread(127.0.0.1:44186)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:34:33.189 [Curator-Framework-0-SendThread(127.0.0.1:54974)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:34:33.487 [main] INFO  o.a.c.f.i.CuratorFrameworkImpl - Starting
16:34:33.493 [main] INFO  k.u.VerifiableProperties - Verifying properties
16:34:33.493 [main] INFO  k.u.VerifiableProperties - Property broker.id is overridden to 0
16:34:33.493 [main] INFO  k.u.VerifiableProperties - Property log.dirs is overridden to /tmp/kafka/logs/kafka-test-43929
16:34:33.494 [main] INFO  k.u.VerifiableProperties - Property port is overridden to 43929
16:34:33.494 [main] INFO  k.u.VerifiableProperties - Property zookeeper.connect is overridden to 127.0.0.1:52791
16:34:33.494 [main] INFO  k.s.KafkaServer - [Kafka Server 0], starting
16:34:33.494 [main] INFO  k.s.KafkaServer - [Kafka Server 0], Connecting to zookeeper on 127.0.0.1:52791
16:34:33.494 [ZkClient-EventThread-978-127.0.0.1:52791] INFO  o.I.z.ZkEventThread - Starting ZkClient event thread.
16:34:33.899 [Curator-Framework-0-SendThread(127.0.0.1:34460)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:34:33.921 [Curator-Framework-0-SendThread(127.0.0.1:44710)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:34:33.936 [Curator-Framework-0-SendThread(127.0.0.1:59679)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:34:33.986 [Curator-Framework-0-SendThread(127.0.0.1:58965)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:34:34.067 [Curator-Framework-0-SendThread(127.0.0.1:44186)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:34:34.172 [main-EventThread] INFO  o.a.c.f.s.ConnectionStateManager - State change: CONNECTED
16:34:34.290 [Curator-Framework-0-SendThread(127.0.0.1:54974)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:34:34.342 [main-EventThread] INFO  o.I.z.ZkClient - zookeeper state changed (SyncConnected)
16:34:34.921 [main] INFO  k.l.LogManager - Log directory '/tmp/kafka/logs/kafka-test-43929' not found, creating it.
16:34:34.922 [main] INFO  k.l.LogManager - Loading logs.
16:34:34.922 [main] INFO  k.l.LogManager - Logs loading complete.
16:34:34.922 [main] INFO  k.l.LogManager - Starting log cleanup with a period of 300000 ms.
16:34:34.922 [main] INFO  k.l.LogManager - Starting log flusher with a default period of 9223372036854775807 ms.
16:34:34.924 [main] INFO  k.n.Acceptor - Awaiting socket connections on 0.0.0.0:43929.
16:34:34.925 [main] INFO  k.n.SocketServer - [Socket Server on Broker 0], Started
16:34:34.938 [main] INFO  k.u.Mx4jLoader$ - Will not load MX4J, mx4j-tools.jar is not in the classpath
16:34:34.939 [main] INFO  k.c.KafkaController - [Controller 0]: Controller starting up
16:34:34.953 [main] INFO  k.s.ZookeeperLeaderElector - 0 successfully elected as leader
16:34:34.954 [main] INFO  k.c.KafkaController - [Controller 0]: Broker 0 starting become controller state transition
16:34:34.976 [main] INFO  k.c.KafkaController - [Controller 0]: Controller 0 incremented epoch to 1
16:34:34.978 [main] INFO  k.c.KafkaController - [Controller 0]: Partitions undergoing preferred replica election: 
16:34:34.978 [main] INFO  k.c.KafkaController - [Controller 0]: Partitions that completed preferred replica election: 
16:34:34.978 [main] INFO  k.c.KafkaController - [Controller 0]: Resuming preferred replica election for partitions: 
16:34:34.987 [main] INFO  k.c.KafkaController - [Controller 0]: Partitions being reassigned: Map()
16:34:34.987 [main] INFO  k.c.KafkaController - [Controller 0]: Partitions already reassigned: List()
16:34:34.987 [main] INFO  k.c.KafkaController - [Controller 0]: Resuming reassignment of partitions: Map()
16:34:34.987 [main] INFO  k.c.KafkaController - [Controller 0]: List of topics to be deleted: 
16:34:34.987 [main] INFO  k.c.KafkaController - [Controller 0]: List of topics ineligible for deletion: 
16:34:34.988 [main] INFO  k.c.KafkaController - [Controller 0]: Currently active brokers in the cluster: Set()
16:34:34.988 [main] INFO  k.c.KafkaController - [Controller 0]: Currently shutting brokers in the cluster: Set()
16:34:34.988 [main] INFO  k.c.KafkaController - [Controller 0]: Current list of topics in the cluster: Set()
16:34:34.988 [main] INFO  k.c.ReplicaStateMachine - [Replica state machine on controller 0]: Started replica state machine with initial state -> Map()
16:34:34.988 [main] INFO  k.c.PartitionStateMachine - [Partition state machine on Controller 0]: Started partition state machine with initial state -> Map()
16:34:34.988 [main] INFO  k.c.KafkaController - [Controller 0]: Broker 0 is ready to serve as the new controller with epoch 1
16:34:34.988 [main] INFO  k.c.KafkaController - [Controller 0]: Starting preferred replica leader election for partitions 
16:34:34.988 [main] INFO  k.c.PartitionStateMachine - [Partition state machine on Controller 0]: Invoking state change to OnlinePartition for partitions 
16:34:34.993 [main] INFO  k.c.KafkaController - [Controller 0]: starting the partition rebalance scheduler
16:34:34.994 [main] INFO  k.c.KafkaController - [Controller 0]: Controller startup complete
16:34:34.994 [ZkClient-EventThread-978-127.0.0.1:52791] INFO  k.s.ZookeeperLeaderElector$LeaderChangeListener - New leader is 0
16:34:35.000 [Curator-Framework-0-SendThread(127.0.0.1:34460)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:34:35.004 [main] INFO  k.u.ZkUtils$ - Registered broker 0 at path /brokers/ids/0 with address testing-worker-linux-docker-a83abded-3380-linux-4.prod.travis-ci.org:43929.
16:34:35.004 [main] INFO  k.s.KafkaServer - [Kafka Server 0], started
16:34:35.005 [main] INFO  o.a.k.c.p.ProducerConfig - ProducerConfig values: 
	compression.type = none
	metric.reporters = []
	metadata.max.age.ms = 300000
	metadata.fetch.timeout.ms = 1000
	acks = 1
	batch.size = 16384
	reconnect.backoff.ms = 10
	bootstrap.servers = [localhost:43929]
	receive.buffer.bytes = 32768
	retry.backoff.ms = 100
	buffer.memory = 33554432
	timeout.ms = 30000
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	retries = 0
	max.request.size = 1048576
	block.on.buffer.full = true
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
	metrics.sample.window.ms = 30000
	send.buffer.bytes = 131072
	max.in.flight.requests.per.connection = 5
	metrics.num.samples = 2
	linger.ms = 0
	client.id = 

16:34:35.006 [main] INFO  o.a.k.c.p.ProducerConfig - ProducerConfig values: 
	compression.type = none
	metric.reporters = []
	metadata.max.age.ms = 300000
	metadata.fetch.timeout.ms = 1000
	acks = 1
	batch.size = 16384
	reconnect.backoff.ms = 10
	bootstrap.servers = [localhost:43929]
	receive.buffer.bytes = 32768
	retry.backoff.ms = 100
	buffer.memory = 33554432
	timeout.ms = 30000
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	retries = 0
	max.request.size = 1048576
	block.on.buffer.full = true
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	metrics.sample.window.ms = 30000
	send.buffer.bytes = 131072
	max.in.flight.requests.per.connection = 5
	metrics.num.samples = 2
	linger.ms = 0
	client.id = 

16:34:35.007 [ZkClient-EventThread-978-127.0.0.1:52791] INFO  k.c.ReplicaStateMachine$BrokerChangeListener - [BrokerChangeListener on Controller 0]: Broker change listener fired for path /brokers/ids with children 0
16:34:35.010 [ZkClient-EventThread-978-127.0.0.1:52791] INFO  k.c.ReplicaStateMachine$BrokerChangeListener - [BrokerChangeListener on Controller 0]: Newly added brokers: 0, deleted brokers: , all live brokers: 0
16:34:35.022 [Curator-Framework-0-SendThread(127.0.0.1:44710)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:34:35.023 [main] INFO  k.s.KafkaServer - [Kafka Server 0], shutting down
16:34:35.023 [main] INFO  k.s.KafkaServer - [Kafka Server 0], Starting controlled shutdown
16:34:35.023 [ZkClient-EventThread-978-127.0.0.1:52791] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Controller 0 connected to id:0,host:testing-worker-linux-docker-a83abded-3380-linux-4.prod.travis-ci.org,port:43929 for sending state change requests
16:34:35.024 [ZkClient-EventThread-978-127.0.0.1:52791] INFO  k.c.KafkaController - [Controller 0]: New broker startup callback for 0
16:34:35.027 [Controller-0-to-broker-0-send-thread] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Starting 
16:34:35.027 [kafka-request-handler-0] INFO  k.c.KafkaController - [Controller 0]: Shutting down broker 0
16:34:35.027 [main] INFO  k.s.KafkaServer - [Kafka Server 0], Controlled shutdown succeeded
16:34:35.028 [main] INFO  k.n.SocketServer - [Socket Server on Broker 0], Shutting down
16:34:35.028 [kafka-network-thread-43929-1] INFO  k.n.Processor - Closing socket connection to /172.17.1.249.
16:34:35.028 [main] INFO  k.n.SocketServer - [Socket Server on Broker 0], Shutdown completed
16:34:35.028 [main] INFO  k.s.KafkaRequestHandlerPool - [Kafka Request Handler on Broker 0], shutting down
16:34:35.029 [main] INFO  k.s.KafkaRequestHandlerPool - [Kafka Request Handler on Broker 0], shut down completely
16:34:35.037 [Curator-Framework-0-SendThread(127.0.0.1:59679)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:34:35.087 [Curator-Framework-0-SendThread(127.0.0.1:58965)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:34:35.137 [main] INFO  k.s.ReplicaManager - [Replica Manager on Broker 0]: Shut down
16:34:35.137 [main] INFO  k.s.ReplicaFetcherManager - [ReplicaFetcherManager on broker 0] shutting down
16:34:35.137 [main] INFO  k.s.ReplicaFetcherManager - [ReplicaFetcherManager on broker 0] shutdown completed
16:34:35.137 [main] INFO  k.s.ReplicaManager - [Replica Manager on Broker 0]: Shut down completely
16:34:35.137 [main] INFO  k.l.LogManager - Shutting down.
16:34:35.137 [main] INFO  k.l.LogManager - Shutdown complete.
16:34:35.138 [main] INFO  k.c.PartitionStateMachine - [Partition state machine on Controller 0]: Stopped partition state machine
16:34:35.138 [main] INFO  k.c.ReplicaStateMachine - [Replica state machine on controller 0]: Stopped replica state machine
16:34:35.138 [main] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Shutting down
16:34:35.138 [Controller-0-to-broker-0-send-thread] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Stopped 
16:34:35.138 [main] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Shutdown completed
16:34:35.138 [ZkClient-EventThread-978-127.0.0.1:52791] INFO  o.I.z.ZkEventThread - Terminate ZkClient event thread.
16:34:35.168 [Curator-Framework-0-SendThread(127.0.0.1:44186)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:34:35.197 [main] INFO  k.s.KafkaServer - [Kafka Server 0], shut down completed
16:34:35.198 [Curator-Framework-0] INFO  o.a.c.f.i.CuratorFrameworkImpl - backgroundOperationsLoop exiting

--------------------------------------------------
--------------------------------------------------
classname: org.apache.storm.kafka.bolt.KafkaBoltTest / testname: executeWithByteArrayKeyAndMessageAsync
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.kafka.bolt.KafkaBoltTest.generateTestTuple(KafkaBoltTest.java:290)
	at org.apache.storm.kafka.bolt.KafkaBoltTest.executeWithByteArrayKeyAndMessageAsync(KafkaBoltTest.java:146)

-------------------- system-out --------------------
16:34:35.391 [Curator-Framework-0-SendThread(127.0.0.1:54974)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:34:36.101 [Curator-Framework-0-SendThread(127.0.0.1:34460)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:34:36.123 [Curator-Framework-0-SendThread(127.0.0.1:44710)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:34:36.138 [Curator-Framework-0-SendThread(127.0.0.1:59679)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:34:36.188 [Curator-Framework-0-SendThread(127.0.0.1:58965)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:34:36.269 [Curator-Framework-0-SendThread(127.0.0.1:44186)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:34:36.299 [main] INFO  o.a.c.f.i.CuratorFrameworkImpl - Starting
16:34:36.303 [main] INFO  k.u.VerifiableProperties - Verifying properties
16:34:36.303 [main] INFO  k.u.VerifiableProperties - Property broker.id is overridden to 0
16:34:36.303 [main] INFO  k.u.VerifiableProperties - Property log.dirs is overridden to /tmp/kafka/logs/kafka-test-59611
16:34:36.303 [main] INFO  k.u.VerifiableProperties - Property port is overridden to 59611
16:34:36.303 [main] INFO  k.u.VerifiableProperties - Property zookeeper.connect is overridden to 127.0.0.1:35123
16:34:36.303 [main] INFO  k.s.KafkaServer - [Kafka Server 0], starting
16:34:36.303 [main] INFO  k.s.KafkaServer - [Kafka Server 0], Connecting to zookeeper on 127.0.0.1:35123
16:34:36.304 [ZkClient-EventThread-1015-127.0.0.1:35123] INFO  o.I.z.ZkEventThread - Starting ZkClient event thread.
16:34:36.491 [Curator-Framework-0-SendThread(127.0.0.1:54974)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:34:36.503 [main-EventThread] INFO  o.a.c.f.s.ConnectionStateManager - State change: CONNECTED
16:34:36.684 [main-EventThread] INFO  o.I.z.ZkClient - zookeeper state changed (SyncConnected)
16:34:37.202 [Curator-Framework-0-SendThread(127.0.0.1:34460)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:34:37.224 [Curator-Framework-0-SendThread(127.0.0.1:44710)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:34:37.239 [Curator-Framework-0-SendThread(127.0.0.1:59679)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:34:37.289 [Curator-Framework-0-SendThread(127.0.0.1:58965)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:34:37.370 [Curator-Framework-0-SendThread(127.0.0.1:44186)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:34:37.592 [Curator-Framework-0-SendThread(127.0.0.1:54974)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:34:38.303 [Curator-Framework-0-SendThread(127.0.0.1:34460)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:34:38.324 [Curator-Framework-0-SendThread(127.0.0.1:44710)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:34:38.339 [Curator-Framework-0-SendThread(127.0.0.1:59679)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:34:38.390 [Curator-Framework-0-SendThread(127.0.0.1:58965)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:34:38.471 [Curator-Framework-0-SendThread(127.0.0.1:44186)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:34:38.693 [Curator-Framework-0-SendThread(127.0.0.1:54974)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:34:39.404 [Curator-Framework-0-SendThread(127.0.0.1:34460)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:34:39.425 [Curator-Framework-0-SendThread(127.0.0.1:44710)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:34:39.440 [Curator-Framework-0-SendThread(127.0.0.1:59679)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:34:39.490 [Curator-Framework-0-SendThread(127.0.0.1:58965)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:34:39.572 [Curator-Framework-0-SendThread(127.0.0.1:44186)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:34:39.794 [Curator-Framework-0-SendThread(127.0.0.1:54974)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:34:40.441 [main] INFO  k.l.LogManager - Log directory '/tmp/kafka/logs/kafka-test-59611' not found, creating it.
16:34:40.442 [main] INFO  k.l.LogManager - Loading logs.
16:34:40.442 [main] INFO  k.l.LogManager - Logs loading complete.
16:34:40.442 [main] INFO  k.l.LogManager - Starting log cleanup with a period of 300000 ms.
16:34:40.442 [main] INFO  k.l.LogManager - Starting log flusher with a default period of 9223372036854775807 ms.
16:34:40.445 [main] INFO  k.n.Acceptor - Awaiting socket connections on 0.0.0.0:59611.
16:34:40.446 [main] INFO  k.n.SocketServer - [Socket Server on Broker 0], Started
16:34:40.448 [main] INFO  k.u.Mx4jLoader$ - Will not load MX4J, mx4j-tools.jar is not in the classpath
16:34:40.449 [main] INFO  k.c.KafkaController - [Controller 0]: Controller starting up
16:34:40.471 [main] INFO  k.s.ZookeeperLeaderElector - 0 successfully elected as leader
16:34:40.471 [main] INFO  k.c.KafkaController - [Controller 0]: Broker 0 starting become controller state transition
16:34:40.505 [Curator-Framework-0-SendThread(127.0.0.1:34460)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:34:40.507 [main] INFO  k.c.KafkaController - [Controller 0]: Controller 0 incremented epoch to 1
16:34:40.511 [main] INFO  k.c.KafkaController - [Controller 0]: Partitions undergoing preferred replica election: 
16:34:40.511 [main] INFO  k.c.KafkaController - [Controller 0]: Partitions that completed preferred replica election: 
16:34:40.511 [main] INFO  k.c.KafkaController - [Controller 0]: Resuming preferred replica election for partitions: 
16:34:40.512 [main] INFO  k.c.KafkaController - [Controller 0]: Partitions being reassigned: Map()
16:34:40.512 [main] INFO  k.c.KafkaController - [Controller 0]: Partitions already reassigned: List()
16:34:40.512 [main] INFO  k.c.KafkaController - [Controller 0]: Resuming reassignment of partitions: Map()
16:34:40.512 [main] INFO  k.c.KafkaController - [Controller 0]: List of topics to be deleted: 
16:34:40.513 [main] INFO  k.c.KafkaController - [Controller 0]: List of topics ineligible for deletion: 
16:34:40.513 [main] INFO  k.c.KafkaController - [Controller 0]: Currently active brokers in the cluster: Set()
16:34:40.513 [main] INFO  k.c.KafkaController - [Controller 0]: Currently shutting brokers in the cluster: Set()
16:34:40.513 [main] INFO  k.c.KafkaController - [Controller 0]: Current list of topics in the cluster: Set()
16:34:40.513 [main] INFO  k.c.ReplicaStateMachine - [Replica state machine on controller 0]: Started replica state machine with initial state -> Map()
16:34:40.513 [main] INFO  k.c.PartitionStateMachine - [Partition state machine on Controller 0]: Started partition state machine with initial state -> Map()
16:34:40.513 [main] INFO  k.c.KafkaController - [Controller 0]: Broker 0 is ready to serve as the new controller with epoch 1
16:34:40.513 [main] INFO  k.c.KafkaController - [Controller 0]: Starting preferred replica leader election for partitions 
16:34:40.513 [main] INFO  k.c.PartitionStateMachine - [Partition state machine on Controller 0]: Invoking state change to OnlinePartition for partitions 
16:34:40.526 [Curator-Framework-0-SendThread(127.0.0.1:44710)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:34:40.541 [Curator-Framework-0-SendThread(127.0.0.1:59679)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:34:40.550 [main] INFO  k.c.KafkaController - [Controller 0]: starting the partition rebalance scheduler
16:34:40.550 [main] INFO  k.c.KafkaController - [Controller 0]: Controller startup complete
16:34:40.552 [ZkClient-EventThread-1015-127.0.0.1:35123] INFO  k.s.ZookeeperLeaderElector$LeaderChangeListener - New leader is 0
16:34:40.581 [main] INFO  k.u.ZkUtils$ - Registered broker 0 at path /brokers/ids/0 with address testing-worker-linux-docker-a83abded-3380-linux-4.prod.travis-ci.org:59611.
16:34:40.581 [main] INFO  k.s.KafkaServer - [Kafka Server 0], started
16:34:40.581 [main] INFO  o.a.k.c.p.ProducerConfig - ProducerConfig values: 
	compression.type = none
	metric.reporters = []
	metadata.max.age.ms = 300000
	metadata.fetch.timeout.ms = 1000
	acks = 1
	batch.size = 16384
	reconnect.backoff.ms = 10
	bootstrap.servers = [localhost:59611]
	receive.buffer.bytes = 32768
	retry.backoff.ms = 100
	buffer.memory = 33554432
	timeout.ms = 30000
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	retries = 0
	max.request.size = 1048576
	block.on.buffer.full = true
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
	metrics.sample.window.ms = 30000
	send.buffer.bytes = 131072
	max.in.flight.requests.per.connection = 5
	metrics.num.samples = 2
	linger.ms = 0
	client.id = 

16:34:40.584 [main] INFO  k.s.KafkaServer - [Kafka Server 0], shutting down
16:34:40.584 [main] INFO  k.s.KafkaServer - [Kafka Server 0], Starting controlled shutdown
16:34:40.585 [ZkClient-EventThread-1015-127.0.0.1:35123] INFO  k.c.ReplicaStateMachine$BrokerChangeListener - [BrokerChangeListener on Controller 0]: Broker change listener fired for path /brokers/ids with children 0
16:34:40.592 [Curator-Framework-0-SendThread(127.0.0.1:58965)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:34:40.593 [ZkClient-EventThread-1015-127.0.0.1:35123] INFO  k.c.ReplicaStateMachine$BrokerChangeListener - [BrokerChangeListener on Controller 0]: Newly added brokers: 0, deleted brokers: , all live brokers: 0
16:34:40.594 [ZkClient-EventThread-1015-127.0.0.1:35123] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Controller 0 connected to id:0,host:testing-worker-linux-docker-a83abded-3380-linux-4.prod.travis-ci.org,port:59611 for sending state change requests
16:34:40.594 [ZkClient-EventThread-1015-127.0.0.1:35123] INFO  k.c.KafkaController - [Controller 0]: New broker startup callback for 0
16:34:40.594 [Controller-0-to-broker-0-send-thread] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Starting 
16:34:40.595 [kafka-request-handler-0] INFO  k.c.KafkaController - [Controller 0]: Shutting down broker 0
16:34:40.595 [main] INFO  k.s.KafkaServer - [Kafka Server 0], Controlled shutdown succeeded
16:34:40.595 [main] INFO  k.n.SocketServer - [Socket Server on Broker 0], Shutting down
16:34:40.596 [kafka-network-thread-59611-0] INFO  k.n.Processor - Closing socket connection to /172.17.1.249.
16:34:40.597 [main] INFO  k.n.SocketServer - [Socket Server on Broker 0], Shutdown completed
16:34:40.597 [main] INFO  k.s.KafkaRequestHandlerPool - [Kafka Request Handler on Broker 0], shutting down
16:34:40.597 [main] INFO  k.s.KafkaRequestHandlerPool - [Kafka Request Handler on Broker 0], shut down completely
16:34:40.672 [Curator-Framework-0-SendThread(127.0.0.1:44186)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:34:40.848 [main] INFO  k.s.ReplicaManager - [Replica Manager on Broker 0]: Shut down
16:34:40.848 [main] INFO  k.s.ReplicaFetcherManager - [ReplicaFetcherManager on broker 0] shutting down
16:34:40.848 [main] INFO  k.s.ReplicaFetcherManager - [ReplicaFetcherManager on broker 0] shutdown completed
16:34:40.848 [main] INFO  k.s.ReplicaManager - [Replica Manager on Broker 0]: Shut down completely
16:34:40.848 [main] INFO  k.l.LogManager - Shutting down.
16:34:40.848 [main] INFO  k.l.LogManager - Shutdown complete.
16:34:40.849 [main] INFO  k.c.PartitionStateMachine - [Partition state machine on Controller 0]: Stopped partition state machine
16:34:40.849 [main] INFO  k.c.ReplicaStateMachine - [Replica state machine on controller 0]: Stopped replica state machine
16:34:40.849 [main] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Shutting down
16:34:40.849 [main] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Shutdown completed
16:34:40.852 [ZkClient-EventThread-1015-127.0.0.1:35123] INFO  o.I.z.ZkEventThread - Terminate ZkClient event thread.
16:34:40.852 [Controller-0-to-broker-0-send-thread] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Stopped 
16:34:40.854 [main] INFO  k.s.KafkaServer - [Kafka Server 0], shut down completed
16:34:40.854 [Curator-Framework-0] INFO  o.a.c.f.i.CuratorFrameworkImpl - backgroundOperationsLoop exiting

--------------------------------------------------
--------------------------------------------------
classname: org.apache.storm.kafka.bolt.KafkaBoltTest / testname: executeWithKey
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.kafka.bolt.KafkaBoltTest.generateTestTuple(KafkaBoltTest.java:290)
	at org.apache.storm.kafka.bolt.KafkaBoltTest.executeWithKey(KafkaBoltTest.java:115)

-------------------- system-out --------------------
16:34:40.895 [Curator-Framework-0-SendThread(127.0.0.1:54974)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:34:41.606 [Curator-Framework-0-SendThread(127.0.0.1:34460)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:34:41.627 [Curator-Framework-0-SendThread(127.0.0.1:44710)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:34:41.642 [Curator-Framework-0-SendThread(127.0.0.1:59679)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:34:41.693 [Curator-Framework-0-SendThread(127.0.0.1:58965)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:34:41.773 [Curator-Framework-0-SendThread(127.0.0.1:44186)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:34:41.876 [main] INFO  o.a.c.f.i.CuratorFrameworkImpl - Starting
16:34:41.880 [main] INFO  k.u.VerifiableProperties - Verifying properties
16:34:41.880 [main] INFO  k.u.VerifiableProperties - Property broker.id is overridden to 0
16:34:41.880 [main] INFO  k.u.VerifiableProperties - Property log.dirs is overridden to /tmp/kafka/logs/kafka-test-45510
16:34:41.880 [main] INFO  k.u.VerifiableProperties - Property port is overridden to 45510
16:34:41.880 [main] INFO  k.u.VerifiableProperties - Property zookeeper.connect is overridden to 127.0.0.1:54998
16:34:41.880 [main] INFO  k.s.KafkaServer - [Kafka Server 0], starting
16:34:41.880 [main] INFO  k.s.KafkaServer - [Kafka Server 0], Connecting to zookeeper on 127.0.0.1:54998
16:34:41.880 [ZkClient-EventThread-1051-127.0.0.1:54998] INFO  o.I.z.ZkEventThread - Starting ZkClient event thread.
16:34:41.885 [main-EventThread] INFO  o.a.c.f.s.ConnectionStateManager - State change: CONNECTED
16:34:41.888 [main-EventThread] INFO  o.I.z.ZkClient - zookeeper state changed (SyncConnected)
16:34:41.903 [main] INFO  k.l.LogManager - Log directory '/tmp/kafka/logs/kafka-test-45510' not found, creating it.
16:34:41.903 [main] INFO  k.l.LogManager - Loading logs.
16:34:41.903 [main] INFO  k.l.LogManager - Logs loading complete.
16:34:41.903 [main] INFO  k.l.LogManager - Starting log cleanup with a period of 300000 ms.
16:34:41.904 [main] INFO  k.l.LogManager - Starting log flusher with a default period of 9223372036854775807 ms.
16:34:41.909 [main] INFO  k.n.Acceptor - Awaiting socket connections on 0.0.0.0:45510.
16:34:41.909 [main] INFO  k.n.SocketServer - [Socket Server on Broker 0], Started
16:34:41.911 [main] INFO  k.u.Mx4jLoader$ - Will not load MX4J, mx4j-tools.jar is not in the classpath
16:34:41.911 [main] INFO  k.c.KafkaController - [Controller 0]: Controller starting up
16:34:41.915 [main] INFO  k.s.ZookeeperLeaderElector - 0 successfully elected as leader
16:34:41.915 [main] INFO  k.c.KafkaController - [Controller 0]: Broker 0 starting become controller state transition
16:34:41.922 [main] INFO  k.c.KafkaController - [Controller 0]: Controller 0 incremented epoch to 1
16:34:41.924 [main] INFO  k.c.KafkaController - [Controller 0]: Partitions undergoing preferred replica election: 
16:34:41.924 [main] INFO  k.c.KafkaController - [Controller 0]: Partitions that completed preferred replica election: 
16:34:41.924 [main] INFO  k.c.KafkaController - [Controller 0]: Resuming preferred replica election for partitions: 
16:34:41.924 [main] INFO  k.c.KafkaController - [Controller 0]: Partitions being reassigned: Map()
16:34:41.924 [main] INFO  k.c.KafkaController - [Controller 0]: Partitions already reassigned: List()
16:34:41.925 [main] INFO  k.c.KafkaController - [Controller 0]: Resuming reassignment of partitions: Map()
16:34:41.925 [main] INFO  k.c.KafkaController - [Controller 0]: List of topics to be deleted: 
16:34:41.925 [main] INFO  k.c.KafkaController - [Controller 0]: List of topics ineligible for deletion: 
16:34:41.925 [main] INFO  k.c.KafkaController - [Controller 0]: Currently active brokers in the cluster: Set()
16:34:41.925 [main] INFO  k.c.KafkaController - [Controller 0]: Currently shutting brokers in the cluster: Set()
16:34:41.925 [main] INFO  k.c.KafkaController - [Controller 0]: Current list of topics in the cluster: Set()
16:34:41.925 [main] INFO  k.c.ReplicaStateMachine - [Replica state machine on controller 0]: Started replica state machine with initial state -> Map()
16:34:41.925 [main] INFO  k.c.PartitionStateMachine - [Partition state machine on Controller 0]: Started partition state machine with initial state -> Map()
16:34:41.925 [main] INFO  k.c.KafkaController - [Controller 0]: Broker 0 is ready to serve as the new controller with epoch 1
16:34:41.925 [main] INFO  k.c.KafkaController - [Controller 0]: Starting preferred replica leader election for partitions 
16:34:41.925 [main] INFO  k.c.PartitionStateMachine - [Partition state machine on Controller 0]: Invoking state change to OnlinePartition for partitions 
16:34:41.930 [main] INFO  k.c.KafkaController - [Controller 0]: starting the partition rebalance scheduler
16:34:41.930 [main] INFO  k.c.KafkaController - [Controller 0]: Controller startup complete
16:34:41.931 [ZkClient-EventThread-1051-127.0.0.1:54998] INFO  k.s.ZookeeperLeaderElector$LeaderChangeListener - New leader is 0
16:34:41.934 [main] INFO  k.u.ZkUtils$ - Registered broker 0 at path /brokers/ids/0 with address testing-worker-linux-docker-a83abded-3380-linux-4.prod.travis-ci.org:45510.
16:34:41.934 [main] INFO  k.s.KafkaServer - [Kafka Server 0], started
16:34:41.935 [main] INFO  o.a.k.c.p.ProducerConfig - ProducerConfig values: 
	compression.type = none
	metric.reporters = []
	metadata.max.age.ms = 300000
	metadata.fetch.timeout.ms = 1000
	acks = 1
	batch.size = 16384
	reconnect.backoff.ms = 10
	bootstrap.servers = [localhost:45510]
	receive.buffer.bytes = 32768
	retry.backoff.ms = 100
	buffer.memory = 33554432
	timeout.ms = 30000
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	retries = 0
	max.request.size = 1048576
	block.on.buffer.full = true
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
	metrics.sample.window.ms = 30000
	send.buffer.bytes = 131072
	max.in.flight.requests.per.connection = 5
	metrics.num.samples = 2
	linger.ms = 0
	client.id = 

16:34:41.937 [main] INFO  k.s.KafkaServer - [Kafka Server 0], shutting down
16:34:41.937 [main] INFO  k.s.KafkaServer - [Kafka Server 0], Starting controlled shutdown
16:34:41.939 [ZkClient-EventThread-1051-127.0.0.1:54998] INFO  k.c.ReplicaStateMachine$BrokerChangeListener - [BrokerChangeListener on Controller 0]: Broker change listener fired for path /brokers/ids with children 0
16:34:41.945 [ZkClient-EventThread-1051-127.0.0.1:54998] INFO  k.c.ReplicaStateMachine$BrokerChangeListener - [BrokerChangeListener on Controller 0]: Newly added brokers: 0, deleted brokers: , all live brokers: 0
16:34:41.945 [ZkClient-EventThread-1051-127.0.0.1:54998] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Controller 0 connected to id:0,host:testing-worker-linux-docker-a83abded-3380-linux-4.prod.travis-ci.org,port:45510 for sending state change requests
16:34:41.946 [ZkClient-EventThread-1051-127.0.0.1:54998] INFO  k.c.KafkaController - [Controller 0]: New broker startup callback for 0
16:34:41.946 [Controller-0-to-broker-0-send-thread] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Starting 
16:34:41.946 [kafka-request-handler-0] INFO  k.c.KafkaController - [Controller 0]: Shutting down broker 0
16:34:41.946 [main] INFO  k.s.KafkaServer - [Kafka Server 0], Controlled shutdown succeeded
16:34:41.946 [kafka-network-thread-45510-0] INFO  k.n.Processor - Closing socket connection to /172.17.1.249.
16:34:41.947 [main] INFO  k.n.SocketServer - [Socket Server on Broker 0], Shutting down
16:34:41.947 [main] INFO  k.n.SocketServer - [Socket Server on Broker 0], Shutdown completed
16:34:41.947 [main] INFO  k.s.KafkaRequestHandlerPool - [Kafka Request Handler on Broker 0], shutting down
16:34:41.948 [main] INFO  k.s.KafkaRequestHandlerPool - [Kafka Request Handler on Broker 0], shut down completely
16:34:41.996 [Curator-Framework-0-SendThread(127.0.0.1:54974)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:34:42.310 [main] INFO  k.s.ReplicaManager - [Replica Manager on Broker 0]: Shut down
16:34:42.310 [main] INFO  k.s.ReplicaFetcherManager - [ReplicaFetcherManager on broker 0] shutting down
16:34:42.310 [main] INFO  k.s.ReplicaFetcherManager - [ReplicaFetcherManager on broker 0] shutdown completed
16:34:42.310 [main] INFO  k.s.ReplicaManager - [Replica Manager on Broker 0]: Shut down completely
16:34:42.311 [main] INFO  k.l.LogManager - Shutting down.
16:34:42.311 [main] INFO  k.l.LogManager - Shutdown complete.
16:34:42.312 [main] INFO  k.c.PartitionStateMachine - [Partition state machine on Controller 0]: Stopped partition state machine
16:34:42.312 [main] INFO  k.c.ReplicaStateMachine - [Replica state machine on controller 0]: Stopped replica state machine
16:34:42.312 [main] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Shutting down
16:34:42.312 [main] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Shutdown completed
16:34:42.312 [ZkClient-EventThread-1051-127.0.0.1:54998] INFO  o.I.z.ZkEventThread - Terminate ZkClient event thread.
16:34:42.312 [Controller-0-to-broker-0-send-thread] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Stopped 
16:34:42.315 [main] INFO  k.s.KafkaServer - [Kafka Server 0], shut down completed
16:34:42.315 [Curator-Framework-0] INFO  o.a.c.f.i.CuratorFrameworkImpl - backgroundOperationsLoop exiting

--------------------------------------------------
--------------------------------------------------
classname: org.apache.storm.kafka.bolt.KafkaBoltTest / testname: executeWithBoltSpecifiedProperties
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.kafka.bolt.KafkaBoltTest.generateTestTuple(KafkaBoltTest.java:290)
	at org.apache.storm.kafka.bolt.KafkaBoltTest.executeWithBoltSpecifiedProperties(KafkaBoltTest.java:199)

-------------------- system-out --------------------
16:34:42.706 [Curator-Framework-0-SendThread(127.0.0.1:34460)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:34:42.728 [Curator-Framework-0-SendThread(127.0.0.1:44710)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:34:42.742 [Curator-Framework-0-SendThread(127.0.0.1:59679)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:34:42.793 [Curator-Framework-0-SendThread(127.0.0.1:58965)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:34:42.874 [Curator-Framework-0-SendThread(127.0.0.1:44186)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:34:43.097 [Curator-Framework-0-SendThread(127.0.0.1:54974)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:34:43.325 [main] INFO  o.a.c.f.i.CuratorFrameworkImpl - Starting
16:34:43.326 [main] INFO  k.u.VerifiableProperties - Verifying properties
16:34:43.326 [main] INFO  k.u.VerifiableProperties - Property broker.id is overridden to 0
16:34:43.326 [main] INFO  k.u.VerifiableProperties - Property log.dirs is overridden to /tmp/kafka/logs/kafka-test-46818
16:34:43.326 [main] INFO  k.u.VerifiableProperties - Property port is overridden to 46818
16:34:43.326 [main] INFO  k.u.VerifiableProperties - Property zookeeper.connect is overridden to 127.0.0.1:39682
16:34:43.327 [main] INFO  k.s.KafkaServer - [Kafka Server 0], starting
16:34:43.327 [main] INFO  k.s.KafkaServer - [Kafka Server 0], Connecting to zookeeper on 127.0.0.1:39682
16:34:43.327 [ZkClient-EventThread-1087-127.0.0.1:39682] INFO  o.I.z.ZkEventThread - Starting ZkClient event thread.
16:34:43.330 [main-EventThread] INFO  o.a.c.f.s.ConnectionStateManager - State change: CONNECTED
16:34:43.331 [main-EventThread] INFO  o.I.z.ZkClient - zookeeper state changed (SyncConnected)
16:34:43.389 [main] INFO  k.l.LogManager - Log directory '/tmp/kafka/logs/kafka-test-46818' not found, creating it.
16:34:43.389 [main] INFO  k.l.LogManager - Loading logs.
16:34:43.389 [main] INFO  k.l.LogManager - Logs loading complete.
16:34:43.389 [main] INFO  k.l.LogManager - Starting log cleanup with a period of 300000 ms.
16:34:43.390 [main] INFO  k.l.LogManager - Starting log flusher with a default period of 9223372036854775807 ms.
16:34:43.391 [main] INFO  k.n.Acceptor - Awaiting socket connections on 0.0.0.0:46818.
16:34:43.391 [main] INFO  k.n.SocketServer - [Socket Server on Broker 0], Started
16:34:43.395 [main] INFO  k.u.Mx4jLoader$ - Will not load MX4J, mx4j-tools.jar is not in the classpath
16:34:43.395 [main] INFO  k.c.KafkaController - [Controller 0]: Controller starting up
16:34:43.399 [main] INFO  k.s.ZookeeperLeaderElector - 0 successfully elected as leader
16:34:43.399 [main] INFO  k.c.KafkaController - [Controller 0]: Broker 0 starting become controller state transition
16:34:43.409 [main] INFO  k.c.KafkaController - [Controller 0]: Controller 0 incremented epoch to 1
16:34:43.414 [main] INFO  k.c.KafkaController - [Controller 0]: Partitions undergoing preferred replica election: 
16:34:43.414 [main] INFO  k.c.KafkaController - [Controller 0]: Partitions that completed preferred replica election: 
16:34:43.414 [main] INFO  k.c.KafkaController - [Controller 0]: Resuming preferred replica election for partitions: 
16:34:43.414 [main] INFO  k.c.KafkaController - [Controller 0]: Partitions being reassigned: Map()
16:34:43.414 [main] INFO  k.c.KafkaController - [Controller 0]: Partitions already reassigned: List()
16:34:43.415 [main] INFO  k.c.KafkaController - [Controller 0]: Resuming reassignment of partitions: Map()
16:34:43.423 [main] INFO  k.c.KafkaController - [Controller 0]: List of topics to be deleted: 
16:34:43.423 [main] INFO  k.c.KafkaController - [Controller 0]: List of topics ineligible for deletion: 
16:34:43.423 [main] INFO  k.c.KafkaController - [Controller 0]: Currently active brokers in the cluster: Set()
16:34:43.423 [main] INFO  k.c.KafkaController - [Controller 0]: Currently shutting brokers in the cluster: Set()
16:34:43.423 [main] INFO  k.c.KafkaController - [Controller 0]: Current list of topics in the cluster: Set()
16:34:43.424 [main] INFO  k.c.ReplicaStateMachine - [Replica state machine on controller 0]: Started replica state machine with initial state -> Map()
16:34:43.424 [main] INFO  k.c.PartitionStateMachine - [Partition state machine on Controller 0]: Started partition state machine with initial state -> Map()
16:34:43.424 [main] INFO  k.c.KafkaController - [Controller 0]: Broker 0 is ready to serve as the new controller with epoch 1
16:34:43.424 [main] INFO  k.c.KafkaController - [Controller 0]: Starting preferred replica leader election for partitions 
16:34:43.424 [main] INFO  k.c.PartitionStateMachine - [Partition state machine on Controller 0]: Invoking state change to OnlinePartition for partitions 
16:34:43.431 [main] INFO  k.c.KafkaController - [Controller 0]: starting the partition rebalance scheduler
16:34:43.432 [main] INFO  k.c.KafkaController - [Controller 0]: Controller startup complete
16:34:43.433 [ZkClient-EventThread-1087-127.0.0.1:39682] INFO  k.s.ZookeeperLeaderElector$LeaderChangeListener - New leader is 0
16:34:43.439 [main] INFO  k.u.ZkUtils$ - Registered broker 0 at path /brokers/ids/0 with address testing-worker-linux-docker-a83abded-3380-linux-4.prod.travis-ci.org:46818.
16:34:43.439 [main] INFO  k.s.KafkaServer - [Kafka Server 0], started
16:34:43.439 [main] INFO  o.a.k.c.p.ProducerConfig - ProducerConfig values: 
	compression.type = none
	metric.reporters = []
	metadata.max.age.ms = 300000
	metadata.fetch.timeout.ms = 1000
	acks = 1
	batch.size = 16384
	reconnect.backoff.ms = 10
	bootstrap.servers = [localhost:46818]
	receive.buffer.bytes = 32768
	retry.backoff.ms = 100
	buffer.memory = 33554432
	timeout.ms = 30000
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	retries = 0
	max.request.size = 1048576
	block.on.buffer.full = true
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
	metrics.sample.window.ms = 30000
	send.buffer.bytes = 131072
	max.in.flight.requests.per.connection = 5
	metrics.num.samples = 2
	linger.ms = 0
	client.id = 

16:34:43.441 [ZkClient-EventThread-1087-127.0.0.1:39682] INFO  k.c.ReplicaStateMachine$BrokerChangeListener - [BrokerChangeListener on Controller 0]: Broker change listener fired for path /brokers/ids with children 0
16:34:43.444 [ZkClient-EventThread-1087-127.0.0.1:39682] INFO  k.c.ReplicaStateMachine$BrokerChangeListener - [BrokerChangeListener on Controller 0]: Newly added brokers: 0, deleted brokers: , all live brokers: 0
16:34:43.459 [main] INFO  o.a.k.c.p.ProducerConfig - ProducerConfig values: 
	compression.type = none
	metric.reporters = []
	metadata.max.age.ms = 300000
	metadata.fetch.timeout.ms = 1000
	acks = 1
	batch.size = 16384
	reconnect.backoff.ms = 10
	bootstrap.servers = [localhost:46818]
	receive.buffer.bytes = 32768
	retry.backoff.ms = 100
	buffer.memory = 33554432
	timeout.ms = 30000
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	retries = 0
	max.request.size = 1048576
	block.on.buffer.full = true
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	metrics.sample.window.ms = 30000
	send.buffer.bytes = 131072
	max.in.flight.requests.per.connection = 5
	metrics.num.samples = 2
	linger.ms = 0
	client.id = 

16:34:43.461 [main] INFO  k.s.KafkaServer - [Kafka Server 0], shutting down
16:34:43.461 [main] INFO  k.s.KafkaServer - [Kafka Server 0], Starting controlled shutdown
16:34:43.461 [ZkClient-EventThread-1087-127.0.0.1:39682] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Controller 0 connected to id:0,host:testing-worker-linux-docker-a83abded-3380-linux-4.prod.travis-ci.org,port:46818 for sending state change requests
16:34:43.466 [ZkClient-EventThread-1087-127.0.0.1:39682] INFO  k.c.KafkaController - [Controller 0]: New broker startup callback for 0
16:34:43.466 [kafka-request-handler-0] INFO  k.c.KafkaController - [Controller 0]: Shutting down broker 0
16:34:43.467 [main] INFO  k.s.KafkaServer - [Kafka Server 0], Controlled shutdown succeeded
16:34:43.467 [main] INFO  k.n.SocketServer - [Socket Server on Broker 0], Shutting down
16:34:43.468 [main] INFO  k.n.SocketServer - [Socket Server on Broker 0], Shutdown completed
16:34:43.468 [main] INFO  k.s.KafkaRequestHandlerPool - [Kafka Request Handler on Broker 0], shutting down
16:34:43.468 [main] INFO  k.s.KafkaRequestHandlerPool - [Kafka Request Handler on Broker 0], shut down completely
16:34:43.469 [Controller-0-to-broker-0-send-thread] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Starting 
16:34:43.594 [main] INFO  k.s.ReplicaManager - [Replica Manager on Broker 0]: Shut down
16:34:43.594 [main] INFO  k.s.ReplicaFetcherManager - [ReplicaFetcherManager on broker 0] shutting down
16:34:43.594 [main] INFO  k.s.ReplicaFetcherManager - [ReplicaFetcherManager on broker 0] shutdown completed
16:34:43.594 [main] INFO  k.s.ReplicaManager - [Replica Manager on Broker 0]: Shut down completely
16:34:43.594 [main] INFO  k.l.LogManager - Shutting down.
16:34:43.595 [main] INFO  k.l.LogManager - Shutdown complete.
16:34:43.595 [main] INFO  k.c.PartitionStateMachine - [Partition state machine on Controller 0]: Stopped partition state machine
16:34:43.595 [main] INFO  k.c.ReplicaStateMachine - [Replica state machine on controller 0]: Stopped replica state machine
16:34:43.595 [main] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Shutting down
16:34:43.596 [main] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Shutdown completed
16:34:43.596 [Controller-0-to-broker-0-send-thread] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Stopped 
16:34:43.596 [ZkClient-EventThread-1087-127.0.0.1:39682] INFO  o.I.z.ZkEventThread - Terminate ZkClient event thread.
16:34:43.598 [main] INFO  k.s.KafkaServer - [Kafka Server 0], shut down completed
16:34:43.598 [Curator-Framework-0] INFO  o.a.c.f.i.CuratorFrameworkImpl - backgroundOperationsLoop exiting

--------------------------------------------------
Checking ./external/storm-kafka/target/surefire-reports/TEST-org.apache.storm.kafka.ExponentialBackoffMsgRetryManagerTest.xml
Checking ./external/storm-kafka/target/surefire-reports/TEST-org.apache.storm.kafka.ZkCoordinatorTest.xml
Checking ./external/storm-kafka/target/surefire-reports/TEST-org.apache.storm.kafka.TridentKafkaTest.xml
Looking for errors in ./external/storm-redis/target/surefire-reports
Checking ./external/storm-redis/target/surefire-reports/TEST-org.apache.storm.redis.state.DefaultStateSerializerTest.xml
Checking ./external/storm-redis/target/surefire-reports/TEST-org.apache.storm.redis.state.RedisKeyValueStateTest.xml
Checking ./external/storm-redis/target/surefire-reports/TEST-org.apache.storm.redis.state.RedisKeyValueStateProviderTest.xml

travis_time:end:14930bdd:start=1466699591536652232,finish=1466699940257532122,duration=348720879890[0K
[31;1mThe command "/bin/bash ./dev-tools/travis/travis-script.sh `pwd` $MODULES" exited with 1.[0m
travis_fold:start:cache.2[0Kstore build cache
travis_time:start:08d41c6b[0K
travis_time:end:08d41c6b:start=1466699940261740603,finish=1466699940265035947,duration=3295344[0Ktravis_time:start:0872ce68[0K[32;1mchange detected (content changed, file is created, or file is deleted):
/home/travis/.m2/repository/eigenbase/eigenbase-properties/1.1.4/eigenbase-properties-1.1.4.pom.lastUpdated
/home/travis/.m2/repository/io/confluent/kafka-avro-serializer/1.0/kafka-avro-serializer-1.0.pom.lastUpdated
/home/travis/.m2/repository/io/confluent/kafka-schema-registry-client/1.0/kafka-schema-registry-client-1.0.pom.lastUpdated
/home/travis/.m2/repository/net/hydromatic/linq4j/0.4/linq4j-0.4.pom.lastUpdated
/home/travis/.m2/repository/net/hydromatic/quidem/0.1.1/quidem-0.1.1.pom.lastUpdated
/home/travis/.m2/repository/org/apache/storm/flux/2.0.0-SNAPSHOT/maven-metadata-local.xml
/home/travis/.m2/repository/org/apache/storm/flux/2.0.0-SNAPSHOT/_remote.repositories
/home/travis/.m2/repository/org/apache/storm/flux-core/2.0.0-SNAPSHOT/flux-core-2.0.0-SNAPSHOT.jar
/home/travis/.m2/repository/org/apache/storm/flux-core/2.0.0-SNAPSHOT/maven-metadata-local.xml
/home/travis/.m2/repository/org/apache/storm/flux-core/2.0.0-SNAPSHOT/_remote.repositories
/home/travis/.m2/repository/org/ap
[0m
[32;1m...
[0m
[32;1mchanges detected, packing new archive[0m
.
.
.
.
.
.
.
[32;1muploading archive[0m

travis_time:end:0872ce68:start=1466699940268831072,finish=1466699998745436078,duration=58476605006[0Ktravis_fold:end:cache.2[0K
Done. Your build exited with 1.
