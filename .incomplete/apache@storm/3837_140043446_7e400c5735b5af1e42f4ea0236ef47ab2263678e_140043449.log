Using worker: worker-linux-docker-021d9b47.prod.travis-ci.org:travis-linux-15

travis_fold:start:system_info[0K[33;1mBuild system information[0m
Build language: java
Build group: stable
Build dist: precise
[34m[1mBuild image provisioning date and time[0m
Thu Feb  5 15:09:33 UTC 2015
[34m[1mOperating System Details[0m
Distributor ID:	Ubuntu
Description:	Ubuntu 12.04.5 LTS
Release:	12.04
Codename:	precise
[34m[1mLinux Version[0m
3.13.0-29-generic
[34m[1mCookbooks Version[0m
a68419e https://github.com/travis-ci/travis-cookbooks/tree/a68419e
[34m[1mGCC version[0m
gcc (Ubuntu/Linaro 4.6.3-1ubuntu5) 4.6.3
Copyright (C) 2011 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.

[34m[1mLLVM version[0m
clang version 3.4 (tags/RELEASE_34/final)
Target: x86_64-unknown-linux-gnu
Thread model: posix
[34m[1mPre-installed Ruby versions[0m
ruby-1.9.3-p551
[34m[1mPre-installed Node.js versions[0m
v0.10.36
[34m[1mPre-installed Go versions[0m
1.4.1
[34m[1mRedis version[0m
redis-server 2.8.19
[34m[1mriak version[0m
2.0.2
[34m[1mMongoDB version[0m
MongoDB 2.4.12
[34m[1mCouchDB version[0m
couchdb 1.6.1
[34m[1mNeo4j version[0m
1.9.4
[34m[1mRabbitMQ Version[0m
3.4.3
[34m[1mElasticSearch version[0m
1.4.0
[34m[1mInstalled Sphinx versions[0m
2.0.10
2.1.9
2.2.6
[34m[1mDefault Sphinx version[0m
2.2.6
[34m[1mInstalled Firefox version[0m
firefox 31.0esr
[34m[1mPhantomJS version[0m
1.9.8
[34m[1mant -version[0m
Apache Ant(TM) version 1.8.2 compiled on December 3 2011
[34m[1mmvn -version[0m
Apache Maven 3.2.5 (12a6b3acb947671f09b81f49094c53f426d8cea1; 2014-12-14T17:29:23+00:00)
Maven home: /usr/local/maven
Java version: 1.7.0_76, vendor: Oracle Corporation
Java home: /usr/lib/jvm/java-7-oracle/jre
Default locale: en_US, platform encoding: ANSI_X3.4-1968
OS name: "linux", version: "3.13.0-29-generic", arch: "amd64", family: "unix"
travis_fold:end:system_info[0K
travis_fold:start:fix.CVE-2015-7547[0K$ export DEBIAN_FRONTEND=noninteractive
W: Size of file /var/lib/apt/lists/us.archive.ubuntu.com_ubuntu_dists_precise-backports_multiverse_source_Sources.gz is not what the server reported 5886 5888
W: Size of file /var/lib/apt/lists/ppa.launchpad.net_ubuntugis_ppa_ubuntu_dists_precise_main_binary-amd64_Packages.gz is not what the server reported 36669 36677
W: Size of file /var/lib/apt/lists/ppa.launchpad.net_ubuntugis_ppa_ubuntu_dists_precise_main_binary-i386_Packages.gz is not what the server reported 36729 36733
Reading package lists...
Building dependency tree...
Reading state information...
The following extra packages will be installed:
  libc-bin libc-dev-bin libc6-dev
Suggested packages:
  glibc-doc
The following packages will be upgraded:
  libc-bin libc-dev-bin libc6 libc6-dev
4 upgraded, 0 newly installed, 0 to remove and 247 not upgraded.
Need to get 8,840 kB of archives.
After this operation, 14.3 kB disk space will be freed.
Get:1 http://us.archive.ubuntu.com/ubuntu/ precise-updates/main libc6-dev amd64 2.15-0ubuntu10.15 [2,943 kB]
Get:2 http://us.archive.ubuntu.com/ubuntu/ precise-updates/main libc-dev-bin amd64 2.15-0ubuntu10.15 [84.7 kB]
Get:3 http://us.archive.ubuntu.com/ubuntu/ precise-updates/main libc-bin amd64 2.15-0ubuntu10.15 [1,177 kB]
Get:4 http://us.archive.ubuntu.com/ubuntu/ precise-updates/main libc6 amd64 2.15-0ubuntu10.15 [4,636 kB]
Fetched 8,840 kB in 0s (34.4 MB/s)
Preconfiguring packages ...
(Reading database ... (Reading database ... 5%(Reading database ... 10%(Reading database ... 15%(Reading database ... 20%(Reading database ... 25%(Reading database ... 30%(Reading database ... 35%(Reading database ... 40%(Reading database ... 45%(Reading database ... 50%(Reading database ... 55%(Reading database ... 60%(Reading database ... 65%(Reading database ... 70%(Reading database ... 75%(Reading database ... 80%(Reading database ... 85%(Reading database ... 90%(Reading database ... 95%(Reading database ... 100%(Reading database ... 72019 files and directories currently installed.)
Preparing to replace libc6-dev 2.15-0ubuntu10.10 (using .../libc6-dev_2.15-0ubuntu10.15_amd64.deb) ...
Unpacking replacement libc6-dev ...
Preparing to replace libc-dev-bin 2.15-0ubuntu10.10 (using .../libc-dev-bin_2.15-0ubuntu10.15_amd64.deb) ...
Unpacking replacement libc-dev-bin ...
Preparing to replace libc-bin 2.15-0ubuntu10.10 (using .../libc-bin_2.15-0ubuntu10.15_amd64.deb) ...
Unpacking replacement libc-bin ...
Processing triggers for man-db ...
Setting up libc-bin (2.15-0ubuntu10.15) ...
(Reading database ... (Reading database ... 5%(Reading database ... 10%(Reading database ... 15%(Reading database ... 20%(Reading database ... 25%(Reading database ... 30%(Reading database ... 35%(Reading database ... 40%(Reading database ... 45%(Reading database ... 50%(Reading database ... 55%(Reading database ... 60%(Reading database ... 65%(Reading database ... 70%(Reading database ... 75%(Reading database ... 80%(Reading database ... 85%(Reading database ... 90%(Reading database ... 95%(Reading database ... 100%(Reading database ... 72018 files and directories currently installed.)
Preparing to replace libc6 2.15-0ubuntu10.10 (using .../libc6_2.15-0ubuntu10.15_amd64.deb) ...
Unpacking replacement libc6 ...
Setting up libc6 (2.15-0ubuntu10.15) ...
Setting up libc-dev-bin (2.15-0ubuntu10.15) ...
Setting up libc6-dev (2.15-0ubuntu10.15) ...
Processing triggers for libc-bin ...
ldconfig deferred processing now taking place
travis_fold:end:fix.CVE-2015-7547[0Ktravis_fold:start:git.checkout[0Ktravis_time:start:0004c72b[0K$ git clone --depth=50 https://github.com/apache/storm.git apache/storm
Cloning into 'apache/storm'...
remote: Counting objects: 19310, done.[K
remote: Compressing objects:   0% (1/7822)   [Kremote: Compressing objects:   1% (79/7822)   [Kremote: Compressing objects:   2% (157/7822)   [Kremote: Compressing objects:   3% (235/7822)   [Kremote: Compressing objects:   4% (313/7822)   [Kremote: Compressing objects:   5% (392/7822)   [Kremote: Compressing objects:   6% (470/7822)   [Kremote: Compressing objects:   7% (548/7822)   [Kremote: Compressing objects:   8% (626/7822)   [Kremote: Compressing objects:   9% (704/7822)   [Kremote: Compressing objects:  10% (783/7822)   [Kremote: Compressing objects:  11% (861/7822)   [Kremote: Compressing objects:  12% (939/7822)   [Kremote: Compressing objects:  13% (1017/7822)   [Kremote: Compressing objects:  14% (1096/7822)   [Kremote: Compressing objects:  15% (1174/7822)   [Kremote: Compressing objects:  16% (1252/7822)   [Kremote: Compressing objects:  17% (1330/7822)   [Kremote: Compressing objects:  18% (1408/7822)   [Kremote: Compressing objects:  19% (1487/7822)   [Kremote: Compressing objects:  20% (1565/7822)   [Kremote: Compressing objects:  21% (1643/7822)   [Kremote: Compressing objects:  22% (1721/7822)   [Kremote: Compressing objects:  23% (1800/7822)   [Kremote: Compressing objects:  24% (1878/7822)   [Kremote: Compressing objects:  25% (1956/7822)   [Kremote: Compressing objects:  26% (2034/7822)   [Kremote: Compressing objects:  27% (2112/7822)   [Kremote: Compressing objects:  28% (2191/7822)   [Kremote: Compressing objects:  29% (2269/7822)   [Kremote: Compressing objects:  30% (2347/7822)   [Kremote: Compressing objects:  31% (2425/7822)   [Kremote: Compressing objects:  32% (2504/7822)   [Kremote: Compressing objects:  33% (2582/7822)   [Kremote: Compressing objects:  34% (2660/7822)   [Kremote: Compressing objects:  35% (2738/7822)   [Kremote: Compressing objects:  36% (2816/7822)   [Kremote: Compressing objects:  37% (2895/7822)   [Kremote: Compressing objects:  38% (2973/7822)   [Kremote: Compressing objects:  39% (3051/7822)   [Kremote: Compressing objects:  40% (3129/7822)   [Kremote: Compressing objects:  41% (3208/7822)   [Kremote: Compressing objects:  42% (3286/7822)   [Kremote: Compressing objects:  43% (3364/7822)   [Kremote: Compressing objects:  44% (3442/7822)   [Kremote: Compressing objects:  45% (3520/7822)   [Kremote: Compressing objects:  46% (3599/7822)   [Kremote: Compressing objects:  47% (3677/7822)   [Kremote: Compressing objects:  48% (3755/7822)   [Kremote: Compressing objects:  49% (3833/7822)   [Kremote: Compressing objects:  50% (3911/7822)   [Kremote: Compressing objects:  51% (3990/7822)   [Kremote: Compressing objects:  52% (4068/7822)   [Kremote: Compressing objects:  53% (4146/7822)   [Kremote: Compressing objects:  54% (4224/7822)   [Kremote: Compressing objects:  55% (4303/7822)   [Kremote: Compressing objects:  56% (4381/7822)   [Kremote: Compressing objects:  57% (4459/7822)   [Kremote: Compressing objects:  58% (4537/7822)   [Kremote: Compressing objects:  59% (4615/7822)   [Kremote: Compressing objects:  60% (4694/7822)   [Kremote: Compressing objects:  61% (4772/7822)   [Kremote: Compressing objects:  62% (4850/7822)   [Kremote: Compressing objects:  63% (4928/7822)   [Kremote: Compressing objects:  64% (5007/7822)   [Kremote: Compressing objects:  65% (5085/7822)   [Kremote: Compressing objects:  66% (5163/7822)   [Kremote: Compressing objects:  67% (5241/7822)   [Kremote: Compressing objects:  68% (5319/7822)   [Kremote: Compressing objects:  69% (5398/7822)   [Kremote: Compressing objects:  70% (5476/7822)   [Kremote: Compressing objects:  71% (5554/7822)   [Kremote: Compressing objects:  72% (5632/7822)   [Kremote: Compressing objects:  73% (5711/7822)   [Kremote: Compressing objects:  74% (5789/7822)   [Kremote: Compressing objects:  75% (5867/7822)   [Kremote: Compressing objects:  76% (5945/7822)   [Kremote: Compressing objects:  77% (6023/7822)   [Kremote: Compressing objects:  78% (6102/7822)   [Kremote: Compressing objects:  79% (6180/7822)   [Kremote: Compressing objects:  80% (6258/7822)   [Kremote: Compressing objects:  81% (6336/7822)   [Kremote: Compressing objects:  82% (6415/7822)   [Kremote: Compressing objects:  83% (6493/7822)   [Kremote: Compressing objects:  84% (6571/7822)   [Kremote: Compressing objects:  85% (6649/7822)   [Kremote: Compressing objects:  86% (6727/7822)   [Kremote: Compressing objects:  87% (6806/7822)   [Kremote: Compressing objects:  88% (6884/7822)   [Kremote: Compressing objects:  89% (6962/7822)   [Kremote: Compressing objects:  90% (7040/7822)   [Kremote: Compressing objects:  91% (7119/7822)   [Kremote: Compressing objects:  92% (7197/7822)   [Kremote: Compressing objects:  93% (7275/7822)   [Kremote: Compressing objects:  94% (7353/7822)   [Kremote: Compressing objects:  95% (7431/7822)   [Kremote: Compressing objects:  96% (7510/7822)   [Kremote: Compressing objects:  97% (7588/7822)   [Kremote: Compressing objects:  98% (7666/7822)   [Kremote: Compressing objects:  99% (7744/7822)   [Kremote: Compressing objects: 100% (7822/7822)   [Kremote: Compressing objects: 100% (7822/7822), done.[K
Receiving objects:   0% (1/19310)   Receiving objects:   1% (194/19310)   Receiving objects:   2% (387/19310)   Receiving objects:   3% (580/19310)   Receiving objects:   4% (773/19310)   Receiving objects:   5% (966/19310)   Receiving objects:   6% (1159/19310)   Receiving objects:   7% (1352/19310)   Receiving objects:   8% (1545/19310)   Receiving objects:   9% (1738/19310)   Receiving objects:  10% (1931/19310)   Receiving objects:  11% (2125/19310)   Receiving objects:  12% (2318/19310)   Receiving objects:  13% (2511/19310)   Receiving objects:  14% (2704/19310)   Receiving objects:  15% (2897/19310)   Receiving objects:  16% (3090/19310)   Receiving objects:  17% (3283/19310)   Receiving objects:  18% (3476/19310)   Receiving objects:  19% (3669/19310)   Receiving objects:  20% (3862/19310)   Receiving objects:  21% (4056/19310)   Receiving objects:  22% (4249/19310)   Receiving objects:  23% (4442/19310)   Receiving objects:  24% (4635/19310)   Receiving objects:  25% (4828/19310)   Receiving objects:  26% (5021/19310)   Receiving objects:  27% (5214/19310)   Receiving objects:  28% (5407/19310)   Receiving objects:  29% (5600/19310)   Receiving objects:  30% (5793/19310)   Receiving objects:  31% (5987/19310)   Receiving objects:  32% (6180/19310)   Receiving objects:  33% (6373/19310)   Receiving objects:  34% (6566/19310)   Receiving objects:  35% (6759/19310)   Receiving objects:  36% (6952/19310)   Receiving objects:  37% (7145/19310)   Receiving objects:  38% (7338/19310)   Receiving objects:  39% (7531/19310)   Receiving objects:  40% (7724/19310)   Receiving objects:  41% (7918/19310)   Receiving objects:  42% (8111/19310)   Receiving objects:  43% (8304/19310)   Receiving objects:  44% (8497/19310)   Receiving objects:  45% (8690/19310)   Receiving objects:  46% (8883/19310)   Receiving objects:  47% (9076/19310)   Receiving objects:  48% (9269/19310)   Receiving objects:  49% (9462/19310)   Receiving objects:  50% (9655/19310)   Receiving objects:  51% (9849/19310)   Receiving objects:  52% (10042/19310)   Receiving objects:  53% (10235/19310)   Receiving objects:  54% (10428/19310)   Receiving objects:  55% (10621/19310)   Receiving objects:  56% (10814/19310)   Receiving objects:  57% (11007/19310)   Receiving objects:  58% (11200/19310)   Receiving objects:  59% (11393/19310)   Receiving objects:  60% (11586/19310)   Receiving objects:  61% (11780/19310)   Receiving objects:  62% (11973/19310)   Receiving objects:  63% (12166/19310)   Receiving objects:  64% (12359/19310), 10.57 MiB | 21.08 MiB/s   Receiving objects:  65% (12552/19310), 10.57 MiB | 21.08 MiB/s   Receiving objects:  66% (12745/19310), 10.57 MiB | 21.08 MiB/s   Receiving objects:  67% (12938/19310), 10.57 MiB | 21.08 MiB/s   Receiving objects:  68% (13131/19310), 10.57 MiB | 21.08 MiB/s   Receiving objects:  69% (13324/19310), 10.57 MiB | 21.08 MiB/s   Receiving objects:  70% (13517/19310), 10.57 MiB | 21.08 MiB/s   Receiving objects:  71% (13711/19310), 10.57 MiB | 21.08 MiB/s   Receiving objects:  72% (13904/19310), 10.57 MiB | 21.08 MiB/s   Receiving objects:  73% (14097/19310), 10.57 MiB | 21.08 MiB/s   Receiving objects:  74% (14290/19310), 10.57 MiB | 21.08 MiB/s   Receiving objects:  75% (14483/19310), 10.57 MiB | 21.08 MiB/s   Receiving objects:  76% (14676/19310), 10.57 MiB | 21.08 MiB/s   Receiving objects:  77% (14869/19310), 10.57 MiB | 21.08 MiB/s   Receiving objects:  78% (15062/19310), 10.57 MiB | 21.08 MiB/s   Receiving objects:  79% (15255/19310), 10.57 MiB | 21.08 MiB/s   Receiving objects:  80% (15448/19310), 10.57 MiB | 21.08 MiB/s   Receiving objects:  81% (15642/19310), 10.57 MiB | 21.08 MiB/s   Receiving objects:  82% (15835/19310), 10.57 MiB | 21.08 MiB/s   Receiving objects:  83% (16028/19310), 10.57 MiB | 21.08 MiB/s   Receiving objects:  84% (16221/19310), 10.57 MiB | 21.08 MiB/s   Receiving objects:  85% (16414/19310), 10.57 MiB | 21.08 MiB/s   Receiving objects:  86% (16607/19310), 10.57 MiB | 21.08 MiB/s   Receiving objects:  87% (16800/19310), 10.57 MiB | 21.08 MiB/s   Receiving objects:  88% (16993/19310), 10.57 MiB | 21.08 MiB/s   Receiving objects:  89% (17186/19310), 10.57 MiB | 21.08 MiB/s   Receiving objects:  90% (17379/19310), 10.57 MiB | 21.08 MiB/s   Receiving objects:  91% (17573/19310), 10.57 MiB | 21.08 MiB/s   Receiving objects:  92% (17766/19310), 10.57 MiB | 21.08 MiB/s   Receiving objects:  93% (17959/19310), 10.57 MiB | 21.08 MiB/s   Receiving objects:  94% (18152/19310), 10.57 MiB | 21.08 MiB/s   Receiving objects:  95% (18345/19310), 10.57 MiB | 21.08 MiB/s   Receiving objects:  96% (18538/19310), 10.57 MiB | 21.08 MiB/s   Receiving objects:  97% (18731/19310), 10.57 MiB | 21.08 MiB/s   Receiving objects:  98% (18924/19310), 10.57 MiB | 21.08 MiB/s   remote: Total 19310 (delta 9316), reused 16551 (delta 7163), pack-reused 0[K
Receiving objects:  99% (19117/19310), 10.57 MiB | 21.08 MiB/s   Receiving objects: 100% (19310/19310), 10.57 MiB | 21.08 MiB/s   Receiving objects: 100% (19310/19310), 14.72 MiB | 21.08 MiB/s, done.
Resolving deltas:   0% (0/9316)   Resolving deltas:   1% (127/9316)   Resolving deltas:   2% (236/9316)   Resolving deltas:   3% (280/9316)   Resolving deltas:   4% (383/9316)   Resolving deltas:   5% (469/9316)   Resolving deltas:   6% (562/9316)   Resolving deltas:   7% (658/9316)   Resolving deltas:   8% (811/9316)   Resolving deltas:   9% (840/9316)   Resolving deltas:  10% (938/9316)   Resolving deltas:  11% (1034/9316)   Resolving deltas:  12% (1120/9316)   Resolving deltas:  13% (1213/9316)   Resolving deltas:  14% (1307/9316)   Resolving deltas:  15% (1398/9316)   Resolving deltas:  16% (1513/9316)   Resolving deltas:  17% (1589/9316)   Resolving deltas:  18% (1677/9316)   Resolving deltas:  19% (1776/9316)   Resolving deltas:  20% (1869/9316)   Resolving deltas:  21% (1960/9316)   Resolving deltas:  22% (2050/9316)   Resolving deltas:  23% (2149/9316)   Resolving deltas:  24% (2274/9316)   Resolving deltas:  25% (2331/9316)   Resolving deltas:  26% (2427/9316)   Resolving deltas:  27% (2517/9316)   Resolving deltas:  31% (2951/9316)   Resolving deltas:  32% (2990/9316)   Resolving deltas:  33% (3088/9316)   Resolving deltas:  34% (3245/9316)   Resolving deltas:  35% (3294/9316)   Resolving deltas:  39% (3644/9316)   Resolving deltas:  40% (3738/9316)   Resolving deltas:  41% (3820/9316)   Resolving deltas:  42% (3914/9316)   Resolving deltas:  43% (4009/9316)   Resolving deltas:  44% (4101/9316)   Resolving deltas:  45% (4194/9316)   Resolving deltas:  46% (4298/9316)   Resolving deltas:  47% (4388/9316)   Resolving deltas:  48% (4473/9316)   Resolving deltas:  49% (4566/9316)   Resolving deltas:  50% (4660/9316)   Resolving deltas:  51% (4752/9316)   Resolving deltas:  52% (4846/9316)   Resolving deltas:  53% (4951/9316)   Resolving deltas:  54% (5035/9316)   Resolving deltas:  55% (5125/9316)   Resolving deltas:  56% (5219/9316)   Resolving deltas:  57% (5311/9316)   Resolving deltas:  58% (5405/9316)   Resolving deltas:  60% (5605/9316)   Resolving deltas:  61% (5684/9316)   Resolving deltas:  62% (5778/9316)   Resolving deltas:  63% (5873/9316)   Resolving deltas:  64% (5989/9316)   Resolving deltas:  65% (6074/9316)   Resolving deltas:  66% (6158/9316)   Resolving deltas:  67% (6243/9316)   Resolving deltas:  68% (6335/9316)   Resolving deltas:  69% (6432/9316)   Resolving deltas:  71% (6638/9316)   Resolving deltas:  73% (6820/9316)   Resolving deltas:  74% (6934/9316)   Resolving deltas:  79% (7443/9316)   Resolving deltas:  80% (7456/9316)   Resolving deltas:  81% (7554/9316)   Resolving deltas:  82% (7643/9316)   Resolving deltas:  83% (7733/9316)   Resolving deltas:  84% (7832/9316)   Resolving deltas:  85% (7919/9316)   Resolving deltas:  86% (8038/9316)   Resolving deltas:  87% (8105/9316)   Resolving deltas:  88% (8255/9316)   Resolving deltas:  91% (8505/9316)   Resolving deltas:  92% (8571/9316)   Resolving deltas:  93% (8687/9316)   Resolving deltas:  94% (8765/9316)   Resolving deltas:  95% (8857/9316)   Resolving deltas:  96% (8950/9316)   Resolving deltas:  97% (9044/9316)   Resolving deltas:  98% (9142/9316)   Resolving deltas:  99% (9259/9316)   Resolving deltas: 100% (9316/9316)   Resolving deltas: 100% (9316/9316), done.
Checking connectivity... done.

travis_time:end:0004c72b:start=1466779555585029008,finish=1466779558346499202,duration=2761470194[0K$ cd apache/storm
travis_time:start:002d6450[0K$ git fetch origin +refs/pull/1515/merge:
remote: Counting objects: 14, done.[K
remote: Compressing objects:   9% (1/11)   [Kremote: Compressing objects:  18% (2/11)   [Kremote: Compressing objects:  27% (3/11)   [Kremote: Compressing objects:  36% (4/11)   [Kremote: Compressing objects:  45% (5/11)   [Kremote: Compressing objects:  54% (6/11)   [Kremote: Compressing objects:  63% (7/11)   [Kremote: Compressing objects:  72% (8/11)   [Kremote: Compressing objects:  81% (9/11)   [Kremote: Compressing objects:  90% (10/11)   [Kremote: Compressing objects: 100% (11/11)   [Kremote: Compressing objects: 100% (11/11), done.[K
remote: Total 14 (delta 7), reused 5 (delta 0), pack-reused 0[K
Unpacking objects:   7% (1/14)   Unpacking objects:  14% (2/14)   Unpacking objects:  21% (3/14)   Unpacking objects:  28% (4/14)   Unpacking objects:  35% (5/14)   Unpacking objects:  42% (6/14)   Unpacking objects:  50% (7/14)   Unpacking objects:  57% (8/14)   Unpacking objects:  64% (9/14)   Unpacking objects:  71% (10/14)   Unpacking objects:  78% (11/14)   Unpacking objects:  85% (12/14)   Unpacking objects:  92% (13/14)   Unpacking objects: 100% (14/14)   Unpacking objects: 100% (14/14), done.
From https://github.com/apache/storm
 * branch            refs/pull/1515/merge -> FETCH_HEAD

travis_time:end:002d6450:start=1466779558350690396,finish=1466779558666787806,duration=316097410[0K$ git checkout -qf FETCH_HEAD
travis_fold:end:git.checkout[0K
[33;1mThis job is running on container-based infrastructure, which does not allow use of 'sudo', setuid and setguid executables.[0m
[33;1mIf you require sudo, add 'sudo: required' to your .travis.yml[0m
[33;1mSee https://docs.travis-ci.com/user/workers/container-based-infrastructure/ for details.[0m

[33;1mSetting environment variables from .travis.yml[0m
$ export MODULES='!storm-core'

$ jdk_switcher use oraclejdk7
Switching to Oracle JDK7 (java-7-oracle), JAVA_HOME will be set to /usr/lib/jvm/java-7-oracle
travis_fold:start:cache.1[0KSetting up build cache
$ export CASHER_DIR=$HOME/.casher
travis_time:start:0d02bf49[0K$ Installing caching utilities

travis_time:end:0d02bf49:start=1466779560537073587,finish=1466779560583935095,duration=46861508[0Ktravis_time:start:181e7bc6[0K
travis_time:end:181e7bc6:start=1466779560588695864,finish=1466779560591847379,duration=3151515[0Ktravis_time:start:1b730b6f[0K[32;1mattempting to download cache archive[0m
[32;1mfetching PR.1515/cache-linux-precise-0b3559b5dae2b3e32156c6d840f23972d650aa066922e3318f7e5db8246681fa--jdk-oraclejdk7.tgz[0m
[32;1mfound cache[0m

travis_time:end:1b730b6f:start=1466779560595695410,finish=1466779571753600146,duration=11157904736[0Ktravis_time:start:072ba383[0K
travis_time:end:072ba383:start=1466779571757394834,finish=1466779571760693789,duration=3298955[0Ktravis_time:start:1243b900[0K[32;1madding /home/travis/.m2/repository to cache[0m
[32;1madding /home/travis/.rvm to cache[0m
[32;1madding /home/travis/.nvm to cache[0m

travis_time:end:1243b900:start=1466779571764486620,finish=1466779581534547535,duration=9770060915[0Ktravis_fold:end:cache.1[0K$ java -Xmx32m -version
java version "1.7.0_76"
Java(TM) SE Runtime Environment (build 1.7.0_76-b13)
Java HotSpot(TM) 64-Bit Server VM (build 24.76-b04, mixed mode)
$ javac -J-Xmx32m -version
javac 1.7.0_76
travis_fold:start:before_install.1[0Ktravis_time:start:21a1e354[0K$ rvm use 2.1.5 --install
[32mUsing /home/travis/.rvm/gems/ruby-2.1.5[0m

travis_time:end:21a1e354:start=1466779581881393678,finish=1466779582128667882,duration=247274204[0Ktravis_fold:end:before_install.1[0Ktravis_fold:start:before_install.2[0Ktravis_time:start:0a202dc6[0K$ nvm install 0.12.2
v0.12.2 is already installed.
Now using node v0.12.2

travis_time:end:0a202dc6:start=1466779582133353941,finish=1466779582405739062,duration=272385121[0Ktravis_fold:end:before_install.2[0Ktravis_fold:start:before_install.3[0Ktravis_time:start:0176a228[0K$ nvm use 0.12.2
Now using node v0.12.2

travis_time:end:0176a228:start=1466779582410394407,finish=1466779582466835157,duration=56440750[0Ktravis_fold:end:before_install.3[0Ktravis_fold:start:install[0Ktravis_time:start:003e4013[0K$ /bin/bash ./dev-tools/travis/travis-install.sh `pwd`
Python version :   Python 2.7.3
Maven version  :   Apache Maven 3.2.5 (12a6b3acb947671f09b81f49094c53f426d8cea1; 2014-12-14T17:29:23+00:00) Maven home: /usr/local/maven Java version: 1.7.0_76, vendor: Oracle Corporation Java home: /usr/lib/jvm/java-7-oracle/jre Default locale: en_US, platform encoding: UTF-8 OS name: "linux", version: "3.13.0-40-generic", arch: "amd64", family: "unix"
['mvn', 'clean', 'install', '-DskipTests', '-Pnative', '--batch-mode'] writing to install.txt
1 seconds 1 log lines11 seconds 239 log lines32 seconds 323 log lines45 seconds 334 log lines55 seconds 571 log lines72 seconds 652 log lines86 seconds 865 log lines97 seconds 914 log lines107 seconds 980 log lines117 seconds 1273 log lines138 seconds 1461 log lines150 seconds 1663 log lines160 seconds 1919 log lines172 seconds 2199 log lines189 seconds 2361 log lines201 seconds 2611 log lines203 seconds 2664 log lines
['mvn', 'clean', 'install', '-DskipTests', '-Pnative', '--batch-mode'] done 0

travis_time:end:003e4013:start=1466779582471454929,finish=1466779786418791599,duration=203947336670[0Ktravis_fold:end:install[0Ktravis_time:start:1bfca116[0K$ /bin/bash ./dev-tools/travis/travis-script.sh `pwd` $MODULES
Python version :   Python 2.7.3
Ruby version   :   ruby 2.1.5p273 (2014-11-13 revision 48405) [x86_64-linux]
NodeJs version :   v0.12.2
Maven version  :   Apache Maven 3.2.5 (12a6b3acb947671f09b81f49094c53f426d8cea1; 2014-12-14T17:29:23+00:00) Maven home: /usr/local/maven Java version: 1.7.0_76, vendor: Oracle Corporation Java home: /usr/lib/jvm/java-7-oracle/jre Default locale: en_US, platform encoding: UTF-8 OS name: "linux", version: "3.13.0-40-generic", arch: "amd64", family: "unix"
[INFO] Scanning for projects...
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Build Order:
[INFO] 
[INFO] Storm
[INFO] multilang-javascript
[INFO] multilang-python
[INFO] multilang-ruby
[INFO] maven-shade-clojure-transformer
[INFO] storm-maven-plugins
[INFO] storm-rename-hack
[INFO] storm-kafka
[INFO] storm-hdfs
[INFO] storm-hbase
[INFO] storm-hive
[INFO] storm-jdbc
[INFO] storm-redis
[INFO] storm-eventhubs
[INFO] flux
[INFO] flux-wrappers
[INFO] flux-core
[INFO] flux-examples
[INFO] storm-sql-runtime
[INFO] storm-sql-core
[INFO] storm-sql-kafka
[INFO] sql
[INFO] storm-elasticsearch
[INFO] storm-solr
[INFO] storm-metrics
[INFO] storm-cassandra
[INFO] storm-mqtt-parent
[INFO] storm-mqtt
[INFO] storm-mqtt-examples
[INFO] storm-mongodb
[INFO] storm-clojure
[INFO] storm-starter
[INFO] storm-kafka-client
[INFO] storm-opentsdb
[INFO] storm-kafka-monitor
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Storm 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ storm ---
[INFO] 
[INFO] --- apache-rat-plugin:0.11:check (default) @ storm ---
[INFO] 51 implicit excludes (use -debug for more details).
[INFO] Exclude: **/target/**
[INFO] Exclude: **/*.iml
[INFO] Exclude: **/.idea/**
[INFO] Exclude: **/metastore_db/**
[INFO] Exclude: **/build/**
[INFO] Exclude: **/logs/**
[INFO] Exclude: **/CHANGELOG.md
[INFO] Exclude: **/README.md
[INFO] Exclude: **/README.markdown
[INFO] Exclude: **/DEVELOPER.md
[INFO] Exclude: **/BYLAWS.md
[INFO] Exclude: **/STORM-UI-REST-API.md
[INFO] Exclude: SECURITY.md
[INFO] Exclude: VERSION
[INFO] Exclude: TODO
[INFO] Exclude: **/src/py/**
[INFO] Exclude: **/src/ui/public/js/jquery.dataTables.1.10.4.min.js
[INFO] Exclude: **/src/ui/public/css/jquery.dataTables.1.10.4.min.css
[INFO] Exclude: **/src/ui/public/images/*
[INFO] Exclude: **/src/ui/public/js/bootstrap-3.3.1.min.js
[INFO] Exclude: **/src/ui/public/css/bootstrap-3.3.1.min.css
[INFO] Exclude: **/src/ui/public/js/dataTables.bootstrap.min.js
[INFO] Exclude: **/src/ui/public/css/dataTables.bootstrap.css
[INFO] Exclude: **/src/ui/public/js/jsonFormatter.min.js
[INFO] Exclude: **/src/ui/public/css/jsonFormatter.min.css
[INFO] Exclude: **/src/ui/public/js/jquery-1.11.1.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.cookies.2.2.0.min.js
[INFO] Exclude: **/src/ui/public/js/moment.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.blockUI.min.js
[INFO] Exclude: **/src/ui/public/js/url.min.js
[INFO] Exclude: **/src/ui/public/js/arbor.js
[INFO] Exclude: **/src/ui/public/js/arbor-graphics.js
[INFO] Exclude: **/src/ui/public/js/arbor-tween.js
[INFO] Exclude: **/src/ui/public/js/jquery.mustache.js
[INFO] Exclude: **/src/ui/public/js/typeahead.jquery.min.js
[INFO] Exclude: **/dependency-reduced-pom.xml
[INFO] Exclude: **/docs/**
[INFO] Exclude: **/.git/**
[INFO] Exclude: **/derby.log
[INFO] Exclude: **/src/dev/**
[INFO] Exclude: **/src/codegen/config.fmpp
[INFO] Exclude: **/src/codegen/data/Parser.tdd
[INFO] Exclude: **/src/test/resources/FixedAvroSerializer.config
[INFO] 1856 resources included (use -debug for more details)
Warning:  org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
Compiler warnings:
  WARNING:  'org.apache.xerces.jaxp.SAXParserImpl: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.'
Warning:  org.apache.xerces.parsers.SAXParser: Feature 'http://javax.xml.XMLConstants/feature/secure-processing' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
[INFO] Rat check: Summary of files. Unapproved: 0 unknown: 0 generated: 0 approved: 1843 licence.
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building multilang-javascript 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ multilang-javascript ---
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ multilang-javascript ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 1 resource
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ multilang-javascript ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ multilang-javascript ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/storm-multilang/javascript/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ multilang-javascript ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-surefire-plugin:2.18.1:test (default-test) @ multilang-javascript ---
[INFO] 
[INFO] --- apache-rat-plugin:0.11:check (default) @ multilang-javascript ---
[INFO] 51 implicit excludes (use -debug for more details).
[INFO] Exclude: **/target/**
[INFO] Exclude: **/*.iml
[INFO] Exclude: **/.idea/**
[INFO] Exclude: **/metastore_db/**
[INFO] Exclude: **/build/**
[INFO] Exclude: **/logs/**
[INFO] Exclude: **/CHANGELOG.md
[INFO] Exclude: **/README.md
[INFO] Exclude: **/README.markdown
[INFO] Exclude: **/DEVELOPER.md
[INFO] Exclude: **/BYLAWS.md
[INFO] Exclude: **/STORM-UI-REST-API.md
[INFO] Exclude: SECURITY.md
[INFO] Exclude: VERSION
[INFO] Exclude: TODO
[INFO] Exclude: **/src/py/**
[INFO] Exclude: **/src/ui/public/js/jquery.dataTables.1.10.4.min.js
[INFO] Exclude: **/src/ui/public/css/jquery.dataTables.1.10.4.min.css
[INFO] Exclude: **/src/ui/public/images/*
[INFO] Exclude: **/src/ui/public/js/bootstrap-3.3.1.min.js
[INFO] Exclude: **/src/ui/public/css/bootstrap-3.3.1.min.css
[INFO] Exclude: **/src/ui/public/js/dataTables.bootstrap.min.js
[INFO] Exclude: **/src/ui/public/css/dataTables.bootstrap.css
[INFO] Exclude: **/src/ui/public/js/jsonFormatter.min.js
[INFO] Exclude: **/src/ui/public/css/jsonFormatter.min.css
[INFO] Exclude: **/src/ui/public/js/jquery-1.11.1.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.cookies.2.2.0.min.js
[INFO] Exclude: **/src/ui/public/js/moment.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.blockUI.min.js
[INFO] Exclude: **/src/ui/public/js/url.min.js
[INFO] Exclude: **/src/ui/public/js/arbor.js
[INFO] Exclude: **/src/ui/public/js/arbor-graphics.js
[INFO] Exclude: **/src/ui/public/js/arbor-tween.js
[INFO] Exclude: **/src/ui/public/js/jquery.mustache.js
[INFO] Exclude: **/src/ui/public/js/typeahead.jquery.min.js
[INFO] Exclude: **/dependency-reduced-pom.xml
[INFO] Exclude: **/docs/**
[INFO] Exclude: **/.git/**
[INFO] Exclude: **/derby.log
[INFO] Exclude: **/src/dev/**
[INFO] Exclude: **/src/codegen/config.fmpp
[INFO] Exclude: **/src/codegen/data/Parser.tdd
[INFO] Exclude: **/src/test/resources/FixedAvroSerializer.config
[INFO] 2 resources included (use -debug for more details)
Warning:  org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
Compiler warnings:
  WARNING:  'org.apache.xerces.jaxp.SAXParserImpl: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.'
Warning:  org.apache.xerces.parsers.SAXParser: Feature 'http://javax.xml.XMLConstants/feature/secure-processing' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
[INFO] Rat check: Summary of files. Unapproved: 0 unknown: 0 generated: 0 approved: 2 licence.
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building multilang-python 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ multilang-python ---
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ multilang-python ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 1 resource
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ multilang-python ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ multilang-python ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/storm-multilang/python/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ multilang-python ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-surefire-plugin:2.18.1:test (default-test) @ multilang-python ---
[INFO] 
[INFO] --- apache-rat-plugin:0.11:check (default) @ multilang-python ---
[INFO] 51 implicit excludes (use -debug for more details).
[INFO] Exclude: **/target/**
[INFO] Exclude: **/*.iml
[INFO] Exclude: **/.idea/**
[INFO] Exclude: **/metastore_db/**
[INFO] Exclude: **/build/**
[INFO] Exclude: **/logs/**
[INFO] Exclude: **/CHANGELOG.md
[INFO] Exclude: **/README.md
[INFO] Exclude: **/README.markdown
[INFO] Exclude: **/DEVELOPER.md
[INFO] Exclude: **/BYLAWS.md
[INFO] Exclude: **/STORM-UI-REST-API.md
[INFO] Exclude: SECURITY.md
[INFO] Exclude: VERSION
[INFO] Exclude: TODO
[INFO] Exclude: **/src/py/**
[INFO] Exclude: **/src/ui/public/js/jquery.dataTables.1.10.4.min.js
[INFO] Exclude: **/src/ui/public/css/jquery.dataTables.1.10.4.min.css
[INFO] Exclude: **/src/ui/public/images/*
[INFO] Exclude: **/src/ui/public/js/bootstrap-3.3.1.min.js
[INFO] Exclude: **/src/ui/public/css/bootstrap-3.3.1.min.css
[INFO] Exclude: **/src/ui/public/js/dataTables.bootstrap.min.js
[INFO] Exclude: **/src/ui/public/css/dataTables.bootstrap.css
[INFO] Exclude: **/src/ui/public/js/jsonFormatter.min.js
[INFO] Exclude: **/src/ui/public/css/jsonFormatter.min.css
[INFO] Exclude: **/src/ui/public/js/jquery-1.11.1.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.cookies.2.2.0.min.js
[INFO] Exclude: **/src/ui/public/js/moment.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.blockUI.min.js
[INFO] Exclude: **/src/ui/public/js/url.min.js
[INFO] Exclude: **/src/ui/public/js/arbor.js
[INFO] Exclude: **/src/ui/public/js/arbor-graphics.js
[INFO] Exclude: **/src/ui/public/js/arbor-tween.js
[INFO] Exclude: **/src/ui/public/js/jquery.mustache.js
[INFO] Exclude: **/src/ui/public/js/typeahead.jquery.min.js
[INFO] Exclude: **/dependency-reduced-pom.xml
[INFO] Exclude: **/docs/**
[INFO] Exclude: **/.git/**
[INFO] Exclude: **/derby.log
[INFO] Exclude: **/src/dev/**
[INFO] Exclude: **/src/codegen/config.fmpp
[INFO] Exclude: **/src/codegen/data/Parser.tdd
[INFO] Exclude: **/src/test/resources/FixedAvroSerializer.config
[INFO] 2 resources included (use -debug for more details)
Warning:  org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
Compiler warnings:
  WARNING:  'org.apache.xerces.jaxp.SAXParserImpl: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.'
Warning:  org.apache.xerces.parsers.SAXParser: Feature 'http://javax.xml.XMLConstants/feature/secure-processing' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
[INFO] Rat check: Summary of files. Unapproved: 0 unknown: 0 generated: 0 approved: 2 licence.
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building multilang-ruby 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ multilang-ruby ---
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ multilang-ruby ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 1 resource
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ multilang-ruby ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ multilang-ruby ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/storm-multilang/ruby/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ multilang-ruby ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-surefire-plugin:2.18.1:test (default-test) @ multilang-ruby ---
[INFO] 
[INFO] --- apache-rat-plugin:0.11:check (default) @ multilang-ruby ---
[INFO] 51 implicit excludes (use -debug for more details).
[INFO] Exclude: **/target/**
[INFO] Exclude: **/*.iml
[INFO] Exclude: **/.idea/**
[INFO] Exclude: **/metastore_db/**
[INFO] Exclude: **/build/**
[INFO] Exclude: **/logs/**
[INFO] Exclude: **/CHANGELOG.md
[INFO] Exclude: **/README.md
[INFO] Exclude: **/README.markdown
[INFO] Exclude: **/DEVELOPER.md
[INFO] Exclude: **/BYLAWS.md
[INFO] Exclude: **/STORM-UI-REST-API.md
[INFO] Exclude: SECURITY.md
[INFO] Exclude: VERSION
[INFO] Exclude: TODO
[INFO] Exclude: **/src/py/**
[INFO] Exclude: **/src/ui/public/js/jquery.dataTables.1.10.4.min.js
[INFO] Exclude: **/src/ui/public/css/jquery.dataTables.1.10.4.min.css
[INFO] Exclude: **/src/ui/public/images/*
[INFO] Exclude: **/src/ui/public/js/bootstrap-3.3.1.min.js
[INFO] Exclude: **/src/ui/public/css/bootstrap-3.3.1.min.css
[INFO] Exclude: **/src/ui/public/js/dataTables.bootstrap.min.js
[INFO] Exclude: **/src/ui/public/css/dataTables.bootstrap.css
[INFO] Exclude: **/src/ui/public/js/jsonFormatter.min.js
[INFO] Exclude: **/src/ui/public/css/jsonFormatter.min.css
[INFO] Exclude: **/src/ui/public/js/jquery-1.11.1.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.cookies.2.2.0.min.js
[INFO] Exclude: **/src/ui/public/js/moment.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.blockUI.min.js
[INFO] Exclude: **/src/ui/public/js/url.min.js
[INFO] Exclude: **/src/ui/public/js/arbor.js
[INFO] Exclude: **/src/ui/public/js/arbor-graphics.js
[INFO] Exclude: **/src/ui/public/js/arbor-tween.js
[INFO] Exclude: **/src/ui/public/js/jquery.mustache.js
[INFO] Exclude: **/src/ui/public/js/typeahead.jquery.min.js
[INFO] Exclude: **/dependency-reduced-pom.xml
[INFO] Exclude: **/docs/**
[INFO] Exclude: **/.git/**
[INFO] Exclude: **/derby.log
[INFO] Exclude: **/src/dev/**
[INFO] Exclude: **/src/codegen/config.fmpp
[INFO] Exclude: **/src/codegen/data/Parser.tdd
[INFO] Exclude: **/src/test/resources/FixedAvroSerializer.config
[INFO] 2 resources included (use -debug for more details)
Warning:  org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
Compiler warnings:
  WARNING:  'org.apache.xerces.jaxp.SAXParserImpl: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.'
Warning:  org.apache.xerces.parsers.SAXParser: Feature 'http://javax.xml.XMLConstants/feature/secure-processing' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
[INFO] Rat check: Summary of files. Unapproved: 0 unknown: 0 generated: 0 approved: 2 licence.
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building maven-shade-clojure-transformer 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ maven-shade-clojure-transformer ---
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ maven-shade-clojure-transformer ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/storm-buildtools/maven-shade-clojure-transformer/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ maven-shade-clojure-transformer ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ maven-shade-clojure-transformer ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/storm-buildtools/maven-shade-clojure-transformer/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ maven-shade-clojure-transformer ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-surefire-plugin:2.18.1:test (default-test) @ maven-shade-clojure-transformer ---
[INFO] 
[INFO] --- apache-rat-plugin:0.11:check (default) @ maven-shade-clojure-transformer ---
[INFO] 51 implicit excludes (use -debug for more details).
[INFO] Exclude: **/target/**
[INFO] Exclude: **/*.iml
[INFO] Exclude: **/.idea/**
[INFO] Exclude: **/metastore_db/**
[INFO] Exclude: **/build/**
[INFO] Exclude: **/logs/**
[INFO] Exclude: **/CHANGELOG.md
[INFO] Exclude: **/README.md
[INFO] Exclude: **/README.markdown
[INFO] Exclude: **/DEVELOPER.md
[INFO] Exclude: **/BYLAWS.md
[INFO] Exclude: **/STORM-UI-REST-API.md
[INFO] Exclude: SECURITY.md
[INFO] Exclude: VERSION
[INFO] Exclude: TODO
[INFO] Exclude: **/src/py/**
[INFO] Exclude: **/src/ui/public/js/jquery.dataTables.1.10.4.min.js
[INFO] Exclude: **/src/ui/public/css/jquery.dataTables.1.10.4.min.css
[INFO] Exclude: **/src/ui/public/images/*
[INFO] Exclude: **/src/ui/public/js/bootstrap-3.3.1.min.js
[INFO] Exclude: **/src/ui/public/css/bootstrap-3.3.1.min.css
[INFO] Exclude: **/src/ui/public/js/dataTables.bootstrap.min.js
[INFO] Exclude: **/src/ui/public/css/dataTables.bootstrap.css
[INFO] Exclude: **/src/ui/public/js/jsonFormatter.min.js
[INFO] Exclude: **/src/ui/public/css/jsonFormatter.min.css
[INFO] Exclude: **/src/ui/public/js/jquery-1.11.1.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.cookies.2.2.0.min.js
[INFO] Exclude: **/src/ui/public/js/moment.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.blockUI.min.js
[INFO] Exclude: **/src/ui/public/js/url.min.js
[INFO] Exclude: **/src/ui/public/js/arbor.js
[INFO] Exclude: **/src/ui/public/js/arbor-graphics.js
[INFO] Exclude: **/src/ui/public/js/arbor-tween.js
[INFO] Exclude: **/src/ui/public/js/jquery.mustache.js
[INFO] Exclude: **/src/ui/public/js/typeahead.jquery.min.js
[INFO] Exclude: **/dependency-reduced-pom.xml
[INFO] Exclude: **/docs/**
[INFO] Exclude: **/.git/**
[INFO] Exclude: **/derby.log
[INFO] Exclude: **/src/dev/**
[INFO] Exclude: **/src/codegen/config.fmpp
[INFO] Exclude: **/src/codegen/data/Parser.tdd
[INFO] Exclude: **/src/test/resources/FixedAvroSerializer.config
[INFO] 2 resources included (use -debug for more details)
Warning:  org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
Compiler warnings:
  WARNING:  'org.apache.xerces.jaxp.SAXParserImpl: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.'
Warning:  org.apache.xerces.parsers.SAXParser: Feature 'http://javax.xml.XMLConstants/feature/secure-processing' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
[INFO] Rat check: Summary of files. Unapproved: 0 unknown: 0 generated: 0 approved: 2 licence.
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building storm-maven-plugins 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ storm-maven-plugins ---
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ storm-maven-plugins ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/storm-buildtools/storm-maven-plugins/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ storm-maven-plugins ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-plugin-plugin:3.0:descriptor (default-descriptor) @ storm-maven-plugins ---
[INFO] Using 'UTF-8' encoding to read mojo metadata.
[INFO] Applying mojo extractor for language: java-annotations
[INFO] Mojo extractor for language: java-annotations found 1 mojo descriptors.
[INFO] Applying mojo extractor for language: java
[INFO] Mojo extractor for language: java found 0 mojo descriptors.
[INFO] Applying mojo extractor for language: bsh
[INFO] Mojo extractor for language: bsh found 0 mojo descriptors.
[INFO] 
[INFO] --- maven-plugin-plugin:3.0:descriptor (mojo-descriptor) @ storm-maven-plugins ---
[INFO] Using 'UTF-8' encoding to read mojo metadata.
[INFO] Applying mojo extractor for language: java-annotations
[INFO] Mojo extractor for language: java-annotations found 1 mojo descriptors.
[INFO] Applying mojo extractor for language: java
[INFO] Mojo extractor for language: java found 0 mojo descriptors.
[INFO] Applying mojo extractor for language: bsh
[INFO] Mojo extractor for language: bsh found 0 mojo descriptors.
[INFO] 
[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ storm-maven-plugins ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/storm-buildtools/storm-maven-plugins/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ storm-maven-plugins ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-surefire-plugin:2.18.1:test (default-test) @ storm-maven-plugins ---
[INFO] 
[INFO] --- apache-rat-plugin:0.11:check (default) @ storm-maven-plugins ---
[INFO] 51 implicit excludes (use -debug for more details).
[INFO] Exclude: **/target/**
[INFO] Exclude: **/*.iml
[INFO] Exclude: **/.idea/**
[INFO] Exclude: **/metastore_db/**
[INFO] Exclude: **/build/**
[INFO] Exclude: **/logs/**
[INFO] Exclude: **/CHANGELOG.md
[INFO] Exclude: **/README.md
[INFO] Exclude: **/README.markdown
[INFO] Exclude: **/DEVELOPER.md
[INFO] Exclude: **/BYLAWS.md
[INFO] Exclude: **/STORM-UI-REST-API.md
[INFO] Exclude: SECURITY.md
[INFO] Exclude: VERSION
[INFO] Exclude: TODO
[INFO] Exclude: **/src/py/**
[INFO] Exclude: **/src/ui/public/js/jquery.dataTables.1.10.4.min.js
[INFO] Exclude: **/src/ui/public/css/jquery.dataTables.1.10.4.min.css
[INFO] Exclude: **/src/ui/public/images/*
[INFO] Exclude: **/src/ui/public/js/bootstrap-3.3.1.min.js
[INFO] Exclude: **/src/ui/public/css/bootstrap-3.3.1.min.css
[INFO] Exclude: **/src/ui/public/js/dataTables.bootstrap.min.js
[INFO] Exclude: **/src/ui/public/css/dataTables.bootstrap.css
[INFO] Exclude: **/src/ui/public/js/jsonFormatter.min.js
[INFO] Exclude: **/src/ui/public/css/jsonFormatter.min.css
[INFO] Exclude: **/src/ui/public/js/jquery-1.11.1.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.cookies.2.2.0.min.js
[INFO] Exclude: **/src/ui/public/js/moment.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.blockUI.min.js
[INFO] Exclude: **/src/ui/public/js/url.min.js
[INFO] Exclude: **/src/ui/public/js/arbor.js
[INFO] Exclude: **/src/ui/public/js/arbor-graphics.js
[INFO] Exclude: **/src/ui/public/js/arbor-tween.js
[INFO] Exclude: **/src/ui/public/js/jquery.mustache.js
[INFO] Exclude: **/src/ui/public/js/typeahead.jquery.min.js
[INFO] Exclude: **/dependency-reduced-pom.xml
[INFO] Exclude: **/docs/**
[INFO] Exclude: **/.git/**
[INFO] Exclude: **/derby.log
[INFO] Exclude: **/src/dev/**
[INFO] Exclude: **/src/codegen/config.fmpp
[INFO] Exclude: **/src/codegen/data/Parser.tdd
[INFO] Exclude: **/src/test/resources/FixedAvroSerializer.config
[INFO] 3 resources included (use -debug for more details)
Warning:  org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
Compiler warnings:
  WARNING:  'org.apache.xerces.jaxp.SAXParserImpl: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.'
Warning:  org.apache.xerces.parsers.SAXParser: Feature 'http://javax.xml.XMLConstants/feature/secure-processing' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
[INFO] Rat check: Summary of files. Unapproved: 0 unknown: 0 generated: 0 approved: 3 licence.
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building storm-rename-hack 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] Downloading: https://repository.apache.org/snapshots/org/apache/storm/storm-core/2.0.0-SNAPSHOT/maven-metadata.xml
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ storm-rename-hack ---
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ storm-rename-hack ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/storm-rename-hack/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ storm-rename-hack ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ storm-rename-hack ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/storm-rename-hack/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ storm-rename-hack ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-surefire-plugin:2.18.1:test (default-test) @ storm-rename-hack ---
[INFO] 
[INFO] --- apache-rat-plugin:0.11:check (default) @ storm-rename-hack ---
[INFO] 51 implicit excludes (use -debug for more details).
[INFO] Exclude: **/target/**
[INFO] Exclude: **/*.iml
[INFO] Exclude: **/.idea/**
[INFO] Exclude: **/metastore_db/**
[INFO] Exclude: **/build/**
[INFO] Exclude: **/logs/**
[INFO] Exclude: **/CHANGELOG.md
[INFO] Exclude: **/README.md
[INFO] Exclude: **/README.markdown
[INFO] Exclude: **/DEVELOPER.md
[INFO] Exclude: **/BYLAWS.md
[INFO] Exclude: **/STORM-UI-REST-API.md
[INFO] Exclude: SECURITY.md
[INFO] Exclude: VERSION
[INFO] Exclude: TODO
[INFO] Exclude: **/src/py/**
[INFO] Exclude: **/src/ui/public/js/jquery.dataTables.1.10.4.min.js
[INFO] Exclude: **/src/ui/public/css/jquery.dataTables.1.10.4.min.css
[INFO] Exclude: **/src/ui/public/images/*
[INFO] Exclude: **/src/ui/public/js/bootstrap-3.3.1.min.js
[INFO] Exclude: **/src/ui/public/css/bootstrap-3.3.1.min.css
[INFO] Exclude: **/src/ui/public/js/dataTables.bootstrap.min.js
[INFO] Exclude: **/src/ui/public/css/dataTables.bootstrap.css
[INFO] Exclude: **/src/ui/public/js/jsonFormatter.min.js
[INFO] Exclude: **/src/ui/public/css/jsonFormatter.min.css
[INFO] Exclude: **/src/ui/public/js/jquery-1.11.1.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.cookies.2.2.0.min.js
[INFO] Exclude: **/src/ui/public/js/moment.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.blockUI.min.js
[INFO] Exclude: **/src/ui/public/js/url.min.js
[INFO] Exclude: **/src/ui/public/js/arbor.js
[INFO] Exclude: **/src/ui/public/js/arbor-graphics.js
[INFO] Exclude: **/src/ui/public/js/arbor-tween.js
[INFO] Exclude: **/src/ui/public/js/jquery.mustache.js
[INFO] Exclude: **/src/ui/public/js/typeahead.jquery.min.js
[INFO] Exclude: **/dependency-reduced-pom.xml
[INFO] Exclude: **/docs/**
[INFO] Exclude: **/.git/**
[INFO] Exclude: **/derby.log
[INFO] Exclude: **/src/dev/**
[INFO] Exclude: **/src/codegen/config.fmpp
[INFO] Exclude: **/src/codegen/data/Parser.tdd
[INFO] Exclude: **/src/test/resources/FixedAvroSerializer.config
[INFO] 10 resources included (use -debug for more details)
Warning:  org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
Compiler warnings:
  WARNING:  'org.apache.xerces.jaxp.SAXParserImpl: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.'
Warning:  org.apache.xerces.parsers.SAXParser: Feature 'http://javax.xml.XMLConstants/feature/secure-processing' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
[INFO] Rat check: Summary of files. Unapproved: 0 unknown: 0 generated: 0 approved: 10 licence.
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building storm-kafka 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.2.201409121644:prepare-agent (jacoco-initialize) @ storm-kafka ---
[INFO] argLine set to -javaagent:/home/travis/.m2/repository/org/jacoco/org.jacoco.agent/0.7.2.201409121644/org.jacoco.agent-0.7.2.201409121644-runtime.jar=destfile=/home/travis/build/apache/storm/external/storm-kafka/target/jacoco.exec
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ storm-kafka ---
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ storm-kafka ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/storm-kafka/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ storm-kafka ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ storm-kafka ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/storm-kafka/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ storm-kafka ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:2.18.1:test (default-test) @ storm-kafka ---
[INFO] Surefire report directory: /home/travis/build/apache/storm/external/storm-kafka/target/surefire-reports
[INFO] parallel='none', perCoreThreadCount=true, threadCount=0, useUnlimitedThreads=false, threadCountSuites=0, threadCountClasses=0, threadCountMethods=0, parallelOptimized=true

-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running org.apache.storm.kafka.KafkaErrorTest
Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.699 sec - in org.apache.storm.kafka.KafkaErrorTest
Running org.apache.storm.kafka.KafkaUtilsTest
Tests run: 18, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 31.211 sec - in org.apache.storm.kafka.KafkaUtilsTest
Running org.apache.storm.kafka.StringKeyValueSchemeTest
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.075 sec - in org.apache.storm.kafka.StringKeyValueSchemeTest
Running org.apache.storm.kafka.ZkCoordinatorTest
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 5.435 sec - in org.apache.storm.kafka.ZkCoordinatorTest
Running org.apache.storm.kafka.TridentKafkaTest
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.652 sec - in org.apache.storm.kafka.TridentKafkaTest
Running org.apache.storm.kafka.ExponentialBackoffMsgRetryManagerTest
Tests run: 12, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.723 sec - in org.apache.storm.kafka.ExponentialBackoffMsgRetryManagerTest
Running org.apache.storm.kafka.TestStringScheme
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0 sec - in org.apache.storm.kafka.TestStringScheme
Running org.apache.storm.kafka.DynamicBrokersReaderTest
Tests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 6.543 sec - in org.apache.storm.kafka.DynamicBrokersReaderTest
Running org.apache.storm.kafka.bolt.KafkaBoltTest
Tests run: 8, Failures: 0, Errors: 7, Skipped: 0, Time elapsed: 11.671 sec <<< FAILURE! - in org.apache.storm.kafka.bolt.KafkaBoltTest
executeWithBrokerDown(org.apache.storm.kafka.bolt.KafkaBoltTest)  Time elapsed: 1.479 sec  <<< ERROR!
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.kafka.bolt.KafkaBoltTest.generateTestTuple(KafkaBoltTest.java:301)
	at org.apache.storm.kafka.bolt.KafkaBoltTest.executeWithBrokerDown(KafkaBoltTest.java:266)

executeWithoutKey(org.apache.storm.kafka.bolt.KafkaBoltTest)  Time elapsed: 1.491 sec  <<< ERROR!
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.kafka.bolt.KafkaBoltTest.generateTestTuple(KafkaBoltTest.java:301)
	at org.apache.storm.kafka.bolt.KafkaBoltTest.executeWithoutKey(KafkaBoltTest.java:255)

executeWithByteArrayKeyAndMessageFire(org.apache.storm.kafka.bolt.KafkaBoltTest)  Time elapsed: 1.454 sec  <<< ERROR!
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.kafka.bolt.KafkaBoltTest.generateTestTuple(KafkaBoltTest.java:290)
	at org.apache.storm.kafka.bolt.KafkaBoltTest.executeWithByteArrayKeyAndMessageFire(KafkaBoltTest.java:176)

executeWithByteArrayKeyAndMessageSync(org.apache.storm.kafka.bolt.KafkaBoltTest)  Time elapsed: 1.454 sec  <<< ERROR!
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.kafka.bolt.KafkaBoltTest.generateTestTuple(KafkaBoltTest.java:290)
	at org.apache.storm.kafka.bolt.KafkaBoltTest.executeWithByteArrayKeyAndMessageSync(KafkaBoltTest.java:131)

executeWithByteArrayKeyAndMessageAsync(org.apache.storm.kafka.bolt.KafkaBoltTest)  Time elapsed: 1.475 sec  <<< ERROR!
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.kafka.bolt.KafkaBoltTest.generateTestTuple(KafkaBoltTest.java:290)
	at org.apache.storm.kafka.bolt.KafkaBoltTest.executeWithByteArrayKeyAndMessageAsync(KafkaBoltTest.java:146)

executeWithKey(org.apache.storm.kafka.bolt.KafkaBoltTest)  Time elapsed: 1.447 sec  <<< ERROR!
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.kafka.bolt.KafkaBoltTest.generateTestTuple(KafkaBoltTest.java:290)
	at org.apache.storm.kafka.bolt.KafkaBoltTest.executeWithKey(KafkaBoltTest.java:115)

executeWithBoltSpecifiedProperties(org.apache.storm.kafka.bolt.KafkaBoltTest)  Time elapsed: 1.431 sec  <<< ERROR!
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.kafka.bolt.KafkaBoltTest.generateTestTuple(KafkaBoltTest.java:290)
	at org.apache.storm.kafka.bolt.KafkaBoltTest.executeWithBoltSpecifiedProperties(KafkaBoltTest.java:199)


Results :

Tests in error: 
  KafkaBoltTest.executeWithBoltSpecifiedProperties:199->generateTestTuple:290 » IllegalArgument
  KafkaBoltTest.executeWithBrokerDown:266->generateTestTuple:301 » IllegalArgument
  KafkaBoltTest.executeWithByteArrayKeyAndMessageAsync:146->generateTestTuple:290 » IllegalArgument
  KafkaBoltTest.executeWithByteArrayKeyAndMessageFire:176->generateTestTuple:290 » IllegalArgument
  KafkaBoltTest.executeWithByteArrayKeyAndMessageSync:131->generateTestTuple:290 » IllegalArgument
  KafkaBoltTest.executeWithKey:115->generateTestTuple:290 » IllegalArgument Spou...
  KafkaBoltTest.executeWithoutKey:255->generateTestTuple:301 » IllegalArgument S...

Tests run: 57, Failures: 0, Errors: 7, Skipped: 0

[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building storm-hdfs 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ storm-hdfs ---
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ storm-hdfs ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/storm-hdfs/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:2.5.1:compile (default-compile) @ storm-hdfs ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ storm-hdfs ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 2 resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:2.5.1:testCompile (default-testCompile) @ storm-hdfs ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:2.18.1:test (default-test) @ storm-hdfs ---
[INFO] Surefire report directory: /home/travis/build/apache/storm/external/storm-hdfs/target/surefire-reports
[INFO] parallel='none', perCoreThreadCount=true, threadCount=0, useUnlimitedThreads=false, threadCountSuites=0, threadCountClasses=0, threadCountMethods=0, parallelOptimized=true

-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running org.apache.storm.hdfs.bolt.TestHdfsBolt
Tests run: 6, Failures: 0, Errors: 6, Skipped: 0, Time elapsed: 0.495 sec <<< FAILURE! - in org.apache.storm.hdfs.bolt.TestHdfsBolt
testTwoTuplesTwoFiles(org.apache.storm.hdfs.bolt.TestHdfsBolt)  Time elapsed: 0.008 sec  <<< ERROR!
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hdfs.bolt.TestHdfsBolt.generateTestTuple(TestHdfsBolt.java:243)
	at org.apache.storm.hdfs.bolt.TestHdfsBolt.<init>(TestHdfsBolt.java:69)

testPartitionedOutput(org.apache.storm.hdfs.bolt.TestHdfsBolt)  Time elapsed: 0.001 sec  <<< ERROR!
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hdfs.bolt.TestHdfsBolt.generateTestTuple(TestHdfsBolt.java:243)
	at org.apache.storm.hdfs.bolt.TestHdfsBolt.<init>(TestHdfsBolt.java:69)

testTwoTuplesOneFile(org.apache.storm.hdfs.bolt.TestHdfsBolt)  Time elapsed: 0.001 sec  <<< ERROR!
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hdfs.bolt.TestHdfsBolt.generateTestTuple(TestHdfsBolt.java:243)
	at org.apache.storm.hdfs.bolt.TestHdfsBolt.<init>(TestHdfsBolt.java:69)

testFailedSync(org.apache.storm.hdfs.bolt.TestHdfsBolt)  Time elapsed: 0.001 sec  <<< ERROR!
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hdfs.bolt.TestHdfsBolt.generateTestTuple(TestHdfsBolt.java:243)
	at org.apache.storm.hdfs.bolt.TestHdfsBolt.<init>(TestHdfsBolt.java:69)

testTickTuples(org.apache.storm.hdfs.bolt.TestHdfsBolt)  Time elapsed: 0 sec  <<< ERROR!
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hdfs.bolt.TestHdfsBolt.generateTestTuple(TestHdfsBolt.java:243)
	at org.apache.storm.hdfs.bolt.TestHdfsBolt.<init>(TestHdfsBolt.java:69)

testFailureFilecount(org.apache.storm.hdfs.bolt.TestHdfsBolt)  Time elapsed: 0 sec  <<< ERROR!
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hdfs.bolt.TestHdfsBolt.generateTestTuple(TestHdfsBolt.java:243)
	at org.apache.storm.hdfs.bolt.TestHdfsBolt.<init>(TestHdfsBolt.java:69)

Running org.apache.storm.hdfs.bolt.TestSequenceFileBolt
Tests run: 3, Failures: 0, Errors: 3, Skipped: 0, Time elapsed: 0.17 sec <<< FAILURE! - in org.apache.storm.hdfs.bolt.TestSequenceFileBolt
testTwoTuplesTwoFiles(org.apache.storm.hdfs.bolt.TestSequenceFileBolt)  Time elapsed: 0.001 sec  <<< ERROR!
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hdfs.bolt.TestSequenceFileBolt.generateTestTuple(TestSequenceFileBolt.java:161)
	at org.apache.storm.hdfs.bolt.TestSequenceFileBolt.<init>(TestSequenceFileBolt.java:68)

testTwoTuplesOneFile(org.apache.storm.hdfs.bolt.TestSequenceFileBolt)  Time elapsed: 0 sec  <<< ERROR!
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hdfs.bolt.TestSequenceFileBolt.generateTestTuple(TestSequenceFileBolt.java:161)
	at org.apache.storm.hdfs.bolt.TestSequenceFileBolt.<init>(TestSequenceFileBolt.java:68)

testFailedSync(org.apache.storm.hdfs.bolt.TestSequenceFileBolt)  Time elapsed: 0 sec  <<< ERROR!
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hdfs.bolt.TestSequenceFileBolt.generateTestTuple(TestSequenceFileBolt.java:161)
	at org.apache.storm.hdfs.bolt.TestSequenceFileBolt.<init>(TestSequenceFileBolt.java:68)

Running org.apache.storm.hdfs.bolt.AvroGenericRecordBoltTest
Tests run: 5, Failures: 0, Errors: 5, Skipped: 0, Time elapsed: 0.004 sec <<< FAILURE! - in org.apache.storm.hdfs.bolt.AvroGenericRecordBoltTest
schemaThrashing(org.apache.storm.hdfs.bolt.AvroGenericRecordBoltTest)  Time elapsed: 0.001 sec  <<< ERROR!
java.lang.ExceptionInInitializerError: null
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hdfs.bolt.AvroGenericRecordBoltTest.generateTestTuple(AvroGenericRecordBoltTest.java:220)
	at org.apache.storm.hdfs.bolt.AvroGenericRecordBoltTest.<clinit>(AvroGenericRecordBoltTest.java:95)

forwardSchemaChangeWorks(org.apache.storm.hdfs.bolt.AvroGenericRecordBoltTest)  Time elapsed: 0 sec  <<< ERROR!
java.lang.NoClassDefFoundError: Could not initialize class org.apache.storm.hdfs.bolt.AvroGenericRecordBoltTest
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.junit.runners.BlockJUnit4ClassRunner.createTest(BlockJUnit4ClassRunner.java:195)
	at org.junit.runners.BlockJUnit4ClassRunner$1.runReflectiveCall(BlockJUnit4ClassRunner.java:244)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.BlockJUnit4ClassRunner.methodBlock(BlockJUnit4ClassRunner.java:241)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:108)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:78)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:54)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:144)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:203)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:155)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:103)

multipleTuplesOneFile(org.apache.storm.hdfs.bolt.AvroGenericRecordBoltTest)  Time elapsed: 0 sec  <<< ERROR!
java.lang.NoClassDefFoundError: Could not initialize class org.apache.storm.hdfs.bolt.AvroGenericRecordBoltTest
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.junit.runners.BlockJUnit4ClassRunner.createTest(BlockJUnit4ClassRunner.java:195)
	at org.junit.runners.BlockJUnit4ClassRunner$1.runReflectiveCall(BlockJUnit4ClassRunner.java:244)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.BlockJUnit4ClassRunner.methodBlock(BlockJUnit4ClassRunner.java:241)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:108)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:78)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:54)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:144)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:203)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:155)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:103)

multipleTuplesMutliplesFiles(org.apache.storm.hdfs.bolt.AvroGenericRecordBoltTest)  Time elapsed: 0.001 sec  <<< ERROR!
java.lang.NoClassDefFoundError: Could not initialize class org.apache.storm.hdfs.bolt.AvroGenericRecordBoltTest
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.junit.runners.BlockJUnit4ClassRunner.createTest(BlockJUnit4ClassRunner.java:195)
	at org.junit.runners.BlockJUnit4ClassRunner$1.runReflectiveCall(BlockJUnit4ClassRunner.java:244)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.BlockJUnit4ClassRunner.methodBlock(BlockJUnit4ClassRunner.java:241)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:108)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:78)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:54)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:144)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:203)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:155)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:103)

backwardSchemaChangeWorks(org.apache.storm.hdfs.bolt.AvroGenericRecordBoltTest)  Time elapsed: 0.001 sec  <<< ERROR!
java.lang.NoClassDefFoundError: Could not initialize class org.apache.storm.hdfs.bolt.AvroGenericRecordBoltTest
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.junit.runners.BlockJUnit4ClassRunner.createTest(BlockJUnit4ClassRunner.java:195)
	at org.junit.runners.BlockJUnit4ClassRunner$1.runReflectiveCall(BlockJUnit4ClassRunner.java:244)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.BlockJUnit4ClassRunner.methodBlock(BlockJUnit4ClassRunner.java:241)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:108)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:78)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:54)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:144)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:203)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:155)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:103)

Running org.apache.storm.hdfs.bolt.TestWritersMap
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.001 sec - in org.apache.storm.hdfs.bolt.TestWritersMap
Running org.apache.storm.hdfs.bolt.format.TestSimpleFileNameFormat
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 4.009 sec - in org.apache.storm.hdfs.bolt.format.TestSimpleFileNameFormat
Running org.apache.storm.hdfs.spout.TestDirLock
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 18.983 sec - in org.apache.storm.hdfs.spout.TestDirLock
Running org.apache.storm.hdfs.spout.TestHdfsSemantics
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.159 sec - in org.apache.storm.hdfs.spout.TestHdfsSemantics
Running org.apache.storm.hdfs.spout.TestHdfsSpout
Tests run: 10, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 8.631 sec - in org.apache.storm.hdfs.spout.TestHdfsSpout
Running org.apache.storm.hdfs.spout.TestProgressTracker
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.752 sec - in org.apache.storm.hdfs.spout.TestProgressTracker
Running org.apache.storm.hdfs.spout.TestFileLock
Tests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 24.265 sec - in org.apache.storm.hdfs.spout.TestFileLock
Running org.apache.storm.hdfs.trident.format.TestSimpleFileNameFormat
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0 sec - in org.apache.storm.hdfs.trident.format.TestSimpleFileNameFormat
Running org.apache.storm.hdfs.trident.HdfsStateTest
Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.224 sec - in org.apache.storm.hdfs.trident.HdfsStateTest
Running org.apache.storm.hdfs.blobstore.BlobStoreTest
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 10.938 sec - in org.apache.storm.hdfs.blobstore.BlobStoreTest
Running org.apache.storm.hdfs.blobstore.HdfsBlobStoreImplTest
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.606 sec - in org.apache.storm.hdfs.blobstore.HdfsBlobStoreImplTest
Running org.apache.storm.hdfs.avro.TestFixedAvroSerializer
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.001 sec - in org.apache.storm.hdfs.avro.TestFixedAvroSerializer
Running org.apache.storm.hdfs.avro.TestGenericAvroSerializer
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.002 sec - in org.apache.storm.hdfs.avro.TestGenericAvroSerializer

Results :

Tests in error: 
  AvroGenericRecordBoltTest.backwardSchemaChangeWorks » NoClassDefFound Could no...
  AvroGenericRecordBoltTest.forwardSchemaChangeWorks » NoClassDefFound Could not...
  AvroGenericRecordBoltTest.multipleTuplesMutliplesFiles » NoClassDefFound Could...
  AvroGenericRecordBoltTest.multipleTuplesOneFile » NoClassDefFound Could not in...
  AvroGenericRecordBoltTest.schemaThrashing » ExceptionInInitializer
  TestHdfsBolt.<init>:69->generateTestTuple:243 » IllegalArgument Spouts is not ...
  TestHdfsBolt.<init>:69->generateTestTuple:243 » IllegalArgument Spouts is not ...
  TestHdfsBolt.<init>:69->generateTestTuple:243 » IllegalArgument Spouts is not ...
  TestHdfsBolt.<init>:69->generateTestTuple:243 » IllegalArgument Spouts is not ...
  TestHdfsBolt.<init>:69->generateTestTuple:243 » IllegalArgument Spouts is not ...
  TestHdfsBolt.<init>:69->generateTestTuple:243 » IllegalArgument Spouts is not ...
  TestSequenceFileBolt.<init>:68->generateTestTuple:161 » IllegalArgument Spouts...
  TestSequenceFileBolt.<init>:68->generateTestTuple:161 » IllegalArgument Spouts...
  TestSequenceFileBolt.<init>:68->generateTestTuple:161 » IllegalArgument Spouts...

Tests run: 60, Failures: 0, Errors: 14, Skipped: 0

[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building storm-hbase 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ storm-hbase ---
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ storm-hbase ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/storm-hbase/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ storm-hbase ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ storm-hbase ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/storm-hbase/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ storm-hbase ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:2.18.1:test (default-test) @ storm-hbase ---
[INFO] 
[INFO] --- apache-rat-plugin:0.11:check (default) @ storm-hbase ---
[INFO] 51 implicit excludes (use -debug for more details).
[INFO] Exclude: **/target/**
[INFO] Exclude: **/*.iml
[INFO] Exclude: **/.idea/**
[INFO] Exclude: **/metastore_db/**
[INFO] Exclude: **/build/**
[INFO] Exclude: **/logs/**
[INFO] Exclude: **/CHANGELOG.md
[INFO] Exclude: **/README.md
[INFO] Exclude: **/README.markdown
[INFO] Exclude: **/DEVELOPER.md
[INFO] Exclude: **/BYLAWS.md
[INFO] Exclude: **/STORM-UI-REST-API.md
[INFO] Exclude: SECURITY.md
[INFO] Exclude: VERSION
[INFO] Exclude: TODO
[INFO] Exclude: **/src/py/**
[INFO] Exclude: **/src/ui/public/js/jquery.dataTables.1.10.4.min.js
[INFO] Exclude: **/src/ui/public/css/jquery.dataTables.1.10.4.min.css
[INFO] Exclude: **/src/ui/public/images/*
[INFO] Exclude: **/src/ui/public/js/bootstrap-3.3.1.min.js
[INFO] Exclude: **/src/ui/public/css/bootstrap-3.3.1.min.css
[INFO] Exclude: **/src/ui/public/js/dataTables.bootstrap.min.js
[INFO] Exclude: **/src/ui/public/css/dataTables.bootstrap.css
[INFO] Exclude: **/src/ui/public/js/jsonFormatter.min.js
[INFO] Exclude: **/src/ui/public/css/jsonFormatter.min.css
[INFO] Exclude: **/src/ui/public/js/jquery-1.11.1.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.cookies.2.2.0.min.js
[INFO] Exclude: **/src/ui/public/js/moment.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.blockUI.min.js
[INFO] Exclude: **/src/ui/public/js/url.min.js
[INFO] Exclude: **/src/ui/public/js/arbor.js
[INFO] Exclude: **/src/ui/public/js/arbor-graphics.js
[INFO] Exclude: **/src/ui/public/js/arbor-tween.js
[INFO] Exclude: **/src/ui/public/js/jquery.mustache.js
[INFO] Exclude: **/src/ui/public/js/typeahead.jquery.min.js
[INFO] Exclude: **/dependency-reduced-pom.xml
[INFO] Exclude: **/docs/**
[INFO] Exclude: **/.git/**
[INFO] Exclude: **/derby.log
[INFO] Exclude: **/src/dev/**
[INFO] Exclude: **/src/codegen/config.fmpp
[INFO] Exclude: **/src/codegen/data/Parser.tdd
[INFO] Exclude: **/src/test/resources/FixedAvroSerializer.config
[INFO] 35 resources included (use -debug for more details)
Warning:  org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
Compiler warnings:
  WARNING:  'org.apache.xerces.jaxp.SAXParserImpl: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.'
Warning:  org.apache.xerces.parsers.SAXParser: Feature 'http://javax.xml.XMLConstants/feature/secure-processing' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
[INFO] Rat check: Summary of files. Unapproved: 0 unknown: 0 generated: 0 approved: 35 licence.
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building storm-hive 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ storm-hive ---
[INFO] Downloading: https://oss.sonatype.org/content/repositories/releases/net/hydromatic/linq4j/0.4/linq4j-0.4.pom
[INFO] Downloading: https://repository.apache.org/releases/net/hydromatic/linq4j/0.4/linq4j-0.4.pom
[INFO] Downloading: https://oss.sonatype.org/content/repositories/releases/net/hydromatic/quidem/0.1.1/quidem-0.1.1.pom
[INFO] Downloading: https://repository.apache.org/releases/net/hydromatic/quidem/0.1.1/quidem-0.1.1.pom
[INFO] Downloading: https://oss.sonatype.org/content/repositories/releases/org/pentaho/pentaho-aggdesigner-algorithm/5.1.3-jhyde/pentaho-aggdesigner-algorithm-5.1.3-jhyde.pom
[INFO] Downloading: https://repository.apache.org/releases/org/pentaho/pentaho-aggdesigner-algorithm/5.1.3-jhyde/pentaho-aggdesigner-algorithm-5.1.3-jhyde.pom
[INFO] Downloading: https://oss.sonatype.org/content/repositories/releases/eigenbase/eigenbase-properties/1.1.4/eigenbase-properties-1.1.4.pom
[INFO] Downloading: https://repository.apache.org/releases/eigenbase/eigenbase-properties/1.1.4/eigenbase-properties-1.1.4.pom
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ storm-hive ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/storm-hive/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ storm-hive ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ storm-hive ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/storm-hive/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ storm-hive ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (cleanup) @ storm-hive ---
[INFO] 
[INFO] --- maven-surefire-plugin:2.18.1:test (default-test) @ storm-hive ---
[INFO] Surefire report directory: /home/travis/build/apache/storm/external/storm-hive/target/surefire-reports
[INFO] parallel='none', perCoreThreadCount=true, threadCount=0, useUnlimitedThreads=false, threadCountSuites=0, threadCountClasses=0, threadCountMethods=0, parallelOptimized=true

-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running org.apache.storm.hive.bolt.TestHiveBolt
Tests run: 11, Failures: 0, Errors: 9, Skipped: 0, Time elapsed: 14.84 sec <<< FAILURE! - in org.apache.storm.hive.bolt.TestHiveBolt
testMultiPartitionTuples(org.apache.storm.hive.bolt.TestHiveBolt)  Time elapsed: 1.876 sec  <<< ERROR!
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hive.bolt.TestHiveBolt.generateTestTuple(TestHiveBolt.java:447)
	at org.apache.storm.hive.bolt.TestHiveBolt.testMultiPartitionTuples(TestHiveBolt.java:411)

testNoAcksUntilFlushed(org.apache.storm.hive.bolt.TestHiveBolt)  Time elapsed: 2.031 sec  <<< ERROR!
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hive.bolt.TestHiveBolt.generateTestTuple(TestHiveBolt.java:447)
	at org.apache.storm.hive.bolt.TestHiveBolt.testNoAcksUntilFlushed(TestHiveBolt.java:297)

testData(org.apache.storm.hive.bolt.TestHiveBolt)  Time elapsed: 0.921 sec  <<< ERROR!
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hive.bolt.TestHiveBolt.generateTestTuple(TestHiveBolt.java:447)
	at org.apache.storm.hive.bolt.TestHiveBolt.testData(TestHiveBolt.java:251)

testWithoutPartitions(org.apache.storm.hive.bolt.TestHiveBolt)  Time elapsed: 1.042 sec  <<< ERROR!
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hive.bolt.TestHiveBolt.generateTestTuple(TestHiveBolt.java:447)
	at org.apache.storm.hive.bolt.TestHiveBolt.testWithoutPartitions(TestHiveBolt.java:194)

testJsonWriter(org.apache.storm.hive.bolt.TestHiveBolt)  Time elapsed: 0.515 sec  <<< ERROR!
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hive.bolt.TestHiveBolt.generateTestTuple(TestHiveBolt.java:447)
	at org.apache.storm.hive.bolt.TestHiveBolt.testJsonWriter(TestHiveBolt.java:274)

testTickTuple(org.apache.storm.hive.bolt.TestHiveBolt)  Time elapsed: 0.604 sec  <<< ERROR!
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hive.bolt.TestHiveBolt.generateTestTuple(TestHiveBolt.java:447)
	at org.apache.storm.hive.bolt.TestHiveBolt.testTickTuple(TestHiveBolt.java:352)

testWithTimeformat(org.apache.storm.hive.bolt.TestHiveBolt)  Time elapsed: 0.761 sec  <<< ERROR!
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hive.bolt.TestHiveBolt.generateTestTuple(TestHiveBolt.java:447)
	at org.apache.storm.hive.bolt.TestHiveBolt.testWithTimeformat(TestHiveBolt.java:230)

testWithByteArrayIdandMessage(org.apache.storm.hive.bolt.TestHiveBolt)  Time elapsed: 0.575 sec  <<< ERROR!
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hive.bolt.TestHiveBolt.generateTestTuple(TestHiveBolt.java:447)
	at org.apache.storm.hive.bolt.TestHiveBolt.testWithByteArrayIdandMessage(TestHiveBolt.java:161)

testNoAcksIfFlushFails(org.apache.storm.hive.bolt.TestHiveBolt)  Time elapsed: 0.667 sec  <<< ERROR!
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hive.bolt.TestHiveBolt.generateTestTuple(TestHiveBolt.java:447)
	at org.apache.storm.hive.bolt.TestHiveBolt.testNoAcksIfFlushFails(TestHiveBolt.java:327)

Running org.apache.storm.hive.common.TestHiveWriter
Tests run: 3, Failures: 0, Errors: 2, Skipped: 0, Time elapsed: 3.444 sec <<< FAILURE! - in org.apache.storm.hive.common.TestHiveWriter
testWriteBasic(org.apache.storm.hive.common.TestHiveWriter)  Time elapsed: 0.753 sec  <<< ERROR!
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hive.common.TestHiveWriter.generateTestTuple(TestHiveWriter.java:164)
	at org.apache.storm.hive.common.TestHiveWriter.writeTuples(TestHiveWriter.java:179)
	at org.apache.storm.hive.common.TestHiveWriter.testWriteBasic(TestHiveWriter.java:127)

testWriteMultiFlush(org.apache.storm.hive.common.TestHiveWriter)  Time elapsed: 0.911 sec  <<< ERROR!
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hive.common.TestHiveWriter.generateTestTuple(TestHiveWriter.java:164)
	at org.apache.storm.hive.common.TestHiveWriter.testWriteMultiFlush(TestHiveWriter.java:142)


Results :

Tests in error: 
  TestHiveBolt.testData:251->generateTestTuple:447 » IllegalArgument Spouts is n...
  TestHiveBolt.testJsonWriter:274->generateTestTuple:447 » IllegalArgument Spout...
  TestHiveBolt.testMultiPartitionTuples:411->generateTestTuple:447 » IllegalArgument
  TestHiveBolt.testNoAcksIfFlushFails:327->generateTestTuple:447 » IllegalArgument
  TestHiveBolt.testNoAcksUntilFlushed:297->generateTestTuple:447 » IllegalArgument
  TestHiveBolt.testTickTuple:352->generateTestTuple:447 » IllegalArgument Spouts...
  TestHiveBolt.testWithByteArrayIdandMessage:161->generateTestTuple:447 » IllegalArgument
  TestHiveBolt.testWithTimeformat:230->generateTestTuple:447 » IllegalArgument S...
  TestHiveBolt.testWithoutPartitions:194->generateTestTuple:447 » IllegalArgument
  TestHiveWriter.testWriteBasic:127->writeTuples:179->generateTestTuple:164 » IllegalArgument
  TestHiveWriter.testWriteMultiFlush:142->generateTestTuple:164 » IllegalArgument

Tests run: 14, Failures: 0, Errors: 11, Skipped: 0

[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building storm-jdbc 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ storm-jdbc ---
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ storm-jdbc ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/storm-jdbc/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ storm-jdbc ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ storm-jdbc ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/storm-jdbc/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- sql-maven-plugin:1.5:execute (create-db) @ storm-jdbc ---
[INFO] Executing file: /tmp/test.1314906669sql
[INFO] 1 of 1 SQL statements executed successfully
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ storm-jdbc ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:2.18.1:test (default-test) @ storm-jdbc ---
[INFO] Surefire report directory: /home/travis/build/apache/storm/external/storm-jdbc/target/surefire-reports
[INFO] parallel='none', perCoreThreadCount=true, threadCount=0, useUnlimitedThreads=false, threadCountSuites=0, threadCountClasses=0, threadCountMethods=0, parallelOptimized=true

-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running org.apache.storm.jdbc.bolt.JdbcInsertBoltTest
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.602 sec - in org.apache.storm.jdbc.bolt.JdbcInsertBoltTest
Running org.apache.storm.jdbc.bolt.JdbcLookupBoltTest
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.004 sec - in org.apache.storm.jdbc.bolt.JdbcLookupBoltTest
Running org.apache.storm.jdbc.common.UtilTest
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.003 sec - in org.apache.storm.jdbc.common.UtilTest
Running org.apache.storm.jdbc.common.JdbcClientTest
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.52 sec - in org.apache.storm.jdbc.common.JdbcClientTest

Results :

Tests run: 5, Failures: 0, Errors: 0, Skipped: 0

[INFO] 
[INFO] --- apache-rat-plugin:0.11:check (default) @ storm-jdbc ---
[INFO] 51 implicit excludes (use -debug for more details).
[INFO] Exclude: **/target/**
[INFO] Exclude: **/*.iml
[INFO] Exclude: **/.idea/**
[INFO] Exclude: **/metastore_db/**
[INFO] Exclude: **/build/**
[INFO] Exclude: **/logs/**
[INFO] Exclude: **/CHANGELOG.md
[INFO] Exclude: **/README.md
[INFO] Exclude: **/README.markdown
[INFO] Exclude: **/DEVELOPER.md
[INFO] Exclude: **/BYLAWS.md
[INFO] Exclude: **/STORM-UI-REST-API.md
[INFO] Exclude: SECURITY.md
[INFO] Exclude: VERSION
[INFO] Exclude: TODO
[INFO] Exclude: **/src/py/**
[INFO] Exclude: **/src/ui/public/js/jquery.dataTables.1.10.4.min.js
[INFO] Exclude: **/src/ui/public/css/jquery.dataTables.1.10.4.min.css
[INFO] Exclude: **/src/ui/public/images/*
[INFO] Exclude: **/src/ui/public/js/bootstrap-3.3.1.min.js
[INFO] Exclude: **/src/ui/public/css/bootstrap-3.3.1.min.css
[INFO] Exclude: **/src/ui/public/js/dataTables.bootstrap.min.js
[INFO] Exclude: **/src/ui/public/css/dataTables.bootstrap.css
[INFO] Exclude: **/src/ui/public/js/jsonFormatter.min.js
[INFO] Exclude: **/src/ui/public/css/jsonFormatter.min.css
[INFO] Exclude: **/src/ui/public/js/jquery-1.11.1.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.cookies.2.2.0.min.js
[INFO] Exclude: **/src/ui/public/js/moment.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.blockUI.min.js
[INFO] Exclude: **/src/ui/public/js/url.min.js
[INFO] Exclude: **/src/ui/public/js/arbor.js
[INFO] Exclude: **/src/ui/public/js/arbor-graphics.js
[INFO] Exclude: **/src/ui/public/js/arbor-tween.js
[INFO] Exclude: **/src/ui/public/js/jquery.mustache.js
[INFO] Exclude: **/src/ui/public/js/typeahead.jquery.min.js
[INFO] Exclude: **/dependency-reduced-pom.xml
[INFO] Exclude: **/docs/**
[INFO] Exclude: **/.git/**
[INFO] Exclude: **/derby.log
[INFO] Exclude: **/src/dev/**
[INFO] Exclude: **/src/codegen/config.fmpp
[INFO] Exclude: **/src/codegen/data/Parser.tdd
[INFO] Exclude: **/src/test/resources/FixedAvroSerializer.config
[INFO] 26 resources included (use -debug for more details)
Warning:  org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
Compiler warnings:
  WARNING:  'org.apache.xerces.jaxp.SAXParserImpl: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.'
Warning:  org.apache.xerces.parsers.SAXParser: Feature 'http://javax.xml.XMLConstants/feature/secure-processing' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
[INFO] Rat check: Summary of files. Unapproved: 0 unknown: 0 generated: 0 approved: 26 licence.
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building storm-redis 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ storm-redis ---
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ storm-redis ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/storm-redis/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ storm-redis ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ storm-redis ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/storm-redis/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ storm-redis ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:2.18.1:test (default-test) @ storm-redis ---
[INFO] Surefire report directory: /home/travis/build/apache/storm/external/storm-redis/target/surefire-reports
[INFO] parallel='none', perCoreThreadCount=true, threadCount=0, useUnlimitedThreads=false, threadCountSuites=0, threadCountClasses=0, threadCountMethods=0, parallelOptimized=true

-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running org.apache.storm.redis.state.RedisKeyValueStateProviderTest
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.741 sec - in org.apache.storm.redis.state.RedisKeyValueStateProviderTest
Running org.apache.storm.redis.state.DefaultStateSerializerTest
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.058 sec - in org.apache.storm.redis.state.DefaultStateSerializerTest
Running org.apache.storm.redis.state.RedisKeyValueStateTest
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.246 sec - in org.apache.storm.redis.state.RedisKeyValueStateTest

Results :

Tests run: 5, Failures: 0, Errors: 0, Skipped: 0

[INFO] 
[INFO] --- apache-rat-plugin:0.11:check (default) @ storm-redis ---
[INFO] 51 implicit excludes (use -debug for more details).
[INFO] Exclude: **/target/**
[INFO] Exclude: **/*.iml
[INFO] Exclude: **/.idea/**
[INFO] Exclude: **/metastore_db/**
[INFO] Exclude: **/build/**
[INFO] Exclude: **/logs/**
[INFO] Exclude: **/CHANGELOG.md
[INFO] Exclude: **/README.md
[INFO] Exclude: **/README.markdown
[INFO] Exclude: **/DEVELOPER.md
[INFO] Exclude: **/BYLAWS.md
[INFO] Exclude: **/STORM-UI-REST-API.md
[INFO] Exclude: SECURITY.md
[INFO] Exclude: VERSION
[INFO] Exclude: TODO
[INFO] Exclude: **/src/py/**
[INFO] Exclude: **/src/ui/public/js/jquery.dataTables.1.10.4.min.js
[INFO] Exclude: **/src/ui/public/css/jquery.dataTables.1.10.4.min.css
[INFO] Exclude: **/src/ui/public/images/*
[INFO] Exclude: **/src/ui/public/js/bootstrap-3.3.1.min.js
[INFO] Exclude: **/src/ui/public/css/bootstrap-3.3.1.min.css
[INFO] Exclude: **/src/ui/public/js/dataTables.bootstrap.min.js
[INFO] Exclude: **/src/ui/public/css/dataTables.bootstrap.css
[INFO] Exclude: **/src/ui/public/js/jsonFormatter.min.js
[INFO] Exclude: **/src/ui/public/css/jsonFormatter.min.css
[INFO] Exclude: **/src/ui/public/js/jquery-1.11.1.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.cookies.2.2.0.min.js
[INFO] Exclude: **/src/ui/public/js/moment.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.blockUI.min.js
[INFO] Exclude: **/src/ui/public/js/url.min.js
[INFO] Exclude: **/src/ui/public/js/arbor.js
[INFO] Exclude: **/src/ui/public/js/arbor-graphics.js
[INFO] Exclude: **/src/ui/public/js/arbor-tween.js
[INFO] Exclude: **/src/ui/public/js/jquery.mustache.js
[INFO] Exclude: **/src/ui/public/js/typeahead.jquery.min.js
[INFO] Exclude: **/dependency-reduced-pom.xml
[INFO] Exclude: **/docs/**
[INFO] Exclude: **/.git/**
[INFO] Exclude: **/derby.log
[INFO] Exclude: **/src/dev/**
[INFO] Exclude: **/src/codegen/config.fmpp
[INFO] Exclude: **/src/codegen/data/Parser.tdd
[INFO] Exclude: **/src/test/resources/FixedAvroSerializer.config
[INFO] 44 resources included (use -debug for more details)
Warning:  org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
Compiler warnings:
  WARNING:  'org.apache.xerces.jaxp.SAXParserImpl: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.'
Warning:  org.apache.xerces.parsers.SAXParser: Feature 'http://javax.xml.XMLConstants/feature/secure-processing' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
[INFO] Rat check: Summary of files. Unapproved: 0 unknown: 0 generated: 0 approved: 44 licence.
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building storm-eventhubs 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ storm-eventhubs ---
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ storm-eventhubs ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 1 resource
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ storm-eventhubs ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ storm-eventhubs ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/storm-eventhubs/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ storm-eventhubs ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:2.18.1:test (default-test) @ storm-eventhubs ---
[INFO] Surefire report directory: /home/travis/build/apache/storm/external/storm-eventhubs/target/surefire-reports
[INFO] parallel='none', perCoreThreadCount=true, threadCount=0, useUnlimitedThreads=false, threadCountSuites=0, threadCountClasses=0, threadCountMethods=0, parallelOptimized=true

-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running org.apache.storm.eventhubs.trident.TestTransactionalTridentEmitter
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.471 sec - in org.apache.storm.eventhubs.trident.TestTransactionalTridentEmitter
Running org.apache.storm.eventhubs.spout.TestPartitionManager
Tests run: 8, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.038 sec - in org.apache.storm.eventhubs.spout.TestPartitionManager
Running org.apache.storm.eventhubs.spout.TestEventData
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.001 sec - in org.apache.storm.eventhubs.spout.TestEventData
Running org.apache.storm.eventhubs.spout.TestEventHubSpout
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 2.018 sec - in org.apache.storm.eventhubs.spout.TestEventHubSpout

Results :

Tests run: 14, Failures: 0, Errors: 0, Skipped: 0

[INFO] 
[INFO] --- apache-rat-plugin:0.11:check (default) @ storm-eventhubs ---
[INFO] 51 implicit excludes (use -debug for more details).
[INFO] Exclude: **/target/**
[INFO] Exclude: **/*.iml
[INFO] Exclude: **/.idea/**
[INFO] Exclude: **/metastore_db/**
[INFO] Exclude: **/build/**
[INFO] Exclude: **/logs/**
[INFO] Exclude: **/CHANGELOG.md
[INFO] Exclude: **/README.md
[INFO] Exclude: **/README.markdown
[INFO] Exclude: **/DEVELOPER.md
[INFO] Exclude: **/BYLAWS.md
[INFO] Exclude: **/STORM-UI-REST-API.md
[INFO] Exclude: SECURITY.md
[INFO] Exclude: VERSION
[INFO] Exclude: TODO
[INFO] Exclude: **/src/py/**
[INFO] Exclude: **/src/ui/public/js/jquery.dataTables.1.10.4.min.js
[INFO] Exclude: **/src/ui/public/css/jquery.dataTables.1.10.4.min.css
[INFO] Exclude: **/src/ui/public/images/*
[INFO] Exclude: **/src/ui/public/js/bootstrap-3.3.1.min.js
[INFO] Exclude: **/src/ui/public/css/bootstrap-3.3.1.min.css
[INFO] Exclude: **/src/ui/public/js/dataTables.bootstrap.min.js
[INFO] Exclude: **/src/ui/public/css/dataTables.bootstrap.css
[INFO] Exclude: **/src/ui/public/js/jsonFormatter.min.js
[INFO] Exclude: **/src/ui/public/css/jsonFormatter.min.css
[INFO] Exclude: **/src/ui/public/js/jquery-1.11.1.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.cookies.2.2.0.min.js
[INFO] Exclude: **/src/ui/public/js/moment.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.blockUI.min.js
[INFO] Exclude: **/src/ui/public/js/url.min.js
[INFO] Exclude: **/src/ui/public/js/arbor.js
[INFO] Exclude: **/src/ui/public/js/arbor-graphics.js
[INFO] Exclude: **/src/ui/public/js/arbor-tween.js
[INFO] Exclude: **/src/ui/public/js/jquery.mustache.js
[INFO] Exclude: **/src/ui/public/js/typeahead.jquery.min.js
[INFO] Exclude: **/dependency-reduced-pom.xml
[INFO] Exclude: **/docs/**
[INFO] Exclude: **/.git/**
[INFO] Exclude: **/derby.log
[INFO] Exclude: **/src/dev/**
[INFO] Exclude: **/src/codegen/config.fmpp
[INFO] Exclude: **/src/codegen/data/Parser.tdd
[INFO] Exclude: **/src/test/resources/FixedAvroSerializer.config
[INFO] 52 resources included (use -debug for more details)
Warning:  org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
Compiler warnings:
  WARNING:  'org.apache.xerces.jaxp.SAXParserImpl: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.'
Warning:  org.apache.xerces.parsers.SAXParser: Feature 'http://javax.xml.XMLConstants/feature/secure-processing' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
[INFO] Rat check: Summary of files. Unapproved: 0 unknown: 0 generated: 0 approved: 52 licence.
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building flux 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ flux ---
[INFO] 
[INFO] --- apache-rat-plugin:0.11:check (default) @ flux ---
[INFO] 51 implicit excludes (use -debug for more details).
[INFO] Exclude: **/target/**
[INFO] Exclude: **/*.iml
[INFO] Exclude: **/.idea/**
[INFO] Exclude: **/metastore_db/**
[INFO] Exclude: **/build/**
[INFO] Exclude: **/logs/**
[INFO] Exclude: **/CHANGELOG.md
[INFO] Exclude: **/README.md
[INFO] Exclude: **/README.markdown
[INFO] Exclude: **/DEVELOPER.md
[INFO] Exclude: **/BYLAWS.md
[INFO] Exclude: **/STORM-UI-REST-API.md
[INFO] Exclude: SECURITY.md
[INFO] Exclude: VERSION
[INFO] Exclude: TODO
[INFO] Exclude: **/src/py/**
[INFO] Exclude: **/src/ui/public/js/jquery.dataTables.1.10.4.min.js
[INFO] Exclude: **/src/ui/public/css/jquery.dataTables.1.10.4.min.css
[INFO] Exclude: **/src/ui/public/images/*
[INFO] Exclude: **/src/ui/public/js/bootstrap-3.3.1.min.js
[INFO] Exclude: **/src/ui/public/css/bootstrap-3.3.1.min.css
[INFO] Exclude: **/src/ui/public/js/dataTables.bootstrap.min.js
[INFO] Exclude: **/src/ui/public/css/dataTables.bootstrap.css
[INFO] Exclude: **/src/ui/public/js/jsonFormatter.min.js
[INFO] Exclude: **/src/ui/public/css/jsonFormatter.min.css
[INFO] Exclude: **/src/ui/public/js/jquery-1.11.1.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.cookies.2.2.0.min.js
[INFO] Exclude: **/src/ui/public/js/moment.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.blockUI.min.js
[INFO] Exclude: **/src/ui/public/js/url.min.js
[INFO] Exclude: **/src/ui/public/js/arbor.js
[INFO] Exclude: **/src/ui/public/js/arbor-graphics.js
[INFO] Exclude: **/src/ui/public/js/arbor-tween.js
[INFO] Exclude: **/src/ui/public/js/jquery.mustache.js
[INFO] Exclude: **/src/ui/public/js/typeahead.jquery.min.js
[INFO] Exclude: **/dependency-reduced-pom.xml
[INFO] Exclude: **/docs/**
[INFO] Exclude: **/.git/**
[INFO] Exclude: **/derby.log
[INFO] Exclude: **/src/dev/**
[INFO] Exclude: **/src/codegen/config.fmpp
[INFO] Exclude: **/src/codegen/data/Parser.tdd
[INFO] Exclude: **/src/test/resources/FixedAvroSerializer.config
[INFO] 69 resources included (use -debug for more details)
Warning:  org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
Compiler warnings:
  WARNING:  'org.apache.xerces.jaxp.SAXParserImpl: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.'
Warning:  org.apache.xerces.parsers.SAXParser: Feature 'http://javax.xml.XMLConstants/feature/secure-processing' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
[INFO] Rat check: Summary of files. Unapproved: 0 unknown: 0 generated: 0 approved: 68 licence.
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building flux-wrappers 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ flux-wrappers ---
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ flux-wrappers ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 2 resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.3:compile (default-compile) @ flux-wrappers ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ flux-wrappers ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/flux/flux-wrappers/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.3:testCompile (default-testCompile) @ flux-wrappers ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-surefire-plugin:2.18.1:test (default-test) @ flux-wrappers ---
[INFO] 
[INFO] --- apache-rat-plugin:0.11:check (default) @ flux-wrappers ---
[INFO] 51 implicit excludes (use -debug for more details).
[INFO] Exclude: **/target/**
[INFO] Exclude: **/*.iml
[INFO] Exclude: **/.idea/**
[INFO] Exclude: **/metastore_db/**
[INFO] Exclude: **/build/**
[INFO] Exclude: **/logs/**
[INFO] Exclude: **/CHANGELOG.md
[INFO] Exclude: **/README.md
[INFO] Exclude: **/README.markdown
[INFO] Exclude: **/DEVELOPER.md
[INFO] Exclude: **/BYLAWS.md
[INFO] Exclude: **/STORM-UI-REST-API.md
[INFO] Exclude: SECURITY.md
[INFO] Exclude: VERSION
[INFO] Exclude: TODO
[INFO] Exclude: **/src/py/**
[INFO] Exclude: **/src/ui/public/js/jquery.dataTables.1.10.4.min.js
[INFO] Exclude: **/src/ui/public/css/jquery.dataTables.1.10.4.min.css
[INFO] Exclude: **/src/ui/public/images/*
[INFO] Exclude: **/src/ui/public/js/bootstrap-3.3.1.min.js
[INFO] Exclude: **/src/ui/public/css/bootstrap-3.3.1.min.css
[INFO] Exclude: **/src/ui/public/js/dataTables.bootstrap.min.js
[INFO] Exclude: **/src/ui/public/css/dataTables.bootstrap.css
[INFO] Exclude: **/src/ui/public/js/jsonFormatter.min.js
[INFO] Exclude: **/src/ui/public/css/jsonFormatter.min.css
[INFO] Exclude: **/src/ui/public/js/jquery-1.11.1.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.cookies.2.2.0.min.js
[INFO] Exclude: **/src/ui/public/js/moment.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.blockUI.min.js
[INFO] Exclude: **/src/ui/public/js/url.min.js
[INFO] Exclude: **/src/ui/public/js/arbor.js
[INFO] Exclude: **/src/ui/public/js/arbor-graphics.js
[INFO] Exclude: **/src/ui/public/js/arbor-tween.js
[INFO] Exclude: **/src/ui/public/js/jquery.mustache.js
[INFO] Exclude: **/src/ui/public/js/typeahead.jquery.min.js
[INFO] Exclude: **/dependency-reduced-pom.xml
[INFO] Exclude: **/docs/**
[INFO] Exclude: **/.git/**
[INFO] Exclude: **/derby.log
[INFO] Exclude: **/src/dev/**
[INFO] Exclude: **/src/codegen/config.fmpp
[INFO] Exclude: **/src/codegen/data/Parser.tdd
[INFO] Exclude: **/src/test/resources/FixedAvroSerializer.config
[INFO] 6 resources included (use -debug for more details)
Warning:  org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
Compiler warnings:
  WARNING:  'org.apache.xerces.jaxp.SAXParserImpl: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.'
Warning:  org.apache.xerces.parsers.SAXParser: Feature 'http://javax.xml.XMLConstants/feature/secure-processing' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
[INFO] Rat check: Summary of files. Unapproved: 0 unknown: 0 generated: 0 approved: 6 licence.
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Skipping flux-core
[INFO] This project has been banned from the build due to previous failures.
[INFO] ------------------------------------------------------------------------
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Skipping flux-examples
[INFO] This project has been banned from the build due to previous failures.
[INFO] ------------------------------------------------------------------------
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building storm-sql-runtime 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ storm-sql-runtime ---
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ storm-sql-runtime ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/sql/storm-sql-runtime/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ storm-sql-runtime ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ storm-sql-runtime ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/sql/storm-sql-runtime/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ storm-sql-runtime ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:2.18.1:test (default-test) @ storm-sql-runtime ---
[INFO] Surefire report directory: /home/travis/build/apache/storm/external/sql/storm-sql-runtime/target/surefire-reports
[INFO] parallel='none', perCoreThreadCount=true, threadCount=0, useUnlimitedThreads=false, threadCountSuites=0, threadCountClasses=0, threadCountMethods=0, parallelOptimized=true

-------------------------------------------------------
 T E S T S
-------------------------------------------------------

Results :

Tests run: 0, Failures: 0, Errors: 0, Skipped: 0

[INFO] 
[INFO] --- apache-rat-plugin:0.11:check (default) @ storm-sql-runtime ---
[INFO] 51 implicit excludes (use -debug for more details).
[INFO] Exclude: **/target/**
[INFO] Exclude: **/*.iml
[INFO] Exclude: **/.idea/**
[INFO] Exclude: **/metastore_db/**
[INFO] Exclude: **/build/**
[INFO] Exclude: **/logs/**
[INFO] Exclude: **/CHANGELOG.md
[INFO] Exclude: **/README.md
[INFO] Exclude: **/README.markdown
[INFO] Exclude: **/DEVELOPER.md
[INFO] Exclude: **/BYLAWS.md
[INFO] Exclude: **/STORM-UI-REST-API.md
[INFO] Exclude: SECURITY.md
[INFO] Exclude: VERSION
[INFO] Exclude: TODO
[INFO] Exclude: **/src/py/**
[INFO] Exclude: **/src/ui/public/js/jquery.dataTables.1.10.4.min.js
[INFO] Exclude: **/src/ui/public/css/jquery.dataTables.1.10.4.min.css
[INFO] Exclude: **/src/ui/public/images/*
[INFO] Exclude: **/src/ui/public/js/bootstrap-3.3.1.min.js
[INFO] Exclude: **/src/ui/public/css/bootstrap-3.3.1.min.css
[INFO] Exclude: **/src/ui/public/js/dataTables.bootstrap.min.js
[INFO] Exclude: **/src/ui/public/css/dataTables.bootstrap.css
[INFO] Exclude: **/src/ui/public/js/jsonFormatter.min.js
[INFO] Exclude: **/src/ui/public/css/jsonFormatter.min.css
[INFO] Exclude: **/src/ui/public/js/jquery-1.11.1.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.cookies.2.2.0.min.js
[INFO] Exclude: **/src/ui/public/js/moment.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.blockUI.min.js
[INFO] Exclude: **/src/ui/public/js/url.min.js
[INFO] Exclude: **/src/ui/public/js/arbor.js
[INFO] Exclude: **/src/ui/public/js/arbor-graphics.js
[INFO] Exclude: **/src/ui/public/js/arbor-tween.js
[INFO] Exclude: **/src/ui/public/js/jquery.mustache.js
[INFO] Exclude: **/src/ui/public/js/typeahead.jquery.min.js
[INFO] Exclude: **/dependency-reduced-pom.xml
[INFO] Exclude: **/docs/**
[INFO] Exclude: **/.git/**
[INFO] Exclude: **/derby.log
[INFO] Exclude: **/src/dev/**
[INFO] Exclude: **/src/codegen/config.fmpp
[INFO] Exclude: **/src/codegen/data/Parser.tdd
[INFO] Exclude: **/src/test/resources/FixedAvroSerializer.config
[INFO] 15 resources included (use -debug for more details)
Warning:  org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
Compiler warnings:
  WARNING:  'org.apache.xerces.jaxp.SAXParserImpl: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.'
Warning:  org.apache.xerces.parsers.SAXParser: Feature 'http://javax.xml.XMLConstants/feature/secure-processing' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
[INFO] Rat check: Summary of files. Unapproved: 0 unknown: 0 generated: 0 approved: 15 licence.
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building storm-sql-core 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-resources-plugin:2.5:copy-resources (copy-fmpp-resources) @ storm-sql-core ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 4 resources
[INFO] 
[INFO] --- maven-dependency-plugin:2.8:unpack (unpack-parser-template) @ storm-sql-core ---
[INFO] Configured Artifact: org.apache.calcite:calcite-core:?:jar
[INFO] Unpacking /home/travis/.m2/repository/org/apache/calcite/calcite-core/1.4.0-incubating/calcite-core-1.4.0-incubating.jar to /home/travis/build/apache/storm/external/sql/storm-sql-core/target with includes "**/Parser.jj" and excludes ""
[INFO] 
[INFO] --- fmpp-maven-plugin:1.0:generate (generate-fmpp-sources) @ storm-sql-core ---
- Executing: Parser.jj
log4j:WARN No appenders could be found for logger (freemarker.cache).
log4j:WARN Please initialize the log4j system properly.
[INFO] Done
[INFO] 
[INFO] --- javacc-maven-plugin:2.4:javacc (javacc) @ storm-sql-core ---
Java Compiler Compiler Version 4.0 (Parser Generator)
(type "javacc" with no arguments for help)
Reading from file /home/travis/build/apache/storm/external/sql/storm-sql-core/target/generated-sources/javacc/Parser.jj . . .
Note: UNICODE_INPUT option is specified. Please make sure you create the parser/lexer using a Reader with the correct character encoding.
Warning: Lookahead adequacy checking not being performed since option LOOKAHEAD is more than 1.  Set option FORCE_LA_CHECK to true to force checking.
Parser generated with 0 errors and 1 warnings.
[INFO] Processed 1 grammar
[INFO] 
[INFO] --- maven-resources-plugin:2.5:copy-resources (copy-java-sources) @ storm-sql-core ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 19 resources
[INFO] Copying 8 resources
[INFO] Copying 8 resources
[INFO] 
[INFO] --- build-helper-maven-plugin:1.5:add-source (add-generated-sources) @ storm-sql-core ---
[INFO] Source directory: /home/travis/build/apache/storm/external/sql/storm-sql-core/target/generated-sources added.
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ storm-sql-core ---
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ storm-sql-core ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/sql/storm-sql-core/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ storm-sql-core ---
[INFO] Changes detected - recompiling the module!
[INFO] Compiling 26 source files to /home/travis/build/apache/storm/external/sql/storm-sql-core/target/classes
[WARNING] /home/travis/build/apache/storm/external/sql/storm-sql-core/target/generated-sources/org/apache/storm/sql/parser/impl/StormParserImpl.java: Some input files use or override a deprecated API.
[WARNING] /home/travis/build/apache/storm/external/sql/storm-sql-core/target/generated-sources/org/apache/storm/sql/parser/impl/StormParserImpl.java: Recompile with -Xlint:deprecation for details.
[WARNING] /home/travis/build/apache/storm/external/sql/storm-sql-core/target/generated-sources/org/apache/storm/sql/parser/impl/StormParserImpl.java: /home/travis/build/apache/storm/external/sql/storm-sql-core/target/generated-sources/org/apache/storm/sql/parser/impl/StormParserImpl.java uses unchecked or unsafe operations.
[WARNING] /home/travis/build/apache/storm/external/sql/storm-sql-core/target/generated-sources/org/apache/storm/sql/parser/impl/StormParserImpl.java: Recompile with -Xlint:unchecked for details.
[INFO] 
[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ storm-sql-core ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/sql/storm-sql-core/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ storm-sql-core ---
[INFO] Changes detected - recompiling the module!
[INFO] Compiling 15 source files to /home/travis/build/apache/storm/external/sql/storm-sql-core/target/test-classes
[WARNING] /home/travis/build/apache/storm/external/sql/storm-sql-core/target/generated-sources/org/apache/storm/sql/parser/impl/StormParserImpl.java: /home/travis/build/apache/storm/external/sql/storm-sql-core/target/generated-sources/org/apache/storm/sql/parser/impl/StormParserImpl.java uses or overrides a deprecated API.
[WARNING] /home/travis/build/apache/storm/external/sql/storm-sql-core/target/generated-sources/org/apache/storm/sql/parser/impl/StormParserImpl.java: Recompile with -Xlint:deprecation for details.
[WARNING] /home/travis/build/apache/storm/external/sql/storm-sql-core/target/generated-sources/org/apache/storm/sql/parser/impl/StormParserImpl.java: /home/travis/build/apache/storm/external/sql/storm-sql-core/target/generated-sources/org/apache/storm/sql/parser/impl/StormParserImpl.java uses unchecked or unsafe operations.
[WARNING] /home/travis/build/apache/storm/external/sql/storm-sql-core/target/generated-sources/org/apache/storm/sql/parser/impl/StormParserImpl.java: Recompile with -Xlint:unchecked for details.
[INFO] 
[INFO] --- maven-surefire-plugin:2.18.1:test (default-test) @ storm-sql-core ---
[INFO] Surefire report directory: /home/travis/build/apache/storm/external/sql/storm-sql-core/target/surefire-reports
[INFO] parallel='none', perCoreThreadCount=true, threadCount=0, useUnlimitedThreads=false, threadCountSuites=0, threadCountClasses=0, threadCountMethods=0, parallelOptimized=true

-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running org.apache.storm.sql.parser.TestSqlParser
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.453 sec - in org.apache.storm.sql.parser.TestSqlParser
Running org.apache.storm.sql.compiler.backends.trident.TestPlanCompiler
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 32.792 sec - in org.apache.storm.sql.compiler.backends.trident.TestPlanCompiler
Running org.apache.storm.sql.compiler.backends.standalone.TestPlanCompiler
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.496 sec - in org.apache.storm.sql.compiler.backends.standalone.TestPlanCompiler
Running org.apache.storm.sql.compiler.backends.standalone.TestRelNodeCompiler
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.026 sec - in org.apache.storm.sql.compiler.backends.standalone.TestRelNodeCompiler
Running org.apache.storm.sql.compiler.TestExprCompiler
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.021 sec - in org.apache.storm.sql.compiler.TestExprCompiler
Running org.apache.storm.sql.compiler.TestExprSemantic
Tests run: 10, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.966 sec - in org.apache.storm.sql.compiler.TestExprSemantic
Running org.apache.storm.sql.TestStormSql
Tests run: 15, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.423 sec - in org.apache.storm.sql.TestStormSql

Results :

Tests run: 39, Failures: 0, Errors: 0, Skipped: 0

[INFO] 
[INFO] --- apache-rat-plugin:0.11:check (default) @ storm-sql-core ---
[INFO] 51 implicit excludes (use -debug for more details).
[INFO] Exclude: **/target/**
[INFO] Exclude: **/*.iml
[INFO] Exclude: **/.idea/**
[INFO] Exclude: **/metastore_db/**
[INFO] Exclude: **/build/**
[INFO] Exclude: **/logs/**
[INFO] Exclude: **/CHANGELOG.md
[INFO] Exclude: **/README.md
[INFO] Exclude: **/README.markdown
[INFO] Exclude: **/DEVELOPER.md
[INFO] Exclude: **/BYLAWS.md
[INFO] Exclude: **/STORM-UI-REST-API.md
[INFO] Exclude: SECURITY.md
[INFO] Exclude: VERSION
[INFO] Exclude: TODO
[INFO] Exclude: **/src/py/**
[INFO] Exclude: **/src/ui/public/js/jquery.dataTables.1.10.4.min.js
[INFO] Exclude: **/src/ui/public/css/jquery.dataTables.1.10.4.min.css
[INFO] Exclude: **/src/ui/public/images/*
[INFO] Exclude: **/src/ui/public/js/bootstrap-3.3.1.min.js
[INFO] Exclude: **/src/ui/public/css/bootstrap-3.3.1.min.css
[INFO] Exclude: **/src/ui/public/js/dataTables.bootstrap.min.js
[INFO] Exclude: **/src/ui/public/css/dataTables.bootstrap.css
[INFO] Exclude: **/src/ui/public/js/jsonFormatter.min.js
[INFO] Exclude: **/src/ui/public/css/jsonFormatter.min.css
[INFO] Exclude: **/src/ui/public/js/jquery-1.11.1.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.cookies.2.2.0.min.js
[INFO] Exclude: **/src/ui/public/js/moment.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.blockUI.min.js
[INFO] Exclude: **/src/ui/public/js/url.min.js
[INFO] Exclude: **/src/ui/public/js/arbor.js
[INFO] Exclude: **/src/ui/public/js/arbor-graphics.js
[INFO] Exclude: **/src/ui/public/js/arbor-tween.js
[INFO] Exclude: **/src/ui/public/js/jquery.mustache.js
[INFO] Exclude: **/src/ui/public/js/typeahead.jquery.min.js
[INFO] Exclude: **/dependency-reduced-pom.xml
[INFO] Exclude: **/docs/**
[INFO] Exclude: **/.git/**
[INFO] Exclude: **/derby.log
[INFO] Exclude: **/src/dev/**
[INFO] Exclude: **/src/codegen/config.fmpp
[INFO] Exclude: **/src/codegen/data/Parser.tdd
[INFO] Exclude: **/src/test/resources/FixedAvroSerializer.config
[INFO] 30 resources included (use -debug for more details)
Warning:  org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
Compiler warnings:
  WARNING:  'org.apache.xerces.jaxp.SAXParserImpl: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.'
Warning:  org.apache.xerces.parsers.SAXParser: Feature 'http://javax.xml.XMLConstants/feature/secure-processing' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
[INFO] Rat check: Summary of files. Unapproved: 0 unknown: 0 generated: 0 approved: 30 licence.
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Skipping storm-sql-kafka
[INFO] This project has been banned from the build due to previous failures.
[INFO] ------------------------------------------------------------------------
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building sql 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ sql ---
[INFO] 
[INFO] --- apache-rat-plugin:0.11:check (default) @ sql ---
[INFO] 51 implicit excludes (use -debug for more details).
[INFO] Exclude: **/target/**
[INFO] Exclude: **/*.iml
[INFO] Exclude: **/.idea/**
[INFO] Exclude: **/metastore_db/**
[INFO] Exclude: **/build/**
[INFO] Exclude: **/logs/**
[INFO] Exclude: **/CHANGELOG.md
[INFO] Exclude: **/README.md
[INFO] Exclude: **/README.markdown
[INFO] Exclude: **/DEVELOPER.md
[INFO] Exclude: **/BYLAWS.md
[INFO] Exclude: **/STORM-UI-REST-API.md
[INFO] Exclude: SECURITY.md
[INFO] Exclude: VERSION
[INFO] Exclude: TODO
[INFO] Exclude: **/src/py/**
[INFO] Exclude: **/src/ui/public/js/jquery.dataTables.1.10.4.min.js
[INFO] Exclude: **/src/ui/public/css/jquery.dataTables.1.10.4.min.css
[INFO] Exclude: **/src/ui/public/images/*
[INFO] Exclude: **/src/ui/public/js/bootstrap-3.3.1.min.js
[INFO] Exclude: **/src/ui/public/css/bootstrap-3.3.1.min.css
[INFO] Exclude: **/src/ui/public/js/dataTables.bootstrap.min.js
[INFO] Exclude: **/src/ui/public/css/dataTables.bootstrap.css
[INFO] Exclude: **/src/ui/public/js/jsonFormatter.min.js
[INFO] Exclude: **/src/ui/public/css/jsonFormatter.min.css
[INFO] Exclude: **/src/ui/public/js/jquery-1.11.1.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.cookies.2.2.0.min.js
[INFO] Exclude: **/src/ui/public/js/moment.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.blockUI.min.js
[INFO] Exclude: **/src/ui/public/js/url.min.js
[INFO] Exclude: **/src/ui/public/js/arbor.js
[INFO] Exclude: **/src/ui/public/js/arbor-graphics.js
[INFO] Exclude: **/src/ui/public/js/arbor-tween.js
[INFO] Exclude: **/src/ui/public/js/jquery.mustache.js
[INFO] Exclude: **/src/ui/public/js/typeahead.jquery.min.js
[INFO] Exclude: **/dependency-reduced-pom.xml
[INFO] Exclude: **/docs/**
[INFO] Exclude: **/.git/**
[INFO] Exclude: **/derby.log
[INFO] Exclude: **/src/dev/**
[INFO] Exclude: **/src/codegen/config.fmpp
[INFO] Exclude: **/src/codegen/data/Parser.tdd
[INFO] Exclude: **/src/test/resources/FixedAvroSerializer.config
[INFO] 53 resources included (use -debug for more details)
Warning:  org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
Compiler warnings:
  WARNING:  'org.apache.xerces.jaxp.SAXParserImpl: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.'
Warning:  org.apache.xerces.parsers.SAXParser: Feature 'http://javax.xml.XMLConstants/feature/secure-processing' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
[INFO] Rat check: Summary of files. Unapproved: 0 unknown: 0 generated: 0 approved: 53 licence.
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building storm-elasticsearch 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ storm-elasticsearch ---
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ storm-elasticsearch ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/storm-elasticsearch/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ storm-elasticsearch ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ storm-elasticsearch ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/storm-elasticsearch/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ storm-elasticsearch ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (cleanup) @ storm-elasticsearch ---
[INFO] 
[INFO] --- maven-surefire-plugin:2.18.1:test (default-test) @ storm-elasticsearch ---
[INFO] Surefire report directory: /home/travis/build/apache/storm/external/storm-elasticsearch/target/surefire-reports
[INFO] parallel='none', perCoreThreadCount=true, threadCount=0, useUnlimitedThreads=false, threadCountSuites=0, threadCountClasses=0, threadCountMethods=0, parallelOptimized=true

-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running org.apache.storm.elasticsearch.common.TransportAddressesTest
Tests run: 9, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.054 sec - in org.apache.storm.elasticsearch.common.TransportAddressesTest
Running org.apache.storm.elasticsearch.common.EsConfigTest
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.045 sec - in org.apache.storm.elasticsearch.common.EsConfigTest
Running org.apache.storm.elasticsearch.bolt.EsLookupBoltTest
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.088 sec - in org.apache.storm.elasticsearch.bolt.EsLookupBoltTest
Running org.apache.storm.elasticsearch.trident.EsStateFactoryTest
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.005 sec - in org.apache.storm.elasticsearch.trident.EsStateFactoryTest

Results :

Tests run: 18, Failures: 0, Errors: 0, Skipped: 0

[INFO] 
[INFO] --- apache-rat-plugin:0.11:check (default) @ storm-elasticsearch ---
[INFO] 51 implicit excludes (use -debug for more details).
[INFO] Exclude: **/target/**
[INFO] Exclude: **/*.iml
[INFO] Exclude: **/.idea/**
[INFO] Exclude: **/metastore_db/**
[INFO] Exclude: **/build/**
[INFO] Exclude: **/logs/**
[INFO] Exclude: **/CHANGELOG.md
[INFO] Exclude: **/README.md
[INFO] Exclude: **/README.markdown
[INFO] Exclude: **/DEVELOPER.md
[INFO] Exclude: **/BYLAWS.md
[INFO] Exclude: **/STORM-UI-REST-API.md
[INFO] Exclude: SECURITY.md
[INFO] Exclude: VERSION
[INFO] Exclude: TODO
[INFO] Exclude: **/src/py/**
[INFO] Exclude: **/src/ui/public/js/jquery.dataTables.1.10.4.min.js
[INFO] Exclude: **/src/ui/public/css/jquery.dataTables.1.10.4.min.css
[INFO] Exclude: **/src/ui/public/images/*
[INFO] Exclude: **/src/ui/public/js/bootstrap-3.3.1.min.js
[INFO] Exclude: **/src/ui/public/css/bootstrap-3.3.1.min.css
[INFO] Exclude: **/src/ui/public/js/dataTables.bootstrap.min.js
[INFO] Exclude: **/src/ui/public/css/dataTables.bootstrap.css
[INFO] Exclude: **/src/ui/public/js/jsonFormatter.min.js
[INFO] Exclude: **/src/ui/public/css/jsonFormatter.min.css
[INFO] Exclude: **/src/ui/public/js/jquery-1.11.1.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.cookies.2.2.0.min.js
[INFO] Exclude: **/src/ui/public/js/moment.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.blockUI.min.js
[INFO] Exclude: **/src/ui/public/js/url.min.js
[INFO] Exclude: **/src/ui/public/js/arbor.js
[INFO] Exclude: **/src/ui/public/js/arbor-graphics.js
[INFO] Exclude: **/src/ui/public/js/arbor-tween.js
[INFO] Exclude: **/src/ui/public/js/jquery.mustache.js
[INFO] Exclude: **/src/ui/public/js/typeahead.jquery.min.js
[INFO] Exclude: **/dependency-reduced-pom.xml
[INFO] Exclude: **/docs/**
[INFO] Exclude: **/.git/**
[INFO] Exclude: **/derby.log
[INFO] Exclude: **/src/dev/**
[INFO] Exclude: **/src/codegen/config.fmpp
[INFO] Exclude: **/src/codegen/data/Parser.tdd
[INFO] Exclude: **/src/test/resources/FixedAvroSerializer.config
[INFO] 28 resources included (use -debug for more details)
Warning:  org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
Compiler warnings:
  WARNING:  'org.apache.xerces.jaxp.SAXParserImpl: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.'
Warning:  org.apache.xerces.parsers.SAXParser: Feature 'http://javax.xml.XMLConstants/feature/secure-processing' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
[INFO] Rat check: Summary of files. Unapproved: 0 unknown: 0 generated: 0 approved: 28 licence.
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building storm-solr 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ storm-solr ---
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ storm-solr ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/storm-solr/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ storm-solr ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ storm-solr ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/storm-solr/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ storm-solr ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:2.18.1:test (default-test) @ storm-solr ---
[INFO] Surefire report directory: /home/travis/build/apache/storm/external/storm-solr/target/surefire-reports
[INFO] parallel='none', perCoreThreadCount=true, threadCount=0, useUnlimitedThreads=false, threadCountSuites=0, threadCountClasses=0, threadCountMethods=0, parallelOptimized=true

-------------------------------------------------------
 T E S T S
-------------------------------------------------------

Results :

Tests run: 0, Failures: 0, Errors: 0, Skipped: 0

[INFO] 
[INFO] --- apache-rat-plugin:0.11:check (default) @ storm-solr ---
[INFO] 51 implicit excludes (use -debug for more details).
[INFO] Exclude: **/target/**
[INFO] Exclude: **/*.iml
[INFO] Exclude: **/.idea/**
[INFO] Exclude: **/metastore_db/**
[INFO] Exclude: **/build/**
[INFO] Exclude: **/logs/**
[INFO] Exclude: **/CHANGELOG.md
[INFO] Exclude: **/README.md
[INFO] Exclude: **/README.markdown
[INFO] Exclude: **/DEVELOPER.md
[INFO] Exclude: **/BYLAWS.md
[INFO] Exclude: **/STORM-UI-REST-API.md
[INFO] Exclude: SECURITY.md
[INFO] Exclude: VERSION
[INFO] Exclude: TODO
[INFO] Exclude: **/src/py/**
[INFO] Exclude: **/src/ui/public/js/jquery.dataTables.1.10.4.min.js
[INFO] Exclude: **/src/ui/public/css/jquery.dataTables.1.10.4.min.css
[INFO] Exclude: **/src/ui/public/images/*
[INFO] Exclude: **/src/ui/public/js/bootstrap-3.3.1.min.js
[INFO] Exclude: **/src/ui/public/css/bootstrap-3.3.1.min.css
[INFO] Exclude: **/src/ui/public/js/dataTables.bootstrap.min.js
[INFO] Exclude: **/src/ui/public/css/dataTables.bootstrap.css
[INFO] Exclude: **/src/ui/public/js/jsonFormatter.min.js
[INFO] Exclude: **/src/ui/public/css/jsonFormatter.min.css
[INFO] Exclude: **/src/ui/public/js/jquery-1.11.1.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.cookies.2.2.0.min.js
[INFO] Exclude: **/src/ui/public/js/moment.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.blockUI.min.js
[INFO] Exclude: **/src/ui/public/js/url.min.js
[INFO] Exclude: **/src/ui/public/js/arbor.js
[INFO] Exclude: **/src/ui/public/js/arbor-graphics.js
[INFO] Exclude: **/src/ui/public/js/arbor-tween.js
[INFO] Exclude: **/src/ui/public/js/jquery.mustache.js
[INFO] Exclude: **/src/ui/public/js/typeahead.jquery.min.js
[INFO] Exclude: **/dependency-reduced-pom.xml
[INFO] Exclude: **/docs/**
[INFO] Exclude: **/.git/**
[INFO] Exclude: **/derby.log
[INFO] Exclude: **/src/dev/**
[INFO] Exclude: **/src/codegen/config.fmpp
[INFO] Exclude: **/src/codegen/data/Parser.tdd
[INFO] Exclude: **/src/test/resources/FixedAvroSerializer.config
[INFO] 27 resources included (use -debug for more details)
Warning:  org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
Compiler warnings:
  WARNING:  'org.apache.xerces.jaxp.SAXParserImpl: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.'
Warning:  org.apache.xerces.parsers.SAXParser: Feature 'http://javax.xml.XMLConstants/feature/secure-processing' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
[INFO] Rat check: Summary of files. Unapproved: 0 unknown: 0 generated: 0 approved: 27 licence.
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building storm-metrics 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-antrun-plugin:1.6:run (prepare) @ storm-metrics ---
[WARNING] Parameter tasks is deprecated, use target instead
[INFO] Executing tasks

main:
     [echo] Downloading sigar native binaries...
      [get] Destination already exists (skipping): /home/travis/.m2/repository/org/fusesource/sigar/1.6.4/hyperic-sigar-1.6.4.zip
    [unzip] Expanding: /home/travis/.m2/repository/org/fusesource/sigar/1.6.4/hyperic-sigar-1.6.4.zip into /home/travis/build/apache/storm/external/storm-metrics/target/classes/resources
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ storm-metrics ---
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ storm-metrics ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/storm-metrics/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ storm-metrics ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ storm-metrics ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/storm-metrics/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ storm-metrics ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-surefire-plugin:2.18.1:test (default-test) @ storm-metrics ---
[INFO] 
[INFO] --- apache-rat-plugin:0.11:check (default) @ storm-metrics ---
[INFO] 51 implicit excludes (use -debug for more details).
[INFO] Exclude: **/target/**
[INFO] Exclude: **/*.iml
[INFO] Exclude: **/.idea/**
[INFO] Exclude: **/metastore_db/**
[INFO] Exclude: **/build/**
[INFO] Exclude: **/logs/**
[INFO] Exclude: **/CHANGELOG.md
[INFO] Exclude: **/README.md
[INFO] Exclude: **/README.markdown
[INFO] Exclude: **/DEVELOPER.md
[INFO] Exclude: **/BYLAWS.md
[INFO] Exclude: **/STORM-UI-REST-API.md
[INFO] Exclude: SECURITY.md
[INFO] Exclude: VERSION
[INFO] Exclude: TODO
[INFO] Exclude: **/src/py/**
[INFO] Exclude: **/src/ui/public/js/jquery.dataTables.1.10.4.min.js
[INFO] Exclude: **/src/ui/public/css/jquery.dataTables.1.10.4.min.css
[INFO] Exclude: **/src/ui/public/images/*
[INFO] Exclude: **/src/ui/public/js/bootstrap-3.3.1.min.js
[INFO] Exclude: **/src/ui/public/css/bootstrap-3.3.1.min.css
[INFO] Exclude: **/src/ui/public/js/dataTables.bootstrap.min.js
[INFO] Exclude: **/src/ui/public/css/dataTables.bootstrap.css
[INFO] Exclude: **/src/ui/public/js/jsonFormatter.min.js
[INFO] Exclude: **/src/ui/public/css/jsonFormatter.min.css
[INFO] Exclude: **/src/ui/public/js/jquery-1.11.1.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.cookies.2.2.0.min.js
[INFO] Exclude: **/src/ui/public/js/moment.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.blockUI.min.js
[INFO] Exclude: **/src/ui/public/js/url.min.js
[INFO] Exclude: **/src/ui/public/js/arbor.js
[INFO] Exclude: **/src/ui/public/js/arbor-graphics.js
[INFO] Exclude: **/src/ui/public/js/arbor-tween.js
[INFO] Exclude: **/src/ui/public/js/jquery.mustache.js
[INFO] Exclude: **/src/ui/public/js/typeahead.jquery.min.js
[INFO] Exclude: **/dependency-reduced-pom.xml
[INFO] Exclude: **/docs/**
[INFO] Exclude: **/.git/**
[INFO] Exclude: **/derby.log
[INFO] Exclude: **/src/dev/**
[INFO] Exclude: **/src/codegen/config.fmpp
[INFO] Exclude: **/src/codegen/data/Parser.tdd
[INFO] Exclude: **/src/test/resources/FixedAvroSerializer.config
[INFO] 3 resources included (use -debug for more details)
Warning:  org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
Compiler warnings:
  WARNING:  'org.apache.xerces.jaxp.SAXParserImpl: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.'
Warning:  org.apache.xerces.parsers.SAXParser: Feature 'http://javax.xml.XMLConstants/feature/secure-processing' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
[INFO] Rat check: Summary of files. Unapproved: 0 unknown: 0 generated: 0 approved: 3 licence.
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building storm-cassandra 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ storm-cassandra ---
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ storm-cassandra ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/storm-cassandra/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ storm-cassandra ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ storm-cassandra ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 1 resource
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ storm-cassandra ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:2.18.1:test (default-test) @ storm-cassandra ---
[INFO] 
[INFO] --- apache-rat-plugin:0.11:check (default) @ storm-cassandra ---
[INFO] 51 implicit excludes (use -debug for more details).
[INFO] Exclude: **/target/**
[INFO] Exclude: **/*.iml
[INFO] Exclude: **/.idea/**
[INFO] Exclude: **/metastore_db/**
[INFO] Exclude: **/build/**
[INFO] Exclude: **/logs/**
[INFO] Exclude: **/CHANGELOG.md
[INFO] Exclude: **/README.md
[INFO] Exclude: **/README.markdown
[INFO] Exclude: **/DEVELOPER.md
[INFO] Exclude: **/BYLAWS.md
[INFO] Exclude: **/STORM-UI-REST-API.md
[INFO] Exclude: SECURITY.md
[INFO] Exclude: VERSION
[INFO] Exclude: TODO
[INFO] Exclude: **/src/py/**
[INFO] Exclude: **/src/ui/public/js/jquery.dataTables.1.10.4.min.js
[INFO] Exclude: **/src/ui/public/css/jquery.dataTables.1.10.4.min.css
[INFO] Exclude: **/src/ui/public/images/*
[INFO] Exclude: **/src/ui/public/js/bootstrap-3.3.1.min.js
[INFO] Exclude: **/src/ui/public/css/bootstrap-3.3.1.min.css
[INFO] Exclude: **/src/ui/public/js/dataTables.bootstrap.min.js
[INFO] Exclude: **/src/ui/public/css/dataTables.bootstrap.css
[INFO] Exclude: **/src/ui/public/js/jsonFormatter.min.js
[INFO] Exclude: **/src/ui/public/css/jsonFormatter.min.css
[INFO] Exclude: **/src/ui/public/js/jquery-1.11.1.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.cookies.2.2.0.min.js
[INFO] Exclude: **/src/ui/public/js/moment.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.blockUI.min.js
[INFO] Exclude: **/src/ui/public/js/url.min.js
[INFO] Exclude: **/src/ui/public/js/arbor.js
[INFO] Exclude: **/src/ui/public/js/arbor-graphics.js
[INFO] Exclude: **/src/ui/public/js/arbor-tween.js
[INFO] Exclude: **/src/ui/public/js/jquery.mustache.js
[INFO] Exclude: **/src/ui/public/js/typeahead.jquery.min.js
[INFO] Exclude: **/dependency-reduced-pom.xml
[INFO] Exclude: **/docs/**
[INFO] Exclude: **/.git/**
[INFO] Exclude: **/derby.log
[INFO] Exclude: **/src/dev/**
[INFO] Exclude: **/src/codegen/config.fmpp
[INFO] Exclude: **/src/codegen/data/Parser.tdd
[INFO] Exclude: **/src/test/resources/FixedAvroSerializer.config
[INFO] 50 resources included (use -debug for more details)
Warning:  org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
Compiler warnings:
  WARNING:  'org.apache.xerces.jaxp.SAXParserImpl: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.'
Warning:  org.apache.xerces.parsers.SAXParser: Feature 'http://javax.xml.XMLConstants/feature/secure-processing' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
[INFO] Rat check: Summary of files. Unapproved: 0 unknown: 0 generated: 0 approved: 50 licence.
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building storm-mqtt-parent 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ storm-mqtt-parent ---
[INFO] 
[INFO] --- apache-rat-plugin:0.11:check (default) @ storm-mqtt-parent ---
[INFO] 51 implicit excludes (use -debug for more details).
[INFO] Exclude: **/target/**
[INFO] Exclude: **/*.iml
[INFO] Exclude: **/.idea/**
[INFO] Exclude: **/metastore_db/**
[INFO] Exclude: **/build/**
[INFO] Exclude: **/logs/**
[INFO] Exclude: **/CHANGELOG.md
[INFO] Exclude: **/README.md
[INFO] Exclude: **/README.markdown
[INFO] Exclude: **/DEVELOPER.md
[INFO] Exclude: **/BYLAWS.md
[INFO] Exclude: **/STORM-UI-REST-API.md
[INFO] Exclude: SECURITY.md
[INFO] Exclude: VERSION
[INFO] Exclude: TODO
[INFO] Exclude: **/src/py/**
[INFO] Exclude: **/src/ui/public/js/jquery.dataTables.1.10.4.min.js
[INFO] Exclude: **/src/ui/public/css/jquery.dataTables.1.10.4.min.css
[INFO] Exclude: **/src/ui/public/images/*
[INFO] Exclude: **/src/ui/public/js/bootstrap-3.3.1.min.js
[INFO] Exclude: **/src/ui/public/css/bootstrap-3.3.1.min.css
[INFO] Exclude: **/src/ui/public/js/dataTables.bootstrap.min.js
[INFO] Exclude: **/src/ui/public/css/dataTables.bootstrap.css
[INFO] Exclude: **/src/ui/public/js/jsonFormatter.min.js
[INFO] Exclude: **/src/ui/public/css/jsonFormatter.min.css
[INFO] Exclude: **/src/ui/public/js/jquery-1.11.1.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.cookies.2.2.0.min.js
[INFO] Exclude: **/src/ui/public/js/moment.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.blockUI.min.js
[INFO] Exclude: **/src/ui/public/js/url.min.js
[INFO] Exclude: **/src/ui/public/js/arbor.js
[INFO] Exclude: **/src/ui/public/js/arbor-graphics.js
[INFO] Exclude: **/src/ui/public/js/arbor-tween.js
[INFO] Exclude: **/src/ui/public/js/jquery.mustache.js
[INFO] Exclude: **/src/ui/public/js/typeahead.jquery.min.js
[INFO] Exclude: **/dependency-reduced-pom.xml
[INFO] Exclude: **/docs/**
[INFO] Exclude: **/.git/**
[INFO] Exclude: **/derby.log
[INFO] Exclude: **/src/dev/**
[INFO] Exclude: **/src/codegen/config.fmpp
[INFO] Exclude: **/src/codegen/data/Parser.tdd
[INFO] Exclude: **/src/test/resources/FixedAvroSerializer.config
[INFO] 25 resources included (use -debug for more details)
Warning:  org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
Compiler warnings:
  WARNING:  'org.apache.xerces.jaxp.SAXParserImpl: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.'
Warning:  org.apache.xerces.parsers.SAXParser: Feature 'http://javax.xml.XMLConstants/feature/secure-processing' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
[INFO] Rat check: Summary of files. Unapproved: 0 unknown: 0 generated: 0 approved: 25 licence.
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building storm-mqtt 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ storm-mqtt ---
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ storm-mqtt ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/storm-mqtt/core/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ storm-mqtt ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ storm-mqtt ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/storm-mqtt/core/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ storm-mqtt ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:2.18.1:test (default-test) @ storm-mqtt ---
[WARNING] The parameter forkMode is deprecated since version 2.14. Use forkCount and reuseForks instead.
[INFO] Surefire report directory: /home/travis/build/apache/storm/external/storm-mqtt/core/target/surefire-reports
[INFO] parallel='none', perCoreThreadCount=true, threadCount=0, useUnlimitedThreads=false, threadCountSuites=0, threadCountClasses=0, threadCountMethods=0, parallelOptimized=true

-------------------------------------------------------
 T E S T S
-------------------------------------------------------

Results :

Tests run: 0, Failures: 0, Errors: 0, Skipped: 0

[INFO] 
[INFO] --- apache-rat-plugin:0.11:check (default) @ storm-mqtt ---
[INFO] 51 implicit excludes (use -debug for more details).
[INFO] Exclude: **/target/**
[INFO] Exclude: **/*.iml
[INFO] Exclude: **/.idea/**
[INFO] Exclude: **/metastore_db/**
[INFO] Exclude: **/build/**
[INFO] Exclude: **/logs/**
[INFO] Exclude: **/CHANGELOG.md
[INFO] Exclude: **/README.md
[INFO] Exclude: **/README.markdown
[INFO] Exclude: **/DEVELOPER.md
[INFO] Exclude: **/BYLAWS.md
[INFO] Exclude: **/STORM-UI-REST-API.md
[INFO] Exclude: SECURITY.md
[INFO] Exclude: VERSION
[INFO] Exclude: TODO
[INFO] Exclude: **/src/py/**
[INFO] Exclude: **/src/ui/public/js/jquery.dataTables.1.10.4.min.js
[INFO] Exclude: **/src/ui/public/css/jquery.dataTables.1.10.4.min.css
[INFO] Exclude: **/src/ui/public/images/*
[INFO] Exclude: **/src/ui/public/js/bootstrap-3.3.1.min.js
[INFO] Exclude: **/src/ui/public/css/bootstrap-3.3.1.min.css
[INFO] Exclude: **/src/ui/public/js/dataTables.bootstrap.min.js
[INFO] Exclude: **/src/ui/public/css/dataTables.bootstrap.css
[INFO] Exclude: **/src/ui/public/js/jsonFormatter.min.js
[INFO] Exclude: **/src/ui/public/css/jsonFormatter.min.css
[INFO] Exclude: **/src/ui/public/js/jquery-1.11.1.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.cookies.2.2.0.min.js
[INFO] Exclude: **/src/ui/public/js/moment.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.blockUI.min.js
[INFO] Exclude: **/src/ui/public/js/url.min.js
[INFO] Exclude: **/src/ui/public/js/arbor.js
[INFO] Exclude: **/src/ui/public/js/arbor-graphics.js
[INFO] Exclude: **/src/ui/public/js/arbor-tween.js
[INFO] Exclude: **/src/ui/public/js/jquery.mustache.js
[INFO] Exclude: **/src/ui/public/js/typeahead.jquery.min.js
[INFO] Exclude: **/dependency-reduced-pom.xml
[INFO] Exclude: **/docs/**
[INFO] Exclude: **/.git/**
[INFO] Exclude: **/derby.log
[INFO] Exclude: **/src/dev/**
[INFO] Exclude: **/src/codegen/config.fmpp
[INFO] Exclude: **/src/codegen/data/Parser.tdd
[INFO] Exclude: **/src/test/resources/FixedAvroSerializer.config
[INFO] 18 resources included (use -debug for more details)
Warning:  org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
Compiler warnings:
  WARNING:  'org.apache.xerces.jaxp.SAXParserImpl: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.'
Warning:  org.apache.xerces.parsers.SAXParser: Feature 'http://javax.xml.XMLConstants/feature/secure-processing' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
[INFO] Rat check: Summary of files. Unapproved: 0 unknown: 0 generated: 0 approved: 18 licence.
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Skipping storm-mqtt-examples
[INFO] This project has been banned from the build due to previous failures.
[INFO] ------------------------------------------------------------------------
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building storm-mongodb 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ storm-mongodb ---
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ storm-mongodb ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/storm-mongodb/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ storm-mongodb ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ storm-mongodb ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/storm-mongodb/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ storm-mongodb ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:2.18.1:test (default-test) @ storm-mongodb ---
[INFO] 
[INFO] --- apache-rat-plugin:0.11:check (default) @ storm-mongodb ---
[INFO] 51 implicit excludes (use -debug for more details).
[INFO] Exclude: **/target/**
[INFO] Exclude: **/*.iml
[INFO] Exclude: **/.idea/**
[INFO] Exclude: **/metastore_db/**
[INFO] Exclude: **/build/**
[INFO] Exclude: **/logs/**
[INFO] Exclude: **/CHANGELOG.md
[INFO] Exclude: **/README.md
[INFO] Exclude: **/README.markdown
[INFO] Exclude: **/DEVELOPER.md
[INFO] Exclude: **/BYLAWS.md
[INFO] Exclude: **/STORM-UI-REST-API.md
[INFO] Exclude: SECURITY.md
[INFO] Exclude: VERSION
[INFO] Exclude: TODO
[INFO] Exclude: **/src/py/**
[INFO] Exclude: **/src/ui/public/js/jquery.dataTables.1.10.4.min.js
[INFO] Exclude: **/src/ui/public/css/jquery.dataTables.1.10.4.min.css
[INFO] Exclude: **/src/ui/public/images/*
[INFO] Exclude: **/src/ui/public/js/bootstrap-3.3.1.min.js
[INFO] Exclude: **/src/ui/public/css/bootstrap-3.3.1.min.css
[INFO] Exclude: **/src/ui/public/js/dataTables.bootstrap.min.js
[INFO] Exclude: **/src/ui/public/css/dataTables.bootstrap.css
[INFO] Exclude: **/src/ui/public/js/jsonFormatter.min.js
[INFO] Exclude: **/src/ui/public/css/jsonFormatter.min.css
[INFO] Exclude: **/src/ui/public/js/jquery-1.11.1.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.cookies.2.2.0.min.js
[INFO] Exclude: **/src/ui/public/js/moment.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.blockUI.min.js
[INFO] Exclude: **/src/ui/public/js/url.min.js
[INFO] Exclude: **/src/ui/public/js/arbor.js
[INFO] Exclude: **/src/ui/public/js/arbor-graphics.js
[INFO] Exclude: **/src/ui/public/js/arbor-tween.js
[INFO] Exclude: **/src/ui/public/js/jquery.mustache.js
[INFO] Exclude: **/src/ui/public/js/typeahead.jquery.min.js
[INFO] Exclude: **/dependency-reduced-pom.xml
[INFO] Exclude: **/docs/**
[INFO] Exclude: **/.git/**
[INFO] Exclude: **/derby.log
[INFO] Exclude: **/src/dev/**
[INFO] Exclude: **/src/codegen/config.fmpp
[INFO] Exclude: **/src/codegen/data/Parser.tdd
[INFO] Exclude: **/src/test/resources/FixedAvroSerializer.config
[INFO] 18 resources included (use -debug for more details)
Warning:  org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
Compiler warnings:
  WARNING:  'org.apache.xerces.jaxp.SAXParserImpl: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.'
Warning:  org.apache.xerces.parsers.SAXParser: Feature 'http://javax.xml.XMLConstants/feature/secure-processing' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
[INFO] Rat check: Summary of files. Unapproved: 0 unknown: 0 generated: 0 approved: 18 licence.
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building storm-clojure 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ storm-clojure ---
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ storm-clojure ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/storm-clojure/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ storm-clojure ---
[INFO] No sources to compile
[INFO] 
[INFO] --- clojure-maven-plugin:1.7.1:compile (compile) @ storm-clojure ---
[INFO] 
[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ storm-clojure ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/storm-clojure/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ storm-clojure ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-surefire-plugin:2.18.1:test (default-test) @ storm-clojure ---
[INFO] 
[INFO] --- apache-rat-plugin:0.11:check (default) @ storm-clojure ---
[INFO] 51 implicit excludes (use -debug for more details).
[INFO] Exclude: **/target/**
[INFO] Exclude: **/*.iml
[INFO] Exclude: **/.idea/**
[INFO] Exclude: **/metastore_db/**
[INFO] Exclude: **/build/**
[INFO] Exclude: **/logs/**
[INFO] Exclude: **/CHANGELOG.md
[INFO] Exclude: **/README.md
[INFO] Exclude: **/README.markdown
[INFO] Exclude: **/DEVELOPER.md
[INFO] Exclude: **/BYLAWS.md
[INFO] Exclude: **/STORM-UI-REST-API.md
[INFO] Exclude: SECURITY.md
[INFO] Exclude: VERSION
[INFO] Exclude: TODO
[INFO] Exclude: **/src/py/**
[INFO] Exclude: **/src/ui/public/js/jquery.dataTables.1.10.4.min.js
[INFO] Exclude: **/src/ui/public/css/jquery.dataTables.1.10.4.min.css
[INFO] Exclude: **/src/ui/public/images/*
[INFO] Exclude: **/src/ui/public/js/bootstrap-3.3.1.min.js
[INFO] Exclude: **/src/ui/public/css/bootstrap-3.3.1.min.css
[INFO] Exclude: **/src/ui/public/js/dataTables.bootstrap.min.js
[INFO] Exclude: **/src/ui/public/css/dataTables.bootstrap.css
[INFO] Exclude: **/src/ui/public/js/jsonFormatter.min.js
[INFO] Exclude: **/src/ui/public/css/jsonFormatter.min.css
[INFO] Exclude: **/src/ui/public/js/jquery-1.11.1.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.cookies.2.2.0.min.js
[INFO] Exclude: **/src/ui/public/js/moment.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.blockUI.min.js
[INFO] Exclude: **/src/ui/public/js/url.min.js
[INFO] Exclude: **/src/ui/public/js/arbor.js
[INFO] Exclude: **/src/ui/public/js/arbor-graphics.js
[INFO] Exclude: **/src/ui/public/js/arbor-tween.js
[INFO] Exclude: **/src/ui/public/js/jquery.mustache.js
[INFO] Exclude: **/src/ui/public/js/typeahead.jquery.min.js
[INFO] Exclude: **/dependency-reduced-pom.xml
[INFO] Exclude: **/docs/**
[INFO] Exclude: **/.git/**
[INFO] Exclude: **/derby.log
[INFO] Exclude: **/src/dev/**
[INFO] Exclude: **/src/codegen/config.fmpp
[INFO] Exclude: **/src/codegen/data/Parser.tdd
[INFO] Exclude: **/src/test/resources/FixedAvroSerializer.config
[INFO] 4 resources included (use -debug for more details)
Warning:  org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
Compiler warnings:
  WARNING:  'org.apache.xerces.jaxp.SAXParserImpl: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.'
Warning:  org.apache.xerces.parsers.SAXParser: Feature 'http://javax.xml.XMLConstants/feature/secure-processing' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
[INFO] Rat check: Summary of files. Unapproved: 0 unknown: 0 generated: 0 approved: 4 licence.
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Skipping storm-starter
[INFO] This project has been banned from the build due to previous failures.
[INFO] ------------------------------------------------------------------------
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building storm-kafka-client 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ storm-kafka-client ---
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ storm-kafka-client ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/storm-kafka-client/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ storm-kafka-client ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ storm-kafka-client ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/storm-kafka-client/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ storm-kafka-client ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:2.18.1:test (default-test) @ storm-kafka-client ---
[INFO] 
[INFO] --- apache-rat-plugin:0.11:check (default) @ storm-kafka-client ---
[INFO] 51 implicit excludes (use -debug for more details).
[INFO] Exclude: **/target/**
[INFO] Exclude: **/*.iml
[INFO] Exclude: **/.idea/**
[INFO] Exclude: **/metastore_db/**
[INFO] Exclude: **/build/**
[INFO] Exclude: **/logs/**
[INFO] Exclude: **/CHANGELOG.md
[INFO] Exclude: **/README.md
[INFO] Exclude: **/README.markdown
[INFO] Exclude: **/DEVELOPER.md
[INFO] Exclude: **/BYLAWS.md
[INFO] Exclude: **/STORM-UI-REST-API.md
[INFO] Exclude: SECURITY.md
[INFO] Exclude: VERSION
[INFO] Exclude: TODO
[INFO] Exclude: **/src/py/**
[INFO] Exclude: **/src/ui/public/js/jquery.dataTables.1.10.4.min.js
[INFO] Exclude: **/src/ui/public/css/jquery.dataTables.1.10.4.min.css
[INFO] Exclude: **/src/ui/public/images/*
[INFO] Exclude: **/src/ui/public/js/bootstrap-3.3.1.min.js
[INFO] Exclude: **/src/ui/public/css/bootstrap-3.3.1.min.css
[INFO] Exclude: **/src/ui/public/js/dataTables.bootstrap.min.js
[INFO] Exclude: **/src/ui/public/css/dataTables.bootstrap.css
[INFO] Exclude: **/src/ui/public/js/jsonFormatter.min.js
[INFO] Exclude: **/src/ui/public/css/jsonFormatter.min.css
[INFO] Exclude: **/src/ui/public/js/jquery-1.11.1.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.cookies.2.2.0.min.js
[INFO] Exclude: **/src/ui/public/js/moment.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.blockUI.min.js
[INFO] Exclude: **/src/ui/public/js/url.min.js
[INFO] Exclude: **/src/ui/public/js/arbor.js
[INFO] Exclude: **/src/ui/public/js/arbor-graphics.js
[INFO] Exclude: **/src/ui/public/js/arbor-tween.js
[INFO] Exclude: **/src/ui/public/js/jquery.mustache.js
[INFO] Exclude: **/src/ui/public/js/typeahead.jquery.min.js
[INFO] Exclude: **/dependency-reduced-pom.xml
[INFO] Exclude: **/docs/**
[INFO] Exclude: **/.git/**
[INFO] Exclude: **/derby.log
[INFO] Exclude: **/src/dev/**
[INFO] Exclude: **/src/codegen/config.fmpp
[INFO] Exclude: **/src/codegen/data/Parser.tdd
[INFO] Exclude: **/src/test/resources/FixedAvroSerializer.config
[INFO] 14 resources included (use -debug for more details)
Warning:  org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
Compiler warnings:
  WARNING:  'org.apache.xerces.jaxp.SAXParserImpl: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.'
Warning:  org.apache.xerces.parsers.SAXParser: Feature 'http://javax.xml.XMLConstants/feature/secure-processing' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
[INFO] Rat check: Summary of files. Unapproved: 0 unknown: 0 generated: 0 approved: 14 licence.
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building storm-opentsdb 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ storm-opentsdb ---
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ storm-opentsdb ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/storm-opentsdb/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ storm-opentsdb ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ storm-opentsdb ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/storm-opentsdb/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ storm-opentsdb ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:2.18.1:test (default-test) @ storm-opentsdb ---
[INFO] 
[INFO] --- apache-rat-plugin:0.11:check (default) @ storm-opentsdb ---
[INFO] 51 implicit excludes (use -debug for more details).
[INFO] Exclude: **/target/**
[INFO] Exclude: **/*.iml
[INFO] Exclude: **/.idea/**
[INFO] Exclude: **/metastore_db/**
[INFO] Exclude: **/build/**
[INFO] Exclude: **/logs/**
[INFO] Exclude: **/CHANGELOG.md
[INFO] Exclude: **/README.md
[INFO] Exclude: **/README.markdown
[INFO] Exclude: **/DEVELOPER.md
[INFO] Exclude: **/BYLAWS.md
[INFO] Exclude: **/STORM-UI-REST-API.md
[INFO] Exclude: SECURITY.md
[INFO] Exclude: VERSION
[INFO] Exclude: TODO
[INFO] Exclude: **/src/py/**
[INFO] Exclude: **/src/ui/public/js/jquery.dataTables.1.10.4.min.js
[INFO] Exclude: **/src/ui/public/css/jquery.dataTables.1.10.4.min.css
[INFO] Exclude: **/src/ui/public/images/*
[INFO] Exclude: **/src/ui/public/js/bootstrap-3.3.1.min.js
[INFO] Exclude: **/src/ui/public/css/bootstrap-3.3.1.min.css
[INFO] Exclude: **/src/ui/public/js/dataTables.bootstrap.min.js
[INFO] Exclude: **/src/ui/public/css/dataTables.bootstrap.css
[INFO] Exclude: **/src/ui/public/js/jsonFormatter.min.js
[INFO] Exclude: **/src/ui/public/css/jsonFormatter.min.css
[INFO] Exclude: **/src/ui/public/js/jquery-1.11.1.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.cookies.2.2.0.min.js
[INFO] Exclude: **/src/ui/public/js/moment.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.blockUI.min.js
[INFO] Exclude: **/src/ui/public/js/url.min.js
[INFO] Exclude: **/src/ui/public/js/arbor.js
[INFO] Exclude: **/src/ui/public/js/arbor-graphics.js
[INFO] Exclude: **/src/ui/public/js/arbor-tween.js
[INFO] Exclude: **/src/ui/public/js/jquery.mustache.js
[INFO] Exclude: **/src/ui/public/js/typeahead.jquery.min.js
[INFO] Exclude: **/dependency-reduced-pom.xml
[INFO] Exclude: **/docs/**
[INFO] Exclude: **/.git/**
[INFO] Exclude: **/derby.log
[INFO] Exclude: **/src/dev/**
[INFO] Exclude: **/src/codegen/config.fmpp
[INFO] Exclude: **/src/codegen/data/Parser.tdd
[INFO] Exclude: **/src/test/resources/FixedAvroSerializer.config
[INFO] 14 resources included (use -debug for more details)
Warning:  org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
Compiler warnings:
  WARNING:  'org.apache.xerces.jaxp.SAXParserImpl: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.'
Warning:  org.apache.xerces.parsers.SAXParser: Feature 'http://javax.xml.XMLConstants/feature/secure-processing' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
[INFO] Rat check: Summary of files. Unapproved: 0 unknown: 0 generated: 0 approved: 14 licence.
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building storm-kafka-monitor 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ storm-kafka-monitor ---
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ storm-kafka-monitor ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/storm-kafka-monitor/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ storm-kafka-monitor ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ storm-kafka-monitor ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/storm-kafka-monitor/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ storm-kafka-monitor ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-surefire-plugin:2.18.1:test (default-test) @ storm-kafka-monitor ---
[INFO] 
[INFO] --- apache-rat-plugin:0.11:check (default) @ storm-kafka-monitor ---
[INFO] 51 implicit excludes (use -debug for more details).
[INFO] Exclude: **/target/**
[INFO] Exclude: **/*.iml
[INFO] Exclude: **/.idea/**
[INFO] Exclude: **/metastore_db/**
[INFO] Exclude: **/build/**
[INFO] Exclude: **/logs/**
[INFO] Exclude: **/CHANGELOG.md
[INFO] Exclude: **/README.md
[INFO] Exclude: **/README.markdown
[INFO] Exclude: **/DEVELOPER.md
[INFO] Exclude: **/BYLAWS.md
[INFO] Exclude: **/STORM-UI-REST-API.md
[INFO] Exclude: SECURITY.md
[INFO] Exclude: VERSION
[INFO] Exclude: TODO
[INFO] Exclude: **/src/py/**
[INFO] Exclude: **/src/ui/public/js/jquery.dataTables.1.10.4.min.js
[INFO] Exclude: **/src/ui/public/css/jquery.dataTables.1.10.4.min.css
[INFO] Exclude: **/src/ui/public/images/*
[INFO] Exclude: **/src/ui/public/js/bootstrap-3.3.1.min.js
[INFO] Exclude: **/src/ui/public/css/bootstrap-3.3.1.min.css
[INFO] Exclude: **/src/ui/public/js/dataTables.bootstrap.min.js
[INFO] Exclude: **/src/ui/public/css/dataTables.bootstrap.css
[INFO] Exclude: **/src/ui/public/js/jsonFormatter.min.js
[INFO] Exclude: **/src/ui/public/css/jsonFormatter.min.css
[INFO] Exclude: **/src/ui/public/js/jquery-1.11.1.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.cookies.2.2.0.min.js
[INFO] Exclude: **/src/ui/public/js/moment.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.blockUI.min.js
[INFO] Exclude: **/src/ui/public/js/url.min.js
[INFO] Exclude: **/src/ui/public/js/arbor.js
[INFO] Exclude: **/src/ui/public/js/arbor-graphics.js
[INFO] Exclude: **/src/ui/public/js/arbor-tween.js
[INFO] Exclude: **/src/ui/public/js/jquery.mustache.js
[INFO] Exclude: **/src/ui/public/js/typeahead.jquery.min.js
[INFO] Exclude: **/dependency-reduced-pom.xml
[INFO] Exclude: **/docs/**
[INFO] Exclude: **/.git/**
[INFO] Exclude: **/derby.log
[INFO] Exclude: **/src/dev/**
[INFO] Exclude: **/src/codegen/config.fmpp
[INFO] Exclude: **/src/codegen/data/Parser.tdd
[INFO] Exclude: **/src/test/resources/FixedAvroSerializer.config
[INFO] 5 resources included (use -debug for more details)
Warning:  org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
Compiler warnings:
  WARNING:  'org.apache.xerces.jaxp.SAXParserImpl: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.'
Warning:  org.apache.xerces.parsers.SAXParser: Feature 'http://javax.xml.XMLConstants/feature/secure-processing' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
[INFO] Rat check: Summary of files. Unapproved: 0 unknown: 0 generated: 0 approved: 5 licence.
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO] 
[INFO] Storm .............................................. SUCCESS [  5.772 s]
[INFO] multilang-javascript ............................... SUCCESS [  1.087 s]
[INFO] multilang-python ................................... SUCCESS [  0.177 s]
[INFO] multilang-ruby ..................................... SUCCESS [  0.101 s]
[INFO] maven-shade-clojure-transformer .................... SUCCESS [  1.338 s]
[INFO] storm-maven-plugins ................................ SUCCESS [  3.850 s]
[INFO] storm-rename-hack .................................. SUCCESS [  1.869 s]
[INFO] storm-kafka ........................................ FAILURE [01:02 min]
[INFO] storm-hdfs ......................................... FAILURE [01:15 min]
[INFO] storm-hbase ........................................ SUCCESS [  3.228 s]
[INFO] storm-hive ......................................... FAILURE [ 39.326 s]
[INFO] storm-jdbc ......................................... SUCCESS [  1.985 s]
[INFO] storm-redis ........................................ SUCCESS [  1.662 s]
[INFO] storm-eventhubs .................................... SUCCESS [  3.056 s]
[INFO] flux ............................................... SUCCESS [  0.120 s]
[INFO] flux-wrappers ...................................... SUCCESS [  0.294 s]
[INFO] flux-core .......................................... SKIPPED
[INFO] flux-examples ...................................... SKIPPED
[INFO] storm-sql-runtime .................................. SUCCESS [  0.363 s]
[INFO] storm-sql-core ..................................... SUCCESS [ 53.110 s]
[INFO] storm-sql-kafka .................................... SKIPPED
[INFO] sql ................................................ SUCCESS [  0.097 s]
[INFO] storm-elasticsearch ................................ SUCCESS [  5.096 s]
[INFO] storm-solr ......................................... SUCCESS [  2.322 s]
[INFO] storm-metrics ...................................... SUCCESS [  0.496 s]
[INFO] storm-cassandra .................................... SUCCESS [  0.319 s]
[INFO] storm-mqtt-parent .................................. SUCCESS [  0.057 s]
[INFO] storm-mqtt ......................................... SUCCESS [  1.289 s]
[INFO] storm-mqtt-examples ................................ SKIPPED
[INFO] storm-mongodb ...................................... SUCCESS [  0.109 s]
[INFO] storm-clojure ...................................... SUCCESS [  1.650 s]
[INFO] storm-starter ...................................... SKIPPED
[INFO] storm-kafka-client ................................. SUCCESS [  0.128 s]
[INFO] storm-opentsdb ..................................... SUCCESS [  0.910 s]
[INFO] storm-kafka-monitor ................................ SUCCESS [  0.576 s]
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 04:32 min
[INFO] Finished at: 2016-06-24T14:54:21+00:00
[INFO] Final Memory: 83M/390M
[INFO] ------------------------------------------------------------------------
[WARNING] The requested profile "native" could not be activated because it does not exist.
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:2.18.1:test (default-test) on project storm-kafka: There are test failures.
[ERROR] 
[ERROR] Please refer to /home/travis/build/apache/storm/external/storm-kafka/target/surefire-reports for the individual test results.
[ERROR] -> [Help 1]
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:2.18.1:test (default-test) on project storm-hdfs: There are test failures.
[ERROR] 
[ERROR] Please refer to /home/travis/build/apache/storm/external/storm-hdfs/target/surefire-reports for the individual test results.
[ERROR] -> [Help 1]
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:2.18.1:test (default-test) on project storm-hive: There are test failures.
[ERROR] 
[ERROR] Please refer to /home/travis/build/apache/storm/external/storm-hive/target/surefire-reports for the individual test results.
[ERROR] -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException
[ERROR] 
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn <goals> -rf :storm-kafka
Looking for errors in ./external/sql/storm-sql-core/target/surefire-reports
Checking ./external/sql/storm-sql-core/target/surefire-reports/TEST-org.apache.storm.sql.parser.TestSqlParser.xml
Checking ./external/sql/storm-sql-core/target/surefire-reports/TEST-org.apache.storm.sql.compiler.backends.trident.TestPlanCompiler.xml
Checking ./external/sql/storm-sql-core/target/surefire-reports/TEST-org.apache.storm.sql.compiler.backends.standalone.TestPlanCompiler.xml
Checking ./external/sql/storm-sql-core/target/surefire-reports/TEST-org.apache.storm.sql.compiler.backends.standalone.TestRelNodeCompiler.xml
Checking ./external/sql/storm-sql-core/target/surefire-reports/TEST-org.apache.storm.sql.compiler.TestExprCompiler.xml
Checking ./external/sql/storm-sql-core/target/surefire-reports/TEST-org.apache.storm.sql.compiler.TestExprSemantic.xml
Checking ./external/sql/storm-sql-core/target/surefire-reports/TEST-org.apache.storm.sql.TestStormSql.xml
Looking for errors in ./external/storm-elasticsearch/target/surefire-reports
Checking ./external/storm-elasticsearch/target/surefire-reports/TEST-org.apache.storm.elasticsearch.common.TransportAddressesTest.xml
Checking ./external/storm-elasticsearch/target/surefire-reports/TEST-org.apache.storm.elasticsearch.common.EsConfigTest.xml
Checking ./external/storm-elasticsearch/target/surefire-reports/TEST-org.apache.storm.elasticsearch.bolt.EsLookupBoltTest.xml
Checking ./external/storm-elasticsearch/target/surefire-reports/TEST-org.apache.storm.elasticsearch.trident.EsStateFactoryTest.xml
Looking for errors in ./external/storm-eventhubs/target/surefire-reports
Checking ./external/storm-eventhubs/target/surefire-reports/TEST-org.apache.storm.eventhubs.trident.TestTransactionalTridentEmitter.xml
Checking ./external/storm-eventhubs/target/surefire-reports/TEST-org.apache.storm.eventhubs.spout.TestPartitionManager.xml
Checking ./external/storm-eventhubs/target/surefire-reports/TEST-org.apache.storm.eventhubs.spout.TestEventData.xml
Checking ./external/storm-eventhubs/target/surefire-reports/TEST-org.apache.storm.eventhubs.spout.TestEventHubSpout.xml
Looking for errors in ./external/storm-hdfs/target/surefire-reports
Checking ./external/storm-hdfs/target/surefire-reports/TEST-org.apache.storm.hdfs.bolt.TestHdfsBolt.xml
--------------------------------------------------
classname: org.apache.storm.hdfs.bolt.TestHdfsBolt / testname: testTwoTuplesTwoFiles
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hdfs.bolt.TestHdfsBolt.generateTestTuple(TestHdfsBolt.java:243)
	at org.apache.storm.hdfs.bolt.TestHdfsBolt.<init>(TestHdfsBolt.java:69)

--------------------------------------------------
--------------------------------------------------
classname: org.apache.storm.hdfs.bolt.TestHdfsBolt / testname: testPartitionedOutput
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hdfs.bolt.TestHdfsBolt.generateTestTuple(TestHdfsBolt.java:243)
	at org.apache.storm.hdfs.bolt.TestHdfsBolt.<init>(TestHdfsBolt.java:69)

--------------------------------------------------
--------------------------------------------------
classname: org.apache.storm.hdfs.bolt.TestHdfsBolt / testname: testTwoTuplesOneFile
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hdfs.bolt.TestHdfsBolt.generateTestTuple(TestHdfsBolt.java:243)
	at org.apache.storm.hdfs.bolt.TestHdfsBolt.<init>(TestHdfsBolt.java:69)

--------------------------------------------------
--------------------------------------------------
classname: org.apache.storm.hdfs.bolt.TestHdfsBolt / testname: testFailedSync
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hdfs.bolt.TestHdfsBolt.generateTestTuple(TestHdfsBolt.java:243)
	at org.apache.storm.hdfs.bolt.TestHdfsBolt.<init>(TestHdfsBolt.java:69)

--------------------------------------------------
--------------------------------------------------
classname: org.apache.storm.hdfs.bolt.TestHdfsBolt / testname: testTickTuples
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hdfs.bolt.TestHdfsBolt.generateTestTuple(TestHdfsBolt.java:243)
	at org.apache.storm.hdfs.bolt.TestHdfsBolt.<init>(TestHdfsBolt.java:69)

--------------------------------------------------
--------------------------------------------------
classname: org.apache.storm.hdfs.bolt.TestHdfsBolt / testname: testFailureFilecount
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hdfs.bolt.TestHdfsBolt.generateTestTuple(TestHdfsBolt.java:243)
	at org.apache.storm.hdfs.bolt.TestHdfsBolt.<init>(TestHdfsBolt.java:69)

--------------------------------------------------
Checking ./external/storm-hdfs/target/surefire-reports/TEST-org.apache.storm.hdfs.bolt.TestSequenceFileBolt.xml
--------------------------------------------------
classname: org.apache.storm.hdfs.bolt.TestSequenceFileBolt / testname: testTwoTuplesTwoFiles
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hdfs.bolt.TestSequenceFileBolt.generateTestTuple(TestSequenceFileBolt.java:161)
	at org.apache.storm.hdfs.bolt.TestSequenceFileBolt.<init>(TestSequenceFileBolt.java:68)

--------------------------------------------------
--------------------------------------------------
classname: org.apache.storm.hdfs.bolt.TestSequenceFileBolt / testname: testTwoTuplesOneFile
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hdfs.bolt.TestSequenceFileBolt.generateTestTuple(TestSequenceFileBolt.java:161)
	at org.apache.storm.hdfs.bolt.TestSequenceFileBolt.<init>(TestSequenceFileBolt.java:68)

--------------------------------------------------
--------------------------------------------------
classname: org.apache.storm.hdfs.bolt.TestSequenceFileBolt / testname: testFailedSync
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hdfs.bolt.TestSequenceFileBolt.generateTestTuple(TestSequenceFileBolt.java:161)
	at org.apache.storm.hdfs.bolt.TestSequenceFileBolt.<init>(TestSequenceFileBolt.java:68)

--------------------------------------------------
Checking ./external/storm-hdfs/target/surefire-reports/TEST-org.apache.storm.hdfs.bolt.AvroGenericRecordBoltTest.xml
--------------------------------------------------
classname: org.apache.storm.hdfs.bolt.AvroGenericRecordBoltTest / testname: schemaThrashing
java.lang.ExceptionInInitializerError: null
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hdfs.bolt.AvroGenericRecordBoltTest.generateTestTuple(AvroGenericRecordBoltTest.java:220)
	at org.apache.storm.hdfs.bolt.AvroGenericRecordBoltTest.<clinit>(AvroGenericRecordBoltTest.java:95)

--------------------------------------------------
--------------------------------------------------
classname: org.apache.storm.hdfs.bolt.AvroGenericRecordBoltTest / testname: forwardSchemaChangeWorks
java.lang.NoClassDefFoundError: Could not initialize class org.apache.storm.hdfs.bolt.AvroGenericRecordBoltTest
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.junit.runners.BlockJUnit4ClassRunner.createTest(BlockJUnit4ClassRunner.java:195)
	at org.junit.runners.BlockJUnit4ClassRunner$1.runReflectiveCall(BlockJUnit4ClassRunner.java:244)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.BlockJUnit4ClassRunner.methodBlock(BlockJUnit4ClassRunner.java:241)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:108)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:78)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:54)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:144)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:203)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:155)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:103)

--------------------------------------------------
--------------------------------------------------
classname: org.apache.storm.hdfs.bolt.AvroGenericRecordBoltTest / testname: multipleTuplesOneFile
java.lang.NoClassDefFoundError: Could not initialize class org.apache.storm.hdfs.bolt.AvroGenericRecordBoltTest
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.junit.runners.BlockJUnit4ClassRunner.createTest(BlockJUnit4ClassRunner.java:195)
	at org.junit.runners.BlockJUnit4ClassRunner$1.runReflectiveCall(BlockJUnit4ClassRunner.java:244)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.BlockJUnit4ClassRunner.methodBlock(BlockJUnit4ClassRunner.java:241)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:108)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:78)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:54)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:144)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:203)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:155)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:103)

--------------------------------------------------
--------------------------------------------------
classname: org.apache.storm.hdfs.bolt.AvroGenericRecordBoltTest / testname: multipleTuplesMutliplesFiles
java.lang.NoClassDefFoundError: Could not initialize class org.apache.storm.hdfs.bolt.AvroGenericRecordBoltTest
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.junit.runners.BlockJUnit4ClassRunner.createTest(BlockJUnit4ClassRunner.java:195)
	at org.junit.runners.BlockJUnit4ClassRunner$1.runReflectiveCall(BlockJUnit4ClassRunner.java:244)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.BlockJUnit4ClassRunner.methodBlock(BlockJUnit4ClassRunner.java:241)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:108)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:78)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:54)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:144)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:203)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:155)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:103)

--------------------------------------------------
--------------------------------------------------
classname: org.apache.storm.hdfs.bolt.AvroGenericRecordBoltTest / testname: backwardSchemaChangeWorks
java.lang.NoClassDefFoundError: Could not initialize class org.apache.storm.hdfs.bolt.AvroGenericRecordBoltTest
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.junit.runners.BlockJUnit4ClassRunner.createTest(BlockJUnit4ClassRunner.java:195)
	at org.junit.runners.BlockJUnit4ClassRunner$1.runReflectiveCall(BlockJUnit4ClassRunner.java:244)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.BlockJUnit4ClassRunner.methodBlock(BlockJUnit4ClassRunner.java:241)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:108)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:78)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:54)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:144)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:203)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:155)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:103)

--------------------------------------------------
Checking ./external/storm-hdfs/target/surefire-reports/TEST-org.apache.storm.hdfs.bolt.TestWritersMap.xml
Checking ./external/storm-hdfs/target/surefire-reports/TEST-org.apache.storm.hdfs.bolt.format.TestSimpleFileNameFormat.xml
Checking ./external/storm-hdfs/target/surefire-reports/TEST-org.apache.storm.hdfs.spout.TestDirLock.xml
Checking ./external/storm-hdfs/target/surefire-reports/TEST-org.apache.storm.hdfs.spout.TestHdfsSemantics.xml
Checking ./external/storm-hdfs/target/surefire-reports/TEST-org.apache.storm.hdfs.spout.TestHdfsSpout.xml
Checking ./external/storm-hdfs/target/surefire-reports/TEST-org.apache.storm.hdfs.spout.TestProgressTracker.xml
Checking ./external/storm-hdfs/target/surefire-reports/TEST-org.apache.storm.hdfs.spout.TestFileLock.xml
Checking ./external/storm-hdfs/target/surefire-reports/TEST-org.apache.storm.hdfs.trident.format.TestSimpleFileNameFormat.xml
Checking ./external/storm-hdfs/target/surefire-reports/TEST-org.apache.storm.hdfs.trident.HdfsStateTest.xml
Checking ./external/storm-hdfs/target/surefire-reports/TEST-org.apache.storm.hdfs.blobstore.BlobStoreTest.xml
Checking ./external/storm-hdfs/target/surefire-reports/TEST-org.apache.storm.hdfs.blobstore.HdfsBlobStoreImplTest.xml
Checking ./external/storm-hdfs/target/surefire-reports/TEST-org.apache.storm.hdfs.avro.TestFixedAvroSerializer.xml
Checking ./external/storm-hdfs/target/surefire-reports/TEST-org.apache.storm.hdfs.avro.TestGenericAvroSerializer.xml
Looking for errors in ./external/storm-hive/target/surefire-reports
Checking ./external/storm-hive/target/surefire-reports/TEST-org.apache.storm.hive.bolt.TestHiveBolt.xml
--------------------------------------------------
classname: org.apache.storm.hive.bolt.TestHiveBolt / testname: testMultiPartitionTuples
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hive.bolt.TestHiveBolt.generateTestTuple(TestHiveBolt.java:447)
	at org.apache.storm.hive.bolt.TestHiveBolt.testMultiPartitionTuples(TestHiveBolt.java:411)

-------------------- system-out --------------------
4726 [main] WARN  o.a.h.u.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
4834 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
4859 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
5217 [main] INFO  D.Persistence - Property datanucleus.cache.level2 unknown - will be ignored
5217 [main] INFO  D.Persistence - Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
6237 [main] INFO  o.a.h.h.m.ObjectStore - Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
6327 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
7419 [main] INFO  D.Datastore - The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
7420 [main] INFO  D.Datastore - The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
9431 [main] INFO  D.Datastore - The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
9431 [main] INFO  D.Datastore - The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
10037 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
10174 [main] WARN  o.a.h.h.m.ObjectStore - Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 0.14.0
10286 [main] WARN  o.a.h.h.m.ObjectStore - Failed to get database default, returning NoSuchObjectException
10774 [main] INFO  o.a.h.h.m.HiveMetaStore - Added admin role in metastore
10776 [main] INFO  o.a.h.h.m.HiveMetaStore - Added public role in metastore
10872 [main] INFO  o.a.h.h.m.HiveMetaStore - No user is added in admin role, since config is empty
11073 [main] INFO  o.a.h.h.q.s.SessionState - Created HDFS directory: /tmp/hive/travis
11076 [main] INFO  o.a.h.h.q.s.SessionState - Created local directory: /tmp/travis
11078 [main] INFO  o.a.h.h.q.s.SessionState - Created local directory: /tmp/83842e28-21a9-47a6-9ae9-40004adf9b69_resources
11081 [main] INFO  o.a.h.h.q.s.SessionState - Created HDFS directory: /tmp/hive/travis/83842e28-21a9-47a6-9ae9-40004adf9b69
11091 [main] INFO  o.a.h.h.q.s.SessionState - Created local directory: /tmp/travis/83842e28-21a9-47a6-9ae9-40004adf9b69
11100 [main] INFO  o.a.h.h.q.s.SessionState - Created HDFS directory: /tmp/hive/travis/83842e28-21a9-47a6-9ae9-40004adf9b69/_tmp_space.db
11101 [main] INFO  o.a.h.h.q.s.SessionState - No Tez session required at this point. hive.execution.engine=mr.
11231 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
11232 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
11240 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
11241 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
11242 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_table_names_by_filter: db = testdb, filter = 
11242 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table_names_by_filter: db = testdb, filter = 	
11274 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_database: testdb
11275 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_database: testdb	
11276 [main] WARN  o.a.h.h.m.ObjectStore - Failed to get database testdb, returning NoSuchObjectException
11279 [main] ERROR o.a.h.h.m.RetryingHMSHandler - NoSuchObjectException(message:testdb)
	at org.apache.hadoop.hive.metastore.ObjectStore.getDatabase(ObjectStore.java:539)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:98)
	at com.sun.proxy.$Proxy23.getDatabase(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_database_core(HiveMetaStore.java:914)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_database(HiveMetaStore.java:888)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:102)
	at com.sun.proxy.$Proxy25.get_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getDatabase(HiveMetaStoreClient.java:1043)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:657)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:644)
	at org.apache.storm.hive.bolt.HiveSetupUtil.dropDB(HiveSetupUtil.java:166)
	at org.apache.storm.hive.bolt.TestHiveBolt.setup(TestHiveBolt.java:119)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:108)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:78)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:54)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:144)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:203)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:155)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:103)

11280 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Shutting down the object store...
11280 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=Shutting down the object store...	
11280 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Metastore shutdown complete.
11280 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=Metastore shutdown complete.	
11281 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: create_database: Database(name:testdb, description:null, locationUri:raw:///tmp/junit7329884202221791440/testdb.db, parameters:null)
11282 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=create_database: Database(name:testdb, description:null, locationUri:raw:///tmp/junit7329884202221791440/testdb.db, parameters:null)	
11282 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
11284 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
11285 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
11288 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
11289 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
11291 [main] WARN  o.a.h.h.m.ObjectStore - Failed to get database testdb, returning NoSuchObjectException
11313 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: create_table: Table(tableName:test_table, dbName:testdb, owner:null, createTime:0, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:), FieldSchema(name:msg, type:string, comment:)], location:raw:///tmp/junit7329884202221791440/testdb.db/test_table, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:1, serdeInfo:SerDeInfo(name:test_table, serializationLib:org.apache.hadoop.hive.ql.io.orc.OrcSerde, parameters:{serialization.format=1}), bucketCols:[], sortCols:null, parameters:null), partitionKeys:[FieldSchema(name:city, type:string, comment:), FieldSchema(name:state, type:string, comment:)], parameters:{}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)
11313 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=create_table: Table(tableName:test_table, dbName:testdb, owner:null, createTime:0, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:), FieldSchema(name:msg, type:string, comment:)], location:raw:///tmp/junit7329884202221791440/testdb.db/test_table, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:1, serdeInfo:SerDeInfo(name:test_table, serializationLib:org.apache.hadoop.hive.ql.io.orc.OrcSerde, parameters:{serialization.format=1}), bucketCols:[], sortCols:null, parameters:null), partitionKeys:[FieldSchema(name:city, type:string, comment:), FieldSchema(name:state, type:string, comment:)], parameters:{}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
11351 [main] WARN  o.a.h.h.m.HiveMetaStore - Location: raw:///tmp/junit7329884202221791440/testdb.db/test_table specified for non-external table:test_table
11355 [main] INFO  o.a.h.h.c.FileUtils - Creating directory if it doesn't exist: raw:/tmp/junit7329884202221791440/testdb.db/test_table
11579 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: add_partition : db=testdb tbl=test_table
11580 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=add_partition : db=testdb tbl=test_table	
11770 [main] INFO  o.a.h.h.c.FileUtils - Creating directory if it doesn't exist: raw:/tmp/junit7329884202221791440/testdb.db/test_table/city=sunnyvale/state=ca
11838 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Shutting down the object store...
11838 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=Shutting down the object store...	
11838 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Metastore shutdown complete.
11839 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=Metastore shutdown complete.	
done
11845 [main] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=compile from=org.apache.hadoop.hive.ql.Driver>
11871 [main] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=parse from=org.apache.hadoop.hive.ql.Driver>
11874 [main] INFO  h.q.p.ParseDriver - Parsing command: select * from testdb.test_table
12278 [main] INFO  h.q.p.ParseDriver - Parse Completed
12280 [main] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=parse start=1466779969332 end=1466779969741 duration=409 from=org.apache.hadoop.hive.ql.Driver>
12323 [main] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=semanticAnalyze from=org.apache.hadoop.hive.ql.Driver>
12377 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Starting Semantic Analysis
12378 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Completed phase 1 of Semantic Analysis
12378 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Get metadata for source tables
12378 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_table : db=testdb tbl=test_table
12379 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table : db=testdb tbl=test_table	
12379 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
12383 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
12384 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
12387 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
12388 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
12422 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Get metadata for subqueries
12430 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Get metadata for destination tables
12444 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Completed getting MetaData in Semantic Analysis
12561 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Set stats collection dir : file:/tmp/travis/83842e28-21a9-47a6-9ae9-40004adf9b69/hive_2016-06-24_14-52-49_331_2696147984789613743-1/-ext-10002
12656 [main] INFO  o.a.h.h.q.p.OpProcFactory - Processing for FS(2)
12656 [main] INFO  o.a.h.h.q.p.OpProcFactory - Processing for SEL(1)
12656 [main] INFO  o.a.h.h.q.p.OpProcFactory - Processing for TS(0)
12681 [main] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=partition-retrieving from=org.apache.hadoop.hive.ql.optimizer.ppr.PartitionPruner>
12681 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_partitions : db=testdb tbl=test_table
12682 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_partitions : db=testdb tbl=test_table	
12884 [main] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=partition-retrieving start=1466779970142 end=1466779970345 duration=203 from=org.apache.hadoop.hive.ql.optimizer.ppr.PartitionPruner>
12920 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Completed plan generation
12920 [main] INFO  o.a.h.h.q.Driver - Semantic Analysis Completed
12920 [main] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=semanticAnalyze start=1466779969784 end=1466779970381 duration=597 from=org.apache.hadoop.hive.ql.Driver>
12941 [main] INFO  o.a.h.h.q.e.TableScanOperator - Initializing Self TS[0]
12941 [main] INFO  o.a.h.h.q.e.TableScanOperator - Operator 0 TS initialized
12941 [main] INFO  o.a.h.h.q.e.TableScanOperator - Initializing children of 0 TS
12941 [main] INFO  o.a.h.h.q.e.SelectOperator - Initializing child 1 SEL
12941 [main] INFO  o.a.h.h.q.e.SelectOperator - Initializing Self SEL[1]
12945 [main] INFO  o.a.h.h.q.e.SelectOperator - SELECT struct<id:int,msg:string,city:string,state:string>
12945 [main] INFO  o.a.h.h.q.e.SelectOperator - Operator 1 SEL initialized
12945 [main] INFO  o.a.h.h.q.e.SelectOperator - Initializing children of 1 SEL
12945 [main] INFO  o.a.h.h.q.e.ListSinkOperator - Initializing child 3 OP
12946 [main] INFO  o.a.h.h.q.e.ListSinkOperator - Initializing Self OP[3]
12947 [main] INFO  o.a.h.h.q.e.ListSinkOperator - Operator 3 OP initialized
12948 [main] INFO  o.a.h.h.q.e.ListSinkOperator - Initialization Done 3 OP
12948 [main] INFO  o.a.h.h.q.e.SelectOperator - Initialization Done 1 SEL
12948 [main] INFO  o.a.h.h.q.e.TableScanOperator - Initialization Done 0 TS
12951 [main] INFO  o.a.h.h.q.Driver - Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:test_table.id, type:int, comment:null), FieldSchema(name:test_table.msg, type:string, comment:null), FieldSchema(name:test_table.city, type:string, comment:null), FieldSchema(name:test_table.state, type:string, comment:null)], properties:null)
12951 [main] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=compile start=1466779969306 end=1466779970412 duration=1106 from=org.apache.hadoop.hive.ql.Driver>

-------------------- system-err --------------------
Unable to drop index HL_TXNID_INDEX Index 'HL_TXNID_INDEX' does not exist.
Unable to drop table TXN_COMPONENTS: 'DROP TABLE' cannot be performed on 'TXN_COMPONENTS' because it does not exist.
Unable to drop table COMPLETED_TXN_COMPONENTS: 'DROP TABLE' cannot be performed on 'COMPLETED_TXN_COMPONENTS' because it does not exist.
Unable to drop table TXNS: 'DROP TABLE' cannot be performed on 'TXNS' because it does not exist.
Unable to drop table NEXT_TXN_ID: 'DROP TABLE' cannot be performed on 'NEXT_TXN_ID' because it does not exist.
Unable to drop table HIVE_LOCKS: 'DROP TABLE' cannot be performed on 'HIVE_LOCKS' because it does not exist.
Unable to drop table NEXT_LOCK_ID: 'DROP TABLE' cannot be performed on 'NEXT_LOCK_ID' because it does not exist.
Unable to drop table COMPACTION_QUEUE: 'DROP TABLE' cannot be performed on 'COMPACTION_QUEUE' because it does not exist.
Unable to drop table NEXT_COMPACTION_QUEUE_ID: 'DROP TABLE' cannot be performed on 'NEXT_COMPACTION_QUEUE_ID' because it does not exist.

--------------------------------------------------
--------------------------------------------------
classname: org.apache.storm.hive.bolt.TestHiveBolt / testname: testNoAcksUntilFlushed
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hive.bolt.TestHiveBolt.generateTestTuple(TestHiveBolt.java:447)
	at org.apache.storm.hive.bolt.TestHiveBolt.testNoAcksUntilFlushed(TestHiveBolt.java:297)

-------------------- system-out --------------------
13396 [main] INFO  o.a.h.h.q.s.SessionState - Created local directory: /tmp/30feaa54-9c23-4ee0-9ecf-328612dcbf98_resources
13398 [main] INFO  o.a.h.h.q.s.SessionState - Created HDFS directory: /tmp/hive/travis/30feaa54-9c23-4ee0-9ecf-328612dcbf98
13409 [main] INFO  o.a.h.h.q.s.SessionState - Created local directory: /tmp/travis/30feaa54-9c23-4ee0-9ecf-328612dcbf98
13412 [main] INFO  o.a.h.h.q.s.SessionState - Created HDFS directory: /tmp/hive/travis/30feaa54-9c23-4ee0-9ecf-328612dcbf98/_tmp_space.db
13412 [main] INFO  o.a.h.h.q.s.SessionState - No Tez session required at this point. hive.execution.engine=mr.
13415 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
13416 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
13418 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
13418 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
13418 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_table_names_by_filter: db = testdb, filter = 
13419 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table_names_by_filter: db = testdb, filter = 	
13422 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_table : db=testdb tbl=test_table
13422 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table : db=testdb tbl=test_table	
13440 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: drop_table : db=testdb tbl=test_table
13440 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=drop_table : db=testdb tbl=test_table	
13456 [main] INFO  D.Datastore - The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
13456 [main] INFO  D.Datastore - The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
13875 [main] INFO  D.Datastore - The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
13876 [main] INFO  D.Datastore - The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
14069 [main] INFO  D.Datastore - The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
14069 [main] INFO  D.Datastore - The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
14524 [main] INFO  D.Datastore - The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
14524 [main] INFO  D.Datastore - The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
14636 [main] INFO  D.Datastore - The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
14636 [main] INFO  D.Datastore - The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
14932 [main] INFO  h.m.hivemetastoressimpl - deleting  raw:/tmp/junit7329884202221791440/testdb.db/test_table
14933 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_database: testdb
14934 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_database: testdb	
14943 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: drop_database: testdb
14944 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=drop_database: testdb	
14945 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_all_tables: db=testdb
14945 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_all_tables: db=testdb	
14958 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_functions: db=testdb pat=*
14958 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_functions: db=testdb pat=*	
14960 [main] INFO  D.Datastore - The class "org.apache.hadoop.hive.metastore.model.MResourceUri" is tagged as "embedded-only" so does not have its own datastore table.
15073 [main] INFO  o.a.h.h.m.ObjectStore - Dropping database testdb along with all tables
15161 [main] INFO  h.m.hivemetastoressimpl - deleting  raw:/tmp/junit7329884202221791440/testdb.db
15163 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
15163 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
15171 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
15171 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
15172 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: create_database: Database(name:testdb, description:null, locationUri:raw:///tmp/junit5683189384854533606/testdb.db, parameters:null)
15172 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=create_database: Database(name:testdb, description:null, locationUri:raw:///tmp/junit5683189384854533606/testdb.db, parameters:null)	
15176 [main] WARN  o.a.h.h.m.ObjectStore - Failed to get database testdb, returning NoSuchObjectException
15184 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: create_table: Table(tableName:test_table, dbName:testdb, owner:null, createTime:0, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:), FieldSchema(name:msg, type:string, comment:)], location:raw:///tmp/junit5683189384854533606/testdb.db/test_table, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:1, serdeInfo:SerDeInfo(name:test_table, serializationLib:org.apache.hadoop.hive.ql.io.orc.OrcSerde, parameters:{serialization.format=1}), bucketCols:[], sortCols:null, parameters:null), partitionKeys:[FieldSchema(name:city, type:string, comment:), FieldSchema(name:state, type:string, comment:)], parameters:{}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)
15184 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=create_table: Table(tableName:test_table, dbName:testdb, owner:null, createTime:0, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:), FieldSchema(name:msg, type:string, comment:)], location:raw:///tmp/junit5683189384854533606/testdb.db/test_table, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:1, serdeInfo:SerDeInfo(name:test_table, serializationLib:org.apache.hadoop.hive.ql.io.orc.OrcSerde, parameters:{serialization.format=1}), bucketCols:[], sortCols:null, parameters:null), partitionKeys:[FieldSchema(name:city, type:string, comment:), FieldSchema(name:state, type:string, comment:)], parameters:{}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
15193 [main] WARN  o.a.h.h.m.HiveMetaStore - Location: raw:///tmp/junit5683189384854533606/testdb.db/test_table specified for non-external table:test_table
15194 [main] INFO  o.a.h.h.c.FileUtils - Creating directory if it doesn't exist: raw:/tmp/junit5683189384854533606/testdb.db/test_table
15279 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: add_partition : db=testdb tbl=test_table
15280 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=add_partition : db=testdb tbl=test_table	
15393 [main] INFO  o.a.h.h.c.FileUtils - Creating directory if it doesn't exist: raw:/tmp/junit5683189384854533606/testdb.db/test_table/city=sunnyvale/state=ca
15430 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Shutting down the object store...
15431 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=Shutting down the object store...	
15431 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Metastore shutdown complete.
15431 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=Metastore shutdown complete.	
done

--------------------------------------------------
--------------------------------------------------
classname: org.apache.storm.hive.bolt.TestHiveBolt / testname: testData
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hive.bolt.TestHiveBolt.generateTestTuple(TestHiveBolt.java:447)
	at org.apache.storm.hive.bolt.TestHiveBolt.testData(TestHiveBolt.java:251)

-------------------- system-out --------------------
15952 [main] INFO  o.a.h.h.q.s.SessionState - Created local directory: /tmp/d5deebe2-a473-41fc-ade0-1e19b3b78787_resources
15955 [main] INFO  o.a.h.h.q.s.SessionState - Created HDFS directory: /tmp/hive/travis/d5deebe2-a473-41fc-ade0-1e19b3b78787
15958 [main] INFO  o.a.h.h.q.s.SessionState - Created local directory: /tmp/travis/d5deebe2-a473-41fc-ade0-1e19b3b78787
15961 [main] INFO  o.a.h.h.q.s.SessionState - Created HDFS directory: /tmp/hive/travis/d5deebe2-a473-41fc-ade0-1e19b3b78787/_tmp_space.db
15961 [main] INFO  o.a.h.h.q.s.SessionState - No Tez session required at this point. hive.execution.engine=mr.
15963 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_table_names_by_filter: db = testdb, filter = 
15963 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table_names_by_filter: db = testdb, filter = 	
15963 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
15964 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
15965 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
15987 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
15987 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
15999 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_table : db=testdb tbl=test_table
16000 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table : db=testdb tbl=test_table	
16016 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: drop_table : db=testdb tbl=test_table
16016 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=drop_table : db=testdb tbl=test_table	
16612 [main] INFO  h.m.hivemetastoressimpl - deleting  raw:/tmp/junit5683189384854533606/testdb.db/test_table
16613 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_database: testdb
16613 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_database: testdb	
16625 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: drop_database: testdb
16625 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=drop_database: testdb	
16627 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_all_tables: db=testdb
16627 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_all_tables: db=testdb	
16635 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_functions: db=testdb pat=*
16635 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_functions: db=testdb pat=*	
16645 [main] INFO  o.a.h.h.m.ObjectStore - Dropping database testdb along with all tables
16670 [main] INFO  h.m.hivemetastoressimpl - deleting  raw:/tmp/junit5683189384854533606/testdb.db
16671 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
16672 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
16675 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
16675 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
16676 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: create_database: Database(name:testdb, description:null, locationUri:raw:///tmp/junit26797544351202148/testdb.db, parameters:null)
16676 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=create_database: Database(name:testdb, description:null, locationUri:raw:///tmp/junit26797544351202148/testdb.db, parameters:null)	
16677 [main] WARN  o.a.h.h.m.ObjectStore - Failed to get database testdb, returning NoSuchObjectException
16685 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: create_table: Table(tableName:test_table, dbName:testdb, owner:null, createTime:0, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:), FieldSchema(name:msg, type:string, comment:)], location:raw:///tmp/junit26797544351202148/testdb.db/test_table, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:1, serdeInfo:SerDeInfo(name:test_table, serializationLib:org.apache.hadoop.hive.ql.io.orc.OrcSerde, parameters:{serialization.format=1}), bucketCols:[], sortCols:null, parameters:null), partitionKeys:[FieldSchema(name:city, type:string, comment:), FieldSchema(name:state, type:string, comment:)], parameters:{}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)
16685 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=create_table: Table(tableName:test_table, dbName:testdb, owner:null, createTime:0, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:), FieldSchema(name:msg, type:string, comment:)], location:raw:///tmp/junit26797544351202148/testdb.db/test_table, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:1, serdeInfo:SerDeInfo(name:test_table, serializationLib:org.apache.hadoop.hive.ql.io.orc.OrcSerde, parameters:{serialization.format=1}), bucketCols:[], sortCols:null, parameters:null), partitionKeys:[FieldSchema(name:city, type:string, comment:), FieldSchema(name:state, type:string, comment:)], parameters:{}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
16688 [main] WARN  o.a.h.h.m.HiveMetaStore - Location: raw:///tmp/junit26797544351202148/testdb.db/test_table specified for non-external table:test_table
16688 [main] INFO  o.a.h.h.c.FileUtils - Creating directory if it doesn't exist: raw:/tmp/junit26797544351202148/testdb.db/test_table
16761 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: add_partition : db=testdb tbl=test_table
16762 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=add_partition : db=testdb tbl=test_table	
16848 [main] INFO  o.a.h.h.c.FileUtils - Creating directory if it doesn't exist: raw:/tmp/junit26797544351202148/testdb.db/test_table/city=sunnyvale/state=ca
16891 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Shutting down the object store...
16891 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=Shutting down the object store...	
16891 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Metastore shutdown complete.
16891 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=Metastore shutdown complete.	
done

--------------------------------------------------
--------------------------------------------------
classname: org.apache.storm.hive.bolt.TestHiveBolt / testname: testWithoutPartitions
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hive.bolt.TestHiveBolt.generateTestTuple(TestHiveBolt.java:447)
	at org.apache.storm.hive.bolt.TestHiveBolt.testWithoutPartitions(TestHiveBolt.java:194)

-------------------- system-out --------------------
17398 [main] INFO  o.a.h.h.q.s.SessionState - Created local directory: /tmp/f5fbff48-6ca5-48fd-b58c-cd90e612fa25_resources
17400 [main] INFO  o.a.h.h.q.s.SessionState - Created HDFS directory: /tmp/hive/travis/f5fbff48-6ca5-48fd-b58c-cd90e612fa25
17403 [main] INFO  o.a.h.h.q.s.SessionState - Created local directory: /tmp/travis/f5fbff48-6ca5-48fd-b58c-cd90e612fa25
17413 [main] INFO  o.a.h.h.q.s.SessionState - Created HDFS directory: /tmp/hive/travis/f5fbff48-6ca5-48fd-b58c-cd90e612fa25/_tmp_space.db
17413 [main] INFO  o.a.h.h.q.s.SessionState - No Tez session required at this point. hive.execution.engine=mr.
17416 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_table_names_by_filter: db = testdb, filter = 
17417 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table_names_by_filter: db = testdb, filter = 	
17417 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
17418 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
17419 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
17422 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
17423 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
17437 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_table : db=testdb tbl=test_table
17437 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table : db=testdb tbl=test_table	
17457 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: drop_table : db=testdb tbl=test_table
17457 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=drop_table : db=testdb tbl=test_table	
17996 [main] INFO  h.m.hivemetastoressimpl - deleting  raw:/tmp/junit26797544351202148/testdb.db/test_table
17996 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_database: testdb
17996 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_database: testdb	
18008 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: drop_database: testdb
18008 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=drop_database: testdb	
18010 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_all_tables: db=testdb
18010 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_all_tables: db=testdb	
18017 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_functions: db=testdb pat=*
18018 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_functions: db=testdb pat=*	
18026 [main] INFO  o.a.h.h.m.ObjectStore - Dropping database testdb along with all tables
18082 [main] INFO  h.m.hivemetastoressimpl - deleting  raw:/tmp/junit26797544351202148/testdb.db
18084 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
18085 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
18088 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
18088 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
18089 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: create_database: Database(name:testdb, description:null, locationUri:raw:///tmp/junit8029605577550535343/testdb.db, parameters:null)
18089 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=create_database: Database(name:testdb, description:null, locationUri:raw:///tmp/junit8029605577550535343/testdb.db, parameters:null)	
18090 [main] WARN  o.a.h.h.m.ObjectStore - Failed to get database testdb, returning NoSuchObjectException
18110 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: create_table: Table(tableName:test_table, dbName:testdb, owner:null, createTime:0, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:), FieldSchema(name:msg, type:string, comment:)], location:raw:///tmp/junit8029605577550535343/testdb.db/test_table, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:1, serdeInfo:SerDeInfo(name:test_table, serializationLib:org.apache.hadoop.hive.ql.io.orc.OrcSerde, parameters:{serialization.format=1}), bucketCols:[], sortCols:null, parameters:null), partitionKeys:[FieldSchema(name:city, type:string, comment:), FieldSchema(name:state, type:string, comment:)], parameters:{}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)
18111 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=create_table: Table(tableName:test_table, dbName:testdb, owner:null, createTime:0, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:), FieldSchema(name:msg, type:string, comment:)], location:raw:///tmp/junit8029605577550535343/testdb.db/test_table, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:1, serdeInfo:SerDeInfo(name:test_table, serializationLib:org.apache.hadoop.hive.ql.io.orc.OrcSerde, parameters:{serialization.format=1}), bucketCols:[], sortCols:null, parameters:null), partitionKeys:[FieldSchema(name:city, type:string, comment:), FieldSchema(name:state, type:string, comment:)], parameters:{}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
18113 [main] WARN  o.a.h.h.m.HiveMetaStore - Location: raw:///tmp/junit8029605577550535343/testdb.db/test_table specified for non-external table:test_table
18114 [main] INFO  o.a.h.h.c.FileUtils - Creating directory if it doesn't exist: raw:/tmp/junit8029605577550535343/testdb.db/test_table
18245 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: add_partition : db=testdb tbl=test_table
18245 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=add_partition : db=testdb tbl=test_table	
18315 [main] INFO  o.a.h.h.c.FileUtils - Creating directory if it doesn't exist: raw:/tmp/junit8029605577550535343/testdb.db/test_table/city=sunnyvale/state=ca
18343 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Shutting down the object store...
18344 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=Shutting down the object store...	
18344 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Metastore shutdown complete.
18344 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=Metastore shutdown complete.	
done
18345 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_table_names_by_filter: db = testdb1, filter = 
18345 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table_names_by_filter: db = testdb1, filter = 	
18345 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
18347 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
18348 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
18350 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
18351 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
18354 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_database: testdb1
18355 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_database: testdb1	
18356 [main] WARN  o.a.h.h.m.ObjectStore - Failed to get database testdb1, returning NoSuchObjectException
18356 [main] ERROR o.a.h.h.m.RetryingHMSHandler - NoSuchObjectException(message:testdb1)
	at org.apache.hadoop.hive.metastore.ObjectStore.getDatabase(ObjectStore.java:539)
	at sun.reflect.GeneratedMethodAccessor10.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:98)
	at com.sun.proxy.$Proxy23.getDatabase(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_database_core(HiveMetaStore.java:914)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_database(HiveMetaStore.java:888)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:102)
	at com.sun.proxy.$Proxy25.get_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getDatabase(HiveMetaStoreClient.java:1043)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:657)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:644)
	at org.apache.storm.hive.bolt.HiveSetupUtil.dropDB(HiveSetupUtil.java:166)
	at org.apache.storm.hive.bolt.TestHiveBolt.testWithoutPartitions(TestHiveBolt.java:175)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:108)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:78)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:54)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:144)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:203)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:155)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:103)

18357 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Shutting down the object store...
18357 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=Shutting down the object store...	
18357 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Metastore shutdown complete.
18357 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=Metastore shutdown complete.	
18358 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: create_database: Database(name:testdb1, description:null, locationUri:raw:///tmp/junit8029605577550535343/testdb.db, parameters:null)
18358 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=create_database: Database(name:testdb1, description:null, locationUri:raw:///tmp/junit8029605577550535343/testdb.db, parameters:null)	
18358 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
18360 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
18361 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
18363 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
18363 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
18366 [main] WARN  o.a.h.h.m.ObjectStore - Failed to get database testdb1, returning NoSuchObjectException
18380 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: create_table: Table(tableName:test_table1, dbName:testdb1, owner:null, createTime:0, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:), FieldSchema(name:msg, type:string, comment:)], location:raw:///tmp/junit8029605577550535343/testdb.db/test_table1, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:1, serdeInfo:SerDeInfo(name:test_table1, serializationLib:org.apache.hadoop.hive.ql.io.orc.OrcSerde, parameters:{serialization.format=1}), bucketCols:[], sortCols:null, parameters:null), partitionKeys:null, parameters:{}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)
18380 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=create_table: Table(tableName:test_table1, dbName:testdb1, owner:null, createTime:0, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:), FieldSchema(name:msg, type:string, comment:)], location:raw:///tmp/junit8029605577550535343/testdb.db/test_table1, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:1, serdeInfo:SerDeInfo(name:test_table1, serializationLib:org.apache.hadoop.hive.ql.io.orc.OrcSerde, parameters:{serialization.format=1}), bucketCols:[], sortCols:null, parameters:null), partitionKeys:null, parameters:{}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
18382 [main] WARN  o.a.h.h.m.HiveMetaStore - Location: raw:///tmp/junit8029605577550535343/testdb.db/test_table1 specified for non-external table:test_table1
18383 [main] INFO  o.a.h.h.c.FileUtils - Creating directory if it doesn't exist: raw:/tmp/junit8029605577550535343/testdb.db/test_table1
18400 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Shutting down the object store...
18400 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=Shutting down the object store...	
18401 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Metastore shutdown complete.
18401 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=Metastore shutdown complete.	
18401 [main] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=compile from=org.apache.hadoop.hive.ql.Driver>
18402 [main] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=parse from=org.apache.hadoop.hive.ql.Driver>
18402 [main] INFO  h.q.p.ParseDriver - Parsing command: select * from testdb1.test_table1
18403 [main] INFO  h.q.p.ParseDriver - Parse Completed
18403 [main] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=parse start=1466779975863 end=1466779975864 duration=1 from=org.apache.hadoop.hive.ql.Driver>
18414 [main] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=semanticAnalyze from=org.apache.hadoop.hive.ql.Driver>
18415 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Starting Semantic Analysis
18415 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Completed phase 1 of Semantic Analysis
18415 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Get metadata for source tables
18415 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_table : db=testdb1 tbl=test_table1
18416 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table : db=testdb1 tbl=test_table1	
18416 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
18416 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
18417 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
18419 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
18420 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
18438 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Get metadata for subqueries
18438 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Get metadata for destination tables
18441 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Completed getting MetaData in Semantic Analysis
18443 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Set stats collection dir : file:/tmp/travis/f5fbff48-6ca5-48fd-b58c-cd90e612fa25/hive_2016-06-24_14-52-55_863_8001954468891966615-1/-ext-10002
18445 [main] INFO  o.a.h.h.q.p.OpProcFactory - Processing for FS(6)
18446 [main] INFO  o.a.h.h.q.p.OpProcFactory - Processing for SEL(5)
18446 [main] INFO  o.a.h.h.q.p.OpProcFactory - Processing for TS(4)
18450 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Completed plan generation
18451 [main] INFO  o.a.h.h.q.Driver - Semantic Analysis Completed
18451 [main] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=semanticAnalyze start=1466779975875 end=1466779975912 duration=37 from=org.apache.hadoop.hive.ql.Driver>
18453 [main] INFO  o.a.h.h.q.e.TableScanOperator - Initializing Self TS[4]
18453 [main] INFO  o.a.h.h.q.e.TableScanOperator - Operator 4 TS initialized
18453 [main] INFO  o.a.h.h.q.e.TableScanOperator - Initializing children of 4 TS
18453 [main] INFO  o.a.h.h.q.e.SelectOperator - Initializing child 5 SEL
18453 [main] INFO  o.a.h.h.q.e.SelectOperator - Initializing Self SEL[5]
18453 [main] INFO  o.a.h.h.q.e.SelectOperator - SELECT struct<id:int,msg:string>
18454 [main] INFO  o.a.h.h.q.e.SelectOperator - Operator 5 SEL initialized
18454 [main] INFO  o.a.h.h.q.e.SelectOperator - Initializing children of 5 SEL
18454 [main] INFO  o.a.h.h.q.e.ListSinkOperator - Initializing child 7 OP
18454 [main] INFO  o.a.h.h.q.e.ListSinkOperator - Initializing Self OP[7]
18454 [main] INFO  o.a.h.h.q.e.ListSinkOperator - Operator 7 OP initialized
18454 [main] INFO  o.a.h.h.q.e.ListSinkOperator - Initialization Done 7 OP
18454 [main] INFO  o.a.h.h.q.e.SelectOperator - Initialization Done 5 SEL
18454 [main] INFO  o.a.h.h.q.e.TableScanOperator - Initialization Done 4 TS
18455 [main] INFO  o.a.h.h.q.Driver - Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:test_table1.id, type:int, comment:null), FieldSchema(name:test_table1.msg, type:string, comment:null)], properties:null)
18455 [main] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=compile start=1466779975862 end=1466779975916 duration=54 from=org.apache.hadoop.hive.ql.Driver>

--------------------------------------------------
--------------------------------------------------
classname: org.apache.storm.hive.bolt.TestHiveBolt / testname: testJsonWriter
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hive.bolt.TestHiveBolt.generateTestTuple(TestHiveBolt.java:447)
	at org.apache.storm.hive.bolt.TestHiveBolt.testJsonWriter(TestHiveBolt.java:274)

-------------------- system-out --------------------
19976 [main] INFO  o.a.h.h.q.s.SessionState - Created local directory: /tmp/f73a8f3b-0eb2-4110-8fff-00ee2d8362b0_resources
19978 [main] INFO  o.a.h.h.q.s.SessionState - Created HDFS directory: /tmp/hive/travis/f73a8f3b-0eb2-4110-8fff-00ee2d8362b0
19980 [main] INFO  o.a.h.h.q.s.SessionState - Created local directory: /tmp/travis/f73a8f3b-0eb2-4110-8fff-00ee2d8362b0
19982 [main] INFO  o.a.h.h.q.s.SessionState - Created HDFS directory: /tmp/hive/travis/f73a8f3b-0eb2-4110-8fff-00ee2d8362b0/_tmp_space.db
19983 [main] INFO  o.a.h.h.q.s.SessionState - No Tez session required at this point. hive.execution.engine=mr.
19984 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_table_names_by_filter: db = testdb, filter = 
19984 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table_names_by_filter: db = testdb, filter = 	
19984 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
19985 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
19986 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
19989 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
19989 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
20001 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_table : db=testdb tbl=test_table
20001 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table : db=testdb tbl=test_table	
20028 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: drop_table : db=testdb tbl=test_table
20028 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=drop_table : db=testdb tbl=test_table	
20322 [main] INFO  h.m.hivemetastoressimpl - deleting  raw:/tmp/junit8100006520220902923/testdb.db/test_table
20323 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_database: testdb
20323 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_database: testdb	
20329 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: drop_database: testdb
20333 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=drop_database: testdb	
20334 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_all_tables: db=testdb
20334 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_all_tables: db=testdb	
20339 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_functions: db=testdb pat=*
20339 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_functions: db=testdb pat=*	
20344 [main] INFO  o.a.h.h.m.ObjectStore - Dropping database testdb along with all tables
20359 [main] INFO  h.m.hivemetastoressimpl - deleting  raw:/tmp/junit8100006520220902923/testdb.db
20360 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
20361 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
20363 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
20363 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
20364 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: create_database: Database(name:testdb, description:null, locationUri:raw:///tmp/junit3941545228912359381/testdb.db, parameters:null)
20364 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=create_database: Database(name:testdb, description:null, locationUri:raw:///tmp/junit3941545228912359381/testdb.db, parameters:null)	
20365 [main] WARN  o.a.h.h.m.ObjectStore - Failed to get database testdb, returning NoSuchObjectException
20369 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: create_table: Table(tableName:test_table, dbName:testdb, owner:null, createTime:0, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:), FieldSchema(name:msg, type:string, comment:)], location:raw:///tmp/junit3941545228912359381/testdb.db/test_table, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:1, serdeInfo:SerDeInfo(name:test_table, serializationLib:org.apache.hadoop.hive.ql.io.orc.OrcSerde, parameters:{serialization.format=1}), bucketCols:[], sortCols:null, parameters:null), partitionKeys:[FieldSchema(name:city, type:string, comment:), FieldSchema(name:state, type:string, comment:)], parameters:{}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)
20369 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=create_table: Table(tableName:test_table, dbName:testdb, owner:null, createTime:0, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:), FieldSchema(name:msg, type:string, comment:)], location:raw:///tmp/junit3941545228912359381/testdb.db/test_table, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:1, serdeInfo:SerDeInfo(name:test_table, serializationLib:org.apache.hadoop.hive.ql.io.orc.OrcSerde, parameters:{serialization.format=1}), bucketCols:[], sortCols:null, parameters:null), partitionKeys:[FieldSchema(name:city, type:string, comment:), FieldSchema(name:state, type:string, comment:)], parameters:{}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
20371 [main] WARN  o.a.h.h.m.HiveMetaStore - Location: raw:///tmp/junit3941545228912359381/testdb.db/test_table specified for non-external table:test_table
20372 [main] INFO  o.a.h.h.c.FileUtils - Creating directory if it doesn't exist: raw:/tmp/junit3941545228912359381/testdb.db/test_table
20417 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: add_partition : db=testdb tbl=test_table
20417 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=add_partition : db=testdb tbl=test_table	
20465 [main] INFO  o.a.h.h.c.FileUtils - Creating directory if it doesn't exist: raw:/tmp/junit3941545228912359381/testdb.db/test_table/city=sunnyvale/state=ca
20491 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Shutting down the object store...
20492 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=Shutting down the object store...	
20492 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Metastore shutdown complete.
20492 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=Metastore shutdown complete.	
done

--------------------------------------------------
--------------------------------------------------
classname: org.apache.storm.hive.bolt.TestHiveBolt / testname: testTickTuple
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hive.bolt.TestHiveBolt.generateTestTuple(TestHiveBolt.java:447)
	at org.apache.storm.hive.bolt.TestHiveBolt.testTickTuple(TestHiveBolt.java:352)

-------------------- system-out --------------------
21669 [main] INFO  o.a.h.h.q.s.SessionState - Created local directory: /tmp/cf79f281-bda0-4795-bac1-86ccceb8f891_resources
21671 [main] INFO  o.a.h.h.q.s.SessionState - Created HDFS directory: /tmp/hive/travis/cf79f281-bda0-4795-bac1-86ccceb8f891
21672 [main] INFO  o.a.h.h.q.s.SessionState - Created local directory: /tmp/travis/cf79f281-bda0-4795-bac1-86ccceb8f891
21674 [main] INFO  o.a.h.h.q.s.SessionState - Created HDFS directory: /tmp/hive/travis/cf79f281-bda0-4795-bac1-86ccceb8f891/_tmp_space.db
21674 [main] INFO  o.a.h.h.q.s.SessionState - No Tez session required at this point. hive.execution.engine=mr.
21676 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_table_names_by_filter: db = testdb, filter = 
21676 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table_names_by_filter: db = testdb, filter = 	
21676 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
21676 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
21677 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
21679 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
21680 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
21688 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_table : db=testdb tbl=test_table
21689 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table : db=testdb tbl=test_table	
21703 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: drop_table : db=testdb tbl=test_table
21704 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=drop_table : db=testdb tbl=test_table	
22043 [main] INFO  h.m.hivemetastoressimpl - deleting  raw:/tmp/junit4076978818858595288/testdb.db/test_table
22044 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_database: testdb
22044 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_database: testdb	
22051 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: drop_database: testdb
22051 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=drop_database: testdb	
22052 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_all_tables: db=testdb
22053 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_all_tables: db=testdb	
22056 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_functions: db=testdb pat=*
22057 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_functions: db=testdb pat=*	
22062 [main] INFO  o.a.h.h.m.ObjectStore - Dropping database testdb along with all tables
22080 [main] INFO  h.m.hivemetastoressimpl - deleting  raw:/tmp/junit4076978818858595288/testdb.db
22080 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
22081 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
22083 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
22083 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
22084 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: create_database: Database(name:testdb, description:null, locationUri:raw:///tmp/junit5301618529196380606/testdb.db, parameters:null)
22084 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=create_database: Database(name:testdb, description:null, locationUri:raw:///tmp/junit5301618529196380606/testdb.db, parameters:null)	
22085 [main] WARN  o.a.h.h.m.ObjectStore - Failed to get database testdb, returning NoSuchObjectException
22090 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: create_table: Table(tableName:test_table, dbName:testdb, owner:null, createTime:0, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:), FieldSchema(name:msg, type:string, comment:)], location:raw:///tmp/junit5301618529196380606/testdb.db/test_table, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:1, serdeInfo:SerDeInfo(name:test_table, serializationLib:org.apache.hadoop.hive.ql.io.orc.OrcSerde, parameters:{serialization.format=1}), bucketCols:[], sortCols:null, parameters:null), partitionKeys:[FieldSchema(name:city, type:string, comment:), FieldSchema(name:state, type:string, comment:)], parameters:{}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)
22090 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=create_table: Table(tableName:test_table, dbName:testdb, owner:null, createTime:0, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:), FieldSchema(name:msg, type:string, comment:)], location:raw:///tmp/junit5301618529196380606/testdb.db/test_table, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:1, serdeInfo:SerDeInfo(name:test_table, serializationLib:org.apache.hadoop.hive.ql.io.orc.OrcSerde, parameters:{serialization.format=1}), bucketCols:[], sortCols:null, parameters:null), partitionKeys:[FieldSchema(name:city, type:string, comment:), FieldSchema(name:state, type:string, comment:)], parameters:{}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
22095 [main] WARN  o.a.h.h.m.HiveMetaStore - Location: raw:///tmp/junit5301618529196380606/testdb.db/test_table specified for non-external table:test_table
22096 [main] INFO  o.a.h.h.c.FileUtils - Creating directory if it doesn't exist: raw:/tmp/junit5301618529196380606/testdb.db/test_table
22198 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: add_partition : db=testdb tbl=test_table
22198 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=add_partition : db=testdb tbl=test_table	
22255 [main] INFO  o.a.h.h.c.FileUtils - Creating directory if it doesn't exist: raw:/tmp/junit5301618529196380606/testdb.db/test_table/city=sunnyvale/state=ca
22281 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Shutting down the object store...
22281 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=Shutting down the object store...	
22281 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Metastore shutdown complete.
22281 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=Metastore shutdown complete.	
done

--------------------------------------------------
--------------------------------------------------
classname: org.apache.storm.hive.bolt.TestHiveBolt / testname: testWithTimeformat
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hive.bolt.TestHiveBolt.generateTestTuple(TestHiveBolt.java:447)
	at org.apache.storm.hive.bolt.TestHiveBolt.testWithTimeformat(TestHiveBolt.java:230)

-------------------- system-out --------------------
22688 [main] INFO  o.a.h.h.q.s.SessionState - Created local directory: /tmp/e7a24ff2-64ec-4795-8664-3ad4323d3f98_resources
22690 [main] INFO  o.a.h.h.q.s.SessionState - Created HDFS directory: /tmp/hive/travis/e7a24ff2-64ec-4795-8664-3ad4323d3f98
22692 [main] INFO  o.a.h.h.q.s.SessionState - Created local directory: /tmp/travis/e7a24ff2-64ec-4795-8664-3ad4323d3f98
22694 [main] INFO  o.a.h.h.q.s.SessionState - Created HDFS directory: /tmp/hive/travis/e7a24ff2-64ec-4795-8664-3ad4323d3f98/_tmp_space.db
22694 [main] INFO  o.a.h.h.q.s.SessionState - No Tez session required at this point. hive.execution.engine=mr.
22696 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_table_names_by_filter: db = testdb, filter = 
22696 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table_names_by_filter: db = testdb, filter = 	
22696 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
22696 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
22697 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
22700 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
22700 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
22708 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_table : db=testdb tbl=test_table
22708 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table : db=testdb tbl=test_table	
22723 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: drop_table : db=testdb tbl=test_table
22724 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=drop_table : db=testdb tbl=test_table	
23012 [main] INFO  h.m.hivemetastoressimpl - deleting  raw:/tmp/junit5301618529196380606/testdb.db/test_table
23012 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_database: testdb
23012 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_database: testdb	
23021 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: drop_database: testdb
23021 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=drop_database: testdb	
23022 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_all_tables: db=testdb
23022 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_all_tables: db=testdb	
23028 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_functions: db=testdb pat=*
23028 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_functions: db=testdb pat=*	
23035 [main] INFO  o.a.h.h.m.ObjectStore - Dropping database testdb along with all tables
23056 [main] INFO  h.m.hivemetastoressimpl - deleting  raw:/tmp/junit5301618529196380606/testdb.db
23057 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
23058 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
23104 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
23116 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
23117 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: create_database: Database(name:testdb, description:null, locationUri:raw:///tmp/junit2201970362256394687/testdb.db, parameters:null)
23117 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=create_database: Database(name:testdb, description:null, locationUri:raw:///tmp/junit2201970362256394687/testdb.db, parameters:null)	
23118 [main] WARN  o.a.h.h.m.ObjectStore - Failed to get database testdb, returning NoSuchObjectException
23135 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: create_table: Table(tableName:test_table, dbName:testdb, owner:null, createTime:0, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:), FieldSchema(name:msg, type:string, comment:)], location:raw:///tmp/junit2201970362256394687/testdb.db/test_table, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:1, serdeInfo:SerDeInfo(name:test_table, serializationLib:org.apache.hadoop.hive.ql.io.orc.OrcSerde, parameters:{serialization.format=1}), bucketCols:[], sortCols:null, parameters:null), partitionKeys:[FieldSchema(name:city, type:string, comment:), FieldSchema(name:state, type:string, comment:)], parameters:{}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)
23136 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=create_table: Table(tableName:test_table, dbName:testdb, owner:null, createTime:0, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:), FieldSchema(name:msg, type:string, comment:)], location:raw:///tmp/junit2201970362256394687/testdb.db/test_table, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:1, serdeInfo:SerDeInfo(name:test_table, serializationLib:org.apache.hadoop.hive.ql.io.orc.OrcSerde, parameters:{serialization.format=1}), bucketCols:[], sortCols:null, parameters:null), partitionKeys:[FieldSchema(name:city, type:string, comment:), FieldSchema(name:state, type:string, comment:)], parameters:{}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
23137 [main] WARN  o.a.h.h.m.HiveMetaStore - Location: raw:///tmp/junit2201970362256394687/testdb.db/test_table specified for non-external table:test_table
23138 [main] INFO  o.a.h.h.c.FileUtils - Creating directory if it doesn't exist: raw:/tmp/junit2201970362256394687/testdb.db/test_table
23185 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: add_partition : db=testdb tbl=test_table
23185 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=add_partition : db=testdb tbl=test_table	
23271 [main] INFO  o.a.h.h.c.FileUtils - Creating directory if it doesn't exist: raw:/tmp/junit2201970362256394687/testdb.db/test_table/city=sunnyvale/state=ca
23291 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Shutting down the object store...
23291 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=Shutting down the object store...	
23291 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Metastore shutdown complete.
23292 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=Metastore shutdown complete.	
done
23292 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_table_names_by_filter: db = testdb1, filter = 
23292 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table_names_by_filter: db = testdb1, filter = 	
23292 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
23293 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
23293 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
23296 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
23296 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
23303 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_table : db=testdb1 tbl=test_table1
23303 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table : db=testdb1 tbl=test_table1	
23334 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: drop_table : db=testdb1 tbl=test_table1
23334 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=drop_table : db=testdb1 tbl=test_table1	
23368 [main] INFO  h.m.hivemetastoressimpl - deleting  raw:/tmp/junit8029605577550535343/testdb.db/test_table1
23368 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_database: testdb1
23368 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_database: testdb1	
23370 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: drop_database: testdb1
23370 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=drop_database: testdb1	
23371 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_all_tables: db=testdb1
23371 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_all_tables: db=testdb1	
23372 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_functions: db=testdb1 pat=*
23372 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_functions: db=testdb1 pat=*	
23373 [main] INFO  o.a.h.h.m.ObjectStore - Dropping database testdb1 along with all tables
23378 [main] INFO  h.m.hivemetastoressimpl - deleting  raw:/tmp/junit8029605577550535343/testdb.db
23378 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
23379 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
23381 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
23381 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
23382 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: create_database: Database(name:testdb1, description:null, locationUri:raw:///tmp/junit2201970362256394687/testdb.db, parameters:null)
23382 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=create_database: Database(name:testdb1, description:null, locationUri:raw:///tmp/junit2201970362256394687/testdb.db, parameters:null)	
23383 [main] WARN  o.a.h.h.m.ObjectStore - Failed to get database testdb1, returning NoSuchObjectException
23386 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: create_table: Table(tableName:test_table1, dbName:testdb1, owner:null, createTime:0, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:), FieldSchema(name:msg, type:string, comment:)], location:raw:///tmp/junit2201970362256394687/testdb.db/test_table1, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:1, serdeInfo:SerDeInfo(name:test_table1, serializationLib:org.apache.hadoop.hive.ql.io.orc.OrcSerde, parameters:{serialization.format=1}), bucketCols:[], sortCols:null, parameters:null), partitionKeys:[FieldSchema(name:dt, type:string, comment:)], parameters:{}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)
23386 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=create_table: Table(tableName:test_table1, dbName:testdb1, owner:null, createTime:0, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:), FieldSchema(name:msg, type:string, comment:)], location:raw:///tmp/junit2201970362256394687/testdb.db/test_table1, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:1, serdeInfo:SerDeInfo(name:test_table1, serializationLib:org.apache.hadoop.hive.ql.io.orc.OrcSerde, parameters:{serialization.format=1}), bucketCols:[], sortCols:null, parameters:null), partitionKeys:[FieldSchema(name:dt, type:string, comment:)], parameters:{}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
23388 [main] WARN  o.a.h.h.m.HiveMetaStore - Location: raw:///tmp/junit2201970362256394687/testdb.db/test_table1 specified for non-external table:test_table1
23388 [main] INFO  o.a.h.h.c.FileUtils - Creating directory if it doesn't exist: raw:/tmp/junit2201970362256394687/testdb.db/test_table1
23405 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Shutting down the object store...
23405 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=Shutting down the object store...	
23405 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Metastore shutdown complete.
23406 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=Metastore shutdown complete.	
23407 [main] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=compile from=org.apache.hadoop.hive.ql.Driver>
23407 [main] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=parse from=org.apache.hadoop.hive.ql.Driver>
23407 [main] INFO  h.q.p.ParseDriver - Parsing command: select * from testdb1.test_table1
23408 [main] INFO  h.q.p.ParseDriver - Parse Completed
23408 [main] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=parse start=1466779980868 end=1466779980869 duration=1 from=org.apache.hadoop.hive.ql.Driver>
23419 [main] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=semanticAnalyze from=org.apache.hadoop.hive.ql.Driver>
23419 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Starting Semantic Analysis
23419 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Completed phase 1 of Semantic Analysis
23419 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Get metadata for source tables
23420 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_table : db=testdb1 tbl=test_table1
23420 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table : db=testdb1 tbl=test_table1	
23420 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
23420 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
23421 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
23423 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
23424 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
23438 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Get metadata for subqueries
23438 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Get metadata for destination tables
23441 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Completed getting MetaData in Semantic Analysis
23442 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Set stats collection dir : file:/tmp/travis/e7a24ff2-64ec-4795-8664-3ad4323d3f98/hive_2016-06-24_14-53-00_868_7132867591017276096-1/-ext-10002
23445 [main] INFO  o.a.h.h.q.p.OpProcFactory - Processing for FS(10)
23445 [main] INFO  o.a.h.h.q.p.OpProcFactory - Processing for SEL(9)
23445 [main] INFO  o.a.h.h.q.p.OpProcFactory - Processing for TS(8)
23448 [main] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=partition-retrieving from=org.apache.hadoop.hive.ql.optimizer.ppr.PartitionPruner>
23448 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_partitions : db=testdb1 tbl=test_table1
23448 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_partitions : db=testdb1 tbl=test_table1	
23456 [main] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=partition-retrieving start=1466779980909 end=1466779980917 duration=8 from=org.apache.hadoop.hive.ql.optimizer.ppr.PartitionPruner>
23457 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Completed plan generation
23457 [main] INFO  o.a.h.h.q.Driver - Semantic Analysis Completed
23457 [main] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=semanticAnalyze start=1466779980880 end=1466779980918 duration=38 from=org.apache.hadoop.hive.ql.Driver>
23458 [main] INFO  o.a.h.h.q.e.TableScanOperator - Initializing Self TS[8]
23458 [main] INFO  o.a.h.h.q.e.TableScanOperator - Operator 8 TS initialized
23458 [main] INFO  o.a.h.h.q.e.TableScanOperator - Initializing children of 8 TS
23458 [main] INFO  o.a.h.h.q.e.SelectOperator - Initializing child 9 SEL
23458 [main] INFO  o.a.h.h.q.e.SelectOperator - Initializing Self SEL[9]
23458 [main] INFO  o.a.h.h.q.e.SelectOperator - SELECT struct<id:int,msg:string,dt:string>
23458 [main] INFO  o.a.h.h.q.e.SelectOperator - Operator 9 SEL initialized
23458 [main] INFO  o.a.h.h.q.e.SelectOperator - Initializing children of 9 SEL
23458 [main] INFO  o.a.h.h.q.e.ListSinkOperator - Initializing child 11 OP
23458 [main] INFO  o.a.h.h.q.e.ListSinkOperator - Initializing Self OP[11]
23459 [main] INFO  o.a.h.h.q.e.ListSinkOperator - Operator 11 OP initialized
23459 [main] INFO  o.a.h.h.q.e.ListSinkOperator - Initialization Done 11 OP
23459 [main] INFO  o.a.h.h.q.e.SelectOperator - Initialization Done 9 SEL
23459 [main] INFO  o.a.h.h.q.e.TableScanOperator - Initialization Done 8 TS
23459 [main] INFO  o.a.h.h.q.Driver - Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:test_table1.id, type:int, comment:null), FieldSchema(name:test_table1.msg, type:string, comment:null), FieldSchema(name:test_table1.dt, type:string, comment:null)], properties:null)
23459 [main] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=compile start=1466779980868 end=1466779980920 duration=52 from=org.apache.hadoop.hive.ql.Driver>

--------------------------------------------------
--------------------------------------------------
classname: org.apache.storm.hive.bolt.TestHiveBolt / testname: testWithByteArrayIdandMessage
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hive.bolt.TestHiveBolt.generateTestTuple(TestHiveBolt.java:447)
	at org.apache.storm.hive.bolt.TestHiveBolt.testWithByteArrayIdandMessage(TestHiveBolt.java:161)

-------------------- system-out --------------------
23737 [main] INFO  o.a.h.h.q.s.SessionState - Created local directory: /tmp/45a736c8-f564-47f9-83ad-2fdd7bc01cc8_resources
23739 [main] INFO  o.a.h.h.q.s.SessionState - Created HDFS directory: /tmp/hive/travis/45a736c8-f564-47f9-83ad-2fdd7bc01cc8
23741 [main] INFO  o.a.h.h.q.s.SessionState - Created local directory: /tmp/travis/45a736c8-f564-47f9-83ad-2fdd7bc01cc8
23743 [main] INFO  o.a.h.h.q.s.SessionState - Created HDFS directory: /tmp/hive/travis/45a736c8-f564-47f9-83ad-2fdd7bc01cc8/_tmp_space.db
23743 [main] INFO  o.a.h.h.q.s.SessionState - No Tez session required at this point. hive.execution.engine=mr.
23745 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
23746 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
23748 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
23749 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
23749 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_table_names_by_filter: db = testdb, filter = 
23749 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table_names_by_filter: db = testdb, filter = 	
23750 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_table : db=testdb tbl=test_table
23750 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table : db=testdb tbl=test_table	
23766 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: drop_table : db=testdb tbl=test_table
23766 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=drop_table : db=testdb tbl=test_table	
24028 [main] INFO  h.m.hivemetastoressimpl - deleting  raw:/tmp/junit2201970362256394687/testdb.db/test_table
24028 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_database: testdb
24029 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_database: testdb	
24037 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: drop_database: testdb
24037 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=drop_database: testdb	
24038 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_all_tables: db=testdb
24038 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_all_tables: db=testdb	
24043 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_functions: db=testdb pat=*
24043 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_functions: db=testdb pat=*	
24049 [main] INFO  o.a.h.h.m.ObjectStore - Dropping database testdb along with all tables
24070 [main] INFO  h.m.hivemetastoressimpl - deleting  raw:/tmp/junit2201970362256394687/testdb.db
24070 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
24071 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
24073 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
24074 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
24074 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: create_database: Database(name:testdb, description:null, locationUri:raw:///tmp/junit3927182545364716070/testdb.db, parameters:null)
24074 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=create_database: Database(name:testdb, description:null, locationUri:raw:///tmp/junit3927182545364716070/testdb.db, parameters:null)	
24075 [main] WARN  o.a.h.h.m.ObjectStore - Failed to get database testdb, returning NoSuchObjectException
24080 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: create_table: Table(tableName:test_table, dbName:testdb, owner:null, createTime:0, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:), FieldSchema(name:msg, type:string, comment:)], location:raw:///tmp/junit3927182545364716070/testdb.db/test_table, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:1, serdeInfo:SerDeInfo(name:test_table, serializationLib:org.apache.hadoop.hive.ql.io.orc.OrcSerde, parameters:{serialization.format=1}), bucketCols:[], sortCols:null, parameters:null), partitionKeys:[FieldSchema(name:city, type:string, comment:), FieldSchema(name:state, type:string, comment:)], parameters:{}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)
24081 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=create_table: Table(tableName:test_table, dbName:testdb, owner:null, createTime:0, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:), FieldSchema(name:msg, type:string, comment:)], location:raw:///tmp/junit3927182545364716070/testdb.db/test_table, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:1, serdeInfo:SerDeInfo(name:test_table, serializationLib:org.apache.hadoop.hive.ql.io.orc.OrcSerde, parameters:{serialization.format=1}), bucketCols:[], sortCols:null, parameters:null), partitionKeys:[FieldSchema(name:city, type:string, comment:), FieldSchema(name:state, type:string, comment:)], parameters:{}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
24082 [main] WARN  o.a.h.h.m.HiveMetaStore - Location: raw:///tmp/junit3927182545364716070/testdb.db/test_table specified for non-external table:test_table
24083 [main] INFO  o.a.h.h.c.FileUtils - Creating directory if it doesn't exist: raw:/tmp/junit3927182545364716070/testdb.db/test_table
24150 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: add_partition : db=testdb tbl=test_table
24150 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=add_partition : db=testdb tbl=test_table	
24198 [main] INFO  o.a.h.h.c.FileUtils - Creating directory if it doesn't exist: raw:/tmp/junit3927182545364716070/testdb.db/test_table/city=sunnyvale/state=ca
24223 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Shutting down the object store...
24223 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=Shutting down the object store...	
24223 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Metastore shutdown complete.
24223 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=Metastore shutdown complete.	
done
24224 [main] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=compile from=org.apache.hadoop.hive.ql.Driver>
24225 [main] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=parse from=org.apache.hadoop.hive.ql.Driver>
24225 [main] INFO  h.q.p.ParseDriver - Parsing command: select * from testdb.test_table
24225 [main] INFO  h.q.p.ParseDriver - Parse Completed
24225 [main] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=parse start=1466779981686 end=1466779981686 duration=0 from=org.apache.hadoop.hive.ql.Driver>
24235 [main] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=semanticAnalyze from=org.apache.hadoop.hive.ql.Driver>
24235 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Starting Semantic Analysis
24235 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Completed phase 1 of Semantic Analysis
24235 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Get metadata for source tables
24236 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_table : db=testdb tbl=test_table
24236 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table : db=testdb tbl=test_table	
24236 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
24236 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
24237 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
24239 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
24240 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
24258 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Get metadata for subqueries
24258 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Get metadata for destination tables
24260 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Completed getting MetaData in Semantic Analysis
24262 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Set stats collection dir : file:/tmp/travis/45a736c8-f564-47f9-83ad-2fdd7bc01cc8/hive_2016-06-24_14-53-01_685_1676707853156758521-1/-ext-10002
24265 [main] INFO  o.a.h.h.q.p.OpProcFactory - Processing for FS(14)
24265 [main] INFO  o.a.h.h.q.p.OpProcFactory - Processing for SEL(13)
24265 [main] INFO  o.a.h.h.q.p.OpProcFactory - Processing for TS(12)
24267 [main] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=partition-retrieving from=org.apache.hadoop.hive.ql.optimizer.ppr.PartitionPruner>
24268 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_partitions : db=testdb tbl=test_table
24268 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_partitions : db=testdb tbl=test_table	
24311 [main] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=partition-retrieving start=1466779981728 end=1466779981772 duration=44 from=org.apache.hadoop.hive.ql.optimizer.ppr.PartitionPruner>
24313 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Completed plan generation
24313 [main] INFO  o.a.h.h.q.Driver - Semantic Analysis Completed
24314 [main] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=semanticAnalyze start=1466779981696 end=1466779981775 duration=79 from=org.apache.hadoop.hive.ql.Driver>
24314 [main] INFO  o.a.h.h.q.e.TableScanOperator - Initializing Self TS[12]
24315 [main] INFO  o.a.h.h.q.e.TableScanOperator - Operator 12 TS initialized
24315 [main] INFO  o.a.h.h.q.e.TableScanOperator - Initializing children of 12 TS
24315 [main] INFO  o.a.h.h.q.e.SelectOperator - Initializing child 13 SEL
24315 [main] INFO  o.a.h.h.q.e.SelectOperator - Initializing Self SEL[13]
24315 [main] INFO  o.a.h.h.q.e.SelectOperator - SELECT struct<id:int,msg:string,city:string,state:string>
24315 [main] INFO  o.a.h.h.q.e.SelectOperator - Operator 13 SEL initialized
24315 [main] INFO  o.a.h.h.q.e.SelectOperator - Initializing children of 13 SEL
24315 [main] INFO  o.a.h.h.q.e.ListSinkOperator - Initializing child 15 OP
24316 [main] INFO  o.a.h.h.q.e.ListSinkOperator - Initializing Self OP[15]
24316 [main] INFO  o.a.h.h.q.e.ListSinkOperator - Operator 15 OP initialized
24316 [main] INFO  o.a.h.h.q.e.ListSinkOperator - Initialization Done 15 OP
24316 [main] INFO  o.a.h.h.q.e.SelectOperator - Initialization Done 13 SEL
24316 [main] INFO  o.a.h.h.q.e.TableScanOperator - Initialization Done 12 TS
24316 [main] INFO  o.a.h.h.q.Driver - Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:test_table.id, type:int, comment:null), FieldSchema(name:test_table.msg, type:string, comment:null), FieldSchema(name:test_table.city, type:string, comment:null), FieldSchema(name:test_table.state, type:string, comment:null)], properties:null)
24317 [main] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=compile start=1466779981685 end=1466779981778 duration=93 from=org.apache.hadoop.hive.ql.Driver>

--------------------------------------------------
--------------------------------------------------
classname: org.apache.storm.hive.bolt.TestHiveBolt / testname: testNoAcksIfFlushFails
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hive.bolt.TestHiveBolt.generateTestTuple(TestHiveBolt.java:447)
	at org.apache.storm.hive.bolt.TestHiveBolt.testNoAcksIfFlushFails(TestHiveBolt.java:327)

-------------------- system-out --------------------
24904 [main] INFO  o.a.h.h.q.s.SessionState - Created local directory: /tmp/a36120ec-f5ec-441c-9229-1537ca79211f_resources
24906 [main] INFO  o.a.h.h.q.s.SessionState - Created HDFS directory: /tmp/hive/travis/a36120ec-f5ec-441c-9229-1537ca79211f
24907 [main] INFO  o.a.h.h.q.s.SessionState - Created local directory: /tmp/travis/a36120ec-f5ec-441c-9229-1537ca79211f
24909 [main] INFO  o.a.h.h.q.s.SessionState - Created HDFS directory: /tmp/hive/travis/a36120ec-f5ec-441c-9229-1537ca79211f/_tmp_space.db
24910 [main] INFO  o.a.h.h.q.s.SessionState - No Tez session required at this point. hive.execution.engine=mr.
24911 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
24912 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
24913 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
24913 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
24914 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_table_names_by_filter: db = testdb, filter = 
24914 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table_names_by_filter: db = testdb, filter = 	
24918 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_table : db=testdb tbl=test_table
24918 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table : db=testdb tbl=test_table	
24935 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: drop_table : db=testdb tbl=test_table
24935 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=drop_table : db=testdb tbl=test_table	
25264 [main] INFO  h.m.hivemetastoressimpl - deleting  raw:/tmp/junit3927182545364716070/testdb.db/test_table
25264 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_database: testdb
25264 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_database: testdb	
25270 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: drop_database: testdb
25270 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=drop_database: testdb	
25271 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_all_tables: db=testdb
25271 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_all_tables: db=testdb	
25274 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_functions: db=testdb pat=*
25274 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_functions: db=testdb pat=*	
25279 [main] INFO  o.a.h.h.m.ObjectStore - Dropping database testdb along with all tables
25343 [main] INFO  h.m.hivemetastoressimpl - deleting  raw:/tmp/junit3927182545364716070/testdb.db
25344 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
25345 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
25347 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
25347 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
25347 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: create_database: Database(name:testdb, description:null, locationUri:raw:///tmp/junit799277879341766816/testdb.db, parameters:null)
25347 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=create_database: Database(name:testdb, description:null, locationUri:raw:///tmp/junit799277879341766816/testdb.db, parameters:null)	
25348 [main] WARN  o.a.h.h.m.ObjectStore - Failed to get database testdb, returning NoSuchObjectException
25352 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: create_table: Table(tableName:test_table, dbName:testdb, owner:null, createTime:0, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:), FieldSchema(name:msg, type:string, comment:)], location:raw:///tmp/junit799277879341766816/testdb.db/test_table, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:1, serdeInfo:SerDeInfo(name:test_table, serializationLib:org.apache.hadoop.hive.ql.io.orc.OrcSerde, parameters:{serialization.format=1}), bucketCols:[], sortCols:null, parameters:null), partitionKeys:[FieldSchema(name:city, type:string, comment:), FieldSchema(name:state, type:string, comment:)], parameters:{}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)
25353 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=create_table: Table(tableName:test_table, dbName:testdb, owner:null, createTime:0, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:), FieldSchema(name:msg, type:string, comment:)], location:raw:///tmp/junit799277879341766816/testdb.db/test_table, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:1, serdeInfo:SerDeInfo(name:test_table, serializationLib:org.apache.hadoop.hive.ql.io.orc.OrcSerde, parameters:{serialization.format=1}), bucketCols:[], sortCols:null, parameters:null), partitionKeys:[FieldSchema(name:city, type:string, comment:), FieldSchema(name:state, type:string, comment:)], parameters:{}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
25354 [main] WARN  o.a.h.h.m.HiveMetaStore - Location: raw:///tmp/junit799277879341766816/testdb.db/test_table specified for non-external table:test_table
25355 [main] INFO  o.a.h.h.c.FileUtils - Creating directory if it doesn't exist: raw:/tmp/junit799277879341766816/testdb.db/test_table
25399 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: add_partition : db=testdb tbl=test_table
25400 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=add_partition : db=testdb tbl=test_table	
25506 [main] INFO  o.a.h.h.c.FileUtils - Creating directory if it doesn't exist: raw:/tmp/junit799277879341766816/testdb.db/test_table/city=sunnyvale/state=ca
25540 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Shutting down the object store...
25540 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=Shutting down the object store...	
25543 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Metastore shutdown complete.
25543 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=Metastore shutdown complete.	
done

--------------------------------------------------
Checking ./external/storm-hive/target/surefire-reports/TEST-org.apache.storm.hive.common.TestHiveWriter.xml
--------------------------------------------------
classname: org.apache.storm.hive.common.TestHiveWriter / testname: testWriteBasic
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hive.common.TestHiveWriter.generateTestTuple(TestHiveWriter.java:164)
	at org.apache.storm.hive.common.TestHiveWriter.writeTuples(TestHiveWriter.java:179)
	at org.apache.storm.hive.common.TestHiveWriter.testWriteBasic(TestHiveWriter.java:127)

-------------------- system-out --------------------
25940 [main] INFO  o.a.h.h.q.s.SessionState - Created local directory: /tmp/fb5aa6c1-61ca-438e-81c7-f6a7108cd5ad_resources
25942 [main] INFO  o.a.h.h.q.s.SessionState - Created HDFS directory: /tmp/hive/travis/fb5aa6c1-61ca-438e-81c7-f6a7108cd5ad
25944 [main] INFO  o.a.h.h.q.s.SessionState - Created local directory: /tmp/travis/fb5aa6c1-61ca-438e-81c7-f6a7108cd5ad
25946 [main] INFO  o.a.h.h.q.s.SessionState - Created HDFS directory: /tmp/hive/travis/fb5aa6c1-61ca-438e-81c7-f6a7108cd5ad/_tmp_space.db
25946 [main] INFO  o.a.h.h.q.s.SessionState - No Tez session required at this point. hive.execution.engine=mr.
25947 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_table_names_by_filter: db = testdb, filter = 
25947 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table_names_by_filter: db = testdb, filter = 	
25947 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
25948 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
25954 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
25957 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
25957 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
25969 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_table : db=testdb tbl=test_table
25969 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table : db=testdb tbl=test_table	
25986 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: drop_table : db=testdb tbl=test_table
25986 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=drop_table : db=testdb tbl=test_table	
26227 [main] INFO  h.m.hivemetastoressimpl - deleting  raw:/tmp/junit799277879341766816/testdb.db/test_table
26227 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_database: testdb
26227 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_database: testdb	
26234 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: drop_database: testdb
26234 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=drop_database: testdb	
26235 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_all_tables: db=testdb
26235 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_all_tables: db=testdb	
26239 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_functions: db=testdb pat=*
26239 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_functions: db=testdb pat=*	
26243 [main] INFO  o.a.h.h.m.ObjectStore - Dropping database testdb along with all tables
26258 [main] INFO  h.m.hivemetastoressimpl - deleting  raw:/tmp/junit799277879341766816/testdb.db
26259 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
26259 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
26261 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
26261 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
26262 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: create_database: Database(name:testdb, description:null, locationUri:/tmp/junit7876091414799495689/testdb.db, parameters:null)
26262 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=create_database: Database(name:testdb, description:null, locationUri:/tmp/junit7876091414799495689/testdb.db, parameters:null)	
26262 [main] WARN  o.a.h.h.m.ObjectStore - Failed to get database testdb, returning NoSuchObjectException
26263 [main] INFO  o.a.h.h.c.FileUtils - Creating directory if it doesn't exist: file:/tmp/junit7876091414799495689/testdb.db
26267 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: create_table: Table(tableName:test_table2, dbName:testdb, owner:null, createTime:0, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:), FieldSchema(name:msg, type:string, comment:)], location:/tmp/junit7876091414799495689/testdb.db/test_table2, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:1, serdeInfo:SerDeInfo(name:test_table2, serializationLib:org.apache.hadoop.hive.ql.io.orc.OrcSerde, parameters:{serialization.format=1}), bucketCols:[], sortCols:null, parameters:null), partitionKeys:[FieldSchema(name:city, type:string, comment:), FieldSchema(name:state, type:string, comment:)], parameters:{}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)
26267 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=create_table: Table(tableName:test_table2, dbName:testdb, owner:null, createTime:0, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:), FieldSchema(name:msg, type:string, comment:)], location:/tmp/junit7876091414799495689/testdb.db/test_table2, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:1, serdeInfo:SerDeInfo(name:test_table2, serializationLib:org.apache.hadoop.hive.ql.io.orc.OrcSerde, parameters:{serialization.format=1}), bucketCols:[], sortCols:null, parameters:null), partitionKeys:[FieldSchema(name:city, type:string, comment:), FieldSchema(name:state, type:string, comment:)], parameters:{}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
26268 [main] WARN  o.a.h.h.m.HiveMetaStore - Location: /tmp/junit7876091414799495689/testdb.db/test_table2 specified for non-external table:test_table2
26269 [main] INFO  o.a.h.h.c.FileUtils - Creating directory if it doesn't exist: file:/tmp/junit7876091414799495689/testdb.db/test_table2
26301 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: add_partition : db=testdb tbl=test_table2
26302 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=add_partition : db=testdb tbl=test_table2	
26340 [main] INFO  o.a.h.h.c.FileUtils - Creating directory if it doesn't exist: file:/tmp/junit7876091414799495689/testdb.db/test_table2/city=sunnyvale/state=ca
26363 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Shutting down the object store...
26363 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=Shutting down the object store...	
26364 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Metastore shutdown complete.
26364 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=Metastore shutdown complete.	
26408 [hiveWriterTest] INFO  o.a.h.h.q.s.SessionState - Created local directory: /tmp/a6be0b99-4afa-471b-8378-5a8dbaecd5b2_resources
26410 [hiveWriterTest] INFO  o.a.h.h.q.s.SessionState - Created HDFS directory: /tmp/hive/travis/a6be0b99-4afa-471b-8378-5a8dbaecd5b2
26412 [hiveWriterTest] INFO  o.a.h.h.q.s.SessionState - Created local directory: /tmp/travis/a6be0b99-4afa-471b-8378-5a8dbaecd5b2
26414 [hiveWriterTest] INFO  o.a.h.h.q.s.SessionState - Created HDFS directory: /tmp/hive/travis/a6be0b99-4afa-471b-8378-5a8dbaecd5b2/_tmp_space.db
26414 [hiveWriterTest] INFO  o.a.h.h.q.s.SessionState - No Tez session required at this point. hive.execution.engine=mr.
26415 [hiveWriterTest] INFO  o.a.h.h.m.HiveMetaStore - 1: get_table : db=testdb tbl=test_table2
26415 [hiveWriterTest] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table : db=testdb tbl=test_table2	
26415 [hiveWriterTest] INFO  o.a.h.h.m.HiveMetaStore - 1: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
26416 [hiveWriterTest] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
26418 [hiveWriterTest] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
26422 [hiveWriterTest] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
26422 [hiveWriterTest] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
26445 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=Driver.run from=org.apache.hadoop.hive.ql.Driver>
26445 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=TimeToSubmit from=org.apache.hadoop.hive.ql.Driver>
26445 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=compile from=org.apache.hadoop.hive.ql.Driver>
26445 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=parse from=org.apache.hadoop.hive.ql.Driver>
26446 [hiveWriterTest] INFO  h.q.p.ParseDriver - Parsing command: use testdb
26447 [hiveWriterTest] INFO  h.q.p.ParseDriver - Parse Completed
26447 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=parse start=1466779983906 end=1466779983908 duration=2 from=org.apache.hadoop.hive.ql.Driver>
26457 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=semanticAnalyze from=org.apache.hadoop.hive.ql.Driver>
26466 [hiveWriterTest] INFO  o.a.h.h.m.HiveMetaStore - 1: get_database: testdb
26467 [hiveWriterTest] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_database: testdb	
26483 [hiveWriterTest] INFO  o.a.h.h.q.Driver - Semantic Analysis Completed
26483 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=semanticAnalyze start=1466779983918 end=1466779983944 duration=26 from=org.apache.hadoop.hive.ql.Driver>
26483 [hiveWriterTest] INFO  o.a.h.h.q.Driver - Returning Hive schema: Schema(fieldSchemas:null, properties:null)
26483 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=compile start=1466779983906 end=1466779983944 duration=38 from=org.apache.hadoop.hive.ql.Driver>
26483 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=acquireReadWriteLocks from=org.apache.hadoop.hive.ql.Driver>
26492 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=acquireReadWriteLocks start=1466779983944 end=1466779983953 duration=9 from=org.apache.hadoop.hive.ql.Driver>
26492 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=Driver.execute from=org.apache.hadoop.hive.ql.Driver>
26492 [hiveWriterTest] INFO  o.a.h.h.q.Driver - Starting command: use testdb
26494 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=TimeToSubmit start=1466779983906 end=1466779983955 duration=49 from=org.apache.hadoop.hive.ql.Driver>
26494 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=runTasks from=org.apache.hadoop.hive.ql.Driver>
26494 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=task.DDL.Stage-0 from=org.apache.hadoop.hive.ql.Driver>
26498 [hiveWriterTest] INFO  o.a.h.h.q.Driver - Starting task [Stage-0:DDL] in serial mode
26498 [hiveWriterTest] INFO  o.a.h.h.m.HiveMetaStore - 1: get_database: testdb
26499 [hiveWriterTest] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_database: testdb	
26500 [hiveWriterTest] INFO  o.a.h.h.m.HiveMetaStore - 1: get_database: testdb
26500 [hiveWriterTest] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_database: testdb	
26501 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=runTasks start=1466779983955 end=1466779983962 duration=7 from=org.apache.hadoop.hive.ql.Driver>
26502 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=Driver.execute start=1466779983953 end=1466779983963 duration=10 from=org.apache.hadoop.hive.ql.Driver>
26502 [hiveWriterTest] INFO  o.a.h.h.q.Driver - OK
26502 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=releaseLocks from=org.apache.hadoop.hive.ql.Driver>
26502 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=releaseLocks start=1466779983963 end=1466779983963 duration=0 from=org.apache.hadoop.hive.ql.Driver>
26502 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=Driver.run start=1466779983905 end=1466779983963 duration=58 from=org.apache.hadoop.hive.ql.Driver>
26503 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=Driver.run from=org.apache.hadoop.hive.ql.Driver>
26503 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=TimeToSubmit from=org.apache.hadoop.hive.ql.Driver>
26503 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=compile from=org.apache.hadoop.hive.ql.Driver>
26504 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=parse from=org.apache.hadoop.hive.ql.Driver>
26504 [hiveWriterTest] INFO  h.q.p.ParseDriver - Parsing command: alter table test_table2 add if not exists partition  ( city='sunnyvale',state='ca' )
26507 [hiveWriterTest] INFO  h.q.p.ParseDriver - Parse Completed
26507 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=parse start=1466779983965 end=1466779983968 duration=3 from=org.apache.hadoop.hive.ql.Driver>
26510 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=semanticAnalyze from=org.apache.hadoop.hive.ql.Driver>
26511 [hiveWriterTest] INFO  o.a.h.h.m.HiveMetaStore - 1: get_table : db=testdb tbl=test_table2
26511 [hiveWriterTest] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table : db=testdb tbl=test_table2	
26533 [hiveWriterTest] INFO  o.a.h.h.q.Driver - Semantic Analysis Completed
26533 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=semanticAnalyze start=1466779983971 end=1466779983994 duration=23 from=org.apache.hadoop.hive.ql.Driver>
26533 [hiveWriterTest] INFO  o.a.h.h.q.Driver - Returning Hive schema: Schema(fieldSchemas:null, properties:null)
26533 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=compile start=1466779983964 end=1466779983994 duration=30 from=org.apache.hadoop.hive.ql.Driver>
26533 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=acquireReadWriteLocks from=org.apache.hadoop.hive.ql.Driver>
26560 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=acquireReadWriteLocks start=1466779983994 end=1466779984021 duration=27 from=org.apache.hadoop.hive.ql.Driver>
26560 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=Driver.execute from=org.apache.hadoop.hive.ql.Driver>
26560 [hiveWriterTest] INFO  o.a.h.h.q.Driver - Starting command: alter table test_table2 add if not exists partition  ( city='sunnyvale',state='ca' )
26561 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=TimeToSubmit start=1466779983964 end=1466779984022 duration=58 from=org.apache.hadoop.hive.ql.Driver>
26561 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=runTasks from=org.apache.hadoop.hive.ql.Driver>
26561 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=task.DDL.Stage-0 from=org.apache.hadoop.hive.ql.Driver>
26561 [hiveWriterTest] INFO  o.a.h.h.q.Driver - Starting task [Stage-0:DDL] in serial mode
26561 [hiveWriterTest] INFO  o.a.h.h.m.HiveMetaStore - 1: get_table : db=testdb tbl=test_table2
26562 [hiveWriterTest] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table : db=testdb tbl=test_table2	
26579 [hiveWriterTest] INFO  o.a.h.h.m.HiveMetaStore - 1: add_partitions
26579 [hiveWriterTest] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=add_partitions	
26607 [hiveWriterTest] INFO  o.a.h.h.m.HiveMetaStore - Not adding partition Partition(values:[sunnyvale, ca], dbName:testdb, tableName:test_table2, createTime:0, lastAccessTime:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:), FieldSchema(name:msg, type:string, comment:)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:1, serdeInfo:SerDeInfo(name:test_table2, serializationLib:org.apache.hadoop.hive.ql.io.orc.OrcSerde, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), parameters:null) as it already exists
26609 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=runTasks start=1466779984022 end=1466779984070 duration=48 from=org.apache.hadoop.hive.ql.Driver>
26609 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=Driver.execute start=1466779984021 end=1466779984070 duration=49 from=org.apache.hadoop.hive.ql.Driver>
26609 [hiveWriterTest] INFO  o.a.h.h.q.Driver - OK
26609 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=releaseLocks from=org.apache.hadoop.hive.ql.Driver>
26617 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=releaseLocks start=1466779984070 end=1466779984078 duration=8 from=org.apache.hadoop.hive.ql.Driver>
26617 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=Driver.run start=1466779983964 end=1466779984078 duration=114 from=org.apache.hadoop.hive.ql.Driver>
26663 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_table : db=testdb tbl=test_table2
26663 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table : db=testdb tbl=test_table2	
26663 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
26664 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
26665 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
26666 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
26667 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
26678 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_partition : db=testdb tbl=test_table2[sunnyvale,ca]
26679 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_partition : db=testdb tbl=test_table2[sunnyvale,ca]	
26704 [hiveWriterTest] INFO  o.a.h.h.m.HiveMetaStore - 1: get_table : db=testdb tbl=test_table2
26704 [hiveWriterTest] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table : db=testdb tbl=test_table2	

-------------------- system-err --------------------
OK
OK

--------------------------------------------------
--------------------------------------------------
classname: org.apache.storm.hive.common.TestHiveWriter / testname: testWriteMultiFlush
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hive.common.TestHiveWriter.generateTestTuple(TestHiveWriter.java:164)
	at org.apache.storm.hive.common.TestHiveWriter.testWriteMultiFlush(TestHiveWriter.java:142)

-------------------- system-out --------------------
27185 [main] INFO  o.a.h.h.q.s.SessionState - Created local directory: /tmp/ef42c7a3-5531-489b-88d7-faf7f867ba91_resources
27193 [main] INFO  o.a.h.h.q.s.SessionState - Created HDFS directory: /tmp/hive/travis/ef42c7a3-5531-489b-88d7-faf7f867ba91
27195 [main] INFO  o.a.h.h.q.s.SessionState - Created local directory: /tmp/travis/ef42c7a3-5531-489b-88d7-faf7f867ba91
27197 [main] INFO  o.a.h.h.q.s.SessionState - Created HDFS directory: /tmp/hive/travis/ef42c7a3-5531-489b-88d7-faf7f867ba91/_tmp_space.db
27197 [main] INFO  o.a.h.h.q.s.SessionState - No Tez session required at this point. hive.execution.engine=mr.
27198 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
27198 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
27200 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
27200 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
27201 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_table_names_by_filter: db = testdb, filter = 
27201 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table_names_by_filter: db = testdb, filter = 	
27208 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_table : db=testdb tbl=test_table2
27208 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table : db=testdb tbl=test_table2	
27235 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: drop_table : db=testdb tbl=test_table2
27235 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=drop_table : db=testdb tbl=test_table2	
27468 [main] INFO  h.m.hivemetastoressimpl - deleting  file:/tmp/junit7876091414799495689/testdb.db/test_table2
27468 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_database: testdb
27468 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_database: testdb	
27484 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: drop_database: testdb
27484 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=drop_database: testdb	
27485 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_all_tables: db=testdb
27485 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_all_tables: db=testdb	
27489 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_functions: db=testdb pat=*
27489 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_functions: db=testdb pat=*	
27495 [main] INFO  o.a.h.h.m.ObjectStore - Dropping database testdb along with all tables
27511 [main] INFO  h.m.hivemetastoressimpl - deleting  file:/tmp/junit7876091414799495689/testdb.db
27512 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
27512 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
27522 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
27522 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
27522 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: create_database: Database(name:testdb, description:null, locationUri:/tmp/junit8126053829846467962/testdb.db, parameters:null)
27523 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=create_database: Database(name:testdb, description:null, locationUri:/tmp/junit8126053829846467962/testdb.db, parameters:null)	
27530 [main] WARN  o.a.h.h.m.ObjectStore - Failed to get database testdb, returning NoSuchObjectException
27531 [main] INFO  o.a.h.h.c.FileUtils - Creating directory if it doesn't exist: file:/tmp/junit8126053829846467962/testdb.db
27540 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: create_table: Table(tableName:test_table2, dbName:testdb, owner:null, createTime:0, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:), FieldSchema(name:msg, type:string, comment:)], location:/tmp/junit8126053829846467962/testdb.db/test_table2, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:1, serdeInfo:SerDeInfo(name:test_table2, serializationLib:org.apache.hadoop.hive.ql.io.orc.OrcSerde, parameters:{serialization.format=1}), bucketCols:[], sortCols:null, parameters:null), partitionKeys:[FieldSchema(name:city, type:string, comment:), FieldSchema(name:state, type:string, comment:)], parameters:{}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)
27541 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=create_table: Table(tableName:test_table2, dbName:testdb, owner:null, createTime:0, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:), FieldSchema(name:msg, type:string, comment:)], location:/tmp/junit8126053829846467962/testdb.db/test_table2, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:1, serdeInfo:SerDeInfo(name:test_table2, serializationLib:org.apache.hadoop.hive.ql.io.orc.OrcSerde, parameters:{serialization.format=1}), bucketCols:[], sortCols:null, parameters:null), partitionKeys:[FieldSchema(name:city, type:string, comment:), FieldSchema(name:state, type:string, comment:)], parameters:{}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
27542 [main] WARN  o.a.h.h.m.HiveMetaStore - Location: /tmp/junit8126053829846467962/testdb.db/test_table2 specified for non-external table:test_table2
27542 [main] INFO  o.a.h.h.c.FileUtils - Creating directory if it doesn't exist: file:/tmp/junit8126053829846467962/testdb.db/test_table2
27595 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: add_partition : db=testdb tbl=test_table2
27595 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=add_partition : db=testdb tbl=test_table2	
27633 [main] INFO  o.a.h.h.c.FileUtils - Creating directory if it doesn't exist: file:/tmp/junit8126053829846467962/testdb.db/test_table2/city=sunnyvale/state=ca
27669 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Shutting down the object store...
27669 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=Shutting down the object store...	
27669 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Metastore shutdown complete.
27669 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=Metastore shutdown complete.	
27734 [hiveWriterTest] INFO  o.a.h.h.q.s.SessionState - Created local directory: /tmp/cb44b8d9-73a1-4854-aaa5-a03cddfabadc_resources
27736 [hiveWriterTest] INFO  o.a.h.h.q.s.SessionState - Created HDFS directory: /tmp/hive/travis/cb44b8d9-73a1-4854-aaa5-a03cddfabadc
27738 [hiveWriterTest] INFO  o.a.h.h.q.s.SessionState - Created local directory: /tmp/travis/cb44b8d9-73a1-4854-aaa5-a03cddfabadc
27740 [hiveWriterTest] INFO  o.a.h.h.q.s.SessionState - Created HDFS directory: /tmp/hive/travis/cb44b8d9-73a1-4854-aaa5-a03cddfabadc/_tmp_space.db
27740 [hiveWriterTest] INFO  o.a.h.h.q.s.SessionState - No Tez session required at this point. hive.execution.engine=mr.
27740 [hiveWriterTest] INFO  o.a.h.h.m.HiveMetaStore - 2: get_table : db=testdb tbl=test_table2
27740 [hiveWriterTest] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table : db=testdb tbl=test_table2	
27740 [hiveWriterTest] INFO  o.a.h.h.m.HiveMetaStore - 2: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
27741 [hiveWriterTest] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
27742 [hiveWriterTest] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
27744 [hiveWriterTest] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
27744 [hiveWriterTest] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
27791 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=Driver.run from=org.apache.hadoop.hive.ql.Driver>
27791 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=TimeToSubmit from=org.apache.hadoop.hive.ql.Driver>
27791 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=compile from=org.apache.hadoop.hive.ql.Driver>
27795 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=parse from=org.apache.hadoop.hive.ql.Driver>
27795 [hiveWriterTest] INFO  h.q.p.ParseDriver - Parsing command: use testdb
27796 [hiveWriterTest] INFO  h.q.p.ParseDriver - Parse Completed
27796 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=parse start=1466779985256 end=1466779985257 duration=1 from=org.apache.hadoop.hive.ql.Driver>
27804 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=semanticAnalyze from=org.apache.hadoop.hive.ql.Driver>
27805 [hiveWriterTest] INFO  o.a.h.h.m.HiveMetaStore - 2: get_database: testdb
27805 [hiveWriterTest] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_database: testdb	
27806 [hiveWriterTest] INFO  o.a.h.h.q.Driver - Semantic Analysis Completed
27806 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=semanticAnalyze start=1466779985265 end=1466779985267 duration=2 from=org.apache.hadoop.hive.ql.Driver>
27807 [hiveWriterTest] INFO  o.a.h.h.q.Driver - Returning Hive schema: Schema(fieldSchemas:null, properties:null)
27807 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=compile start=1466779985252 end=1466779985268 duration=16 from=org.apache.hadoop.hive.ql.Driver>
27807 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=acquireReadWriteLocks from=org.apache.hadoop.hive.ql.Driver>
27807 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=acquireReadWriteLocks start=1466779985268 end=1466779985268 duration=0 from=org.apache.hadoop.hive.ql.Driver>
27807 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=Driver.execute from=org.apache.hadoop.hive.ql.Driver>
27807 [hiveWriterTest] INFO  o.a.h.h.q.Driver - Starting command: use testdb
27807 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=TimeToSubmit start=1466779985252 end=1466779985268 duration=16 from=org.apache.hadoop.hive.ql.Driver>
27807 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=runTasks from=org.apache.hadoop.hive.ql.Driver>
27807 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=task.DDL.Stage-0 from=org.apache.hadoop.hive.ql.Driver>
27808 [hiveWriterTest] INFO  o.a.h.h.q.Driver - Starting task [Stage-0:DDL] in serial mode
27808 [hiveWriterTest] INFO  o.a.h.h.m.HiveMetaStore - 2: get_database: testdb
27808 [hiveWriterTest] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_database: testdb	
27809 [hiveWriterTest] INFO  o.a.h.h.m.HiveMetaStore - 2: get_database: testdb
27809 [hiveWriterTest] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_database: testdb	
27810 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=runTasks start=1466779985268 end=1466779985271 duration=3 from=org.apache.hadoop.hive.ql.Driver>
27810 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=Driver.execute start=1466779985268 end=1466779985271 duration=3 from=org.apache.hadoop.hive.ql.Driver>
27811 [hiveWriterTest] INFO  o.a.h.h.q.Driver - OK
27811 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=releaseLocks from=org.apache.hadoop.hive.ql.Driver>
27811 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=releaseLocks start=1466779985272 end=1466779985272 duration=0 from=org.apache.hadoop.hive.ql.Driver>
27811 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=Driver.run start=1466779985252 end=1466779985272 duration=20 from=org.apache.hadoop.hive.ql.Driver>
27811 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=Driver.run from=org.apache.hadoop.hive.ql.Driver>
27811 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=TimeToSubmit from=org.apache.hadoop.hive.ql.Driver>
27811 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=compile from=org.apache.hadoop.hive.ql.Driver>
27812 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=parse from=org.apache.hadoop.hive.ql.Driver>
27812 [hiveWriterTest] INFO  h.q.p.ParseDriver - Parsing command: alter table test_table2 add if not exists partition  ( city='sunnyvale',state='ca' )
27812 [hiveWriterTest] INFO  h.q.p.ParseDriver - Parse Completed
27812 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=parse start=1466779985273 end=1466779985273 duration=0 from=org.apache.hadoop.hive.ql.Driver>
27816 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=semanticAnalyze from=org.apache.hadoop.hive.ql.Driver>
27816 [hiveWriterTest] INFO  o.a.h.h.m.HiveMetaStore - 2: get_table : db=testdb tbl=test_table2
27816 [hiveWriterTest] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table : db=testdb tbl=test_table2	
27827 [hiveWriterTest] INFO  o.a.h.h.q.Driver - Semantic Analysis Completed
27827 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=semanticAnalyze start=1466779985277 end=1466779985288 duration=11 from=org.apache.hadoop.hive.ql.Driver>
27827 [hiveWriterTest] INFO  o.a.h.h.q.Driver - Returning Hive schema: Schema(fieldSchemas:null, properties:null)
27827 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=compile start=1466779985272 end=1466779985288 duration=16 from=org.apache.hadoop.hive.ql.Driver>
27828 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=acquireReadWriteLocks from=org.apache.hadoop.hive.ql.Driver>
27884 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=acquireReadWriteLocks start=1466779985288 end=1466779985345 duration=57 from=org.apache.hadoop.hive.ql.Driver>
27884 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=Driver.execute from=org.apache.hadoop.hive.ql.Driver>
27884 [hiveWriterTest] INFO  o.a.h.h.q.Driver - Starting command: alter table test_table2 add if not exists partition  ( city='sunnyvale',state='ca' )
27884 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=TimeToSubmit start=1466779985272 end=1466779985345 duration=73 from=org.apache.hadoop.hive.ql.Driver>
27885 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=runTasks from=org.apache.hadoop.hive.ql.Driver>
27885 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=task.DDL.Stage-0 from=org.apache.hadoop.hive.ql.Driver>
27885 [hiveWriterTest] INFO  o.a.h.h.q.Driver - Starting task [Stage-0:DDL] in serial mode
27885 [hiveWriterTest] INFO  o.a.h.h.m.HiveMetaStore - 2: get_table : db=testdb tbl=test_table2
27886 [hiveWriterTest] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table : db=testdb tbl=test_table2	
27905 [hiveWriterTest] INFO  o.a.h.h.m.HiveMetaStore - 2: add_partitions
27906 [hiveWriterTest] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=add_partitions	
27929 [hiveWriterTest] INFO  o.a.h.h.m.HiveMetaStore - Not adding partition Partition(values:[sunnyvale, ca], dbName:testdb, tableName:test_table2, createTime:0, lastAccessTime:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:), FieldSchema(name:msg, type:string, comment:)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:1, serdeInfo:SerDeInfo(name:test_table2, serializationLib:org.apache.hadoop.hive.ql.io.orc.OrcSerde, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), parameters:null) as it already exists
27936 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=runTasks start=1466779985346 end=1466779985397 duration=51 from=org.apache.hadoop.hive.ql.Driver>
27936 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=Driver.execute start=1466779985345 end=1466779985397 duration=52 from=org.apache.hadoop.hive.ql.Driver>
27936 [hiveWriterTest] INFO  o.a.h.h.q.Driver - OK
27936 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=releaseLocks from=org.apache.hadoop.hive.ql.Driver>
27946 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=releaseLocks start=1466779985397 end=1466779985407 duration=10 from=org.apache.hadoop.hive.ql.Driver>
27946 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=Driver.run start=1466779985272 end=1466779985407 duration=135 from=org.apache.hadoop.hive.ql.Driver>
27981 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_table : db=testdb tbl=test_table2
27982 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table : db=testdb tbl=test_table2	
27982 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
27996 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
27997 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
27999 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
27999 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
28026 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_partition : db=testdb tbl=test_table2[sunnyvale,ca]
28026 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_partition : db=testdb tbl=test_table2[sunnyvale,ca]	
28054 [hiveWriterTest] INFO  o.a.h.h.m.HiveMetaStore - 2: get_table : db=testdb tbl=test_table2
28055 [hiveWriterTest] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table : db=testdb tbl=test_table2	

-------------------- system-err --------------------
OK
OK

--------------------------------------------------
Looking for errors in ./external/storm-jdbc/target/surefire-reports
Checking ./external/storm-jdbc/target/surefire-reports/TEST-org.apache.storm.jdbc.bolt.JdbcInsertBoltTest.xml
Checking ./external/storm-jdbc/target/surefire-reports/TEST-org.apache.storm.jdbc.bolt.JdbcLookupBoltTest.xml
Checking ./external/storm-jdbc/target/surefire-reports/TEST-org.apache.storm.jdbc.common.UtilTest.xml
Checking ./external/storm-jdbc/target/surefire-reports/TEST-org.apache.storm.jdbc.common.JdbcClientTest.xml
Looking for errors in ./external/storm-kafka/target/surefire-reports
Checking ./external/storm-kafka/target/surefire-reports/TEST-org.apache.storm.kafka.KafkaErrorTest.xml
Checking ./external/storm-kafka/target/surefire-reports/TEST-org.apache.storm.kafka.KafkaUtilsTest.xml
Checking ./external/storm-kafka/target/surefire-reports/TEST-org.apache.storm.kafka.StringKeyValueSchemeTest.xml
Checking ./external/storm-kafka/target/surefire-reports/TEST-org.apache.storm.kafka.ZkCoordinatorTest.xml
Checking ./external/storm-kafka/target/surefire-reports/TEST-org.apache.storm.kafka.TridentKafkaTest.xml
Checking ./external/storm-kafka/target/surefire-reports/TEST-org.apache.storm.kafka.ExponentialBackoffMsgRetryManagerTest.xml
Checking ./external/storm-kafka/target/surefire-reports/TEST-org.apache.storm.kafka.TestStringScheme.xml
Checking ./external/storm-kafka/target/surefire-reports/TEST-org.apache.storm.kafka.DynamicBrokersReaderTest.xml
Checking ./external/storm-kafka/target/surefire-reports/TEST-org.apache.storm.kafka.bolt.KafkaBoltTest.xml
--------------------------------------------------
classname: org.apache.storm.kafka.bolt.KafkaBoltTest / testname: executeWithBrokerDown
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.kafka.bolt.KafkaBoltTest.generateTestTuple(KafkaBoltTest.java:301)
	at org.apache.storm.kafka.bolt.KafkaBoltTest.executeWithBrokerDown(KafkaBoltTest.java:266)

-------------------- system-out --------------------
14:50:57.223 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-021d9b47-3350-linux-15.prod.travis-ci.org/172.17.9.145
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
14:50:57.233 [Curator-Framework-0-SendThread(127.0.0.1:43546)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
14:50:57.255 [Curator-Framework-0] ERROR o.a.c.f.i.CuratorFrameworkImpl - Background operation retry gave up
org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:99) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.curator.framework.imps.CuratorFrameworkImpl.checkBackgroundRetry(CuratorFrameworkImpl.java:728) [curator-framework-2.10.0.jar:?]
	at org.apache.curator.framework.imps.CuratorFrameworkImpl.performBackgroundOperation(CuratorFrameworkImpl.java:857) [curator-framework-2.10.0.jar:?]
	at org.apache.curator.framework.imps.CuratorFrameworkImpl.backgroundOperationsLoop(CuratorFrameworkImpl.java:809) [curator-framework-2.10.0.jar:?]
	at org.apache.curator.framework.imps.CuratorFrameworkImpl.access$300(CuratorFrameworkImpl.java:64) [curator-framework-2.10.0.jar:?]
	at org.apache.curator.framework.imps.CuratorFrameworkImpl$4.call(CuratorFrameworkImpl.java:267) [curator-framework-2.10.0.jar:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:262) [?:1.7.0_76]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:178) [?:1.7.0_76]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:292) [?:1.7.0_76]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [?:1.7.0_76]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [?:1.7.0_76]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
14:50:57.255 [Curator-Framework-0] ERROR o.a.c.f.i.CuratorFrameworkImpl - Background retry gave up
org.apache.curator.CuratorConnectionLossException: KeeperErrorCode = ConnectionLoss
	at org.apache.curator.framework.imps.CuratorFrameworkImpl.performBackgroundOperation(CuratorFrameworkImpl.java:838) [curator-framework-2.10.0.jar:?]
	at org.apache.curator.framework.imps.CuratorFrameworkImpl.backgroundOperationsLoop(CuratorFrameworkImpl.java:809) [curator-framework-2.10.0.jar:?]
	at org.apache.curator.framework.imps.CuratorFrameworkImpl.access$300(CuratorFrameworkImpl.java:64) [curator-framework-2.10.0.jar:?]
	at org.apache.curator.framework.imps.CuratorFrameworkImpl$4.call(CuratorFrameworkImpl.java:267) [curator-framework-2.10.0.jar:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:262) [?:1.7.0_76]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:178) [?:1.7.0_76]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:292) [?:1.7.0_76]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [?:1.7.0_76]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [?:1.7.0_76]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
14:50:57.264 [main-EventThread] INFO  o.a.c.f.s.ConnectionStateManager - State change: SUSPENDED
14:50:57.324 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-021d9b47-3350-linux-15.prod.travis-ci.org/172.17.9.145
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
14:50:57.345 [Curator-Framework-0] WARN  o.a.c.ConnectionState - Connection attempt unsuccessful after 1427 (greater than max timeout of 1000). Resetting connection and trying again with a new connection.
14:50:57.425 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-021d9b47-3350-linux-15.prod.travis-ci.org/172.17.9.145
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
14:50:57.526 [Curator-Framework-0-SendThread(127.0.0.1:58001)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
14:50:57.526 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-021d9b47-3350-linux-15.prod.travis-ci.org/172.17.9.145
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
14:50:57.546 [main-SendThread(localhost:37688)] WARN  o.a.z.ClientCnxn - Session 0x15582e444330000 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
14:50:57.547 [Curator-Framework-0] INFO  o.a.c.f.s.ConnectionStateManager - State change: LOST
14:50:57.547 [Curator-Framework-0] ERROR o.a.c.f.i.CuratorFrameworkImpl - Background operation retry gave up
org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:99) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.curator.framework.imps.CuratorFrameworkImpl.checkBackgroundRetry(CuratorFrameworkImpl.java:728) [curator-framework-2.10.0.jar:?]
	at org.apache.curator.framework.imps.CuratorFrameworkImpl.performBackgroundOperation(CuratorFrameworkImpl.java:857) [curator-framework-2.10.0.jar:?]
	at org.apache.curator.framework.imps.CuratorFrameworkImpl.backgroundOperationsLoop(CuratorFrameworkImpl.java:809) [curator-framework-2.10.0.jar:?]
	at org.apache.curator.framework.imps.CuratorFrameworkImpl.access$300(CuratorFrameworkImpl.java:64) [curator-framework-2.10.0.jar:?]
	at org.apache.curator.framework.imps.CuratorFrameworkImpl$4.call(CuratorFrameworkImpl.java:267) [curator-framework-2.10.0.jar:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:262) [?:1.7.0_76]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:178) [?:1.7.0_76]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:292) [?:1.7.0_76]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [?:1.7.0_76]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [?:1.7.0_76]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
14:50:57.547 [Curator-Framework-0] ERROR o.a.c.f.i.CuratorFrameworkImpl - Background retry gave up
org.apache.curator.CuratorConnectionLossException: KeeperErrorCode = ConnectionLoss
	at org.apache.curator.framework.imps.CuratorFrameworkImpl.performBackgroundOperation(CuratorFrameworkImpl.java:838) [curator-framework-2.10.0.jar:?]
	at org.apache.curator.framework.imps.CuratorFrameworkImpl.backgroundOperationsLoop(CuratorFrameworkImpl.java:809) [curator-framework-2.10.0.jar:?]
	at org.apache.curator.framework.imps.CuratorFrameworkImpl.access$300(CuratorFrameworkImpl.java:64) [curator-framework-2.10.0.jar:?]
	at org.apache.curator.framework.imps.CuratorFrameworkImpl$4.call(CuratorFrameworkImpl.java:267) [curator-framework-2.10.0.jar:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:262) [?:1.7.0_76]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:178) [?:1.7.0_76]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:292) [?:1.7.0_76]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [?:1.7.0_76]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [?:1.7.0_76]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
14:50:57.554 [Curator-Framework-0-SendThread(127.0.0.1:41155)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
14:50:57.627 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-021d9b47-3350-linux-15.prod.travis-ci.org/172.17.9.145
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
14:50:57.728 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-021d9b47-3350-linux-15.prod.travis-ci.org/172.17.9.145
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
14:50:57.797 [Curator-Framework-0-SendThread(127.0.0.1:60595)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
14:50:57.807 [Curator-Framework-0-SendThread(127.0.0.1:37288)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
14:50:57.829 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-021d9b47-3350-linux-15.prod.travis-ci.org/172.17.9.145
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
14:50:57.930 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-021d9b47-3350-linux-15.prod.travis-ci.org/172.17.9.145
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
14:50:58.031 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-021d9b47-3350-linux-15.prod.travis-ci.org/172.17.9.145
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
14:50:58.132 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-021d9b47-3350-linux-15.prod.travis-ci.org/172.17.9.145
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
14:50:58.153 [main-SendThread(localhost:45310)] WARN  o.a.z.ClientCnxn - Session 0x15582e435570000 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
14:50:58.199 [main] INFO  o.a.c.f.i.CuratorFrameworkImpl - Starting
14:50:58.201 [main] INFO  k.u.VerifiableProperties - Verifying properties
14:50:58.201 [main] INFO  k.u.VerifiableProperties - Property broker.id is overridden to 0
14:50:58.201 [main] INFO  k.u.VerifiableProperties - Property log.dirs is overridden to /tmp/kafka/logs/kafka-test-59284
14:50:58.201 [main] INFO  k.u.VerifiableProperties - Property port is overridden to 59284
14:50:58.201 [main] INFO  k.u.VerifiableProperties - Property zookeeper.connect is overridden to 127.0.0.1:60891
14:50:58.201 [main] INFO  k.s.KafkaServer - [Kafka Server 0], starting
14:50:58.201 [main] INFO  k.s.KafkaServer - [Kafka Server 0], Connecting to zookeeper on 127.0.0.1:60891
14:50:58.202 [ZkClient-EventThread-939-127.0.0.1:60891] INFO  o.I.z.ZkEventThread - Starting ZkClient event thread.
14:50:58.212 [main-EventThread] INFO  o.a.c.f.s.ConnectionStateManager - State change: CONNECTED
14:50:58.213 [main-EventThread] INFO  o.I.z.ZkClient - zookeeper state changed (SyncConnected)
14:50:58.225 [main] INFO  k.l.LogManager - Log directory '/tmp/kafka/logs/kafka-test-59284' not found, creating it.
14:50:58.226 [main] INFO  k.l.LogManager - Loading logs.
14:50:58.226 [main] INFO  k.l.LogManager - Logs loading complete.
14:50:58.226 [main] INFO  k.l.LogManager - Starting log cleanup with a period of 300000 ms.
14:50:58.226 [main] INFO  k.l.LogManager - Starting log flusher with a default period of 9223372036854775807 ms.
14:50:58.228 [main] INFO  k.n.Acceptor - Awaiting socket connections on 0.0.0.0:59284.
14:50:58.228 [main] INFO  k.n.SocketServer - [Socket Server on Broker 0], Started
14:50:58.232 [main] INFO  k.u.Mx4jLoader$ - Will not load MX4J, mx4j-tools.jar is not in the classpath
14:50:58.232 [main] INFO  k.c.KafkaController - [Controller 0]: Controller starting up
14:50:58.233 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-021d9b47-3350-linux-15.prod.travis-ci.org/172.17.9.145
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
14:50:58.235 [main] INFO  k.s.ZookeeperLeaderElector - 0 successfully elected as leader
14:50:58.236 [main] INFO  k.c.KafkaController - [Controller 0]: Broker 0 starting become controller state transition
14:50:58.238 [main] INFO  k.c.KafkaController - [Controller 0]: Controller 0 incremented epoch to 1
14:50:58.241 [main] INFO  k.c.KafkaController - [Controller 0]: Partitions undergoing preferred replica election: 
14:50:58.241 [main] INFO  k.c.KafkaController - [Controller 0]: Partitions that completed preferred replica election: 
14:50:58.241 [main] INFO  k.c.KafkaController - [Controller 0]: Resuming preferred replica election for partitions: 
14:50:58.241 [main] INFO  k.c.KafkaController - [Controller 0]: Partitions being reassigned: Map()
14:50:58.241 [main] INFO  k.c.KafkaController - [Controller 0]: Partitions already reassigned: List()
14:50:58.241 [main] INFO  k.c.KafkaController - [Controller 0]: Resuming reassignment of partitions: Map()
14:50:58.242 [main] INFO  k.c.KafkaController - [Controller 0]: List of topics to be deleted: 
14:50:58.242 [main] INFO  k.c.KafkaController - [Controller 0]: List of topics ineligible for deletion: 
14:50:58.242 [main] INFO  k.c.KafkaController - [Controller 0]: Currently active brokers in the cluster: Set()
14:50:58.242 [main] INFO  k.c.KafkaController - [Controller 0]: Currently shutting brokers in the cluster: Set()
14:50:58.242 [main] INFO  k.c.KafkaController - [Controller 0]: Current list of topics in the cluster: Set()
14:50:58.242 [main] INFO  k.c.ReplicaStateMachine - [Replica state machine on controller 0]: Started replica state machine with initial state -> Map()
14:50:58.242 [main] INFO  k.c.PartitionStateMachine - [Partition state machine on Controller 0]: Started partition state machine with initial state -> Map()
14:50:58.242 [main] INFO  k.c.KafkaController - [Controller 0]: Broker 0 is ready to serve as the new controller with epoch 1
14:50:58.242 [main] INFO  k.c.KafkaController - [Controller 0]: Starting preferred replica leader election for partitions 
14:50:58.242 [main] INFO  k.c.PartitionStateMachine - [Partition state machine on Controller 0]: Invoking state change to OnlinePartition for partitions 
14:50:58.244 [main] INFO  k.c.KafkaController - [Controller 0]: starting the partition rebalance scheduler
14:50:58.249 [main] INFO  k.c.KafkaController - [Controller 0]: Controller startup complete
14:50:58.250 [ZkClient-EventThread-939-127.0.0.1:60891] INFO  k.s.ZookeeperLeaderElector$LeaderChangeListener - New leader is 0
14:50:58.253 [main] INFO  k.u.ZkUtils$ - Registered broker 0 at path /brokers/ids/0 with address testing-worker-linux-docker-021d9b47-3350-linux-15.prod.travis-ci.org:59284.
14:50:58.253 [main] INFO  k.s.KafkaServer - [Kafka Server 0], started
14:50:58.254 [ZkClient-EventThread-939-127.0.0.1:60891] INFO  k.c.ReplicaStateMachine$BrokerChangeListener - [BrokerChangeListener on Controller 0]: Broker change listener fired for path /brokers/ids with children 0
14:50:58.255 [Curator-Framework-0] WARN  o.a.c.ConnectionState - Connection attempt unsuccessful after 1134 (greater than max timeout of 1000). Resetting connection and trying again with a new connection.
14:50:58.256 [main] INFO  o.a.k.c.p.ProducerConfig - ProducerConfig values: 
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	block.on.buffer.full = true
	retry.backoff.ms = 100
	buffer.memory = 33554432
	batch.size = 16384
	metrics.sample.window.ms = 30000
	metadata.max.age.ms = 300000
	receive.buffer.bytes = 32768
	timeout.ms = 30000
	max.in.flight.requests.per.connection = 5
	bootstrap.servers = [localhost:59284]
	metric.reporters = []
	client.id = 
	compression.type = none
	retries = 0
	max.request.size = 1048576
	send.buffer.bytes = 131072
	acks = 1
	reconnect.backoff.ms = 10
	linger.ms = 0
	metrics.num.samples = 2
	metadata.fetch.timeout.ms = 1000

14:50:58.260 [main] INFO  k.s.KafkaServer - [Kafka Server 0], shutting down
14:50:58.261 [main] INFO  k.s.KafkaServer - [Kafka Server 0], Starting controlled shutdown
14:50:58.263 [ZkClient-EventThread-939-127.0.0.1:60891] INFO  k.c.ReplicaStateMachine$BrokerChangeListener - [BrokerChangeListener on Controller 0]: Newly added brokers: 0, deleted brokers: , all live brokers: 0
14:50:58.263 [ZkClient-EventThread-939-127.0.0.1:60891] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Controller 0 connected to id:0,host:testing-worker-linux-docker-021d9b47-3350-linux-15.prod.travis-ci.org,port:59284 for sending state change requests
14:50:58.264 [ZkClient-EventThread-939-127.0.0.1:60891] INFO  k.c.KafkaController - [Controller 0]: New broker startup callback for 0
14:50:58.264 [Controller-0-to-broker-0-send-thread] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Starting 
14:50:58.264 [Curator-Framework-0] ERROR o.a.c.ConnectionState - Connection timed out for connection string (127.0.0.1:54006) and timeout (1000) / elapsed (1000)
org.apache.curator.CuratorConnectionLossException: KeeperErrorCode = ConnectionLoss
	at org.apache.curator.ConnectionState.checkTimeouts(ConnectionState.java:197) [curator-client-2.10.0.jar:?]
	at org.apache.curator.ConnectionState.getZooKeeper(ConnectionState.java:88) [curator-client-2.10.0.jar:?]
	at org.apache.curator.CuratorZookeeperClient.getZooKeeper(CuratorZookeeperClient.java:116) [curator-client-2.10.0.jar:?]
	at org.apache.curator.framework.imps.CuratorFrameworkImpl.performBackgroundOperation(CuratorFrameworkImpl.java:835) [curator-framework-2.10.0.jar:?]
	at org.apache.curator.framework.imps.CuratorFrameworkImpl.backgroundOperationsLoop(CuratorFrameworkImpl.java:809) [curator-framework-2.10.0.jar:?]
	at org.apache.curator.framework.imps.CuratorFrameworkImpl.access$300(CuratorFrameworkImpl.java:64) [curator-framework-2.10.0.jar:?]
	at org.apache.curator.framework.imps.CuratorFrameworkImpl$4.call(CuratorFrameworkImpl.java:267) [curator-framework-2.10.0.jar:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:262) [?:1.7.0_76]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:178) [?:1.7.0_76]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:292) [?:1.7.0_76]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [?:1.7.0_76]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [?:1.7.0_76]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
14:50:58.269 [kafka-request-handler-0] INFO  k.c.KafkaController - [Controller 0]: Shutting down broker 0
14:50:58.269 [main] INFO  k.s.KafkaServer - [Kafka Server 0], Controlled shutdown succeeded
14:50:58.269 [main] INFO  k.n.SocketServer - [Socket Server on Broker 0], Shutting down
14:50:58.269 [kafka-network-thread-59284-1] INFO  k.n.Processor - Closing socket connection to /172.17.9.145.
14:50:58.270 [Curator-Framework-0] WARN  o.a.c.ConnectionState - Connection attempt unsuccessful after 1006 (greater than max timeout of 1000). Resetting connection and trying again with a new connection.
14:50:58.270 [main] INFO  k.n.SocketServer - [Socket Server on Broker 0], Shutdown completed
14:50:58.270 [main] INFO  k.s.KafkaRequestHandlerPool - [Kafka Request Handler on Broker 0], shutting down
14:50:58.270 [main] INFO  k.s.KafkaRequestHandlerPool - [Kafka Request Handler on Broker 0], shut down completely
14:50:58.333 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-021d9b47-3350-linux-15.prod.travis-ci.org/172.17.9.145
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
14:50:58.382 [Curator-Framework-0-SendThread(127.0.0.1:54006)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
14:50:58.398 [Curator-Framework-0] ERROR o.a.c.f.i.CuratorFrameworkImpl - Background operation retry gave up
org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:99) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.curator.framework.imps.CuratorFrameworkImpl.checkBackgroundRetry(CuratorFrameworkImpl.java:728) [curator-framework-2.10.0.jar:?]
	at org.apache.curator.framework.imps.CuratorFrameworkImpl.performBackgroundOperation(CuratorFrameworkImpl.java:857) [curator-framework-2.10.0.jar:?]
	at org.apache.curator.framework.imps.CuratorFrameworkImpl.backgroundOperationsLoop(CuratorFrameworkImpl.java:809) [curator-framework-2.10.0.jar:?]
	at org.apache.curator.framework.imps.CuratorFrameworkImpl.access$300(CuratorFrameworkImpl.java:64) [curator-framework-2.10.0.jar:?]
	at org.apache.curator.framework.imps.CuratorFrameworkImpl$4.call(CuratorFrameworkImpl.java:267) [curator-framework-2.10.0.jar:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:262) [?:1.7.0_76]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:178) [?:1.7.0_76]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:292) [?:1.7.0_76]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [?:1.7.0_76]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [?:1.7.0_76]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
14:50:58.398 [Curator-Framework-0] ERROR o.a.c.f.i.CuratorFrameworkImpl - Background retry gave up
org.apache.curator.CuratorConnectionLossException: KeeperErrorCode = ConnectionLoss
	at org.apache.curator.framework.imps.CuratorFrameworkImpl.performBackgroundOperation(CuratorFrameworkImpl.java:838) [curator-framework-2.10.0.jar:?]
	at org.apache.curator.framework.imps.CuratorFrameworkImpl.backgroundOperationsLoop(CuratorFrameworkImpl.java:809) [curator-framework-2.10.0.jar:?]
	at org.apache.curator.framework.imps.CuratorFrameworkImpl.access$300(CuratorFrameworkImpl.java:64) [curator-framework-2.10.0.jar:?]
	at org.apache.curator.framework.imps.CuratorFrameworkImpl$4.call(CuratorFrameworkImpl.java:267) [curator-framework-2.10.0.jar:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:262) [?:1.7.0_76]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:178) [?:1.7.0_76]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:292) [?:1.7.0_76]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [?:1.7.0_76]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [?:1.7.0_76]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
14:50:58.434 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-021d9b47-3350-linux-15.prod.travis-ci.org/172.17.9.145
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
14:50:58.435 [Curator-Framework-0-SendThread(127.0.0.1:43546)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
14:50:58.438 [main-SendThread(localhost:37688)] WARN  o.a.z.ClientCnxn - Session 0x15582e444330000 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
14:50:58.456 [Curator-Framework-0] INFO  o.a.c.f.s.ConnectionStateManager - State change: LOST
14:50:58.456 [Curator-Framework-0] ERROR o.a.c.f.i.CuratorFrameworkImpl - Background operation retry gave up
org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:99) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.curator.framework.imps.CuratorFrameworkImpl.checkBackgroundRetry(CuratorFrameworkImpl.java:728) [curator-framework-2.10.0.jar:?]
	at org.apache.curator.framework.imps.CuratorFrameworkImpl.performBackgroundOperation(CuratorFrameworkImpl.java:857) [curator-framework-2.10.0.jar:?]
	at org.apache.curator.framework.imps.CuratorFrameworkImpl.backgroundOperationsLoop(CuratorFrameworkImpl.java:809) [curator-framework-2.10.0.jar:?]
	at org.apache.curator.framework.imps.CuratorFrameworkImpl.access$300(CuratorFrameworkImpl.java:64) [curator-framework-2.10.0.jar:?]
	at org.apache.curator.framework.imps.CuratorFrameworkImpl$4.call(CuratorFrameworkImpl.java:267) [curator-framework-2.10.0.jar:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:262) [?:1.7.0_76]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:178) [?:1.7.0_76]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:292) [?:1.7.0_76]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [?:1.7.0_76]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [?:1.7.0_76]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
14:50:58.457 [Curator-Framework-0] ERROR o.a.c.f.i.CuratorFrameworkImpl - Background retry gave up
org.apache.curator.CuratorConnectionLossException: KeeperErrorCode = ConnectionLoss
	at org.apache.curator.framework.imps.CuratorFrameworkImpl.performBackgroundOperation(CuratorFrameworkImpl.java:838) [curator-framework-2.10.0.jar:?]
	at org.apache.curator.framework.imps.CuratorFrameworkImpl.backgroundOperationsLoop(CuratorFrameworkImpl.java:809) [curator-framework-2.10.0.jar:?]
	at org.apache.curator.framework.imps.CuratorFrameworkImpl.access$300(CuratorFrameworkImpl.java:64) [curator-framework-2.10.0.jar:?]
	at org.apache.curator.framework.imps.CuratorFrameworkImpl$4.call(CuratorFrameworkImpl.java:267) [curator-framework-2.10.0.jar:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:262) [?:1.7.0_76]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:178) [?:1.7.0_76]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:292) [?:1.7.0_76]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [?:1.7.0_76]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [?:1.7.0_76]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
14:50:58.535 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-021d9b47-3350-linux-15.prod.travis-ci.org/172.17.9.145
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
14:50:58.627 [Curator-Framework-0-SendThread(127.0.0.1:58001)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
14:50:58.631 [main] INFO  k.s.ReplicaManager - [Replica Manager on Broker 0]: Shut down
14:50:58.631 [main] INFO  k.s.ReplicaFetcherManager - [ReplicaFetcherManager on broker 0] shutting down
14:50:58.631 [main] INFO  k.s.ReplicaFetcherManager - [ReplicaFetcherManager on broker 0] shutdown completed
14:50:58.631 [main] INFO  k.s.ReplicaManager - [Replica Manager on Broker 0]: Shut down completely
14:50:58.631 [main] INFO  k.l.LogManager - Shutting down.
14:50:58.631 [main] INFO  k.l.LogManager - Shutdown complete.
14:50:58.632 [main] INFO  k.c.PartitionStateMachine - [Partition state machine on Controller 0]: Stopped partition state machine
14:50:58.632 [main] INFO  k.c.ReplicaStateMachine - [Replica state machine on controller 0]: Stopped replica state machine
14:50:58.632 [main] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Shutting down
14:50:58.632 [Controller-0-to-broker-0-send-thread] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Stopped 
14:50:58.632 [main] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Shutdown completed
14:50:58.632 [ZkClient-EventThread-939-127.0.0.1:60891] INFO  o.I.z.ZkEventThread - Terminate ZkClient event thread.
14:50:58.635 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-021d9b47-3350-linux-15.prod.travis-ci.org/172.17.9.145
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
14:50:58.636 [main] INFO  k.s.KafkaServer - [Kafka Server 0], shut down completed
14:50:58.636 [Curator-Framework-0] INFO  o.a.c.f.i.CuratorFrameworkImpl - backgroundOperationsLoop exiting

--------------------------------------------------
--------------------------------------------------
classname: org.apache.storm.kafka.bolt.KafkaBoltTest / testname: executeWithoutKey
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.kafka.bolt.KafkaBoltTest.generateTestTuple(KafkaBoltTest.java:301)
	at org.apache.storm.kafka.bolt.KafkaBoltTest.executeWithoutKey(KafkaBoltTest.java:255)

-------------------- system-out --------------------
14:51:00.148 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-021d9b47-3350-linux-15.prod.travis-ci.org/172.17.9.145
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
14:51:00.188 [main-SendThread(localhost:45310)] WARN  o.a.z.ClientCnxn - Session 0x15582e435570000 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
14:51:00.249 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-021d9b47-3350-linux-15.prod.travis-ci.org/172.17.9.145
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
14:51:00.350 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-021d9b47-3350-linux-15.prod.travis-ci.org/172.17.9.145
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
14:51:00.434 [main-SendThread(localhost:45310)] WARN  o.a.z.ClientCnxn - Session 0x15582e435570000 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
14:51:00.451 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-021d9b47-3350-linux-15.prod.travis-ci.org/172.17.9.145
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
14:51:00.552 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-021d9b47-3350-linux-15.prod.travis-ci.org/172.17.9.145
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
14:51:00.637 [Curator-Framework-0-SendThread(127.0.0.1:43546)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
14:51:00.653 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-021d9b47-3350-linux-15.prod.travis-ci.org/172.17.9.145
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
14:51:00.685 [Curator-Framework-0-SendThread(127.0.0.1:54006)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
14:51:00.754 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-021d9b47-3350-linux-15.prod.travis-ci.org/172.17.9.145
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
14:51:00.810 [main-SendThread(localhost:37688)] WARN  o.a.z.ClientCnxn - Session 0x15582e444330000 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
14:51:00.829 [Curator-Framework-0-SendThread(127.0.0.1:58001)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
14:51:00.855 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-021d9b47-3350-linux-15.prod.travis-ci.org/172.17.9.145
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
14:51:00.856 [Curator-Framework-0-SendThread(127.0.0.1:41155)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
14:51:00.956 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-021d9b47-3350-linux-15.prod.travis-ci.org/172.17.9.145
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
14:51:01.057 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-021d9b47-3350-linux-15.prod.travis-ci.org/172.17.9.145
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
14:51:01.100 [Curator-Framework-0-SendThread(127.0.0.1:60595)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
14:51:01.103 [main] INFO  o.a.c.f.i.CuratorFrameworkImpl - Starting
14:51:01.104 [main] INFO  k.u.VerifiableProperties - Verifying properties
14:51:01.104 [main] INFO  k.u.VerifiableProperties - Property broker.id is overridden to 0
14:51:01.104 [main] INFO  k.u.VerifiableProperties - Property log.dirs is overridden to /tmp/kafka/logs/kafka-test-45744
14:51:01.104 [main] INFO  k.u.VerifiableProperties - Property port is overridden to 45744
14:51:01.104 [main] INFO  k.u.VerifiableProperties - Property zookeeper.connect is overridden to 127.0.0.1:50089
14:51:01.104 [main] INFO  k.s.KafkaServer - [Kafka Server 0], starting
14:51:01.104 [main] INFO  k.s.KafkaServer - [Kafka Server 0], Connecting to zookeeper on 127.0.0.1:50089
14:51:01.105 [ZkClient-EventThread-1017-127.0.0.1:50089] INFO  o.I.z.ZkEventThread - Starting ZkClient event thread.
14:51:01.109 [main-EventThread] INFO  o.I.z.ZkClient - zookeeper state changed (SyncConnected)
14:51:01.110 [Curator-Framework-0-SendThread(127.0.0.1:37288)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
14:51:01.113 [main-EventThread] INFO  o.a.c.f.s.ConnectionStateManager - State change: CONNECTED
14:51:01.158 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-021d9b47-3350-linux-15.prod.travis-ci.org/172.17.9.145
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
14:51:01.164 [main] INFO  k.l.LogManager - Log directory '/tmp/kafka/logs/kafka-test-45744' not found, creating it.
14:51:01.165 [main] INFO  k.l.LogManager - Loading logs.
14:51:01.165 [main] INFO  k.l.LogManager - Logs loading complete.
14:51:01.165 [main] INFO  k.l.LogManager - Starting log cleanup with a period of 300000 ms.
14:51:01.165 [main] INFO  k.l.LogManager - Starting log flusher with a default period of 9223372036854775807 ms.
14:51:01.167 [main] INFO  k.n.Acceptor - Awaiting socket connections on 0.0.0.0:45744.
14:51:01.168 [main] INFO  k.n.SocketServer - [Socket Server on Broker 0], Started
14:51:01.170 [main] INFO  k.u.Mx4jLoader$ - Will not load MX4J, mx4j-tools.jar is not in the classpath
14:51:01.170 [main] INFO  k.c.KafkaController - [Controller 0]: Controller starting up
14:51:01.174 [main] INFO  k.s.ZookeeperLeaderElector - 0 successfully elected as leader
14:51:01.174 [main] INFO  k.c.KafkaController - [Controller 0]: Broker 0 starting become controller state transition
14:51:01.178 [main] INFO  k.c.KafkaController - [Controller 0]: Controller 0 incremented epoch to 1
14:51:01.181 [main] INFO  k.c.KafkaController - [Controller 0]: Partitions undergoing preferred replica election: 
14:51:01.181 [main] INFO  k.c.KafkaController - [Controller 0]: Partitions that completed preferred replica election: 
14:51:01.182 [main] INFO  k.c.KafkaController - [Controller 0]: Resuming preferred replica election for partitions: 
14:51:01.182 [main] INFO  k.c.KafkaController - [Controller 0]: Partitions being reassigned: Map()
14:51:01.182 [main] INFO  k.c.KafkaController - [Controller 0]: Partitions already reassigned: List()
14:51:01.182 [main] INFO  k.c.KafkaController - [Controller 0]: Resuming reassignment of partitions: Map()
14:51:01.183 [main] INFO  k.c.KafkaController - [Controller 0]: List of topics to be deleted: 
14:51:01.183 [main] INFO  k.c.KafkaController - [Controller 0]: List of topics ineligible for deletion: 
14:51:01.183 [main] INFO  k.c.KafkaController - [Controller 0]: Currently active brokers in the cluster: Set()
14:51:01.183 [main] INFO  k.c.KafkaController - [Controller 0]: Currently shutting brokers in the cluster: Set()
14:51:01.183 [main] INFO  k.c.KafkaController - [Controller 0]: Current list of topics in the cluster: Set()
14:51:01.183 [main] INFO  k.c.ReplicaStateMachine - [Replica state machine on controller 0]: Started replica state machine with initial state -> Map()
14:51:01.183 [main] INFO  k.c.PartitionStateMachine - [Partition state machine on Controller 0]: Started partition state machine with initial state -> Map()
14:51:01.183 [main] INFO  k.c.KafkaController - [Controller 0]: Broker 0 is ready to serve as the new controller with epoch 1
14:51:01.183 [main] INFO  k.c.KafkaController - [Controller 0]: Starting preferred replica leader election for partitions 
14:51:01.183 [main] INFO  k.c.PartitionStateMachine - [Partition state machine on Controller 0]: Invoking state change to OnlinePartition for partitions 
14:51:01.185 [main] INFO  k.c.KafkaController - [Controller 0]: starting the partition rebalance scheduler
14:51:01.185 [main] INFO  k.c.KafkaController - [Controller 0]: Controller startup complete
14:51:01.187 [ZkClient-EventThread-1017-127.0.0.1:50089] INFO  k.s.ZookeeperLeaderElector$LeaderChangeListener - New leader is 0
14:51:01.193 [main] INFO  k.u.ZkUtils$ - Registered broker 0 at path /brokers/ids/0 with address testing-worker-linux-docker-021d9b47-3350-linux-15.prod.travis-ci.org:45744.
14:51:01.193 [main] INFO  k.s.KafkaServer - [Kafka Server 0], started
14:51:01.193 [main] INFO  o.a.k.c.p.ProducerConfig - ProducerConfig values: 
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	block.on.buffer.full = true
	retry.backoff.ms = 100
	buffer.memory = 33554432
	batch.size = 16384
	metrics.sample.window.ms = 30000
	metadata.max.age.ms = 300000
	receive.buffer.bytes = 32768
	timeout.ms = 30000
	max.in.flight.requests.per.connection = 5
	bootstrap.servers = [localhost:45744]
	metric.reporters = []
	client.id = 
	compression.type = none
	retries = 0
	max.request.size = 1048576
	send.buffer.bytes = 131072
	acks = 1
	reconnect.backoff.ms = 10
	linger.ms = 0
	metrics.num.samples = 2
	metadata.fetch.timeout.ms = 1000

14:51:01.197 [main] INFO  k.s.KafkaServer - [Kafka Server 0], shutting down
14:51:01.197 [ZkClient-EventThread-1017-127.0.0.1:50089] INFO  k.c.ReplicaStateMachine$BrokerChangeListener - [BrokerChangeListener on Controller 0]: Broker change listener fired for path /brokers/ids with children 0
14:51:01.197 [main] INFO  k.s.KafkaServer - [Kafka Server 0], Starting controlled shutdown
14:51:01.204 [ZkClient-EventThread-1017-127.0.0.1:50089] INFO  k.c.ReplicaStateMachine$BrokerChangeListener - [BrokerChangeListener on Controller 0]: Newly added brokers: 0, deleted brokers: , all live brokers: 0
14:51:01.205 [ZkClient-EventThread-1017-127.0.0.1:50089] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Controller 0 connected to id:0,host:testing-worker-linux-docker-021d9b47-3350-linux-15.prod.travis-ci.org,port:45744 for sending state change requests
14:51:01.205 [ZkClient-EventThread-1017-127.0.0.1:50089] INFO  k.c.KafkaController - [Controller 0]: New broker startup callback for 0
14:51:01.205 [Controller-0-to-broker-0-send-thread] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Starting 
14:51:01.205 [kafka-request-handler-0] INFO  k.c.KafkaController - [Controller 0]: Shutting down broker 0
14:51:01.206 [main] INFO  k.s.KafkaServer - [Kafka Server 0], Controlled shutdown succeeded
14:51:01.206 [main] INFO  k.n.SocketServer - [Socket Server on Broker 0], Shutting down
14:51:01.206 [kafka-network-thread-45744-0] INFO  k.n.Processor - Closing socket connection to /172.17.9.145.
14:51:01.207 [main] INFO  k.n.SocketServer - [Socket Server on Broker 0], Shutdown completed
14:51:01.207 [main] INFO  k.s.KafkaRequestHandlerPool - [Kafka Request Handler on Broker 0], shutting down
14:51:01.207 [main] INFO  k.s.KafkaRequestHandlerPool - [Kafka Request Handler on Broker 0], shut down completely
14:51:01.259 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-021d9b47-3350-linux-15.prod.travis-ci.org/172.17.9.145
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
14:51:01.360 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-021d9b47-3350-linux-15.prod.travis-ci.org/172.17.9.145
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
14:51:01.461 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-021d9b47-3350-linux-15.prod.travis-ci.org/172.17.9.145
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
14:51:01.562 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-021d9b47-3350-linux-15.prod.travis-ci.org/172.17.9.145
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
14:51:01.569 [main] INFO  k.s.ReplicaManager - [Replica Manager on Broker 0]: Shut down
14:51:01.569 [main] INFO  k.s.ReplicaFetcherManager - [ReplicaFetcherManager on broker 0] shutting down
14:51:01.570 [main] INFO  k.s.ReplicaFetcherManager - [ReplicaFetcherManager on broker 0] shutdown completed
14:51:01.570 [main] INFO  k.s.ReplicaManager - [Replica Manager on Broker 0]: Shut down completely
14:51:01.570 [main] INFO  k.l.LogManager - Shutting down.
14:51:01.570 [main] INFO  k.l.LogManager - Shutdown complete.
14:51:01.570 [main] INFO  k.c.PartitionStateMachine - [Partition state machine on Controller 0]: Stopped partition state machine
14:51:01.570 [main] INFO  k.c.ReplicaStateMachine - [Replica state machine on controller 0]: Stopped replica state machine
14:51:01.570 [main] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Shutting down
14:51:01.570 [Controller-0-to-broker-0-send-thread] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Stopped 
14:51:01.570 [main] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Shutdown completed
14:51:01.571 [ZkClient-EventThread-1017-127.0.0.1:50089] INFO  o.I.z.ZkEventThread - Terminate ZkClient event thread.
14:51:01.572 [main] INFO  k.s.KafkaServer - [Kafka Server 0], shut down completed
14:51:01.572 [Curator-Framework-0] INFO  o.a.c.f.i.CuratorFrameworkImpl - backgroundOperationsLoop exiting

--------------------------------------------------
--------------------------------------------------
classname: org.apache.storm.kafka.bolt.KafkaBoltTest / testname: executeWithByteArrayKeyAndMessageFire
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.kafka.bolt.KafkaBoltTest.generateTestTuple(KafkaBoltTest.java:290)
	at org.apache.storm.kafka.bolt.KafkaBoltTest.executeWithByteArrayKeyAndMessageFire(KafkaBoltTest.java:176)

-------------------- system-out --------------------
14:51:01.663 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-021d9b47-3350-linux-15.prod.travis-ci.org/172.17.9.145
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
14:51:01.738 [Curator-Framework-0-SendThread(127.0.0.1:43546)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
14:51:01.764 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-021d9b47-3350-linux-15.prod.travis-ci.org/172.17.9.145
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
14:51:01.786 [Curator-Framework-0-SendThread(127.0.0.1:54006)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
14:51:01.865 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-021d9b47-3350-linux-15.prod.travis-ci.org/172.17.9.145
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
14:51:01.930 [Curator-Framework-0-SendThread(127.0.0.1:58001)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
14:51:01.957 [Curator-Framework-0-SendThread(127.0.0.1:41155)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
14:51:01.966 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-021d9b47-3350-linux-15.prod.travis-ci.org/172.17.9.145
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
14:51:02.067 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-021d9b47-3350-linux-15.prod.travis-ci.org/172.17.9.145
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
14:51:02.168 [main-SendThread(localhost:37688)] WARN  o.a.z.ClientCnxn - Session 0x15582e444330000 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
14:51:02.168 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-021d9b47-3350-linux-15.prod.travis-ci.org/172.17.9.145
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
14:51:02.201 [Curator-Framework-0-SendThread(127.0.0.1:60595)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
14:51:02.211 [Curator-Framework-0-SendThread(127.0.0.1:37288)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
14:51:02.269 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-021d9b47-3350-linux-15.prod.travis-ci.org/172.17.9.145
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
14:51:02.300 [main-SendThread(localhost:45310)] WARN  o.a.z.ClientCnxn - Session 0x15582e435570000 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
14:51:02.370 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-021d9b47-3350-linux-15.prod.travis-ci.org/172.17.9.145
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
14:51:02.471 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-021d9b47-3350-linux-15.prod.travis-ci.org/172.17.9.145
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
14:51:02.572 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-021d9b47-3350-linux-15.prod.travis-ci.org/172.17.9.145
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
14:51:02.577 [main] INFO  o.a.c.f.i.CuratorFrameworkImpl - Starting
14:51:02.579 [main] INFO  k.u.VerifiableProperties - Verifying properties
14:51:02.579 [main] INFO  k.u.VerifiableProperties - Property broker.id is overridden to 0
14:51:02.579 [main] INFO  k.u.VerifiableProperties - Property log.dirs is overridden to /tmp/kafka/logs/kafka-test-51124
14:51:02.580 [main] INFO  k.u.VerifiableProperties - Property port is overridden to 51124
14:51:02.580 [main] INFO  k.u.VerifiableProperties - Property zookeeper.connect is overridden to 127.0.0.1:33835
14:51:02.580 [main] INFO  k.s.KafkaServer - [Kafka Server 0], starting
14:51:02.580 [main] INFO  k.s.KafkaServer - [Kafka Server 0], Connecting to zookeeper on 127.0.0.1:33835
14:51:02.581 [ZkClient-EventThread-1053-127.0.0.1:33835] INFO  o.I.z.ZkEventThread - Starting ZkClient event thread.
14:51:02.594 [main-EventThread] INFO  o.a.c.f.s.ConnectionStateManager - State change: CONNECTED
14:51:02.594 [main-EventThread] INFO  o.I.z.ZkClient - zookeeper state changed (SyncConnected)
14:51:02.618 [main] INFO  k.l.LogManager - Log directory '/tmp/kafka/logs/kafka-test-51124' not found, creating it.
14:51:02.619 [main] INFO  k.l.LogManager - Loading logs.
14:51:02.619 [main] INFO  k.l.LogManager - Logs loading complete.
14:51:02.619 [main] INFO  k.l.LogManager - Starting log cleanup with a period of 300000 ms.
14:51:02.619 [main] INFO  k.l.LogManager - Starting log flusher with a default period of 9223372036854775807 ms.
14:51:02.621 [main] INFO  k.n.Acceptor - Awaiting socket connections on 0.0.0.0:51124.
14:51:02.621 [main] INFO  k.n.SocketServer - [Socket Server on Broker 0], Started
14:51:02.624 [main] INFO  k.u.Mx4jLoader$ - Will not load MX4J, mx4j-tools.jar is not in the classpath
14:51:02.624 [main] INFO  k.c.KafkaController - [Controller 0]: Controller starting up
14:51:02.629 [main] INFO  k.s.ZookeeperLeaderElector - 0 successfully elected as leader
14:51:02.629 [main] INFO  k.c.KafkaController - [Controller 0]: Broker 0 starting become controller state transition
14:51:02.632 [main] INFO  k.c.KafkaController - [Controller 0]: Controller 0 incremented epoch to 1
14:51:02.635 [main] INFO  k.c.KafkaController - [Controller 0]: Partitions undergoing preferred replica election: 
14:51:02.635 [main] INFO  k.c.KafkaController - [Controller 0]: Partitions that completed preferred replica election: 
14:51:02.635 [main] INFO  k.c.KafkaController - [Controller 0]: Resuming preferred replica election for partitions: 
14:51:02.636 [main] INFO  k.c.KafkaController - [Controller 0]: Partitions being reassigned: Map()
14:51:02.636 [main] INFO  k.c.KafkaController - [Controller 0]: Partitions already reassigned: List()
14:51:02.636 [main] INFO  k.c.KafkaController - [Controller 0]: Resuming reassignment of partitions: Map()
14:51:02.636 [main] INFO  k.c.KafkaController - [Controller 0]: List of topics to be deleted: 
14:51:02.636 [main] INFO  k.c.KafkaController - [Controller 0]: List of topics ineligible for deletion: 
14:51:02.636 [main] INFO  k.c.KafkaController - [Controller 0]: Currently active brokers in the cluster: Set()
14:51:02.637 [main] INFO  k.c.KafkaController - [Controller 0]: Currently shutting brokers in the cluster: Set()
14:51:02.637 [main] INFO  k.c.KafkaController - [Controller 0]: Current list of topics in the cluster: Set()
14:51:02.637 [main] INFO  k.c.ReplicaStateMachine - [Replica state machine on controller 0]: Started replica state machine with initial state -> Map()
14:51:02.637 [main] INFO  k.c.PartitionStateMachine - [Partition state machine on Controller 0]: Started partition state machine with initial state -> Map()
14:51:02.637 [main] INFO  k.c.KafkaController - [Controller 0]: Broker 0 is ready to serve as the new controller with epoch 1
14:51:02.637 [main] INFO  k.c.KafkaController - [Controller 0]: Starting preferred replica leader election for partitions 
14:51:02.637 [main] INFO  k.c.PartitionStateMachine - [Partition state machine on Controller 0]: Invoking state change to OnlinePartition for partitions 
14:51:02.640 [main] INFO  k.c.KafkaController - [Controller 0]: starting the partition rebalance scheduler
14:51:02.640 [main] INFO  k.c.KafkaController - [Controller 0]: Controller startup complete
14:51:02.641 [ZkClient-EventThread-1053-127.0.0.1:33835] INFO  k.s.ZookeeperLeaderElector$LeaderChangeListener - New leader is 0
14:51:02.644 [main] INFO  k.u.ZkUtils$ - Registered broker 0 at path /brokers/ids/0 with address testing-worker-linux-docker-021d9b47-3350-linux-15.prod.travis-ci.org:51124.
14:51:02.644 [main] INFO  k.s.KafkaServer - [Kafka Server 0], started
14:51:02.645 [main] INFO  o.a.k.c.p.ProducerConfig - ProducerConfig values: 
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	block.on.buffer.full = true
	retry.backoff.ms = 100
	buffer.memory = 33554432
	batch.size = 16384
	metrics.sample.window.ms = 30000
	metadata.max.age.ms = 300000
	receive.buffer.bytes = 32768
	timeout.ms = 30000
	max.in.flight.requests.per.connection = 5
	bootstrap.servers = [localhost:51124]
	metric.reporters = []
	client.id = 
	compression.type = none
	retries = 0
	max.request.size = 1048576
	send.buffer.bytes = 131072
	acks = 1
	reconnect.backoff.ms = 10
	linger.ms = 0
	metrics.num.samples = 2
	metadata.fetch.timeout.ms = 1000

14:51:02.648 [ZkClient-EventThread-1053-127.0.0.1:33835] INFO  k.c.ReplicaStateMachine$BrokerChangeListener - [BrokerChangeListener on Controller 0]: Broker change listener fired for path /brokers/ids with children 0
14:51:02.649 [main] INFO  o.a.k.c.p.ProducerConfig - ProducerConfig values: 
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	block.on.buffer.full = true
	retry.backoff.ms = 100
	buffer.memory = 33554432
	batch.size = 16384
	metrics.sample.window.ms = 30000
	metadata.max.age.ms = 300000
	receive.buffer.bytes = 32768
	timeout.ms = 30000
	max.in.flight.requests.per.connection = 5
	bootstrap.servers = [localhost:51124]
	metric.reporters = []
	client.id = 
	compression.type = none
	retries = 0
	max.request.size = 1048576
	send.buffer.bytes = 131072
	acks = 1
	reconnect.backoff.ms = 10
	linger.ms = 0
	metrics.num.samples = 2
	metadata.fetch.timeout.ms = 1000

14:51:02.662 [main] INFO  k.s.KafkaServer - [Kafka Server 0], shutting down
14:51:02.662 [main] INFO  k.s.KafkaServer - [Kafka Server 0], Starting controlled shutdown
14:51:02.662 [ZkClient-EventThread-1053-127.0.0.1:33835] INFO  k.c.ReplicaStateMachine$BrokerChangeListener - [BrokerChangeListener on Controller 0]: Newly added brokers: 0, deleted brokers: , all live brokers: 0
14:51:02.663 [ZkClient-EventThread-1053-127.0.0.1:33835] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Controller 0 connected to id:0,host:testing-worker-linux-docker-021d9b47-3350-linux-15.prod.travis-ci.org,port:51124 for sending state change requests
14:51:02.663 [ZkClient-EventThread-1053-127.0.0.1:33835] INFO  k.c.KafkaController - [Controller 0]: New broker startup callback for 0
14:51:02.663 [Controller-0-to-broker-0-send-thread] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Starting 
14:51:02.666 [kafka-request-handler-0] INFO  k.c.KafkaController - [Controller 0]: Shutting down broker 0
14:51:02.667 [main] INFO  k.s.KafkaServer - [Kafka Server 0], Controlled shutdown succeeded
14:51:02.667 [main] INFO  k.n.SocketServer - [Socket Server on Broker 0], Shutting down
14:51:02.667 [kafka-network-thread-51124-1] INFO  k.n.Processor - Closing socket connection to /172.17.9.145.
14:51:02.668 [main] INFO  k.n.SocketServer - [Socket Server on Broker 0], Shutdown completed
14:51:02.668 [main] INFO  k.s.KafkaRequestHandlerPool - [Kafka Request Handler on Broker 0], shutting down
14:51:02.668 [main] INFO  k.s.KafkaRequestHandlerPool - [Kafka Request Handler on Broker 0], shut down completely
14:51:02.672 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-021d9b47-3350-linux-15.prod.travis-ci.org/172.17.9.145
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
14:51:02.687 [main-SendThread(localhost:45310)] WARN  o.a.z.ClientCnxn - Session 0x15582e435570000 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
14:51:02.741 [main-SendThread(localhost:37688)] WARN  o.a.z.ClientCnxn - Session 0x15582e444330000 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
14:51:02.773 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-021d9b47-3350-linux-15.prod.travis-ci.org/172.17.9.145
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
14:51:02.839 [Curator-Framework-0-SendThread(127.0.0.1:43546)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
14:51:02.874 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-021d9b47-3350-linux-15.prod.travis-ci.org/172.17.9.145
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
14:51:02.887 [Curator-Framework-0-SendThread(127.0.0.1:54006)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
14:51:02.975 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-021d9b47-3350-linux-15.prod.travis-ci.org/172.17.9.145
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
14:51:03.023 [main] INFO  k.s.ReplicaManager - [Replica Manager on Broker 0]: Shut down
14:51:03.023 [main] INFO  k.s.ReplicaFetcherManager - [ReplicaFetcherManager on broker 0] shutting down
14:51:03.023 [main] INFO  k.s.ReplicaFetcherManager - [ReplicaFetcherManager on broker 0] shutdown completed
14:51:03.024 [main] INFO  k.s.ReplicaManager - [Replica Manager on Broker 0]: Shut down completely
14:51:03.024 [main] INFO  k.l.LogManager - Shutting down.
14:51:03.024 [main] INFO  k.l.LogManager - Shutdown complete.
14:51:03.024 [main] INFO  k.c.PartitionStateMachine - [Partition state machine on Controller 0]: Stopped partition state machine
14:51:03.024 [main] INFO  k.c.ReplicaStateMachine - [Replica state machine on controller 0]: Stopped replica state machine
14:51:03.024 [main] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Shutting down
14:51:03.025 [Controller-0-to-broker-0-send-thread] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Stopped 
14:51:03.025 [main] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Shutdown completed
14:51:03.025 [ZkClient-EventThread-1053-127.0.0.1:33835] INFO  o.I.z.ZkEventThread - Terminate ZkClient event thread.
14:51:03.027 [main] INFO  k.s.KafkaServer - [Kafka Server 0], shut down completed
14:51:03.027 [Curator-Framework-0] INFO  o.a.c.f.i.CuratorFrameworkImpl - backgroundOperationsLoop exiting

--------------------------------------------------
--------------------------------------------------
classname: org.apache.storm.kafka.bolt.KafkaBoltTest / testname: executeWithByteArrayKeyAndMessageSync
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.kafka.bolt.KafkaBoltTest.generateTestTuple(KafkaBoltTest.java:290)
	at org.apache.storm.kafka.bolt.KafkaBoltTest.executeWithByteArrayKeyAndMessageSync(KafkaBoltTest.java:131)

-------------------- system-out --------------------
14:51:03.030 [Curator-Framework-0-SendThread(127.0.0.1:58001)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
14:51:03.058 [Curator-Framework-0-SendThread(127.0.0.1:41155)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
14:51:03.076 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-021d9b47-3350-linux-15.prod.travis-ci.org/172.17.9.145
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
14:51:03.177 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-021d9b47-3350-linux-15.prod.travis-ci.org/172.17.9.145
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
14:51:03.278 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-021d9b47-3350-linux-15.prod.travis-ci.org/172.17.9.145
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
14:51:03.302 [Curator-Framework-0-SendThread(127.0.0.1:60595)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
14:51:03.312 [Curator-Framework-0-SendThread(127.0.0.1:37288)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
14:51:03.379 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-021d9b47-3350-linux-15.prod.travis-ci.org/172.17.9.145
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
14:51:03.480 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-021d9b47-3350-linux-15.prod.travis-ci.org/172.17.9.145
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
14:51:03.581 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-021d9b47-3350-linux-15.prod.travis-ci.org/172.17.9.145
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
14:51:03.682 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-021d9b47-3350-linux-15.prod.travis-ci.org/172.17.9.145
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
14:51:03.784 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-021d9b47-3350-linux-15.prod.travis-ci.org/172.17.9.145
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
14:51:03.879 [main-SendThread(localhost:45310)] WARN  o.a.z.ClientCnxn - Session 0x15582e435570000 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
14:51:03.884 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-021d9b47-3350-linux-15.prod.travis-ci.org/172.17.9.145
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
14:51:03.940 [Curator-Framework-0-SendThread(127.0.0.1:43546)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
14:51:03.985 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-021d9b47-3350-linux-15.prod.travis-ci.org/172.17.9.145
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
14:51:03.988 [Curator-Framework-0-SendThread(127.0.0.1:54006)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
14:51:04.051 [main] INFO  o.a.c.f.i.CuratorFrameworkImpl - Starting
14:51:04.053 [main] INFO  k.u.VerifiableProperties - Verifying properties
14:51:04.053 [main] INFO  k.u.VerifiableProperties - Property broker.id is overridden to 0
14:51:04.053 [main] INFO  k.u.VerifiableProperties - Property log.dirs is overridden to /tmp/kafka/logs/kafka-test-40972
14:51:04.053 [main] INFO  k.u.VerifiableProperties - Property port is overridden to 40972
14:51:04.053 [main] INFO  k.u.VerifiableProperties - Property zookeeper.connect is overridden to 127.0.0.1:40073
14:51:04.053 [main] INFO  k.s.KafkaServer - [Kafka Server 0], starting
14:51:04.053 [main] INFO  k.s.KafkaServer - [Kafka Server 0], Connecting to zookeeper on 127.0.0.1:40073
14:51:04.054 [ZkClient-EventThread-1090-127.0.0.1:40073] INFO  o.I.z.ZkEventThread - Starting ZkClient event thread.
14:51:04.056 [main-EventThread] INFO  o.a.c.f.s.ConnectionStateManager - State change: CONNECTED
14:51:04.056 [main-EventThread] INFO  o.I.z.ZkClient - zookeeper state changed (SyncConnected)
14:51:04.069 [main] INFO  k.l.LogManager - Log directory '/tmp/kafka/logs/kafka-test-40972' not found, creating it.
14:51:04.069 [main] INFO  k.l.LogManager - Loading logs.
14:51:04.069 [main] INFO  k.l.LogManager - Logs loading complete.
14:51:04.069 [main] INFO  k.l.LogManager - Starting log cleanup with a period of 300000 ms.
14:51:04.070 [main] INFO  k.l.LogManager - Starting log flusher with a default period of 9223372036854775807 ms.
14:51:04.071 [main] INFO  k.n.Acceptor - Awaiting socket connections on 0.0.0.0:40972.
14:51:04.072 [main] INFO  k.n.SocketServer - [Socket Server on Broker 0], Started
14:51:04.074 [main] INFO  k.u.Mx4jLoader$ - Will not load MX4J, mx4j-tools.jar is not in the classpath
14:51:04.074 [main] INFO  k.c.KafkaController - [Controller 0]: Controller starting up
14:51:04.076 [main] INFO  k.s.ZookeeperLeaderElector - 0 successfully elected as leader
14:51:04.077 [main] INFO  k.c.KafkaController - [Controller 0]: Broker 0 starting become controller state transition
14:51:04.079 [main] INFO  k.c.KafkaController - [Controller 0]: Controller 0 incremented epoch to 1
14:51:04.082 [main] INFO  k.c.KafkaController - [Controller 0]: Partitions undergoing preferred replica election: 
14:51:04.082 [main] INFO  k.c.KafkaController - [Controller 0]: Partitions that completed preferred replica election: 
14:51:04.082 [main] INFO  k.c.KafkaController - [Controller 0]: Resuming preferred replica election for partitions: 
14:51:04.082 [main] INFO  k.c.KafkaController - [Controller 0]: Partitions being reassigned: Map()
14:51:04.082 [main] INFO  k.c.KafkaController - [Controller 0]: Partitions already reassigned: List()
14:51:04.083 [main] INFO  k.c.KafkaController - [Controller 0]: Resuming reassignment of partitions: Map()
14:51:04.083 [main] INFO  k.c.KafkaController - [Controller 0]: List of topics to be deleted: 
14:51:04.083 [main] INFO  k.c.KafkaController - [Controller 0]: List of topics ineligible for deletion: 
14:51:04.083 [main] INFO  k.c.KafkaController - [Controller 0]: Currently active brokers in the cluster: Set()
14:51:04.083 [main] INFO  k.c.KafkaController - [Controller 0]: Currently shutting brokers in the cluster: Set()
14:51:04.083 [main] INFO  k.c.KafkaController - [Controller 0]: Current list of topics in the cluster: Set()
14:51:04.083 [main] INFO  k.c.ReplicaStateMachine - [Replica state machine on controller 0]: Started replica state machine with initial state -> Map()
14:51:04.083 [main] INFO  k.c.PartitionStateMachine - [Partition state machine on Controller 0]: Started partition state machine with initial state -> Map()
14:51:04.084 [main] INFO  k.c.KafkaController - [Controller 0]: Broker 0 is ready to serve as the new controller with epoch 1
14:51:04.084 [main] INFO  k.c.KafkaController - [Controller 0]: Starting preferred replica leader election for partitions 
14:51:04.084 [main] INFO  k.c.PartitionStateMachine - [Partition state machine on Controller 0]: Invoking state change to OnlinePartition for partitions 
14:51:04.085 [main] INFO  k.c.KafkaController - [Controller 0]: starting the partition rebalance scheduler
14:51:04.085 [main] INFO  k.c.KafkaController - [Controller 0]: Controller startup complete
14:51:04.086 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-021d9b47-3350-linux-15.prod.travis-ci.org/172.17.9.145
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
14:51:04.087 [ZkClient-EventThread-1090-127.0.0.1:40073] INFO  k.s.ZookeeperLeaderElector$LeaderChangeListener - New leader is 0
14:51:04.090 [main] INFO  k.u.ZkUtils$ - Registered broker 0 at path /brokers/ids/0 with address testing-worker-linux-docker-021d9b47-3350-linux-15.prod.travis-ci.org:40972.
14:51:04.090 [main] INFO  k.s.KafkaServer - [Kafka Server 0], started
14:51:04.090 [main] INFO  o.a.k.c.p.ProducerConfig - ProducerConfig values: 
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	block.on.buffer.full = true
	retry.backoff.ms = 100
	buffer.memory = 33554432
	batch.size = 16384
	metrics.sample.window.ms = 30000
	metadata.max.age.ms = 300000
	receive.buffer.bytes = 32768
	timeout.ms = 30000
	max.in.flight.requests.per.connection = 5
	bootstrap.servers = [localhost:40972]
	metric.reporters = []
	client.id = 
	compression.type = none
	retries = 0
	max.request.size = 1048576
	send.buffer.bytes = 131072
	acks = 1
	reconnect.backoff.ms = 10
	linger.ms = 0
	metrics.num.samples = 2
	metadata.fetch.timeout.ms = 1000

14:51:04.093 [main] INFO  o.a.k.c.p.ProducerConfig - ProducerConfig values: 
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	block.on.buffer.full = true
	retry.backoff.ms = 100
	buffer.memory = 33554432
	batch.size = 16384
	metrics.sample.window.ms = 30000
	metadata.max.age.ms = 300000
	receive.buffer.bytes = 32768
	timeout.ms = 30000
	max.in.flight.requests.per.connection = 5
	bootstrap.servers = [localhost:40972]
	metric.reporters = []
	client.id = 
	compression.type = none
	retries = 0
	max.request.size = 1048576
	send.buffer.bytes = 131072
	acks = 1
	reconnect.backoff.ms = 10
	linger.ms = 0
	metrics.num.samples = 2
	metadata.fetch.timeout.ms = 1000

14:51:04.093 [ZkClient-EventThread-1090-127.0.0.1:40073] INFO  k.c.ReplicaStateMachine$BrokerChangeListener - [BrokerChangeListener on Controller 0]: Broker change listener fired for path /brokers/ids with children 0
14:51:04.096 [main] INFO  k.s.KafkaServer - [Kafka Server 0], shutting down
14:51:04.096 [main] INFO  k.s.KafkaServer - [Kafka Server 0], Starting controlled shutdown
14:51:04.097 [ZkClient-EventThread-1090-127.0.0.1:40073] INFO  k.c.ReplicaStateMachine$BrokerChangeListener - [BrokerChangeListener on Controller 0]: Newly added brokers: 0, deleted brokers: , all live brokers: 0
14:51:04.098 [ZkClient-EventThread-1090-127.0.0.1:40073] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Controller 0 connected to id:0,host:testing-worker-linux-docker-021d9b47-3350-linux-15.prod.travis-ci.org,port:40972 for sending state change requests
14:51:04.098 [ZkClient-EventThread-1090-127.0.0.1:40073] INFO  k.c.KafkaController - [Controller 0]: New broker startup callback for 0
14:51:04.098 [Controller-0-to-broker-0-send-thread] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Starting 
14:51:04.101 [kafka-request-handler-0] INFO  k.c.KafkaController - [Controller 0]: Shutting down broker 0
14:51:04.101 [main] INFO  k.s.KafkaServer - [Kafka Server 0], Controlled shutdown succeeded
14:51:04.101 [main] INFO  k.n.SocketServer - [Socket Server on Broker 0], Shutting down
14:51:04.102 [kafka-network-thread-40972-1] INFO  k.n.Processor - Closing socket connection to /172.17.9.145.
14:51:04.102 [main] INFO  k.n.SocketServer - [Socket Server on Broker 0], Shutdown completed
14:51:04.102 [main] INFO  k.s.KafkaRequestHandlerPool - [Kafka Request Handler on Broker 0], shutting down
14:51:04.103 [main] INFO  k.s.KafkaRequestHandlerPool - [Kafka Request Handler on Broker 0], shut down completely
14:51:04.131 [Curator-Framework-0-SendThread(127.0.0.1:58001)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
14:51:04.159 [Curator-Framework-0-SendThread(127.0.0.1:41155)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
14:51:04.187 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-021d9b47-3350-linux-15.prod.travis-ci.org/172.17.9.145
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
14:51:04.288 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-021d9b47-3350-linux-15.prod.travis-ci.org/172.17.9.145
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
14:51:04.388 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-021d9b47-3350-linux-15.prod.travis-ci.org/172.17.9.145
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
14:51:04.403 [Curator-Framework-0-SendThread(127.0.0.1:60595)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
14:51:04.412 [Curator-Framework-0-SendThread(127.0.0.1:37288)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
14:51:04.473 [main] INFO  k.s.ReplicaManager - [Replica Manager on Broker 0]: Shut down
14:51:04.474 [main] INFO  k.s.ReplicaFetcherManager - [ReplicaFetcherManager on broker 0] shutting down
14:51:04.474 [main] INFO  k.s.ReplicaFetcherManager - [ReplicaFetcherManager on broker 0] shutdown completed
14:51:04.474 [main] INFO  k.s.ReplicaManager - [Replica Manager on Broker 0]: Shut down completely
14:51:04.474 [main] INFO  k.l.LogManager - Shutting down.
14:51:04.474 [main] INFO  k.l.LogManager - Shutdown complete.
14:51:04.474 [main] INFO  k.c.PartitionStateMachine - [Partition state machine on Controller 0]: Stopped partition state machine
14:51:04.474 [main] INFO  k.c.ReplicaStateMachine - [Replica state machine on controller 0]: Stopped replica state machine
14:51:04.475 [main] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Shutting down
14:51:04.475 [Controller-0-to-broker-0-send-thread] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Stopped 
14:51:04.475 [main] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Shutdown completed
14:51:04.475 [ZkClient-EventThread-1090-127.0.0.1:40073] INFO  o.I.z.ZkEventThread - Terminate ZkClient event thread.
14:51:04.478 [main] INFO  k.s.KafkaServer - [Kafka Server 0], shut down completed
14:51:04.478 [Curator-Framework-0] INFO  o.a.c.f.i.CuratorFrameworkImpl - backgroundOperationsLoop exiting

--------------------------------------------------
--------------------------------------------------
classname: org.apache.storm.kafka.bolt.KafkaBoltTest / testname: executeWithByteArrayKeyAndMessageAsync
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.kafka.bolt.KafkaBoltTest.generateTestTuple(KafkaBoltTest.java:290)
	at org.apache.storm.kafka.bolt.KafkaBoltTest.executeWithByteArrayKeyAndMessageAsync(KafkaBoltTest.java:146)

-------------------- system-out --------------------
14:51:04.489 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-021d9b47-3350-linux-15.prod.travis-ci.org/172.17.9.145
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
14:51:04.590 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-021d9b47-3350-linux-15.prod.travis-ci.org/172.17.9.145
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
14:51:04.658 [main-SendThread(localhost:37688)] WARN  o.a.z.ClientCnxn - Session 0x15582e444330000 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
14:51:04.691 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-021d9b47-3350-linux-15.prod.travis-ci.org/172.17.9.145
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
14:51:04.698 [Curator-Framework-0] WARN  o.a.c.ConnectionState - Connection attempt unsuccessful after 20006 (greater than max timeout of 20000). Resetting connection and trying again with a new connection.
14:51:04.792 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-021d9b47-3350-linux-15.prod.travis-ci.org/172.17.9.145
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
14:51:04.893 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-021d9b47-3350-linux-15.prod.travis-ci.org/172.17.9.145
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
14:51:04.994 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-021d9b47-3350-linux-15.prod.travis-ci.org/172.17.9.145
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
14:51:05.041 [Curator-Framework-0-SendThread(127.0.0.1:43546)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
14:51:05.069 [Curator-Framework-0-SendThread(localhost:45310)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
14:51:05.089 [Curator-Framework-0-SendThread(127.0.0.1:54006)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
14:51:05.094 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-021d9b47-3350-linux-15.prod.travis-ci.org/172.17.9.145
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
14:51:05.159 [Curator-Framework-0] ERROR o.a.c.f.i.CuratorFrameworkImpl - Background operation retry gave up
org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:99) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.curator.framework.imps.CuratorFrameworkImpl.checkBackgroundRetry(CuratorFrameworkImpl.java:728) [curator-framework-2.10.0.jar:?]
	at org.apache.curator.framework.imps.CuratorFrameworkImpl.performBackgroundOperation(CuratorFrameworkImpl.java:857) [curator-framework-2.10.0.jar:?]
	at org.apache.curator.framework.imps.CuratorFrameworkImpl.backgroundOperationsLoop(CuratorFrameworkImpl.java:809) [curator-framework-2.10.0.jar:?]
	at org.apache.curator.framework.imps.CuratorFrameworkImpl.access$300(CuratorFrameworkImpl.java:64) [curator-framework-2.10.0.jar:?]
	at org.apache.curator.framework.imps.CuratorFrameworkImpl$4.call(CuratorFrameworkImpl.java:267) [curator-framework-2.10.0.jar:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:262) [?:1.7.0_76]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:178) [?:1.7.0_76]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:292) [?:1.7.0_76]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [?:1.7.0_76]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [?:1.7.0_76]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
14:51:05.160 [Curator-Framework-0] ERROR o.a.c.f.i.CuratorFrameworkImpl - Background retry gave up
org.apache.curator.CuratorConnectionLossException: KeeperErrorCode = ConnectionLoss
	at org.apache.curator.framework.imps.CuratorFrameworkImpl.performBackgroundOperation(CuratorFrameworkImpl.java:838) [curator-framework-2.10.0.jar:?]
	at org.apache.curator.framework.imps.CuratorFrameworkImpl.backgroundOperationsLoop(CuratorFrameworkImpl.java:809) [curator-framework-2.10.0.jar:?]
	at org.apache.curator.framework.imps.CuratorFrameworkImpl.access$300(CuratorFrameworkImpl.java:64) [curator-framework-2.10.0.jar:?]
	at org.apache.curator.framework.imps.CuratorFrameworkImpl$4.call(CuratorFrameworkImpl.java:267) [curator-framework-2.10.0.jar:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:262) [?:1.7.0_76]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:178) [?:1.7.0_76]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:292) [?:1.7.0_76]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [?:1.7.0_76]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [?:1.7.0_76]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
14:51:05.170 [Curator-Framework-0-SendThread(localhost:45310)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
14:51:05.195 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-021d9b47-3350-linux-15.prod.travis-ci.org/172.17.9.145
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
14:51:05.232 [Curator-Framework-0-SendThread(127.0.0.1:58001)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
14:51:05.260 [Curator-Framework-0-SendThread(127.0.0.1:41155)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
14:51:05.296 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-021d9b47-3350-linux-15.prod.travis-ci.org/172.17.9.145
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
14:51:05.397 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-021d9b47-3350-linux-15.prod.travis-ci.org/172.17.9.145
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
14:51:05.498 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-021d9b47-3350-linux-15.prod.travis-ci.org/172.17.9.145
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
14:51:05.504 [Curator-Framework-0-SendThread(127.0.0.1:60595)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
14:51:05.507 [main] INFO  o.a.c.f.i.CuratorFrameworkImpl - Starting
14:51:05.508 [main] INFO  k.u.VerifiableProperties - Verifying properties
14:51:05.508 [main] INFO  k.u.VerifiableProperties - Property broker.id is overridden to 0
14:51:05.508 [main] INFO  k.u.VerifiableProperties - Property log.dirs is overridden to /tmp/kafka/logs/kafka-test-43292
14:51:05.508 [main] INFO  k.u.VerifiableProperties - Property port is overridden to 43292
14:51:05.508 [main] INFO  k.u.VerifiableProperties - Property zookeeper.connect is overridden to 127.0.0.1:55015
14:51:05.508 [main] INFO  k.s.KafkaServer - [Kafka Server 0], starting
14:51:05.509 [main] INFO  k.s.KafkaServer - [Kafka Server 0], Connecting to zookeeper on 127.0.0.1:55015
14:51:05.514 [ZkClient-EventThread-1129-127.0.0.1:55015] INFO  o.I.z.ZkEventThread - Starting ZkClient event thread.
14:51:05.515 [main-EventThread] INFO  o.a.c.f.s.ConnectionStateManager - State change: CONNECTED
14:51:05.515 [Curator-Framework-0-SendThread(127.0.0.1:37288)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
14:51:05.516 [main-EventThread] INFO  o.I.z.ZkClient - zookeeper state changed (SyncConnected)
14:51:05.528 [main] INFO  k.l.LogManager - Log directory '/tmp/kafka/logs/kafka-test-43292' not found, creating it.
14:51:05.528 [main] INFO  k.l.LogManager - Loading logs.
14:51:05.528 [main] INFO  k.l.LogManager - Logs loading complete.
14:51:05.529 [main] INFO  k.l.LogManager - Starting log cleanup with a period of 300000 ms.
14:51:05.529 [main] INFO  k.l.LogManager - Starting log flusher with a default period of 9223372036854775807 ms.
14:51:05.531 [main] INFO  k.n.Acceptor - Awaiting socket connections on 0.0.0.0:43292.
14:51:05.531 [main] INFO  k.n.SocketServer - [Socket Server on Broker 0], Started
14:51:05.534 [main] INFO  k.u.Mx4jLoader$ - Will not load MX4J, mx4j-tools.jar is not in the classpath
14:51:05.534 [main] INFO  k.c.KafkaController - [Controller 0]: Controller starting up
14:51:05.536 [main] INFO  k.s.ZookeeperLeaderElector - 0 successfully elected as leader
14:51:05.537 [main] INFO  k.c.KafkaController - [Controller 0]: Broker 0 starting become controller state transition
14:51:05.539 [main] INFO  k.c.KafkaController - [Controller 0]: Controller 0 incremented epoch to 1
14:51:05.542 [main] INFO  k.c.KafkaController - [Controller 0]: Partitions undergoing preferred replica election: 
14:51:05.542 [main] INFO  k.c.KafkaController - [Controller 0]: Partitions that completed preferred replica election: 
14:51:05.542 [main] INFO  k.c.KafkaController - [Controller 0]: Resuming preferred replica election for partitions: 
14:51:05.543 [main] INFO  k.c.KafkaController - [Controller 0]: Partitions being reassigned: Map()
14:51:05.543 [main] INFO  k.c.KafkaController - [Controller 0]: Partitions already reassigned: List()
14:51:05.543 [main] INFO  k.c.KafkaController - [Controller 0]: Resuming reassignment of partitions: Map()
14:51:05.543 [main] INFO  k.c.KafkaController - [Controller 0]: List of topics to be deleted: 
14:51:05.543 [main] INFO  k.c.KafkaController - [Controller 0]: List of topics ineligible for deletion: 
14:51:05.543 [main] INFO  k.c.KafkaController - [Controller 0]: Currently active brokers in the cluster: Set()
14:51:05.544 [main] INFO  k.c.KafkaController - [Controller 0]: Currently shutting brokers in the cluster: Set()
14:51:05.544 [main] INFO  k.c.KafkaController - [Controller 0]: Current list of topics in the cluster: Set()
14:51:05.544 [main] INFO  k.c.ReplicaStateMachine - [Replica state machine on controller 0]: Started replica state machine with initial state -> Map()
14:51:05.544 [main] INFO  k.c.PartitionStateMachine - [Partition state machine on Controller 0]: Started partition state machine with initial state -> Map()
14:51:05.544 [main] INFO  k.c.KafkaController - [Controller 0]: Broker 0 is ready to serve as the new controller with epoch 1
14:51:05.544 [main] INFO  k.c.KafkaController - [Controller 0]: Starting preferred replica leader election for partitions 
14:51:05.544 [main] INFO  k.c.PartitionStateMachine - [Partition state machine on Controller 0]: Invoking state change to OnlinePartition for partitions 
14:51:05.545 [main] INFO  k.c.KafkaController - [Controller 0]: starting the partition rebalance scheduler
14:51:05.546 [main] INFO  k.c.KafkaController - [Controller 0]: Controller startup complete
14:51:05.547 [ZkClient-EventThread-1129-127.0.0.1:55015] INFO  k.s.ZookeeperLeaderElector$LeaderChangeListener - New leader is 0
14:51:05.550 [main] INFO  k.u.ZkUtils$ - Registered broker 0 at path /brokers/ids/0 with address testing-worker-linux-docker-021d9b47-3350-linux-15.prod.travis-ci.org:43292.
14:51:05.550 [main] INFO  k.s.KafkaServer - [Kafka Server 0], started
14:51:05.550 [main] INFO  o.a.k.c.p.ProducerConfig - ProducerConfig values: 
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	block.on.buffer.full = true
	retry.backoff.ms = 100
	buffer.memory = 33554432
	batch.size = 16384
	metrics.sample.window.ms = 30000
	metadata.max.age.ms = 300000
	receive.buffer.bytes = 32768
	timeout.ms = 30000
	max.in.flight.requests.per.connection = 5
	bootstrap.servers = [localhost:43292]
	metric.reporters = []
	client.id = 
	compression.type = none
	retries = 0
	max.request.size = 1048576
	send.buffer.bytes = 131072
	acks = 1
	reconnect.backoff.ms = 10
	linger.ms = 0
	metrics.num.samples = 2
	metadata.fetch.timeout.ms = 1000

14:51:05.550 [ZkClient-EventThread-1129-127.0.0.1:55015] INFO  k.c.ReplicaStateMachine$BrokerChangeListener - [BrokerChangeListener on Controller 0]: Broker change listener fired for path /brokers/ids with children 0
14:51:05.553 [main] INFO  k.s.KafkaServer - [Kafka Server 0], shutting down
14:51:05.553 [main] INFO  k.s.KafkaServer - [Kafka Server 0], Starting controlled shutdown
14:51:05.555 [ZkClient-EventThread-1129-127.0.0.1:55015] INFO  k.c.ReplicaStateMachine$BrokerChangeListener - [BrokerChangeListener on Controller 0]: Newly added brokers: 0, deleted brokers: , all live brokers: 0
14:51:05.555 [ZkClient-EventThread-1129-127.0.0.1:55015] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Controller 0 connected to id:0,host:testing-worker-linux-docker-021d9b47-3350-linux-15.prod.travis-ci.org,port:43292 for sending state change requests
14:51:05.556 [ZkClient-EventThread-1129-127.0.0.1:55015] INFO  k.c.KafkaController - [Controller 0]: New broker startup callback for 0
14:51:05.559 [Controller-0-to-broker-0-send-thread] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Starting 
14:51:05.559 [kafka-request-handler-0] INFO  k.c.KafkaController - [Controller 0]: Shutting down broker 0
14:51:05.560 [main] INFO  k.s.KafkaServer - [Kafka Server 0], Controlled shutdown succeeded
14:51:05.560 [main] INFO  k.n.SocketServer - [Socket Server on Broker 0], Shutting down
14:51:05.560 [kafka-network-thread-43292-1] INFO  k.n.Processor - Closing socket connection to /172.17.9.145.
14:51:05.561 [main] INFO  k.n.SocketServer - [Socket Server on Broker 0], Shutdown completed
14:51:05.561 [main] INFO  k.s.KafkaRequestHandlerPool - [Kafka Request Handler on Broker 0], shutting down
14:51:05.561 [main] INFO  k.s.KafkaRequestHandlerPool - [Kafka Request Handler on Broker 0], shut down completely
14:51:05.599 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-021d9b47-3350-linux-15.prod.travis-ci.org/172.17.9.145
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
14:51:05.700 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-021d9b47-3350-linux-15.prod.travis-ci.org/172.17.9.145
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
14:51:05.743 [main-SendThread(localhost:37688)] WARN  o.a.z.ClientCnxn - Session 0x15582e444330000 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
14:51:05.801 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-021d9b47-3350-linux-15.prod.travis-ci.org/172.17.9.145
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
14:51:05.902 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-021d9b47-3350-linux-15.prod.travis-ci.org/172.17.9.145
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
14:51:05.933 [main] INFO  k.s.ReplicaManager - [Replica Manager on Broker 0]: Shut down
14:51:05.933 [main] INFO  k.s.ReplicaFetcherManager - [ReplicaFetcherManager on broker 0] shutting down
14:51:05.933 [main] INFO  k.s.ReplicaFetcherManager - [ReplicaFetcherManager on broker 0] shutdown completed
14:51:05.933 [main] INFO  k.s.ReplicaManager - [Replica Manager on Broker 0]: Shut down completely
14:51:05.933 [main] INFO  k.l.LogManager - Shutting down.
14:51:05.934 [main] INFO  k.l.LogManager - Shutdown complete.
14:51:05.934 [main] INFO  k.c.PartitionStateMachine - [Partition state machine on Controller 0]: Stopped partition state machine
14:51:05.934 [main] INFO  k.c.ReplicaStateMachine - [Replica state machine on controller 0]: Stopped replica state machine
14:51:05.934 [main] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Shutting down
14:51:05.934 [Controller-0-to-broker-0-send-thread] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Stopped 
14:51:05.934 [main] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Shutdown completed
14:51:05.934 [ZkClient-EventThread-1129-127.0.0.1:55015] INFO  o.I.z.ZkEventThread - Terminate ZkClient event thread.
14:51:05.944 [main] INFO  k.s.KafkaServer - [Kafka Server 0], shut down completed
14:51:05.944 [Curator-Framework-0] INFO  o.a.c.f.i.CuratorFrameworkImpl - backgroundOperationsLoop exiting

--------------------------------------------------
--------------------------------------------------
classname: org.apache.storm.kafka.bolt.KafkaBoltTest / testname: executeWithKey
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.kafka.bolt.KafkaBoltTest.generateTestTuple(KafkaBoltTest.java:290)
	at org.apache.storm.kafka.bolt.KafkaBoltTest.executeWithKey(KafkaBoltTest.java:115)

-------------------- system-out --------------------
14:51:06.003 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-021d9b47-3350-linux-15.prod.travis-ci.org/172.17.9.145
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
14:51:06.104 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-021d9b47-3350-linux-15.prod.travis-ci.org/172.17.9.145
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
14:51:06.142 [Curator-Framework-0-SendThread(127.0.0.1:43546)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
14:51:06.190 [Curator-Framework-0-SendThread(127.0.0.1:54006)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
14:51:06.205 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-021d9b47-3350-linux-15.prod.travis-ci.org/172.17.9.145
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
14:51:06.271 [Curator-Framework-0-SendThread(localhost:45310)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
14:51:06.306 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-021d9b47-3350-linux-15.prod.travis-ci.org/172.17.9.145
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
14:51:06.333 [Curator-Framework-0-SendThread(127.0.0.1:58001)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
14:51:06.361 [Curator-Framework-0-SendThread(127.0.0.1:41155)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
14:51:06.372 [Curator-Framework-0-SendThread(localhost:45310)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
14:51:06.407 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-021d9b47-3350-linux-15.prod.travis-ci.org/172.17.9.145
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
14:51:06.507 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-021d9b47-3350-linux-15.prod.travis-ci.org/172.17.9.145
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
14:51:06.605 [Curator-Framework-0-SendThread(127.0.0.1:60595)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
14:51:06.607 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-021d9b47-3350-linux-15.prod.travis-ci.org/172.17.9.145
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
14:51:06.616 [Curator-Framework-0-SendThread(127.0.0.1:37288)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
14:51:06.708 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-021d9b47-3350-linux-15.prod.travis-ci.org/172.17.9.145
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
14:51:06.809 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-021d9b47-3350-linux-15.prod.travis-ci.org/172.17.9.145
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
14:51:06.910 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-021d9b47-3350-linux-15.prod.travis-ci.org/172.17.9.145
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
14:51:06.971 [main] INFO  o.a.c.f.i.CuratorFrameworkImpl - Starting
14:51:06.972 [main] INFO  k.u.VerifiableProperties - Verifying properties
14:51:06.972 [main] INFO  k.u.VerifiableProperties - Property broker.id is overridden to 0
14:51:06.973 [main] INFO  k.u.VerifiableProperties - Property log.dirs is overridden to /tmp/kafka/logs/kafka-test-41909
14:51:06.973 [main] INFO  k.u.VerifiableProperties - Property port is overridden to 41909
14:51:06.973 [main] INFO  k.u.VerifiableProperties - Property zookeeper.connect is overridden to 127.0.0.1:50749
14:51:06.973 [main] INFO  k.s.KafkaServer - [Kafka Server 0], starting
14:51:06.973 [main] INFO  k.s.KafkaServer - [Kafka Server 0], Connecting to zookeeper on 127.0.0.1:50749
14:51:06.974 [ZkClient-EventThread-1165-127.0.0.1:50749] INFO  o.I.z.ZkEventThread - Starting ZkClient event thread.
14:51:06.976 [main-EventThread] INFO  o.a.c.f.s.ConnectionStateManager - State change: CONNECTED
14:51:06.977 [main-EventThread] INFO  o.I.z.ZkClient - zookeeper state changed (SyncConnected)
14:51:06.996 [main] INFO  k.l.LogManager - Log directory '/tmp/kafka/logs/kafka-test-41909' not found, creating it.
14:51:06.996 [main] INFO  k.l.LogManager - Loading logs.
14:51:06.997 [main] INFO  k.l.LogManager - Logs loading complete.
14:51:06.997 [main] INFO  k.l.LogManager - Starting log cleanup with a period of 300000 ms.
14:51:06.997 [main] INFO  k.l.LogManager - Starting log flusher with a default period of 9223372036854775807 ms.
14:51:06.999 [main] INFO  k.n.Acceptor - Awaiting socket connections on 0.0.0.0:41909.
14:51:06.999 [main] INFO  k.n.SocketServer - [Socket Server on Broker 0], Started
14:51:07.002 [main] INFO  k.u.Mx4jLoader$ - Will not load MX4J, mx4j-tools.jar is not in the classpath
14:51:07.002 [main] INFO  k.c.KafkaController - [Controller 0]: Controller starting up
14:51:07.004 [main] INFO  k.s.ZookeeperLeaderElector - 0 successfully elected as leader
14:51:07.004 [main] INFO  k.c.KafkaController - [Controller 0]: Broker 0 starting become controller state transition
14:51:07.006 [main] INFO  k.c.KafkaController - [Controller 0]: Controller 0 incremented epoch to 1
14:51:07.009 [main] INFO  k.c.KafkaController - [Controller 0]: Partitions undergoing preferred replica election: 
14:51:07.009 [main] INFO  k.c.KafkaController - [Controller 0]: Partitions that completed preferred replica election: 
14:51:07.009 [main] INFO  k.c.KafkaController - [Controller 0]: Resuming preferred replica election for partitions: 
14:51:07.009 [main] INFO  k.c.KafkaController - [Controller 0]: Partitions being reassigned: Map()
14:51:07.009 [main] INFO  k.c.KafkaController - [Controller 0]: Partitions already reassigned: List()
14:51:07.009 [main] INFO  k.c.KafkaController - [Controller 0]: Resuming reassignment of partitions: Map()
14:51:07.010 [main] INFO  k.c.KafkaController - [Controller 0]: List of topics to be deleted: 
14:51:07.010 [main] INFO  k.c.KafkaController - [Controller 0]: List of topics ineligible for deletion: 
14:51:07.010 [main] INFO  k.c.KafkaController - [Controller 0]: Currently active brokers in the cluster: Set()
14:51:07.010 [main] INFO  k.c.KafkaController - [Controller 0]: Currently shutting brokers in the cluster: Set()
14:51:07.010 [main] INFO  k.c.KafkaController - [Controller 0]: Current list of topics in the cluster: Set()
14:51:07.010 [main] INFO  k.c.ReplicaStateMachine - [Replica state machine on controller 0]: Started replica state machine with initial state -> Map()
14:51:07.010 [main] INFO  k.c.PartitionStateMachine - [Partition state machine on Controller 0]: Started partition state machine with initial state -> Map()
14:51:07.010 [main] INFO  k.c.KafkaController - [Controller 0]: Broker 0 is ready to serve as the new controller with epoch 1
14:51:07.010 [main] INFO  k.c.KafkaController - [Controller 0]: Starting preferred replica leader election for partitions 
14:51:07.010 [main] INFO  k.c.PartitionStateMachine - [Partition state machine on Controller 0]: Invoking state change to OnlinePartition for partitions 
14:51:07.011 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-021d9b47-3350-linux-15.prod.travis-ci.org/172.17.9.145
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
14:51:07.012 [main] INFO  k.c.KafkaController - [Controller 0]: starting the partition rebalance scheduler
14:51:07.012 [main] INFO  k.c.KafkaController - [Controller 0]: Controller startup complete
14:51:07.013 [ZkClient-EventThread-1165-127.0.0.1:50749] INFO  k.s.ZookeeperLeaderElector$LeaderChangeListener - New leader is 0
14:51:07.015 [main] INFO  k.u.ZkUtils$ - Registered broker 0 at path /brokers/ids/0 with address testing-worker-linux-docker-021d9b47-3350-linux-15.prod.travis-ci.org:41909.
14:51:07.015 [main] INFO  k.s.KafkaServer - [Kafka Server 0], started
14:51:07.015 [main] INFO  o.a.k.c.p.ProducerConfig - ProducerConfig values: 
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	block.on.buffer.full = true
	retry.backoff.ms = 100
	buffer.memory = 33554432
	batch.size = 16384
	metrics.sample.window.ms = 30000
	metadata.max.age.ms = 300000
	receive.buffer.bytes = 32768
	timeout.ms = 30000
	max.in.flight.requests.per.connection = 5
	bootstrap.servers = [localhost:41909]
	metric.reporters = []
	client.id = 
	compression.type = none
	retries = 0
	max.request.size = 1048576
	send.buffer.bytes = 131072
	acks = 1
	reconnect.backoff.ms = 10
	linger.ms = 0
	metrics.num.samples = 2
	metadata.fetch.timeout.ms = 1000

14:51:07.017 [ZkClient-EventThread-1165-127.0.0.1:50749] INFO  k.c.ReplicaStateMachine$BrokerChangeListener - [BrokerChangeListener on Controller 0]: Broker change listener fired for path /brokers/ids with children 0
14:51:07.019 [main] INFO  k.s.KafkaServer - [Kafka Server 0], shutting down
14:51:07.019 [main] INFO  k.s.KafkaServer - [Kafka Server 0], Starting controlled shutdown
14:51:07.022 [ZkClient-EventThread-1165-127.0.0.1:50749] INFO  k.c.ReplicaStateMachine$BrokerChangeListener - [BrokerChangeListener on Controller 0]: Newly added brokers: 0, deleted brokers: , all live brokers: 0
14:51:07.022 [ZkClient-EventThread-1165-127.0.0.1:50749] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Controller 0 connected to id:0,host:testing-worker-linux-docker-021d9b47-3350-linux-15.prod.travis-ci.org,port:41909 for sending state change requests
14:51:07.023 [ZkClient-EventThread-1165-127.0.0.1:50749] INFO  k.c.KafkaController - [Controller 0]: New broker startup callback for 0
14:51:07.025 [kafka-request-handler-0] INFO  k.c.KafkaController - [Controller 0]: Shutting down broker 0
14:51:07.025 [Controller-0-to-broker-0-send-thread] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Starting 
14:51:07.026 [main] INFO  k.s.KafkaServer - [Kafka Server 0], Controlled shutdown succeeded
14:51:07.026 [main] INFO  k.n.SocketServer - [Socket Server on Broker 0], Shutting down
14:51:07.026 [kafka-network-thread-41909-1] INFO  k.n.Processor - Closing socket connection to /172.17.9.145.
14:51:07.035 [main] INFO  k.n.SocketServer - [Socket Server on Broker 0], Shutdown completed
14:51:07.035 [main] INFO  k.s.KafkaRequestHandlerPool - [Kafka Request Handler on Broker 0], shutting down
14:51:07.035 [main] INFO  k.s.KafkaRequestHandlerPool - [Kafka Request Handler on Broker 0], shut down completely
14:51:07.112 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-021d9b47-3350-linux-15.prod.travis-ci.org/172.17.9.145
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
14:51:07.213 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-021d9b47-3350-linux-15.prod.travis-ci.org/172.17.9.145
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
14:51:07.243 [Curator-Framework-0-SendThread(127.0.0.1:43546)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
14:51:07.291 [Curator-Framework-0-SendThread(127.0.0.1:54006)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
14:51:07.314 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-021d9b47-3350-linux-15.prod.travis-ci.org/172.17.9.145
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
14:51:07.340 [Curator-Framework-0] WARN  o.a.c.ConnectionState - Connection attempt unsuccessful after 20004 (greater than max timeout of 20000). Resetting connection and trying again with a new connection.
14:51:07.401 [main] INFO  k.s.ReplicaManager - [Replica Manager on Broker 0]: Shut down
14:51:07.401 [main] INFO  k.s.ReplicaFetcherManager - [ReplicaFetcherManager on broker 0] shutting down
14:51:07.401 [main] INFO  k.s.ReplicaFetcherManager - [ReplicaFetcherManager on broker 0] shutdown completed
14:51:07.401 [main] INFO  k.s.ReplicaManager - [Replica Manager on Broker 0]: Shut down completely
14:51:07.401 [main] INFO  k.l.LogManager - Shutting down.
14:51:07.401 [main] INFO  k.l.LogManager - Shutdown complete.
14:51:07.402 [main] INFO  k.c.PartitionStateMachine - [Partition state machine on Controller 0]: Stopped partition state machine
14:51:07.402 [main] INFO  k.c.ReplicaStateMachine - [Replica state machine on controller 0]: Stopped replica state machine
14:51:07.402 [main] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Shutting down
14:51:07.402 [Controller-0-to-broker-0-send-thread] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Stopped 
14:51:07.402 [main] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Shutdown completed
14:51:07.402 [ZkClient-EventThread-1165-127.0.0.1:50749] INFO  o.I.z.ZkEventThread - Terminate ZkClient event thread.
14:51:07.404 [main] INFO  k.s.KafkaServer - [Kafka Server 0], shut down completed
14:51:07.404 [Curator-Framework-0] INFO  o.a.c.f.i.CuratorFrameworkImpl - backgroundOperationsLoop exiting

--------------------------------------------------
--------------------------------------------------
classname: org.apache.storm.kafka.bolt.KafkaBoltTest / testname: executeWithBoltSpecifiedProperties
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.kafka.bolt.KafkaBoltTest.generateTestTuple(KafkaBoltTest.java:290)
	at org.apache.storm.kafka.bolt.KafkaBoltTest.executeWithBoltSpecifiedProperties(KafkaBoltTest.java:199)

-------------------- system-out --------------------
14:51:07.417 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-021d9b47-3350-linux-15.prod.travis-ci.org/172.17.9.145
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
14:51:07.434 [Curator-Framework-0-SendThread(127.0.0.1:58001)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
14:51:07.462 [Curator-Framework-0-SendThread(127.0.0.1:41155)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
14:51:07.473 [Curator-Framework-0-SendThread(localhost:45310)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
14:51:07.518 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-021d9b47-3350-linux-15.prod.travis-ci.org/172.17.9.145
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
14:51:07.573 [Curator-Framework-0-SendThread(localhost:45310)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
14:51:07.619 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-021d9b47-3350-linux-15.prod.travis-ci.org/172.17.9.145
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
14:51:07.666 [Curator-Framework-0-SendThread(localhost:37688)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
14:51:07.706 [Curator-Framework-0-SendThread(127.0.0.1:60595)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
14:51:07.717 [Curator-Framework-0-SendThread(127.0.0.1:37288)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
14:51:07.720 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-021d9b47-3350-linux-15.prod.travis-ci.org/172.17.9.145
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
14:51:07.756 [Curator-Framework-0] ERROR o.a.c.f.i.CuratorFrameworkImpl - Background operation retry gave up
org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:99) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.curator.framework.imps.CuratorFrameworkImpl.checkBackgroundRetry(CuratorFrameworkImpl.java:728) [curator-framework-2.10.0.jar:?]
	at org.apache.curator.framework.imps.CuratorFrameworkImpl.performBackgroundOperation(CuratorFrameworkImpl.java:857) [curator-framework-2.10.0.jar:?]
	at org.apache.curator.framework.imps.CuratorFrameworkImpl.backgroundOperationsLoop(CuratorFrameworkImpl.java:809) [curator-framework-2.10.0.jar:?]
	at org.apache.curator.framework.imps.CuratorFrameworkImpl.access$300(CuratorFrameworkImpl.java:64) [curator-framework-2.10.0.jar:?]
	at org.apache.curator.framework.imps.CuratorFrameworkImpl$4.call(CuratorFrameworkImpl.java:267) [curator-framework-2.10.0.jar:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:262) [?:1.7.0_76]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:178) [?:1.7.0_76]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:292) [?:1.7.0_76]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [?:1.7.0_76]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [?:1.7.0_76]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
14:51:07.757 [Curator-Framework-0] ERROR o.a.c.f.i.CuratorFrameworkImpl - Background retry gave up
org.apache.curator.CuratorConnectionLossException: KeeperErrorCode = ConnectionLoss
	at org.apache.curator.framework.imps.CuratorFrameworkImpl.performBackgroundOperation(CuratorFrameworkImpl.java:838) [curator-framework-2.10.0.jar:?]
	at org.apache.curator.framework.imps.CuratorFrameworkImpl.backgroundOperationsLoop(CuratorFrameworkImpl.java:809) [curator-framework-2.10.0.jar:?]
	at org.apache.curator.framework.imps.CuratorFrameworkImpl.access$300(CuratorFrameworkImpl.java:64) [curator-framework-2.10.0.jar:?]
	at org.apache.curator.framework.imps.CuratorFrameworkImpl$4.call(CuratorFrameworkImpl.java:267) [curator-framework-2.10.0.jar:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:262) [?:1.7.0_76]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:178) [?:1.7.0_76]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:292) [?:1.7.0_76]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [?:1.7.0_76]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [?:1.7.0_76]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
14:51:07.767 [Curator-Framework-0-SendThread(localhost:37688)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
14:51:07.821 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-021d9b47-3350-linux-15.prod.travis-ci.org/172.17.9.145
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
14:51:07.922 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-021d9b47-3350-linux-15.prod.travis-ci.org/172.17.9.145
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
14:51:08.023 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-021d9b47-3350-linux-15.prod.travis-ci.org/172.17.9.145
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
14:51:08.124 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-021d9b47-3350-linux-15.prod.travis-ci.org/172.17.9.145
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
14:51:08.225 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-021d9b47-3350-linux-15.prod.travis-ci.org/172.17.9.145
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
14:51:08.326 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-021d9b47-3350-linux-15.prod.travis-ci.org/172.17.9.145
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
14:51:08.344 [Curator-Framework-0-SendThread(127.0.0.1:43546)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
14:51:08.392 [Curator-Framework-0-SendThread(127.0.0.1:54006)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
14:51:08.409 [main] INFO  o.a.c.f.i.CuratorFrameworkImpl - Starting
14:51:08.411 [main] INFO  k.u.VerifiableProperties - Verifying properties
14:51:08.411 [main] INFO  k.u.VerifiableProperties - Property broker.id is overridden to 0
14:51:08.411 [main] INFO  k.u.VerifiableProperties - Property log.dirs is overridden to /tmp/kafka/logs/kafka-test-44612
14:51:08.411 [main] INFO  k.u.VerifiableProperties - Property port is overridden to 44612
14:51:08.411 [main] INFO  k.u.VerifiableProperties - Property zookeeper.connect is overridden to 127.0.0.1:39728
14:51:08.411 [main] INFO  k.s.KafkaServer - [Kafka Server 0], starting
14:51:08.411 [main] INFO  k.s.KafkaServer - [Kafka Server 0], Connecting to zookeeper on 127.0.0.1:39728
14:51:08.411 [ZkClient-EventThread-1203-127.0.0.1:39728] INFO  o.I.z.ZkEventThread - Starting ZkClient event thread.
14:51:08.413 [main-EventThread] INFO  o.a.c.f.s.ConnectionStateManager - State change: CONNECTED
14:51:08.413 [main-EventThread] INFO  o.I.z.ZkClient - zookeeper state changed (SyncConnected)
14:51:08.425 [main] INFO  k.l.LogManager - Log directory '/tmp/kafka/logs/kafka-test-44612' not found, creating it.
14:51:08.426 [main] INFO  k.l.LogManager - Loading logs.
14:51:08.426 [main] INFO  k.l.LogManager - Logs loading complete.
14:51:08.426 [main] INFO  k.l.LogManager - Starting log cleanup with a period of 300000 ms.
14:51:08.426 [main] INFO  k.l.LogManager - Starting log flusher with a default period of 9223372036854775807 ms.
14:51:08.426 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-021d9b47-3350-linux-15.prod.travis-ci.org/172.17.9.145
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
14:51:08.428 [main] INFO  k.n.Acceptor - Awaiting socket connections on 0.0.0.0:44612.
14:51:08.429 [main] INFO  k.n.SocketServer - [Socket Server on Broker 0], Started
14:51:08.432 [main] INFO  k.u.Mx4jLoader$ - Will not load MX4J, mx4j-tools.jar is not in the classpath
14:51:08.433 [main] INFO  k.c.KafkaController - [Controller 0]: Controller starting up
14:51:08.435 [main] INFO  k.s.ZookeeperLeaderElector - 0 successfully elected as leader
14:51:08.435 [main] INFO  k.c.KafkaController - [Controller 0]: Broker 0 starting become controller state transition
14:51:08.437 [main] INFO  k.c.KafkaController - [Controller 0]: Controller 0 incremented epoch to 1
14:51:08.440 [main] INFO  k.c.KafkaController - [Controller 0]: Partitions undergoing preferred replica election: 
14:51:08.440 [main] INFO  k.c.KafkaController - [Controller 0]: Partitions that completed preferred replica election: 
14:51:08.440 [main] INFO  k.c.KafkaController - [Controller 0]: Resuming preferred replica election for partitions: 
14:51:08.441 [main] INFO  k.c.KafkaController - [Controller 0]: Partitions being reassigned: Map()
14:51:08.441 [main] INFO  k.c.KafkaController - [Controller 0]: Partitions already reassigned: List()
14:51:08.441 [main] INFO  k.c.KafkaController - [Controller 0]: Resuming reassignment of partitions: Map()
14:51:08.441 [main] INFO  k.c.KafkaController - [Controller 0]: List of topics to be deleted: 
14:51:08.441 [main] INFO  k.c.KafkaController - [Controller 0]: List of topics ineligible for deletion: 
14:51:08.441 [main] INFO  k.c.KafkaController - [Controller 0]: Currently active brokers in the cluster: Set()
14:51:08.441 [main] INFO  k.c.KafkaController - [Controller 0]: Currently shutting brokers in the cluster: Set()
14:51:08.442 [main] INFO  k.c.KafkaController - [Controller 0]: Current list of topics in the cluster: Set()
14:51:08.442 [main] INFO  k.c.ReplicaStateMachine - [Replica state machine on controller 0]: Started replica state machine with initial state -> Map()
14:51:08.442 [main] INFO  k.c.PartitionStateMachine - [Partition state machine on Controller 0]: Started partition state machine with initial state -> Map()
14:51:08.442 [main] INFO  k.c.KafkaController - [Controller 0]: Broker 0 is ready to serve as the new controller with epoch 1
14:51:08.442 [main] INFO  k.c.KafkaController - [Controller 0]: Starting preferred replica leader election for partitions 
14:51:08.442 [main] INFO  k.c.PartitionStateMachine - [Partition state machine on Controller 0]: Invoking state change to OnlinePartition for partitions 
14:51:08.444 [main] INFO  k.c.KafkaController - [Controller 0]: starting the partition rebalance scheduler
14:51:08.445 [main] INFO  k.c.KafkaController - [Controller 0]: Controller startup complete
14:51:08.446 [ZkClient-EventThread-1203-127.0.0.1:39728] INFO  k.s.ZookeeperLeaderElector$LeaderChangeListener - New leader is 0
14:51:08.448 [main] INFO  k.u.ZkUtils$ - Registered broker 0 at path /brokers/ids/0 with address testing-worker-linux-docker-021d9b47-3350-linux-15.prod.travis-ci.org:44612.
14:51:08.448 [main] INFO  k.s.KafkaServer - [Kafka Server 0], started
14:51:08.449 [main] INFO  o.a.k.c.p.ProducerConfig - ProducerConfig values: 
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	block.on.buffer.full = true
	retry.backoff.ms = 100
	buffer.memory = 33554432
	batch.size = 16384
	metrics.sample.window.ms = 30000
	metadata.max.age.ms = 300000
	receive.buffer.bytes = 32768
	timeout.ms = 30000
	max.in.flight.requests.per.connection = 5
	bootstrap.servers = [localhost:44612]
	metric.reporters = []
	client.id = 
	compression.type = none
	retries = 0
	max.request.size = 1048576
	send.buffer.bytes = 131072
	acks = 1
	reconnect.backoff.ms = 10
	linger.ms = 0
	metrics.num.samples = 2
	metadata.fetch.timeout.ms = 1000

14:51:08.449 [ZkClient-EventThread-1203-127.0.0.1:39728] INFO  k.c.ReplicaStateMachine$BrokerChangeListener - [BrokerChangeListener on Controller 0]: Broker change listener fired for path /brokers/ids with children 0
14:51:08.452 [main] INFO  o.a.k.c.p.ProducerConfig - ProducerConfig values: 
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	block.on.buffer.full = true
	retry.backoff.ms = 100
	buffer.memory = 33554432
	batch.size = 16384
	metrics.sample.window.ms = 30000
	metadata.max.age.ms = 300000
	receive.buffer.bytes = 32768
	timeout.ms = 30000
	max.in.flight.requests.per.connection = 5
	bootstrap.servers = [localhost:44612]
	metric.reporters = []
	client.id = 
	compression.type = none
	retries = 0
	max.request.size = 1048576
	send.buffer.bytes = 131072
	acks = 1
	reconnect.backoff.ms = 10
	linger.ms = 0
	metrics.num.samples = 2
	metadata.fetch.timeout.ms = 1000

14:51:08.456 [ZkClient-EventThread-1203-127.0.0.1:39728] INFO  k.c.ReplicaStateMachine$BrokerChangeListener - [BrokerChangeListener on Controller 0]: Newly added brokers: 0, deleted brokers: , all live brokers: 0
14:51:08.457 [main] INFO  k.s.KafkaServer - [Kafka Server 0], shutting down
14:51:08.457 [main] INFO  k.s.KafkaServer - [Kafka Server 0], Starting controlled shutdown
14:51:08.457 [ZkClient-EventThread-1203-127.0.0.1:39728] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Controller 0 connected to id:0,host:testing-worker-linux-docker-021d9b47-3350-linux-15.prod.travis-ci.org,port:44612 for sending state change requests
14:51:08.458 [ZkClient-EventThread-1203-127.0.0.1:39728] INFO  k.c.KafkaController - [Controller 0]: New broker startup callback for 0
14:51:08.458 [Controller-0-to-broker-0-send-thread] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Starting 
14:51:08.461 [kafka-request-handler-0] INFO  k.c.KafkaController - [Controller 0]: Shutting down broker 0
14:51:08.462 [main] INFO  k.s.KafkaServer - [Kafka Server 0], Controlled shutdown succeeded
14:51:08.462 [main] INFO  k.n.SocketServer - [Socket Server on Broker 0], Shutting down
14:51:08.462 [kafka-network-thread-44612-1] INFO  k.n.Processor - Closing socket connection to /172.17.9.145.
14:51:08.462 [main] INFO  k.n.SocketServer - [Socket Server on Broker 0], Shutdown completed
14:51:08.463 [main] INFO  k.s.KafkaRequestHandlerPool - [Kafka Request Handler on Broker 0], shutting down
14:51:08.463 [main] INFO  k.s.KafkaRequestHandlerPool - [Kafka Request Handler on Broker 0], shut down completely
14:51:08.527 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-021d9b47-3350-linux-15.prod.travis-ci.org/172.17.9.145
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
14:51:08.535 [Curator-Framework-0-SendThread(127.0.0.1:58001)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
14:51:08.563 [Curator-Framework-0-SendThread(127.0.0.1:41155)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
14:51:08.628 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-021d9b47-3350-linux-15.prod.travis-ci.org/172.17.9.145
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
14:51:08.674 [Curator-Framework-0-SendThread(localhost:45310)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
14:51:08.729 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-021d9b47-3350-linux-15.prod.travis-ci.org/172.17.9.145
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
14:51:08.775 [Curator-Framework-0-SendThread(localhost:45310)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
14:51:08.806 [Curator-Framework-0-SendThread(127.0.0.1:60595)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
14:51:08.818 [Curator-Framework-0-SendThread(127.0.0.1:37288)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
14:51:08.830 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-021d9b47-3350-linux-15.prod.travis-ci.org/172.17.9.145
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
14:51:08.832 [main] INFO  k.s.ReplicaManager - [Replica Manager on Broker 0]: Shut down
14:51:08.832 [main] INFO  k.s.ReplicaFetcherManager - [ReplicaFetcherManager on broker 0] shutting down
14:51:08.832 [main] INFO  k.s.ReplicaFetcherManager - [ReplicaFetcherManager on broker 0] shutdown completed
14:51:08.832 [main] INFO  k.s.ReplicaManager - [Replica Manager on Broker 0]: Shut down completely
14:51:08.832 [main] INFO  k.l.LogManager - Shutting down.
14:51:08.832 [main] INFO  k.l.LogManager - Shutdown complete.
14:51:08.832 [main] INFO  k.c.PartitionStateMachine - [Partition state machine on Controller 0]: Stopped partition state machine
14:51:08.833 [main] INFO  k.c.ReplicaStateMachine - [Replica state machine on controller 0]: Stopped replica state machine
14:51:08.833 [main] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Shutting down
14:51:08.833 [Controller-0-to-broker-0-send-thread] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Stopped 
14:51:08.833 [main] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Shutdown completed
14:51:08.833 [ZkClient-EventThread-1203-127.0.0.1:39728] INFO  o.I.z.ZkEventThread - Terminate ZkClient event thread.
14:51:08.835 [main] INFO  k.s.KafkaServer - [Kafka Server 0], shut down completed
14:51:08.835 [Curator-Framework-0] INFO  o.a.c.f.i.CuratorFrameworkImpl - backgroundOperationsLoop exiting

--------------------------------------------------
Looking for errors in ./external/storm-redis/target/surefire-reports
Checking ./external/storm-redis/target/surefire-reports/TEST-org.apache.storm.redis.state.RedisKeyValueStateProviderTest.xml
Checking ./external/storm-redis/target/surefire-reports/TEST-org.apache.storm.redis.state.DefaultStateSerializerTest.xml
Checking ./external/storm-redis/target/surefire-reports/TEST-org.apache.storm.redis.state.RedisKeyValueStateTest.xml

travis_time:end:1bfca116:start=1466779786425156793,finish=1466780064321198647,duration=277896041854[0K
[31;1mThe command "/bin/bash ./dev-tools/travis/travis-script.sh `pwd` $MODULES" exited with 1.[0m
travis_fold:start:cache.2[0Kstore build cache
travis_time:start:370aaac2[0K
travis_time:end:370aaac2:start=1466780064326974051,finish=1466780064331235708,duration=4261657[0Ktravis_time:start:01016bb0[0K[32;1mchange detected (content changed, file is created, or file is deleted):
/home/travis/.m2/repository/eigenbase/eigenbase-properties/1.1.4/eigenbase-properties-1.1.4.pom.lastUpdated
/home/travis/.m2/repository/io/confluent/kafka-avro-serializer/1.0/kafka-avro-serializer-1.0.pom.lastUpdated
/home/travis/.m2/repository/io/confluent/kafka-schema-registry-client/1.0/kafka-schema-registry-client-1.0.pom.lastUpdated
/home/travis/.m2/repository/net/hydromatic/linq4j/0.4/linq4j-0.4.pom.lastUpdated
/home/travis/.m2/repository/net/hydromatic/quidem/0.1.1/quidem-0.1.1.pom.lastUpdated
/home/travis/.m2/repository/org/apache/storm/flux/2.0.0-SNAPSHOT/maven-metadata-local.xml
/home/travis/.m2/repository/org/apache/storm/flux/2.0.0-SNAPSHOT/_remote.repositories
/home/travis/.m2/repository/org/apache/storm/flux-core/2.0.0-SNAPSHOT/flux-core-2.0.0-SNAPSHOT.jar
/home/travis/.m2/repository/org/apache/storm/flux-core/2.0.0-SNAPSHOT/maven-metadata-local.xml
/home/travis/.m2/repository/org/apache/storm/flux-core/2.0.0-SNAPSHOT/_remote.repositories
/home/travis/.m2/repository/org/ap
[0m
[32;1m...
[0m
[32;1mchanges detected, packing new archive[0m
.
.
.
.
.
.
[32;1muploading archive[0m

travis_time:end:01016bb0:start=1466780064336558997,finish=1466780117861725630,duration=53525166633[0Ktravis_fold:end:cache.2[0K
Done. Your build exited with 1.
