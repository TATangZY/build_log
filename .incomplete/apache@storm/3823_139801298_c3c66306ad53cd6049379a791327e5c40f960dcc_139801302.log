Using worker: worker-linux-docker-e9cd6d11.prod.travis-ci.org:travis-linux-10

travis_fold:start:system_info[0K[33;1mBuild system information[0m
Build language: java
Build group: stable
Build dist: precise
[34m[1mBuild image provisioning date and time[0m
Thu Feb  5 15:09:33 UTC 2015
[34m[1mOperating System Details[0m
Distributor ID:	Ubuntu
Description:	Ubuntu 12.04.5 LTS
Release:	12.04
Codename:	precise
[34m[1mLinux Version[0m
3.13.0-29-generic
[34m[1mCookbooks Version[0m
a68419e https://github.com/travis-ci/travis-cookbooks/tree/a68419e
[34m[1mGCC version[0m
gcc (Ubuntu/Linaro 4.6.3-1ubuntu5) 4.6.3
Copyright (C) 2011 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.

[34m[1mLLVM version[0m
clang version 3.4 (tags/RELEASE_34/final)
Target: x86_64-unknown-linux-gnu
Thread model: posix
[34m[1mPre-installed Ruby versions[0m
ruby-1.9.3-p551
[34m[1mPre-installed Node.js versions[0m
v0.10.36
[34m[1mPre-installed Go versions[0m
1.4.1
[34m[1mRedis version[0m
redis-server 2.8.19
[34m[1mriak version[0m
2.0.2
[34m[1mMongoDB version[0m
MongoDB 2.4.12
[34m[1mCouchDB version[0m
couchdb 1.6.1
[34m[1mNeo4j version[0m
1.9.4
[34m[1mRabbitMQ Version[0m
3.4.3
[34m[1mElasticSearch version[0m
1.4.0
[34m[1mInstalled Sphinx versions[0m
2.0.10
2.1.9
2.2.6
[34m[1mDefault Sphinx version[0m
2.2.6
[34m[1mInstalled Firefox version[0m
firefox 31.0esr
[34m[1mPhantomJS version[0m
1.9.8
[34m[1mant -version[0m
Apache Ant(TM) version 1.8.2 compiled on December 3 2011
[34m[1mmvn -version[0m
Apache Maven 3.2.5 (12a6b3acb947671f09b81f49094c53f426d8cea1; 2014-12-14T17:29:23+00:00)
Maven home: /usr/local/maven
Java version: 1.7.0_76, vendor: Oracle Corporation
Java home: /usr/lib/jvm/java-7-oracle/jre
Default locale: en_US, platform encoding: ANSI_X3.4-1968
OS name: "linux", version: "3.13.0-29-generic", arch: "amd64", family: "unix"
travis_fold:end:system_info[0K
travis_fold:start:fix.CVE-2015-7547[0K$ export DEBIAN_FRONTEND=noninteractive
W: Size of file /var/lib/apt/lists/us.archive.ubuntu.com_ubuntu_dists_precise-backports_multiverse_source_Sources.gz is not what the server reported 5886 5888
W: Failed to fetch http://ppa.launchpad.net/couchdb/stable/ubuntu/dists/precise/Release.gpg  Could not connect to ppa.launchpad.net:80 (91.189.95.83), connection timed out

W: Failed to fetch http://ppa.launchpad.net/git-core/v1.8/ubuntu/dists/precise/Release.gpg  Unable to connect to ppa.launchpad.net:http:

W: Failed to fetch http://ppa.launchpad.net/rwky/redis/ubuntu/dists/precise/Release.gpg  Unable to connect to ppa.launchpad.net:http:

W: Failed to fetch http://ppa.launchpad.net/travis-ci/zero-mq/ubuntu/dists/precise/Release.gpg  Unable to connect to ppa.launchpad.net:http:

W: Failed to fetch http://ppa.launchpad.net/ubuntugis/ppa/ubuntu/dists/precise/Release.gpg  Unable to connect to ppa.launchpad.net:http:

W: Failed to fetch http://ppa.launchpad.net/webupd8team/java/ubuntu/dists/precise/Release.gpg  Unable to connect to ppa.launchpad.net:http:

W: Some index files failed to download. They have been ignored, or old ones used instead.
Reading package lists...
Building dependency tree...
Reading state information...
The following extra packages will be installed:
  libc-bin libc-dev-bin libc6-dev
Suggested packages:
  glibc-doc
The following packages will be upgraded:
  libc-bin libc-dev-bin libc6 libc6-dev
4 upgraded, 0 newly installed, 0 to remove and 244 not upgraded.
Need to get 8,840 kB of archives.
After this operation, 14.3 kB disk space will be freed.
Get:1 http://us.archive.ubuntu.com/ubuntu/ precise-updates/main libc6-dev amd64 2.15-0ubuntu10.15 [2,943 kB]
Get:2 http://us.archive.ubuntu.com/ubuntu/ precise-updates/main libc-dev-bin amd64 2.15-0ubuntu10.15 [84.7 kB]
Get:3 http://us.archive.ubuntu.com/ubuntu/ precise-updates/main libc-bin amd64 2.15-0ubuntu10.15 [1,177 kB]
Get:4 http://us.archive.ubuntu.com/ubuntu/ precise-updates/main libc6 amd64 2.15-0ubuntu10.15 [4,636 kB]
Fetched 8,840 kB in 0s (20.1 MB/s)
Preconfiguring packages ...
(Reading database ... (Reading database ... 5%(Reading database ... 10%(Reading database ... 15%(Reading database ... 20%(Reading database ... 25%(Reading database ... 30%(Reading database ... 35%(Reading database ... 40%(Reading database ... 45%(Reading database ... 50%(Reading database ... 55%(Reading database ... 60%(Reading database ... 65%(Reading database ... 70%(Reading database ... 75%(Reading database ... 80%(Reading database ... 85%(Reading database ... 90%(Reading database ... 95%(Reading database ... 100%(Reading database ... 72019 files and directories currently installed.)
Preparing to replace libc6-dev 2.15-0ubuntu10.10 (using .../libc6-dev_2.15-0ubuntu10.15_amd64.deb) ...
Unpacking replacement libc6-dev ...
Preparing to replace libc-dev-bin 2.15-0ubuntu10.10 (using .../libc-dev-bin_2.15-0ubuntu10.15_amd64.deb) ...
Unpacking replacement libc-dev-bin ...
Preparing to replace libc-bin 2.15-0ubuntu10.10 (using .../libc-bin_2.15-0ubuntu10.15_amd64.deb) ...
Unpacking replacement libc-bin ...
Processing triggers for man-db ...
Setting up libc-bin (2.15-0ubuntu10.15) ...
(Reading database ... (Reading database ... 5%(Reading database ... 10%(Reading database ... 15%(Reading database ... 20%(Reading database ... 25%(Reading database ... 30%(Reading database ... 35%(Reading database ... 40%(Reading database ... 45%(Reading database ... 50%(Reading database ... 55%(Reading database ... 60%(Reading database ... 65%(Reading database ... 70%(Reading database ... 75%(Reading database ... 80%(Reading database ... 85%(Reading database ... 90%(Reading database ... 95%(Reading database ... 100%(Reading database ... 72018 files and directories currently installed.)
Preparing to replace libc6 2.15-0ubuntu10.10 (using .../libc6_2.15-0ubuntu10.15_amd64.deb) ...
Unpacking replacement libc6 ...
Setting up libc6 (2.15-0ubuntu10.15) ...
Setting up libc-dev-bin (2.15-0ubuntu10.15) ...
Setting up libc6-dev (2.15-0ubuntu10.15) ...
Processing triggers for libc-bin ...
ldconfig deferred processing now taking place
travis_fold:end:fix.CVE-2015-7547[0Ktravis_fold:start:git.checkout[0Ktravis_time:start:15eeae68[0K$ git clone --depth=50 https://github.com/apache/storm.git apache/storm
Cloning into 'apache/storm'...
remote: Counting objects: 17967, done.[K
remote: Compressing objects:   0% (1/7336)   [Kremote: Compressing objects:   1% (74/7336)   [Kremote: Compressing objects:   2% (147/7336)   [Kremote: Compressing objects:   3% (221/7336)   [Kremote: Compressing objects:   4% (294/7336)   [Kremote: Compressing objects:   5% (367/7336)   [Kremote: Compressing objects:   6% (441/7336)   [Kremote: Compressing objects:   7% (514/7336)   [Kremote: Compressing objects:   8% (587/7336)   [Kremote: Compressing objects:   9% (661/7336)   [Kremote: Compressing objects:  10% (734/7336)   [Kremote: Compressing objects:  11% (807/7336)   [Kremote: Compressing objects:  12% (881/7336)   [Kremote: Compressing objects:  13% (954/7336)   [Kremote: Compressing objects:  14% (1028/7336)   [Kremote: Compressing objects:  15% (1101/7336)   [Kremote: Compressing objects:  16% (1174/7336)   [Kremote: Compressing objects:  17% (1248/7336)   [Kremote: Compressing objects:  18% (1321/7336)   [Kremote: Compressing objects:  19% (1394/7336)   [Kremote: Compressing objects:  20% (1468/7336)   [Kremote: Compressing objects:  21% (1541/7336)   [Kremote: Compressing objects:  22% (1614/7336)   [Kremote: Compressing objects:  23% (1688/7336)   [Kremote: Compressing objects:  24% (1761/7336)   [Kremote: Compressing objects:  25% (1834/7336)   [Kremote: Compressing objects:  26% (1908/7336)   [Kremote: Compressing objects:  27% (1981/7336)   [Kremote: Compressing objects:  28% (2055/7336)   [Kremote: Compressing objects:  29% (2128/7336)   [Kremote: Compressing objects:  30% (2201/7336)   [Kremote: Compressing objects:  31% (2275/7336)   [Kremote: Compressing objects:  32% (2348/7336)   [Kremote: Compressing objects:  33% (2421/7336)   [Kremote: Compressing objects:  34% (2495/7336)   [Kremote: Compressing objects:  35% (2568/7336)   [Kremote: Compressing objects:  36% (2641/7336)   [Kremote: Compressing objects:  37% (2715/7336)   [Kremote: Compressing objects:  38% (2788/7336)   [Kremote: Compressing objects:  39% (2862/7336)   [Kremote: Compressing objects:  40% (2935/7336)   [Kremote: Compressing objects:  41% (3008/7336)   [Kremote: Compressing objects:  42% (3082/7336)   [Kremote: Compressing objects:  43% (3155/7336)   [Kremote: Compressing objects:  44% (3228/7336)   [Kremote: Compressing objects:  45% (3302/7336)   [Kremote: Compressing objects:  46% (3375/7336)   [Kremote: Compressing objects:  47% (3448/7336)   [Kremote: Compressing objects:  48% (3522/7336)   [Kremote: Compressing objects:  49% (3595/7336)   [Kremote: Compressing objects:  50% (3668/7336)   [Kremote: Compressing objects:  51% (3742/7336)   [Kremote: Compressing objects:  52% (3815/7336)   [Kremote: Compressing objects:  53% (3889/7336)   [Kremote: Compressing objects:  54% (3962/7336)   [Kremote: Compressing objects:  55% (4035/7336)   [Kremote: Compressing objects:  56% (4109/7336)   [Kremote: Compressing objects:  57% (4182/7336)   [Kremote: Compressing objects:  58% (4255/7336)   [Kremote: Compressing objects:  59% (4329/7336)   [Kremote: Compressing objects:  60% (4402/7336)   [Kremote: Compressing objects:  61% (4475/7336)   [Kremote: Compressing objects:  62% (4549/7336)   [Kremote: Compressing objects:  63% (4622/7336)   [Kremote: Compressing objects:  64% (4696/7336)   [Kremote: Compressing objects:  65% (4769/7336)   [Kremote: Compressing objects:  66% (4842/7336)   [Kremote: Compressing objects:  67% (4916/7336)   [Kremote: Compressing objects:  68% (4989/7336)   [Kremote: Compressing objects:  69% (5062/7336)   [Kremote: Compressing objects:  70% (5136/7336)   [Kremote: Compressing objects:  71% (5209/7336)   [Kremote: Compressing objects:  72% (5282/7336)   [Kremote: Compressing objects:  73% (5356/7336)   [Kremote: Compressing objects:  74% (5429/7336)   [Kremote: Compressing objects:  75% (5502/7336)   [Kremote: Compressing objects:  76% (5576/7336)   [Kremote: Compressing objects:  77% (5649/7336)   [Kremote: Compressing objects:  78% (5723/7336)   [Kremote: Compressing objects:  79% (5796/7336)   [Kremote: Compressing objects:  80% (5869/7336)   [Kremote: Compressing objects:  81% (5943/7336)   [Kremote: Compressing objects:  82% (6016/7336)   [Kremote: Compressing objects:  83% (6089/7336)   [Kremote: Compressing objects:  84% (6163/7336)   [Kremote: Compressing objects:  85% (6236/7336)   [Kremote: Compressing objects:  86% (6309/7336)   [Kremote: Compressing objects:  87% (6383/7336)   [Kremote: Compressing objects:  88% (6456/7336)   [Kremote: Compressing objects:  89% (6530/7336)   [Kremote: Compressing objects:  90% (6603/7336)   [Kremote: Compressing objects:  91% (6676/7336)   [Kremote: Compressing objects:  92% (6750/7336)   [Kremote: Compressing objects:  93% (6823/7336)   [Kremote: Compressing objects:  94% (6896/7336)   [Kremote: Compressing objects:  95% (6970/7336)   [Kremote: Compressing objects:  96% (7043/7336)   [Kremote: Compressing objects:  97% (7116/7336)   [Kremote: Compressing objects:  98% (7190/7336)   [Kremote: Compressing objects:  99% (7263/7336)   [Kremote: Compressing objects: 100% (7336/7336)   [Kremote: Compressing objects: 100% (7336/7336), done.[K
Receiving objects:   0% (1/17967)   Receiving objects:   1% (180/17967)   Receiving objects:   2% (360/17967)   Receiving objects:   3% (540/17967)   Receiving objects:   4% (719/17967)   Receiving objects:   5% (899/17967)   Receiving objects:   6% (1079/17967)   Receiving objects:   7% (1258/17967)   Receiving objects:   8% (1438/17967)   Receiving objects:   9% (1618/17967)   Receiving objects:  10% (1797/17967)   Receiving objects:  11% (1977/17967)   Receiving objects:  12% (2157/17967)   Receiving objects:  13% (2336/17967)   Receiving objects:  14% (2516/17967)   Receiving objects:  15% (2696/17967)   Receiving objects:  16% (2875/17967)   Receiving objects:  17% (3055/17967)   Receiving objects:  18% (3235/17967)   Receiving objects:  19% (3414/17967)   Receiving objects:  20% (3594/17967)   Receiving objects:  21% (3774/17967)   Receiving objects:  22% (3953/17967)   Receiving objects:  23% (4133/17967)   Receiving objects:  24% (4313/17967)   Receiving objects:  25% (4492/17967)   Receiving objects:  26% (4672/17967)   Receiving objects:  27% (4852/17967)   Receiving objects:  28% (5031/17967)   Receiving objects:  29% (5211/17967)   Receiving objects:  30% (5391/17967)   Receiving objects:  31% (5570/17967)   Receiving objects:  32% (5750/17967)   Receiving objects:  33% (5930/17967)   Receiving objects:  34% (6109/17967)   Receiving objects:  35% (6289/17967)   Receiving objects:  36% (6469/17967)   Receiving objects:  37% (6648/17967)   Receiving objects:  38% (6828/17967)   Receiving objects:  39% (7008/17967)   Receiving objects:  40% (7187/17967)   Receiving objects:  41% (7367/17967)   Receiving objects:  42% (7547/17967)   Receiving objects:  43% (7726/17967)   Receiving objects:  44% (7906/17967)   Receiving objects:  45% (8086/17967)   Receiving objects:  46% (8265/17967)   Receiving objects:  47% (8445/17967)   Receiving objects:  48% (8625/17967)   Receiving objects:  49% (8804/17967)   Receiving objects:  50% (8984/17967)   Receiving objects:  51% (9164/17967)   Receiving objects:  52% (9343/17967)   Receiving objects:  53% (9523/17967)   Receiving objects:  54% (9703/17967)   Receiving objects:  55% (9882/17967)   Receiving objects:  56% (10062/17967)   Receiving objects:  57% (10242/17967)   Receiving objects:  58% (10421/17967)   Receiving objects:  59% (10601/17967)   Receiving objects:  60% (10781/17967)   Receiving objects:  61% (10960/17967)   Receiving objects:  62% (11140/17967)   Receiving objects:  63% (11320/17967)   Receiving objects:  64% (11499/17967)   Receiving objects:  65% (11679/17967)   Receiving objects:  66% (11859/17967)   Receiving objects:  67% (12038/17967)   Receiving objects:  68% (12218/17967)   Receiving objects:  69% (12398/17967)   Receiving objects:  70% (12577/17967)   Receiving objects:  71% (12757/17967)   Receiving objects:  72% (12937/17967)   Receiving objects:  73% (13116/17967)   Receiving objects:  74% (13296/17967)   Receiving objects:  75% (13476/17967)   Receiving objects:  76% (13655/17967)   Receiving objects:  77% (13835/17967)   Receiving objects:  78% (14015/17967)   Receiving objects:  79% (14194/17967)   Receiving objects:  80% (14374/17967), 11.68 MiB | 23.30 MiB/s   Receiving objects:  81% (14554/17967), 11.68 MiB | 23.30 MiB/s   Receiving objects:  82% (14733/17967), 11.68 MiB | 23.30 MiB/s   Receiving objects:  83% (14913/17967), 11.68 MiB | 23.30 MiB/s   Receiving objects:  84% (15093/17967), 11.68 MiB | 23.30 MiB/s   Receiving objects:  85% (15272/17967), 11.68 MiB | 23.30 MiB/s   Receiving objects:  86% (15452/17967), 11.68 MiB | 23.30 MiB/s   Receiving objects:  87% (15632/17967), 11.68 MiB | 23.30 MiB/s   Receiving objects:  88% (15811/17967), 11.68 MiB | 23.30 MiB/s   Receiving objects:  89% (15991/17967), 11.68 MiB | 23.30 MiB/s   Receiving objects:  90% (16171/17967), 11.68 MiB | 23.30 MiB/s   Receiving objects:  91% (16350/17967), 11.68 MiB | 23.30 MiB/s   Receiving objects:  92% (16530/17967), 11.68 MiB | 23.30 MiB/s   Receiving objects:  93% (16710/17967), 11.68 MiB | 23.30 MiB/s   Receiving objects:  94% (16889/17967), 11.68 MiB | 23.30 MiB/s   Receiving objects:  95% (17069/17967), 11.68 MiB | 23.30 MiB/s   Receiving objects:  96% (17249/17967), 11.68 MiB | 23.30 MiB/s   Receiving objects:  97% (17428/17967), 11.68 MiB | 23.30 MiB/s   remote: Total 17967 (delta 8565), reused 15297 (delta 6521), pack-reused 0[K
Receiving objects:  98% (17608/17967), 11.68 MiB | 23.30 MiB/s   Receiving objects:  99% (17788/17967), 11.68 MiB | 23.30 MiB/s   Receiving objects: 100% (17967/17967), 11.68 MiB | 23.30 MiB/s   Receiving objects: 100% (17967/17967), 14.30 MiB | 23.30 MiB/s, done.
Resolving deltas:   0% (0/8565)   Resolving deltas:   1% (89/8565)   Resolving deltas:   2% (172/8565)   Resolving deltas:   3% (287/8565)   Resolving deltas:   4% (344/8565)   Resolving deltas:   5% (440/8565)   Resolving deltas:   6% (514/8565)   Resolving deltas:   7% (603/8565)   Resolving deltas:   8% (710/8565)   Resolving deltas:   9% (773/8565)   Resolving deltas:  10% (861/8565)   Resolving deltas:  11% (962/8565)   Resolving deltas:  12% (1030/8565)   Resolving deltas:  13% (1130/8565)   Resolving deltas:  14% (1213/8565)   Resolving deltas:  15% (1287/8565)   Resolving deltas:  16% (1372/8565)   Resolving deltas:  17% (1463/8565)   Resolving deltas:  18% (1543/8565)   Resolving deltas:  19% (1632/8565)   Resolving deltas:  20% (1717/8565)   Resolving deltas:  21% (1800/8565)   Resolving deltas:  22% (1887/8565)   Resolving deltas:  23% (1970/8565)   Resolving deltas:  24% (2058/8565)   Resolving deltas:  25% (2146/8565)   Resolving deltas:  26% (2230/8565)   Resolving deltas:  27% (2313/8565)   Resolving deltas:  28% (2399/8565)   Resolving deltas:  29% (2530/8565)   Resolving deltas:  30% (2575/8565)   Resolving deltas:  31% (2658/8565)   Resolving deltas:  32% (2752/8565)   Resolving deltas:  33% (2865/8565)   Resolving deltas:  34% (2981/8565)   Resolving deltas:  35% (3051/8565)   Resolving deltas:  36% (3092/8565)   Resolving deltas:  37% (3182/8565)   Resolving deltas:  38% (3265/8565)   Resolving deltas:  39% (3342/8565)   Resolving deltas:  40% (3432/8565)   Resolving deltas:  41% (3512/8565)   Resolving deltas:  42% (3610/8565)   Resolving deltas:  43% (3685/8565)   Resolving deltas:  44% (3769/8565)   Resolving deltas:  45% (3860/8565)   Resolving deltas:  46% (3940/8565)   Resolving deltas:  47% (4026/8565)   Resolving deltas:  48% (4112/8565)   Resolving deltas:  49% (4203/8565)   Resolving deltas:  50% (4283/8565)   Resolving deltas:  51% (4369/8565)   Resolving deltas:  52% (4471/8565)   Resolving deltas:  53% (4542/8565)   Resolving deltas:  54% (4629/8565)   Resolving deltas:  55% (4760/8565)   Resolving deltas:  56% (4798/8565)   Resolving deltas:  57% (4885/8565)   Resolving deltas:  58% (4974/8565)   Resolving deltas:  59% (5058/8565)   Resolving deltas:  60% (5142/8565)   Resolving deltas:  61% (5225/8565)   Resolving deltas:  62% (5321/8565)   Resolving deltas:  64% (5556/8565)   Resolving deltas:  69% (5963/8565)   Resolving deltas:  70% (6068/8565)   Resolving deltas:  71% (6082/8565)   Resolving deltas:  72% (6172/8565)   Resolving deltas:  74% (6423/8565)   Resolving deltas:  75% (6476/8565)   Resolving deltas:  76% (6512/8565)   Resolving deltas:  77% (6673/8565)   Resolving deltas:  78% (6740/8565)   Resolving deltas:  79% (6767/8565)   Resolving deltas:  80% (6853/8565)   Resolving deltas:  81% (6948/8565)   Resolving deltas:  82% (7036/8565)   Resolving deltas:  83% (7109/8565)   Resolving deltas:  84% (7197/8565)   Resolving deltas:  85% (7286/8565)   Resolving deltas:  86% (7371/8565)   Resolving deltas:  87% (7455/8565)   Resolving deltas:  88% (7538/8565)   Resolving deltas:  89% (7626/8565)   Resolving deltas:  92% (7890/8565)   Resolving deltas:  93% (8031/8565)   Resolving deltas:  94% (8061/8565)   Resolving deltas:  95% (8137/8565)   Resolving deltas:  96% (8225/8565)   Resolving deltas:  97% (8314/8565)   Resolving deltas:  98% (8394/8565)   Resolving deltas:  99% (8494/8565)   Resolving deltas: 100% (8565/8565)   Resolving deltas: 100% (8565/8565), done.
Checking connectivity... done.

travis_time:end:15eeae68:start=1466698795605701369,finish=1466698797997489329,duration=2391787960[0K$ cd apache/storm
travis_time:start:3da77e4b[0K$ git fetch origin +refs/pull/1515/merge:
remote: Counting objects: 11, done.[K
remote: Compressing objects:  33% (1/3)   [Kremote: Compressing objects:  66% (2/3)   [Kremote: Compressing objects: 100% (3/3)   [Kremote: Compressing objects: 100% (3/3), done.[K
remote: Total 11 (delta 6), reused 9 (delta 5), pack-reused 0[K
Unpacking objects:   9% (1/11)   Unpacking objects:  18% (2/11)   Unpacking objects:  27% (3/11)   Unpacking objects:  36% (4/11)   Unpacking objects:  45% (5/11)   Unpacking objects:  54% (6/11)   Unpacking objects:  63% (7/11)   Unpacking objects:  72% (8/11)   Unpacking objects:  81% (9/11)   Unpacking objects:  90% (10/11)   Unpacking objects: 100% (11/11)   Unpacking objects: 100% (11/11), done.
From https://github.com/apache/storm
 * branch            refs/pull/1515/merge -> FETCH_HEAD

travis_time:end:3da77e4b:start=1466698798001634967,finish=1466698798300014314,duration=298379347[0K$ git checkout -qf FETCH_HEAD
travis_fold:end:git.checkout[0K
[33;1mThis job is running on container-based infrastructure, which does not allow use of 'sudo', setuid and setguid executables.[0m
[33;1mIf you require sudo, add 'sudo: required' to your .travis.yml[0m
[33;1mSee https://docs.travis-ci.com/user/workers/container-based-infrastructure/ for details.[0m

[33;1mSetting environment variables from .travis.yml[0m
$ export MODULES='!storm-core'

$ jdk_switcher use oraclejdk7
Switching to Oracle JDK7 (java-7-oracle), JAVA_HOME will be set to /usr/lib/jvm/java-7-oracle
travis_fold:start:cache.1[0KSetting up build cache
$ export CASHER_DIR=$HOME/.casher
travis_time:start:256419d2[0K$ Installing caching utilities

travis_time:end:256419d2:start=1466698800252123796,finish=1466698800350424316,duration=98300520[0Ktravis_time:start:04e0fe40[0K
travis_time:end:04e0fe40:start=1466698800355179780,finish=1466698800358459720,duration=3279940[0Ktravis_time:start:19b1d690[0K[32;1mattempting to download cache archive[0m
[32;1mfetching PR.1515/cache-linux-precise-0b3559b5dae2b3e32156c6d840f23972d650aa066922e3318f7e5db8246681fa--jdk-oraclejdk7.tgz[0m
[32;1mfetching PR.1515/cache--jdk-oraclejdk7.tgz[0m
[32;1mfetching master/cache-linux-precise-0b3559b5dae2b3e32156c6d840f23972d650aa066922e3318f7e5db8246681fa--jdk-oraclejdk7.tgz[0m
[32;1mfound cache[0m

travis_time:end:19b1d690:start=1466698800362281742,finish=1466698815024648006,duration=14662366264[0Ktravis_time:start:01d8d271[0K
travis_time:end:01d8d271:start=1466698815028674307,finish=1466698815032351943,duration=3677636[0Ktravis_time:start:170f35e5[0K[32;1madding /home/travis/.m2/repository to cache[0m
[32;1madding /home/travis/.rvm to cache[0m
[32;1madding /home/travis/.nvm to cache[0m

travis_time:end:170f35e5:start=1466698815036686388,finish=1466698824951709106,duration=9915022718[0Ktravis_fold:end:cache.1[0K$ java -Xmx32m -version
java version "1.7.0_76"
Java(TM) SE Runtime Environment (build 1.7.0_76-b13)
Java HotSpot(TM) 64-Bit Server VM (build 24.76-b04, mixed mode)
$ javac -J-Xmx32m -version
javac 1.7.0_76
travis_fold:start:before_install.1[0Ktravis_time:start:08975808[0K$ rvm use 2.1.5 --install
[32mUsing /home/travis/.rvm/gems/ruby-2.1.5[0m

travis_time:end:08975808:start=1466698825447384906,finish=1466698825722237722,duration=274852816[0Ktravis_fold:end:before_install.1[0Ktravis_fold:start:before_install.2[0Ktravis_time:start:0bd10488[0K$ nvm install 0.12.2
v0.12.2 is already installed.
Now using node v0.12.2

travis_time:end:0bd10488:start=1466698825726561948,finish=1466698825988272981,duration=261711033[0Ktravis_fold:end:before_install.2[0Ktravis_fold:start:before_install.3[0Ktravis_time:start:114c6c1c[0K$ nvm use 0.12.2
Now using node v0.12.2

travis_time:end:114c6c1c:start=1466698825992502675,finish=1466698826054773660,duration=62270985[0Ktravis_fold:end:before_install.3[0Ktravis_fold:start:install[0Ktravis_time:start:19c3fdea[0K$ /bin/bash ./dev-tools/travis/travis-install.sh `pwd`
Python version :   Python 2.7.3
Maven version  :   Apache Maven 3.2.5 (12a6b3acb947671f09b81f49094c53f426d8cea1; 2014-12-14T17:29:23+00:00) Maven home: /usr/local/maven Java version: 1.7.0_76, vendor: Oracle Corporation Java home: /usr/lib/jvm/java-7-oracle/jre Default locale: en_US, platform encoding: UTF-8 OS name: "linux", version: "3.13.0-40-generic", arch: "amd64", family: "unix"
['mvn', 'clean', 'install', '-DskipTests', '-Pnative', '--batch-mode'] writing to install.txt
1 seconds 1 log lines12 seconds 237 log lines35 seconds 323 log lines47 seconds 334 log lines57 seconds 554 log lines74 seconds 652 log lines84 seconds 865 log lines95 seconds 914 log lines105 seconds 1141 log lines125 seconds 1473 log lines136 seconds 1796 log lines146 seconds 2199 log lines160 seconds 2377 log lines173 seconds 2679 log lines173 seconds 2680 log lines
['mvn', 'clean', 'install', '-DskipTests', '-Pnative', '--batch-mode'] done 0

travis_time:end:19c3fdea:start=1466698826059131856,finish=1466699000208553578,duration=174149421722[0Ktravis_fold:end:install[0Ktravis_time:start:160d6f40[0K$ /bin/bash ./dev-tools/travis/travis-script.sh `pwd` $MODULES
Python version :   Python 2.7.3
Ruby version   :   ruby 2.1.5p273 (2014-11-13 revision 48405) [x86_64-linux]
NodeJs version :   v0.12.2
Maven version  :   Apache Maven 3.2.5 (12a6b3acb947671f09b81f49094c53f426d8cea1; 2014-12-14T17:29:23+00:00) Maven home: /usr/local/maven Java version: 1.7.0_76, vendor: Oracle Corporation Java home: /usr/lib/jvm/java-7-oracle/jre Default locale: en_US, platform encoding: UTF-8 OS name: "linux", version: "3.13.0-40-generic", arch: "amd64", family: "unix"
[INFO] Scanning for projects...
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Build Order:
[INFO] 
[INFO] Storm
[INFO] multilang-javascript
[INFO] multilang-python
[INFO] multilang-ruby
[INFO] maven-shade-clojure-transformer
[INFO] storm-maven-plugins
[INFO] storm-rename-hack
[INFO] storm-kafka
[INFO] storm-hdfs
[INFO] storm-hbase
[INFO] storm-hive
[INFO] storm-jdbc
[INFO] storm-redis
[INFO] storm-eventhubs
[INFO] flux
[INFO] flux-wrappers
[INFO] flux-core
[INFO] flux-examples
[INFO] storm-sql-runtime
[INFO] storm-sql-core
[INFO] storm-sql-kafka
[INFO] sql
[INFO] storm-elasticsearch
[INFO] storm-solr
[INFO] storm-metrics
[INFO] storm-cassandra
[INFO] storm-mqtt-parent
[INFO] storm-mqtt
[INFO] storm-mqtt-examples
[INFO] storm-mongodb
[INFO] storm-clojure
[INFO] storm-starter
[INFO] storm-kafka-client
[INFO] storm-opentsdb
[INFO] storm-kafka-monitor
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Storm 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ storm ---
[INFO] 
[INFO] --- apache-rat-plugin:0.11:check (default) @ storm ---
[INFO] 51 implicit excludes (use -debug for more details).
[INFO] Exclude: **/target/**
[INFO] Exclude: **/*.iml
[INFO] Exclude: **/.idea/**
[INFO] Exclude: **/metastore_db/**
[INFO] Exclude: **/build/**
[INFO] Exclude: **/logs/**
[INFO] Exclude: **/CHANGELOG.md
[INFO] Exclude: **/README.md
[INFO] Exclude: **/README.markdown
[INFO] Exclude: **/DEVELOPER.md
[INFO] Exclude: **/BYLAWS.md
[INFO] Exclude: **/STORM-UI-REST-API.md
[INFO] Exclude: SECURITY.md
[INFO] Exclude: VERSION
[INFO] Exclude: TODO
[INFO] Exclude: **/src/py/**
[INFO] Exclude: **/src/ui/public/js/jquery.dataTables.1.10.4.min.js
[INFO] Exclude: **/src/ui/public/css/jquery.dataTables.1.10.4.min.css
[INFO] Exclude: **/src/ui/public/images/*
[INFO] Exclude: **/src/ui/public/js/bootstrap-3.3.1.min.js
[INFO] Exclude: **/src/ui/public/css/bootstrap-3.3.1.min.css
[INFO] Exclude: **/src/ui/public/js/dataTables.bootstrap.min.js
[INFO] Exclude: **/src/ui/public/css/dataTables.bootstrap.css
[INFO] Exclude: **/src/ui/public/js/jsonFormatter.min.js
[INFO] Exclude: **/src/ui/public/css/jsonFormatter.min.css
[INFO] Exclude: **/src/ui/public/js/jquery-1.11.1.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.cookies.2.2.0.min.js
[INFO] Exclude: **/src/ui/public/js/moment.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.blockUI.min.js
[INFO] Exclude: **/src/ui/public/js/url.min.js
[INFO] Exclude: **/src/ui/public/js/arbor.js
[INFO] Exclude: **/src/ui/public/js/arbor-graphics.js
[INFO] Exclude: **/src/ui/public/js/arbor-tween.js
[INFO] Exclude: **/src/ui/public/js/jquery.mustache.js
[INFO] Exclude: **/src/ui/public/js/typeahead.jquery.min.js
[INFO] Exclude: **/dependency-reduced-pom.xml
[INFO] Exclude: **/docs/**
[INFO] Exclude: **/.git/**
[INFO] Exclude: **/derby.log
[INFO] Exclude: **/src/dev/**
[INFO] Exclude: **/src/codegen/config.fmpp
[INFO] Exclude: **/src/codegen/data/Parser.tdd
[INFO] Exclude: **/src/test/resources/FixedAvroSerializer.config
[INFO] 1850 resources included (use -debug for more details)
Warning:  org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
Compiler warnings:
  WARNING:  'org.apache.xerces.jaxp.SAXParserImpl: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.'
Warning:  org.apache.xerces.parsers.SAXParser: Feature 'http://javax.xml.XMLConstants/feature/secure-processing' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
[INFO] Rat check: Summary of files. Unapproved: 0 unknown: 0 generated: 0 approved: 1837 licence.
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building multilang-javascript 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ multilang-javascript ---
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ multilang-javascript ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 1 resource
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ multilang-javascript ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ multilang-javascript ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/storm-multilang/javascript/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ multilang-javascript ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-surefire-plugin:2.18.1:test (default-test) @ multilang-javascript ---
[INFO] 
[INFO] --- apache-rat-plugin:0.11:check (default) @ multilang-javascript ---
[INFO] 51 implicit excludes (use -debug for more details).
[INFO] Exclude: **/target/**
[INFO] Exclude: **/*.iml
[INFO] Exclude: **/.idea/**
[INFO] Exclude: **/metastore_db/**
[INFO] Exclude: **/build/**
[INFO] Exclude: **/logs/**
[INFO] Exclude: **/CHANGELOG.md
[INFO] Exclude: **/README.md
[INFO] Exclude: **/README.markdown
[INFO] Exclude: **/DEVELOPER.md
[INFO] Exclude: **/BYLAWS.md
[INFO] Exclude: **/STORM-UI-REST-API.md
[INFO] Exclude: SECURITY.md
[INFO] Exclude: VERSION
[INFO] Exclude: TODO
[INFO] Exclude: **/src/py/**
[INFO] Exclude: **/src/ui/public/js/jquery.dataTables.1.10.4.min.js
[INFO] Exclude: **/src/ui/public/css/jquery.dataTables.1.10.4.min.css
[INFO] Exclude: **/src/ui/public/images/*
[INFO] Exclude: **/src/ui/public/js/bootstrap-3.3.1.min.js
[INFO] Exclude: **/src/ui/public/css/bootstrap-3.3.1.min.css
[INFO] Exclude: **/src/ui/public/js/dataTables.bootstrap.min.js
[INFO] Exclude: **/src/ui/public/css/dataTables.bootstrap.css
[INFO] Exclude: **/src/ui/public/js/jsonFormatter.min.js
[INFO] Exclude: **/src/ui/public/css/jsonFormatter.min.css
[INFO] Exclude: **/src/ui/public/js/jquery-1.11.1.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.cookies.2.2.0.min.js
[INFO] Exclude: **/src/ui/public/js/moment.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.blockUI.min.js
[INFO] Exclude: **/src/ui/public/js/url.min.js
[INFO] Exclude: **/src/ui/public/js/arbor.js
[INFO] Exclude: **/src/ui/public/js/arbor-graphics.js
[INFO] Exclude: **/src/ui/public/js/arbor-tween.js
[INFO] Exclude: **/src/ui/public/js/jquery.mustache.js
[INFO] Exclude: **/src/ui/public/js/typeahead.jquery.min.js
[INFO] Exclude: **/dependency-reduced-pom.xml
[INFO] Exclude: **/docs/**
[INFO] Exclude: **/.git/**
[INFO] Exclude: **/derby.log
[INFO] Exclude: **/src/dev/**
[INFO] Exclude: **/src/codegen/config.fmpp
[INFO] Exclude: **/src/codegen/data/Parser.tdd
[INFO] Exclude: **/src/test/resources/FixedAvroSerializer.config
[INFO] 2 resources included (use -debug for more details)
Warning:  org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
Compiler warnings:
  WARNING:  'org.apache.xerces.jaxp.SAXParserImpl: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.'
Warning:  org.apache.xerces.parsers.SAXParser: Feature 'http://javax.xml.XMLConstants/feature/secure-processing' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
[INFO] Rat check: Summary of files. Unapproved: 0 unknown: 0 generated: 0 approved: 2 licence.
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building multilang-python 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ multilang-python ---
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ multilang-python ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 1 resource
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ multilang-python ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ multilang-python ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/storm-multilang/python/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ multilang-python ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-surefire-plugin:2.18.1:test (default-test) @ multilang-python ---
[INFO] 
[INFO] --- apache-rat-plugin:0.11:check (default) @ multilang-python ---
[INFO] 51 implicit excludes (use -debug for more details).
[INFO] Exclude: **/target/**
[INFO] Exclude: **/*.iml
[INFO] Exclude: **/.idea/**
[INFO] Exclude: **/metastore_db/**
[INFO] Exclude: **/build/**
[INFO] Exclude: **/logs/**
[INFO] Exclude: **/CHANGELOG.md
[INFO] Exclude: **/README.md
[INFO] Exclude: **/README.markdown
[INFO] Exclude: **/DEVELOPER.md
[INFO] Exclude: **/BYLAWS.md
[INFO] Exclude: **/STORM-UI-REST-API.md
[INFO] Exclude: SECURITY.md
[INFO] Exclude: VERSION
[INFO] Exclude: TODO
[INFO] Exclude: **/src/py/**
[INFO] Exclude: **/src/ui/public/js/jquery.dataTables.1.10.4.min.js
[INFO] Exclude: **/src/ui/public/css/jquery.dataTables.1.10.4.min.css
[INFO] Exclude: **/src/ui/public/images/*
[INFO] Exclude: **/src/ui/public/js/bootstrap-3.3.1.min.js
[INFO] Exclude: **/src/ui/public/css/bootstrap-3.3.1.min.css
[INFO] Exclude: **/src/ui/public/js/dataTables.bootstrap.min.js
[INFO] Exclude: **/src/ui/public/css/dataTables.bootstrap.css
[INFO] Exclude: **/src/ui/public/js/jsonFormatter.min.js
[INFO] Exclude: **/src/ui/public/css/jsonFormatter.min.css
[INFO] Exclude: **/src/ui/public/js/jquery-1.11.1.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.cookies.2.2.0.min.js
[INFO] Exclude: **/src/ui/public/js/moment.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.blockUI.min.js
[INFO] Exclude: **/src/ui/public/js/url.min.js
[INFO] Exclude: **/src/ui/public/js/arbor.js
[INFO] Exclude: **/src/ui/public/js/arbor-graphics.js
[INFO] Exclude: **/src/ui/public/js/arbor-tween.js
[INFO] Exclude: **/src/ui/public/js/jquery.mustache.js
[INFO] Exclude: **/src/ui/public/js/typeahead.jquery.min.js
[INFO] Exclude: **/dependency-reduced-pom.xml
[INFO] Exclude: **/docs/**
[INFO] Exclude: **/.git/**
[INFO] Exclude: **/derby.log
[INFO] Exclude: **/src/dev/**
[INFO] Exclude: **/src/codegen/config.fmpp
[INFO] Exclude: **/src/codegen/data/Parser.tdd
[INFO] Exclude: **/src/test/resources/FixedAvroSerializer.config
[INFO] 2 resources included (use -debug for more details)
Warning:  org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
Compiler warnings:
  WARNING:  'org.apache.xerces.jaxp.SAXParserImpl: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.'
Warning:  org.apache.xerces.parsers.SAXParser: Feature 'http://javax.xml.XMLConstants/feature/secure-processing' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
[INFO] Rat check: Summary of files. Unapproved: 0 unknown: 0 generated: 0 approved: 2 licence.
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building multilang-ruby 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ multilang-ruby ---
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ multilang-ruby ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 1 resource
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ multilang-ruby ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ multilang-ruby ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/storm-multilang/ruby/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ multilang-ruby ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-surefire-plugin:2.18.1:test (default-test) @ multilang-ruby ---
[INFO] 
[INFO] --- apache-rat-plugin:0.11:check (default) @ multilang-ruby ---
[INFO] 51 implicit excludes (use -debug for more details).
[INFO] Exclude: **/target/**
[INFO] Exclude: **/*.iml
[INFO] Exclude: **/.idea/**
[INFO] Exclude: **/metastore_db/**
[INFO] Exclude: **/build/**
[INFO] Exclude: **/logs/**
[INFO] Exclude: **/CHANGELOG.md
[INFO] Exclude: **/README.md
[INFO] Exclude: **/README.markdown
[INFO] Exclude: **/DEVELOPER.md
[INFO] Exclude: **/BYLAWS.md
[INFO] Exclude: **/STORM-UI-REST-API.md
[INFO] Exclude: SECURITY.md
[INFO] Exclude: VERSION
[INFO] Exclude: TODO
[INFO] Exclude: **/src/py/**
[INFO] Exclude: **/src/ui/public/js/jquery.dataTables.1.10.4.min.js
[INFO] Exclude: **/src/ui/public/css/jquery.dataTables.1.10.4.min.css
[INFO] Exclude: **/src/ui/public/images/*
[INFO] Exclude: **/src/ui/public/js/bootstrap-3.3.1.min.js
[INFO] Exclude: **/src/ui/public/css/bootstrap-3.3.1.min.css
[INFO] Exclude: **/src/ui/public/js/dataTables.bootstrap.min.js
[INFO] Exclude: **/src/ui/public/css/dataTables.bootstrap.css
[INFO] Exclude: **/src/ui/public/js/jsonFormatter.min.js
[INFO] Exclude: **/src/ui/public/css/jsonFormatter.min.css
[INFO] Exclude: **/src/ui/public/js/jquery-1.11.1.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.cookies.2.2.0.min.js
[INFO] Exclude: **/src/ui/public/js/moment.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.blockUI.min.js
[INFO] Exclude: **/src/ui/public/js/url.min.js
[INFO] Exclude: **/src/ui/public/js/arbor.js
[INFO] Exclude: **/src/ui/public/js/arbor-graphics.js
[INFO] Exclude: **/src/ui/public/js/arbor-tween.js
[INFO] Exclude: **/src/ui/public/js/jquery.mustache.js
[INFO] Exclude: **/src/ui/public/js/typeahead.jquery.min.js
[INFO] Exclude: **/dependency-reduced-pom.xml
[INFO] Exclude: **/docs/**
[INFO] Exclude: **/.git/**
[INFO] Exclude: **/derby.log
[INFO] Exclude: **/src/dev/**
[INFO] Exclude: **/src/codegen/config.fmpp
[INFO] Exclude: **/src/codegen/data/Parser.tdd
[INFO] Exclude: **/src/test/resources/FixedAvroSerializer.config
[INFO] 2 resources included (use -debug for more details)
Warning:  org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
Compiler warnings:
  WARNING:  'org.apache.xerces.jaxp.SAXParserImpl: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.'
Warning:  org.apache.xerces.parsers.SAXParser: Feature 'http://javax.xml.XMLConstants/feature/secure-processing' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
[INFO] Rat check: Summary of files. Unapproved: 0 unknown: 0 generated: 0 approved: 2 licence.
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building maven-shade-clojure-transformer 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ maven-shade-clojure-transformer ---
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ maven-shade-clojure-transformer ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/storm-buildtools/maven-shade-clojure-transformer/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ maven-shade-clojure-transformer ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ maven-shade-clojure-transformer ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/storm-buildtools/maven-shade-clojure-transformer/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ maven-shade-clojure-transformer ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-surefire-plugin:2.18.1:test (default-test) @ maven-shade-clojure-transformer ---
[INFO] 
[INFO] --- apache-rat-plugin:0.11:check (default) @ maven-shade-clojure-transformer ---
[INFO] 51 implicit excludes (use -debug for more details).
[INFO] Exclude: **/target/**
[INFO] Exclude: **/*.iml
[INFO] Exclude: **/.idea/**
[INFO] Exclude: **/metastore_db/**
[INFO] Exclude: **/build/**
[INFO] Exclude: **/logs/**
[INFO] Exclude: **/CHANGELOG.md
[INFO] Exclude: **/README.md
[INFO] Exclude: **/README.markdown
[INFO] Exclude: **/DEVELOPER.md
[INFO] Exclude: **/BYLAWS.md
[INFO] Exclude: **/STORM-UI-REST-API.md
[INFO] Exclude: SECURITY.md
[INFO] Exclude: VERSION
[INFO] Exclude: TODO
[INFO] Exclude: **/src/py/**
[INFO] Exclude: **/src/ui/public/js/jquery.dataTables.1.10.4.min.js
[INFO] Exclude: **/src/ui/public/css/jquery.dataTables.1.10.4.min.css
[INFO] Exclude: **/src/ui/public/images/*
[INFO] Exclude: **/src/ui/public/js/bootstrap-3.3.1.min.js
[INFO] Exclude: **/src/ui/public/css/bootstrap-3.3.1.min.css
[INFO] Exclude: **/src/ui/public/js/dataTables.bootstrap.min.js
[INFO] Exclude: **/src/ui/public/css/dataTables.bootstrap.css
[INFO] Exclude: **/src/ui/public/js/jsonFormatter.min.js
[INFO] Exclude: **/src/ui/public/css/jsonFormatter.min.css
[INFO] Exclude: **/src/ui/public/js/jquery-1.11.1.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.cookies.2.2.0.min.js
[INFO] Exclude: **/src/ui/public/js/moment.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.blockUI.min.js
[INFO] Exclude: **/src/ui/public/js/url.min.js
[INFO] Exclude: **/src/ui/public/js/arbor.js
[INFO] Exclude: **/src/ui/public/js/arbor-graphics.js
[INFO] Exclude: **/src/ui/public/js/arbor-tween.js
[INFO] Exclude: **/src/ui/public/js/jquery.mustache.js
[INFO] Exclude: **/src/ui/public/js/typeahead.jquery.min.js
[INFO] Exclude: **/dependency-reduced-pom.xml
[INFO] Exclude: **/docs/**
[INFO] Exclude: **/.git/**
[INFO] Exclude: **/derby.log
[INFO] Exclude: **/src/dev/**
[INFO] Exclude: **/src/codegen/config.fmpp
[INFO] Exclude: **/src/codegen/data/Parser.tdd
[INFO] Exclude: **/src/test/resources/FixedAvroSerializer.config
[INFO] 2 resources included (use -debug for more details)
Warning:  org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
Compiler warnings:
  WARNING:  'org.apache.xerces.jaxp.SAXParserImpl: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.'
Warning:  org.apache.xerces.parsers.SAXParser: Feature 'http://javax.xml.XMLConstants/feature/secure-processing' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
[INFO] Rat check: Summary of files. Unapproved: 0 unknown: 0 generated: 0 approved: 2 licence.
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building storm-maven-plugins 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ storm-maven-plugins ---
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ storm-maven-plugins ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/storm-buildtools/storm-maven-plugins/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ storm-maven-plugins ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-plugin-plugin:3.0:descriptor (default-descriptor) @ storm-maven-plugins ---
[INFO] Using 'UTF-8' encoding to read mojo metadata.
[INFO] Applying mojo extractor for language: java-annotations
[INFO] Mojo extractor for language: java-annotations found 1 mojo descriptors.
[INFO] Applying mojo extractor for language: java
[INFO] Mojo extractor for language: java found 0 mojo descriptors.
[INFO] Applying mojo extractor for language: bsh
[INFO] Mojo extractor for language: bsh found 0 mojo descriptors.
[INFO] 
[INFO] --- maven-plugin-plugin:3.0:descriptor (mojo-descriptor) @ storm-maven-plugins ---
[INFO] Using 'UTF-8' encoding to read mojo metadata.
[INFO] Applying mojo extractor for language: java-annotations
[INFO] Mojo extractor for language: java-annotations found 1 mojo descriptors.
[INFO] Applying mojo extractor for language: java
[INFO] Mojo extractor for language: java found 0 mojo descriptors.
[INFO] Applying mojo extractor for language: bsh
[INFO] Mojo extractor for language: bsh found 0 mojo descriptors.
[INFO] 
[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ storm-maven-plugins ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/storm-buildtools/storm-maven-plugins/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ storm-maven-plugins ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-surefire-plugin:2.18.1:test (default-test) @ storm-maven-plugins ---
[INFO] 
[INFO] --- apache-rat-plugin:0.11:check (default) @ storm-maven-plugins ---
[INFO] 51 implicit excludes (use -debug for more details).
[INFO] Exclude: **/target/**
[INFO] Exclude: **/*.iml
[INFO] Exclude: **/.idea/**
[INFO] Exclude: **/metastore_db/**
[INFO] Exclude: **/build/**
[INFO] Exclude: **/logs/**
[INFO] Exclude: **/CHANGELOG.md
[INFO] Exclude: **/README.md
[INFO] Exclude: **/README.markdown
[INFO] Exclude: **/DEVELOPER.md
[INFO] Exclude: **/BYLAWS.md
[INFO] Exclude: **/STORM-UI-REST-API.md
[INFO] Exclude: SECURITY.md
[INFO] Exclude: VERSION
[INFO] Exclude: TODO
[INFO] Exclude: **/src/py/**
[INFO] Exclude: **/src/ui/public/js/jquery.dataTables.1.10.4.min.js
[INFO] Exclude: **/src/ui/public/css/jquery.dataTables.1.10.4.min.css
[INFO] Exclude: **/src/ui/public/images/*
[INFO] Exclude: **/src/ui/public/js/bootstrap-3.3.1.min.js
[INFO] Exclude: **/src/ui/public/css/bootstrap-3.3.1.min.css
[INFO] Exclude: **/src/ui/public/js/dataTables.bootstrap.min.js
[INFO] Exclude: **/src/ui/public/css/dataTables.bootstrap.css
[INFO] Exclude: **/src/ui/public/js/jsonFormatter.min.js
[INFO] Exclude: **/src/ui/public/css/jsonFormatter.min.css
[INFO] Exclude: **/src/ui/public/js/jquery-1.11.1.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.cookies.2.2.0.min.js
[INFO] Exclude: **/src/ui/public/js/moment.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.blockUI.min.js
[INFO] Exclude: **/src/ui/public/js/url.min.js
[INFO] Exclude: **/src/ui/public/js/arbor.js
[INFO] Exclude: **/src/ui/public/js/arbor-graphics.js
[INFO] Exclude: **/src/ui/public/js/arbor-tween.js
[INFO] Exclude: **/src/ui/public/js/jquery.mustache.js
[INFO] Exclude: **/src/ui/public/js/typeahead.jquery.min.js
[INFO] Exclude: **/dependency-reduced-pom.xml
[INFO] Exclude: **/docs/**
[INFO] Exclude: **/.git/**
[INFO] Exclude: **/derby.log
[INFO] Exclude: **/src/dev/**
[INFO] Exclude: **/src/codegen/config.fmpp
[INFO] Exclude: **/src/codegen/data/Parser.tdd
[INFO] Exclude: **/src/test/resources/FixedAvroSerializer.config
[INFO] 3 resources included (use -debug for more details)
Warning:  org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
Compiler warnings:
  WARNING:  'org.apache.xerces.jaxp.SAXParserImpl: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.'
Warning:  org.apache.xerces.parsers.SAXParser: Feature 'http://javax.xml.XMLConstants/feature/secure-processing' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
[INFO] Rat check: Summary of files. Unapproved: 0 unknown: 0 generated: 0 approved: 3 licence.
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building storm-rename-hack 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] Downloading: https://repository.apache.org/snapshots/org/apache/storm/storm-core/2.0.0-SNAPSHOT/maven-metadata.xml
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ storm-rename-hack ---
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ storm-rename-hack ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/storm-rename-hack/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ storm-rename-hack ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ storm-rename-hack ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/storm-rename-hack/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ storm-rename-hack ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-surefire-plugin:2.18.1:test (default-test) @ storm-rename-hack ---
[INFO] 
[INFO] --- apache-rat-plugin:0.11:check (default) @ storm-rename-hack ---
[INFO] 51 implicit excludes (use -debug for more details).
[INFO] Exclude: **/target/**
[INFO] Exclude: **/*.iml
[INFO] Exclude: **/.idea/**
[INFO] Exclude: **/metastore_db/**
[INFO] Exclude: **/build/**
[INFO] Exclude: **/logs/**
[INFO] Exclude: **/CHANGELOG.md
[INFO] Exclude: **/README.md
[INFO] Exclude: **/README.markdown
[INFO] Exclude: **/DEVELOPER.md
[INFO] Exclude: **/BYLAWS.md
[INFO] Exclude: **/STORM-UI-REST-API.md
[INFO] Exclude: SECURITY.md
[INFO] Exclude: VERSION
[INFO] Exclude: TODO
[INFO] Exclude: **/src/py/**
[INFO] Exclude: **/src/ui/public/js/jquery.dataTables.1.10.4.min.js
[INFO] Exclude: **/src/ui/public/css/jquery.dataTables.1.10.4.min.css
[INFO] Exclude: **/src/ui/public/images/*
[INFO] Exclude: **/src/ui/public/js/bootstrap-3.3.1.min.js
[INFO] Exclude: **/src/ui/public/css/bootstrap-3.3.1.min.css
[INFO] Exclude: **/src/ui/public/js/dataTables.bootstrap.min.js
[INFO] Exclude: **/src/ui/public/css/dataTables.bootstrap.css
[INFO] Exclude: **/src/ui/public/js/jsonFormatter.min.js
[INFO] Exclude: **/src/ui/public/css/jsonFormatter.min.css
[INFO] Exclude: **/src/ui/public/js/jquery-1.11.1.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.cookies.2.2.0.min.js
[INFO] Exclude: **/src/ui/public/js/moment.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.blockUI.min.js
[INFO] Exclude: **/src/ui/public/js/url.min.js
[INFO] Exclude: **/src/ui/public/js/arbor.js
[INFO] Exclude: **/src/ui/public/js/arbor-graphics.js
[INFO] Exclude: **/src/ui/public/js/arbor-tween.js
[INFO] Exclude: **/src/ui/public/js/jquery.mustache.js
[INFO] Exclude: **/src/ui/public/js/typeahead.jquery.min.js
[INFO] Exclude: **/dependency-reduced-pom.xml
[INFO] Exclude: **/docs/**
[INFO] Exclude: **/.git/**
[INFO] Exclude: **/derby.log
[INFO] Exclude: **/src/dev/**
[INFO] Exclude: **/src/codegen/config.fmpp
[INFO] Exclude: **/src/codegen/data/Parser.tdd
[INFO] Exclude: **/src/test/resources/FixedAvroSerializer.config
[INFO] 10 resources included (use -debug for more details)
Warning:  org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
Compiler warnings:
  WARNING:  'org.apache.xerces.jaxp.SAXParserImpl: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.'
Warning:  org.apache.xerces.parsers.SAXParser: Feature 'http://javax.xml.XMLConstants/feature/secure-processing' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
[INFO] Rat check: Summary of files. Unapproved: 0 unknown: 0 generated: 0 approved: 10 licence.
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building storm-kafka 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.2.201409121644:prepare-agent (jacoco-initialize) @ storm-kafka ---
[INFO] argLine set to -javaagent:/home/travis/.m2/repository/org/jacoco/org.jacoco.agent/0.7.2.201409121644/org.jacoco.agent-0.7.2.201409121644-runtime.jar=destfile=/home/travis/build/apache/storm/external/storm-kafka/target/jacoco.exec
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ storm-kafka ---
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ storm-kafka ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/storm-kafka/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ storm-kafka ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ storm-kafka ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/storm-kafka/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ storm-kafka ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:2.18.1:test (default-test) @ storm-kafka ---
[INFO] Surefire report directory: /home/travis/build/apache/storm/external/storm-kafka/target/surefire-reports
[INFO] parallel='none', perCoreThreadCount=true, threadCount=0, useUnlimitedThreads=false, threadCountSuites=0, threadCountClasses=0, threadCountMethods=0, parallelOptimized=true

-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running org.apache.storm.kafka.KafkaErrorTest
Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.556 sec - in org.apache.storm.kafka.KafkaErrorTest
Running org.apache.storm.kafka.KafkaUtilsTest
Tests run: 18, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 30.705 sec - in org.apache.storm.kafka.KafkaUtilsTest
Running org.apache.storm.kafka.StringKeyValueSchemeTest
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.125 sec - in org.apache.storm.kafka.StringKeyValueSchemeTest
Running org.apache.storm.kafka.ZkCoordinatorTest
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 5.478 sec - in org.apache.storm.kafka.ZkCoordinatorTest
Running org.apache.storm.kafka.TridentKafkaTest
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.677 sec - in org.apache.storm.kafka.TridentKafkaTest
Running org.apache.storm.kafka.ExponentialBackoffMsgRetryManagerTest
Tests run: 12, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.719 sec - in org.apache.storm.kafka.ExponentialBackoffMsgRetryManagerTest
Running org.apache.storm.kafka.TestStringScheme
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0 sec - in org.apache.storm.kafka.TestStringScheme
Running org.apache.storm.kafka.DynamicBrokersReaderTest
Tests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 6.662 sec - in org.apache.storm.kafka.DynamicBrokersReaderTest
Running org.apache.storm.kafka.bolt.KafkaBoltTest
Tests run: 8, Failures: 0, Errors: 7, Skipped: 0, Time elapsed: 12.072 sec <<< FAILURE! - in org.apache.storm.kafka.bolt.KafkaBoltTest
executeWithBrokerDown(org.apache.storm.kafka.bolt.KafkaBoltTest)  Time elapsed: 1.831 sec  <<< ERROR!
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.kafka.bolt.KafkaBoltTest.generateTestTuple(KafkaBoltTest.java:301)
	at org.apache.storm.kafka.bolt.KafkaBoltTest.executeWithBrokerDown(KafkaBoltTest.java:266)

executeWithoutKey(org.apache.storm.kafka.bolt.KafkaBoltTest)  Time elapsed: 1.445 sec  <<< ERROR!
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.kafka.bolt.KafkaBoltTest.generateTestTuple(KafkaBoltTest.java:301)
	at org.apache.storm.kafka.bolt.KafkaBoltTest.executeWithoutKey(KafkaBoltTest.java:255)

executeWithByteArrayKeyAndMessageFire(org.apache.storm.kafka.bolt.KafkaBoltTest)  Time elapsed: 1.503 sec  <<< ERROR!
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.kafka.bolt.KafkaBoltTest.generateTestTuple(KafkaBoltTest.java:290)
	at org.apache.storm.kafka.bolt.KafkaBoltTest.executeWithByteArrayKeyAndMessageFire(KafkaBoltTest.java:176)

executeWithByteArrayKeyAndMessageSync(org.apache.storm.kafka.bolt.KafkaBoltTest)  Time elapsed: 1.434 sec  <<< ERROR!
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.kafka.bolt.KafkaBoltTest.generateTestTuple(KafkaBoltTest.java:290)
	at org.apache.storm.kafka.bolt.KafkaBoltTest.executeWithByteArrayKeyAndMessageSync(KafkaBoltTest.java:131)

executeWithByteArrayKeyAndMessageAsync(org.apache.storm.kafka.bolt.KafkaBoltTest)  Time elapsed: 1.439 sec  <<< ERROR!
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.kafka.bolt.KafkaBoltTest.generateTestTuple(KafkaBoltTest.java:290)
	at org.apache.storm.kafka.bolt.KafkaBoltTest.executeWithByteArrayKeyAndMessageAsync(KafkaBoltTest.java:146)

executeWithKey(org.apache.storm.kafka.bolt.KafkaBoltTest)  Time elapsed: 1.463 sec  <<< ERROR!
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.kafka.bolt.KafkaBoltTest.generateTestTuple(KafkaBoltTest.java:290)
	at org.apache.storm.kafka.bolt.KafkaBoltTest.executeWithKey(KafkaBoltTest.java:115)

executeWithBoltSpecifiedProperties(org.apache.storm.kafka.bolt.KafkaBoltTest)  Time elapsed: 1.465 sec  <<< ERROR!
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.kafka.bolt.KafkaBoltTest.generateTestTuple(KafkaBoltTest.java:290)
	at org.apache.storm.kafka.bolt.KafkaBoltTest.executeWithBoltSpecifiedProperties(KafkaBoltTest.java:199)


Results :

Tests in error: 
  KafkaBoltTest.executeWithBoltSpecifiedProperties:199->generateTestTuple:290 » IllegalArgument
  KafkaBoltTest.executeWithBrokerDown:266->generateTestTuple:301 » IllegalArgument
  KafkaBoltTest.executeWithByteArrayKeyAndMessageAsync:146->generateTestTuple:290 » IllegalArgument
  KafkaBoltTest.executeWithByteArrayKeyAndMessageFire:176->generateTestTuple:290 » IllegalArgument
  KafkaBoltTest.executeWithByteArrayKeyAndMessageSync:131->generateTestTuple:290 » IllegalArgument
  KafkaBoltTest.executeWithKey:115->generateTestTuple:290 » IllegalArgument Spou...
  KafkaBoltTest.executeWithoutKey:255->generateTestTuple:301 » IllegalArgument S...

Tests run: 57, Failures: 0, Errors: 7, Skipped: 0

[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building storm-hdfs 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ storm-hdfs ---
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ storm-hdfs ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/storm-hdfs/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:2.5.1:compile (default-compile) @ storm-hdfs ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ storm-hdfs ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 2 resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:2.5.1:testCompile (default-testCompile) @ storm-hdfs ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:2.18.1:test (default-test) @ storm-hdfs ---
[INFO] Surefire report directory: /home/travis/build/apache/storm/external/storm-hdfs/target/surefire-reports
[INFO] parallel='none', perCoreThreadCount=true, threadCount=0, useUnlimitedThreads=false, threadCountSuites=0, threadCountClasses=0, threadCountMethods=0, parallelOptimized=true

-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running org.apache.storm.hdfs.bolt.TestHdfsBolt
Tests run: 6, Failures: 0, Errors: 6, Skipped: 0, Time elapsed: 0.561 sec <<< FAILURE! - in org.apache.storm.hdfs.bolt.TestHdfsBolt
testTwoTuplesTwoFiles(org.apache.storm.hdfs.bolt.TestHdfsBolt)  Time elapsed: 0.007 sec  <<< ERROR!
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hdfs.bolt.TestHdfsBolt.generateTestTuple(TestHdfsBolt.java:243)
	at org.apache.storm.hdfs.bolt.TestHdfsBolt.<init>(TestHdfsBolt.java:69)

testPartitionedOutput(org.apache.storm.hdfs.bolt.TestHdfsBolt)  Time elapsed: 0 sec  <<< ERROR!
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hdfs.bolt.TestHdfsBolt.generateTestTuple(TestHdfsBolt.java:243)
	at org.apache.storm.hdfs.bolt.TestHdfsBolt.<init>(TestHdfsBolt.java:69)

testTwoTuplesOneFile(org.apache.storm.hdfs.bolt.TestHdfsBolt)  Time elapsed: 0.001 sec  <<< ERROR!
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hdfs.bolt.TestHdfsBolt.generateTestTuple(TestHdfsBolt.java:243)
	at org.apache.storm.hdfs.bolt.TestHdfsBolt.<init>(TestHdfsBolt.java:69)

testFailedSync(org.apache.storm.hdfs.bolt.TestHdfsBolt)  Time elapsed: 0.001 sec  <<< ERROR!
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hdfs.bolt.TestHdfsBolt.generateTestTuple(TestHdfsBolt.java:243)
	at org.apache.storm.hdfs.bolt.TestHdfsBolt.<init>(TestHdfsBolt.java:69)

testTickTuples(org.apache.storm.hdfs.bolt.TestHdfsBolt)  Time elapsed: 0 sec  <<< ERROR!
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hdfs.bolt.TestHdfsBolt.generateTestTuple(TestHdfsBolt.java:243)
	at org.apache.storm.hdfs.bolt.TestHdfsBolt.<init>(TestHdfsBolt.java:69)

testFailureFilecount(org.apache.storm.hdfs.bolt.TestHdfsBolt)  Time elapsed: 0.001 sec  <<< ERROR!
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hdfs.bolt.TestHdfsBolt.generateTestTuple(TestHdfsBolt.java:243)
	at org.apache.storm.hdfs.bolt.TestHdfsBolt.<init>(TestHdfsBolt.java:69)

Running org.apache.storm.hdfs.bolt.TestSequenceFileBolt
Tests run: 3, Failures: 0, Errors: 3, Skipped: 0, Time elapsed: 0.162 sec <<< FAILURE! - in org.apache.storm.hdfs.bolt.TestSequenceFileBolt
testTwoTuplesTwoFiles(org.apache.storm.hdfs.bolt.TestSequenceFileBolt)  Time elapsed: 0 sec  <<< ERROR!
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hdfs.bolt.TestSequenceFileBolt.generateTestTuple(TestSequenceFileBolt.java:161)
	at org.apache.storm.hdfs.bolt.TestSequenceFileBolt.<init>(TestSequenceFileBolt.java:68)

testTwoTuplesOneFile(org.apache.storm.hdfs.bolt.TestSequenceFileBolt)  Time elapsed: 0 sec  <<< ERROR!
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hdfs.bolt.TestSequenceFileBolt.generateTestTuple(TestSequenceFileBolt.java:161)
	at org.apache.storm.hdfs.bolt.TestSequenceFileBolt.<init>(TestSequenceFileBolt.java:68)

testFailedSync(org.apache.storm.hdfs.bolt.TestSequenceFileBolt)  Time elapsed: 0 sec  <<< ERROR!
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hdfs.bolt.TestSequenceFileBolt.generateTestTuple(TestSequenceFileBolt.java:161)
	at org.apache.storm.hdfs.bolt.TestSequenceFileBolt.<init>(TestSequenceFileBolt.java:68)

Running org.apache.storm.hdfs.bolt.AvroGenericRecordBoltTest
Tests run: 5, Failures: 0, Errors: 5, Skipped: 0, Time elapsed: 0.008 sec <<< FAILURE! - in org.apache.storm.hdfs.bolt.AvroGenericRecordBoltTest
schemaThrashing(org.apache.storm.hdfs.bolt.AvroGenericRecordBoltTest)  Time elapsed: 0 sec  <<< ERROR!
java.lang.ExceptionInInitializerError: null
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hdfs.bolt.AvroGenericRecordBoltTest.generateTestTuple(AvroGenericRecordBoltTest.java:220)
	at org.apache.storm.hdfs.bolt.AvroGenericRecordBoltTest.<clinit>(AvroGenericRecordBoltTest.java:95)

forwardSchemaChangeWorks(org.apache.storm.hdfs.bolt.AvroGenericRecordBoltTest)  Time elapsed: 0 sec  <<< ERROR!
java.lang.NoClassDefFoundError: Could not initialize class org.apache.storm.hdfs.bolt.AvroGenericRecordBoltTest
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.junit.runners.BlockJUnit4ClassRunner.createTest(BlockJUnit4ClassRunner.java:195)
	at org.junit.runners.BlockJUnit4ClassRunner$1.runReflectiveCall(BlockJUnit4ClassRunner.java:244)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.BlockJUnit4ClassRunner.methodBlock(BlockJUnit4ClassRunner.java:241)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:108)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:78)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:54)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:144)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:203)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:155)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:103)

multipleTuplesOneFile(org.apache.storm.hdfs.bolt.AvroGenericRecordBoltTest)  Time elapsed: 0.008 sec  <<< ERROR!
java.lang.NoClassDefFoundError: Could not initialize class org.apache.storm.hdfs.bolt.AvroGenericRecordBoltTest
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.junit.runners.BlockJUnit4ClassRunner.createTest(BlockJUnit4ClassRunner.java:195)
	at org.junit.runners.BlockJUnit4ClassRunner$1.runReflectiveCall(BlockJUnit4ClassRunner.java:244)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.BlockJUnit4ClassRunner.methodBlock(BlockJUnit4ClassRunner.java:241)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:108)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:78)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:54)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:144)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:203)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:155)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:103)

multipleTuplesMutliplesFiles(org.apache.storm.hdfs.bolt.AvroGenericRecordBoltTest)  Time elapsed: 0 sec  <<< ERROR!
java.lang.NoClassDefFoundError: Could not initialize class org.apache.storm.hdfs.bolt.AvroGenericRecordBoltTest
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.junit.runners.BlockJUnit4ClassRunner.createTest(BlockJUnit4ClassRunner.java:195)
	at org.junit.runners.BlockJUnit4ClassRunner$1.runReflectiveCall(BlockJUnit4ClassRunner.java:244)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.BlockJUnit4ClassRunner.methodBlock(BlockJUnit4ClassRunner.java:241)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:108)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:78)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:54)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:144)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:203)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:155)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:103)

backwardSchemaChangeWorks(org.apache.storm.hdfs.bolt.AvroGenericRecordBoltTest)  Time elapsed: 0 sec  <<< ERROR!
java.lang.NoClassDefFoundError: Could not initialize class org.apache.storm.hdfs.bolt.AvroGenericRecordBoltTest
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.junit.runners.BlockJUnit4ClassRunner.createTest(BlockJUnit4ClassRunner.java:195)
	at org.junit.runners.BlockJUnit4ClassRunner$1.runReflectiveCall(BlockJUnit4ClassRunner.java:244)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.BlockJUnit4ClassRunner.methodBlock(BlockJUnit4ClassRunner.java:241)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:108)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:78)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:54)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:144)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:203)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:155)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:103)

Running org.apache.storm.hdfs.bolt.TestWritersMap
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 3.111 sec - in org.apache.storm.hdfs.bolt.TestWritersMap
Running org.apache.storm.hdfs.spout.TestDirLock
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 15.567 sec - in org.apache.storm.hdfs.spout.TestDirLock
Running org.apache.storm.hdfs.spout.TestHdfsSemantics
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.892 sec - in org.apache.storm.hdfs.spout.TestHdfsSemantics
Running org.apache.storm.hdfs.spout.TestHdfsSpout
Tests run: 10, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 8.939 sec - in org.apache.storm.hdfs.spout.TestHdfsSpout
Running org.apache.storm.hdfs.spout.TestProgressTracker
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.471 sec - in org.apache.storm.hdfs.spout.TestProgressTracker
Running org.apache.storm.hdfs.spout.TestFileLock
Tests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 22.379 sec - in org.apache.storm.hdfs.spout.TestFileLock
Running org.apache.storm.hdfs.trident.HdfsStateTest
Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.234 sec - in org.apache.storm.hdfs.trident.HdfsStateTest
Running org.apache.storm.hdfs.blobstore.BlobStoreTest
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 10.248 sec - in org.apache.storm.hdfs.blobstore.BlobStoreTest
Running org.apache.storm.hdfs.blobstore.HdfsBlobStoreImplTest
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.544 sec - in org.apache.storm.hdfs.blobstore.HdfsBlobStoreImplTest
Running org.apache.storm.hdfs.avro.TestFixedAvroSerializer
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.002 sec - in org.apache.storm.hdfs.avro.TestFixedAvroSerializer
Running org.apache.storm.hdfs.avro.TestGenericAvroSerializer
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.003 sec - in org.apache.storm.hdfs.avro.TestGenericAvroSerializer

Results :

Tests in error: 
  AvroGenericRecordBoltTest.backwardSchemaChangeWorks » NoClassDefFound Could no...
  AvroGenericRecordBoltTest.forwardSchemaChangeWorks » NoClassDefFound Could not...
  AvroGenericRecordBoltTest.multipleTuplesMutliplesFiles » NoClassDefFound Could...
  AvroGenericRecordBoltTest.multipleTuplesOneFile » NoClassDefFound Could not in...
  AvroGenericRecordBoltTest.schemaThrashing » ExceptionInInitializer
  TestHdfsBolt.<init>:69->generateTestTuple:243 » IllegalArgument Spouts is not ...
  TestHdfsBolt.<init>:69->generateTestTuple:243 » IllegalArgument Spouts is not ...
  TestHdfsBolt.<init>:69->generateTestTuple:243 » IllegalArgument Spouts is not ...
  TestHdfsBolt.<init>:69->generateTestTuple:243 » IllegalArgument Spouts is not ...
  TestHdfsBolt.<init>:69->generateTestTuple:243 » IllegalArgument Spouts is not ...
  TestHdfsBolt.<init>:69->generateTestTuple:243 » IllegalArgument Spouts is not ...
  TestSequenceFileBolt.<init>:68->generateTestTuple:161 » IllegalArgument Spouts...
  TestSequenceFileBolt.<init>:68->generateTestTuple:161 » IllegalArgument Spouts...
  TestSequenceFileBolt.<init>:68->generateTestTuple:161 » IllegalArgument Spouts...

Tests run: 54, Failures: 0, Errors: 14, Skipped: 0

[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building storm-hbase 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ storm-hbase ---
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ storm-hbase ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/storm-hbase/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ storm-hbase ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ storm-hbase ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/storm-hbase/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ storm-hbase ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:2.18.1:test (default-test) @ storm-hbase ---
[INFO] 
[INFO] --- apache-rat-plugin:0.11:check (default) @ storm-hbase ---
[INFO] 51 implicit excludes (use -debug for more details).
[INFO] Exclude: **/target/**
[INFO] Exclude: **/*.iml
[INFO] Exclude: **/.idea/**
[INFO] Exclude: **/metastore_db/**
[INFO] Exclude: **/build/**
[INFO] Exclude: **/logs/**
[INFO] Exclude: **/CHANGELOG.md
[INFO] Exclude: **/README.md
[INFO] Exclude: **/README.markdown
[INFO] Exclude: **/DEVELOPER.md
[INFO] Exclude: **/BYLAWS.md
[INFO] Exclude: **/STORM-UI-REST-API.md
[INFO] Exclude: SECURITY.md
[INFO] Exclude: VERSION
[INFO] Exclude: TODO
[INFO] Exclude: **/src/py/**
[INFO] Exclude: **/src/ui/public/js/jquery.dataTables.1.10.4.min.js
[INFO] Exclude: **/src/ui/public/css/jquery.dataTables.1.10.4.min.css
[INFO] Exclude: **/src/ui/public/images/*
[INFO] Exclude: **/src/ui/public/js/bootstrap-3.3.1.min.js
[INFO] Exclude: **/src/ui/public/css/bootstrap-3.3.1.min.css
[INFO] Exclude: **/src/ui/public/js/dataTables.bootstrap.min.js
[INFO] Exclude: **/src/ui/public/css/dataTables.bootstrap.css
[INFO] Exclude: **/src/ui/public/js/jsonFormatter.min.js
[INFO] Exclude: **/src/ui/public/css/jsonFormatter.min.css
[INFO] Exclude: **/src/ui/public/js/jquery-1.11.1.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.cookies.2.2.0.min.js
[INFO] Exclude: **/src/ui/public/js/moment.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.blockUI.min.js
[INFO] Exclude: **/src/ui/public/js/url.min.js
[INFO] Exclude: **/src/ui/public/js/arbor.js
[INFO] Exclude: **/src/ui/public/js/arbor-graphics.js
[INFO] Exclude: **/src/ui/public/js/arbor-tween.js
[INFO] Exclude: **/src/ui/public/js/jquery.mustache.js
[INFO] Exclude: **/src/ui/public/js/typeahead.jquery.min.js
[INFO] Exclude: **/dependency-reduced-pom.xml
[INFO] Exclude: **/docs/**
[INFO] Exclude: **/.git/**
[INFO] Exclude: **/derby.log
[INFO] Exclude: **/src/dev/**
[INFO] Exclude: **/src/codegen/config.fmpp
[INFO] Exclude: **/src/codegen/data/Parser.tdd
[INFO] Exclude: **/src/test/resources/FixedAvroSerializer.config
[INFO] 35 resources included (use -debug for more details)
Warning:  org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
Compiler warnings:
  WARNING:  'org.apache.xerces.jaxp.SAXParserImpl: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.'
Warning:  org.apache.xerces.parsers.SAXParser: Feature 'http://javax.xml.XMLConstants/feature/secure-processing' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
[INFO] Rat check: Summary of files. Unapproved: 0 unknown: 0 generated: 0 approved: 35 licence.
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building storm-hive 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ storm-hive ---
[INFO] Downloading: https://oss.sonatype.org/content/repositories/releases/net/hydromatic/linq4j/0.4/linq4j-0.4.pom
[INFO] Downloading: https://repository.apache.org/releases/net/hydromatic/linq4j/0.4/linq4j-0.4.pom
[INFO] Downloading: https://oss.sonatype.org/content/repositories/releases/net/hydromatic/quidem/0.1.1/quidem-0.1.1.pom
[INFO] Downloading: https://repository.apache.org/releases/net/hydromatic/quidem/0.1.1/quidem-0.1.1.pom
[INFO] Downloading: https://oss.sonatype.org/content/repositories/releases/org/pentaho/pentaho-aggdesigner-algorithm/5.1.3-jhyde/pentaho-aggdesigner-algorithm-5.1.3-jhyde.pom
[INFO] Downloading: https://repository.apache.org/releases/org/pentaho/pentaho-aggdesigner-algorithm/5.1.3-jhyde/pentaho-aggdesigner-algorithm-5.1.3-jhyde.pom
[INFO] Downloading: https://oss.sonatype.org/content/repositories/releases/eigenbase/eigenbase-properties/1.1.4/eigenbase-properties-1.1.4.pom
[INFO] Downloading: https://repository.apache.org/releases/eigenbase/eigenbase-properties/1.1.4/eigenbase-properties-1.1.4.pom
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ storm-hive ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/storm-hive/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ storm-hive ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ storm-hive ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/storm-hive/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ storm-hive ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (cleanup) @ storm-hive ---
[INFO] 
[INFO] --- maven-surefire-plugin:2.18.1:test (default-test) @ storm-hive ---
[INFO] Surefire report directory: /home/travis/build/apache/storm/external/storm-hive/target/surefire-reports
[INFO] parallel='none', perCoreThreadCount=true, threadCount=0, useUnlimitedThreads=false, threadCountSuites=0, threadCountClasses=0, threadCountMethods=0, parallelOptimized=true

-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running org.apache.storm.hive.bolt.TestHiveBolt
Tests run: 11, Failures: 0, Errors: 9, Skipped: 0, Time elapsed: 14.912 sec <<< FAILURE! - in org.apache.storm.hive.bolt.TestHiveBolt
testMultiPartitionTuples(org.apache.storm.hive.bolt.TestHiveBolt)  Time elapsed: 1.648 sec  <<< ERROR!
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hive.bolt.TestHiveBolt.generateTestTuple(TestHiveBolt.java:447)
	at org.apache.storm.hive.bolt.TestHiveBolt.testMultiPartitionTuples(TestHiveBolt.java:411)

testNoAcksUntilFlushed(org.apache.storm.hive.bolt.TestHiveBolt)  Time elapsed: 1.91 sec  <<< ERROR!
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hive.bolt.TestHiveBolt.generateTestTuple(TestHiveBolt.java:447)
	at org.apache.storm.hive.bolt.TestHiveBolt.testNoAcksUntilFlushed(TestHiveBolt.java:297)

testData(org.apache.storm.hive.bolt.TestHiveBolt)  Time elapsed: 1.355 sec  <<< ERROR!
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hive.bolt.TestHiveBolt.generateTestTuple(TestHiveBolt.java:447)
	at org.apache.storm.hive.bolt.TestHiveBolt.testData(TestHiveBolt.java:251)

testWithoutPartitions(org.apache.storm.hive.bolt.TestHiveBolt)  Time elapsed: 1.061 sec  <<< ERROR!
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hive.bolt.TestHiveBolt.generateTestTuple(TestHiveBolt.java:447)
	at org.apache.storm.hive.bolt.TestHiveBolt.testWithoutPartitions(TestHiveBolt.java:194)

testJsonWriter(org.apache.storm.hive.bolt.TestHiveBolt)  Time elapsed: 0.547 sec  <<< ERROR!
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hive.bolt.TestHiveBolt.generateTestTuple(TestHiveBolt.java:447)
	at org.apache.storm.hive.bolt.TestHiveBolt.testJsonWriter(TestHiveBolt.java:274)

testTickTuple(org.apache.storm.hive.bolt.TestHiveBolt)  Time elapsed: 0.509 sec  <<< ERROR!
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hive.bolt.TestHiveBolt.generateTestTuple(TestHiveBolt.java:447)
	at org.apache.storm.hive.bolt.TestHiveBolt.testTickTuple(TestHiveBolt.java:352)

testWithTimeformat(org.apache.storm.hive.bolt.TestHiveBolt)  Time elapsed: 0.676 sec  <<< ERROR!
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hive.bolt.TestHiveBolt.generateTestTuple(TestHiveBolt.java:447)
	at org.apache.storm.hive.bolt.TestHiveBolt.testWithTimeformat(TestHiveBolt.java:230)

testWithByteArrayIdandMessage(org.apache.storm.hive.bolt.TestHiveBolt)  Time elapsed: 0.561 sec  <<< ERROR!
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hive.bolt.TestHiveBolt.generateTestTuple(TestHiveBolt.java:447)
	at org.apache.storm.hive.bolt.TestHiveBolt.testWithByteArrayIdandMessage(TestHiveBolt.java:161)

testNoAcksIfFlushFails(org.apache.storm.hive.bolt.TestHiveBolt)  Time elapsed: 0.535 sec  <<< ERROR!
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hive.bolt.TestHiveBolt.generateTestTuple(TestHiveBolt.java:447)
	at org.apache.storm.hive.bolt.TestHiveBolt.testNoAcksIfFlushFails(TestHiveBolt.java:327)

Running org.apache.storm.hive.common.TestHiveWriter
Tests run: 3, Failures: 0, Errors: 2, Skipped: 0, Time elapsed: 3.338 sec <<< FAILURE! - in org.apache.storm.hive.common.TestHiveWriter
testWriteBasic(org.apache.storm.hive.common.TestHiveWriter)  Time elapsed: 0.814 sec  <<< ERROR!
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hive.common.TestHiveWriter.generateTestTuple(TestHiveWriter.java:164)
	at org.apache.storm.hive.common.TestHiveWriter.writeTuples(TestHiveWriter.java:179)
	at org.apache.storm.hive.common.TestHiveWriter.testWriteBasic(TestHiveWriter.java:127)

testWriteMultiFlush(org.apache.storm.hive.common.TestHiveWriter)  Time elapsed: 0.691 sec  <<< ERROR!
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hive.common.TestHiveWriter.generateTestTuple(TestHiveWriter.java:164)
	at org.apache.storm.hive.common.TestHiveWriter.testWriteMultiFlush(TestHiveWriter.java:142)


Results :

Tests in error: 
  TestHiveBolt.testData:251->generateTestTuple:447 » IllegalArgument Spouts is n...
  TestHiveBolt.testJsonWriter:274->generateTestTuple:447 » IllegalArgument Spout...
  TestHiveBolt.testMultiPartitionTuples:411->generateTestTuple:447 » IllegalArgument
  TestHiveBolt.testNoAcksIfFlushFails:327->generateTestTuple:447 » IllegalArgument
  TestHiveBolt.testNoAcksUntilFlushed:297->generateTestTuple:447 » IllegalArgument
  TestHiveBolt.testTickTuple:352->generateTestTuple:447 » IllegalArgument Spouts...
  TestHiveBolt.testWithByteArrayIdandMessage:161->generateTestTuple:447 » IllegalArgument
  TestHiveBolt.testWithTimeformat:230->generateTestTuple:447 » IllegalArgument S...
  TestHiveBolt.testWithoutPartitions:194->generateTestTuple:447 » IllegalArgument
  TestHiveWriter.testWriteBasic:127->writeTuples:179->generateTestTuple:164 » IllegalArgument
  TestHiveWriter.testWriteMultiFlush:142->generateTestTuple:164 » IllegalArgument

Tests run: 14, Failures: 0, Errors: 11, Skipped: 0

[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building storm-jdbc 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ storm-jdbc ---
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ storm-jdbc ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/storm-jdbc/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ storm-jdbc ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ storm-jdbc ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/storm-jdbc/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- sql-maven-plugin:1.5:execute (create-db) @ storm-jdbc ---
[INFO] Executing file: /tmp/test.1826562409sql
[INFO] 1 of 1 SQL statements executed successfully
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ storm-jdbc ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:2.18.1:test (default-test) @ storm-jdbc ---
[INFO] Surefire report directory: /home/travis/build/apache/storm/external/storm-jdbc/target/surefire-reports
[INFO] parallel='none', perCoreThreadCount=true, threadCount=0, useUnlimitedThreads=false, threadCountSuites=0, threadCountClasses=0, threadCountMethods=0, parallelOptimized=true

-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running org.apache.storm.jdbc.bolt.JdbcInsertBoltTest
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.552 sec - in org.apache.storm.jdbc.bolt.JdbcInsertBoltTest
Running org.apache.storm.jdbc.bolt.JdbcLookupBoltTest
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.002 sec - in org.apache.storm.jdbc.bolt.JdbcLookupBoltTest
Running org.apache.storm.jdbc.common.UtilTest
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.002 sec - in org.apache.storm.jdbc.common.UtilTest
Running org.apache.storm.jdbc.common.JdbcClientTest
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.523 sec - in org.apache.storm.jdbc.common.JdbcClientTest

Results :

Tests run: 5, Failures: 0, Errors: 0, Skipped: 0

[INFO] 
[INFO] --- apache-rat-plugin:0.11:check (default) @ storm-jdbc ---
[INFO] 51 implicit excludes (use -debug for more details).
[INFO] Exclude: **/target/**
[INFO] Exclude: **/*.iml
[INFO] Exclude: **/.idea/**
[INFO] Exclude: **/metastore_db/**
[INFO] Exclude: **/build/**
[INFO] Exclude: **/logs/**
[INFO] Exclude: **/CHANGELOG.md
[INFO] Exclude: **/README.md
[INFO] Exclude: **/README.markdown
[INFO] Exclude: **/DEVELOPER.md
[INFO] Exclude: **/BYLAWS.md
[INFO] Exclude: **/STORM-UI-REST-API.md
[INFO] Exclude: SECURITY.md
[INFO] Exclude: VERSION
[INFO] Exclude: TODO
[INFO] Exclude: **/src/py/**
[INFO] Exclude: **/src/ui/public/js/jquery.dataTables.1.10.4.min.js
[INFO] Exclude: **/src/ui/public/css/jquery.dataTables.1.10.4.min.css
[INFO] Exclude: **/src/ui/public/images/*
[INFO] Exclude: **/src/ui/public/js/bootstrap-3.3.1.min.js
[INFO] Exclude: **/src/ui/public/css/bootstrap-3.3.1.min.css
[INFO] Exclude: **/src/ui/public/js/dataTables.bootstrap.min.js
[INFO] Exclude: **/src/ui/public/css/dataTables.bootstrap.css
[INFO] Exclude: **/src/ui/public/js/jsonFormatter.min.js
[INFO] Exclude: **/src/ui/public/css/jsonFormatter.min.css
[INFO] Exclude: **/src/ui/public/js/jquery-1.11.1.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.cookies.2.2.0.min.js
[INFO] Exclude: **/src/ui/public/js/moment.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.blockUI.min.js
[INFO] Exclude: **/src/ui/public/js/url.min.js
[INFO] Exclude: **/src/ui/public/js/arbor.js
[INFO] Exclude: **/src/ui/public/js/arbor-graphics.js
[INFO] Exclude: **/src/ui/public/js/arbor-tween.js
[INFO] Exclude: **/src/ui/public/js/jquery.mustache.js
[INFO] Exclude: **/src/ui/public/js/typeahead.jquery.min.js
[INFO] Exclude: **/dependency-reduced-pom.xml
[INFO] Exclude: **/docs/**
[INFO] Exclude: **/.git/**
[INFO] Exclude: **/derby.log
[INFO] Exclude: **/src/dev/**
[INFO] Exclude: **/src/codegen/config.fmpp
[INFO] Exclude: **/src/codegen/data/Parser.tdd
[INFO] Exclude: **/src/test/resources/FixedAvroSerializer.config
[INFO] 26 resources included (use -debug for more details)
Warning:  org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
Compiler warnings:
  WARNING:  'org.apache.xerces.jaxp.SAXParserImpl: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.'
Warning:  org.apache.xerces.parsers.SAXParser: Feature 'http://javax.xml.XMLConstants/feature/secure-processing' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
[INFO] Rat check: Summary of files. Unapproved: 0 unknown: 0 generated: 0 approved: 26 licence.
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building storm-redis 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ storm-redis ---
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ storm-redis ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/storm-redis/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ storm-redis ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ storm-redis ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/storm-redis/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ storm-redis ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:2.18.1:test (default-test) @ storm-redis ---
[INFO] Surefire report directory: /home/travis/build/apache/storm/external/storm-redis/target/surefire-reports
[INFO] parallel='none', perCoreThreadCount=true, threadCount=0, useUnlimitedThreads=false, threadCountSuites=0, threadCountClasses=0, threadCountMethods=0, parallelOptimized=true

-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running org.apache.storm.redis.state.RedisKeyValueStateProviderTest
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.681 sec - in org.apache.storm.redis.state.RedisKeyValueStateProviderTest
Running org.apache.storm.redis.state.DefaultStateSerializerTest
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.068 sec - in org.apache.storm.redis.state.DefaultStateSerializerTest
Running org.apache.storm.redis.state.RedisKeyValueStateTest
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.208 sec - in org.apache.storm.redis.state.RedisKeyValueStateTest

Results :

Tests run: 5, Failures: 0, Errors: 0, Skipped: 0

[INFO] 
[INFO] --- apache-rat-plugin:0.11:check (default) @ storm-redis ---
[INFO] 51 implicit excludes (use -debug for more details).
[INFO] Exclude: **/target/**
[INFO] Exclude: **/*.iml
[INFO] Exclude: **/.idea/**
[INFO] Exclude: **/metastore_db/**
[INFO] Exclude: **/build/**
[INFO] Exclude: **/logs/**
[INFO] Exclude: **/CHANGELOG.md
[INFO] Exclude: **/README.md
[INFO] Exclude: **/README.markdown
[INFO] Exclude: **/DEVELOPER.md
[INFO] Exclude: **/BYLAWS.md
[INFO] Exclude: **/STORM-UI-REST-API.md
[INFO] Exclude: SECURITY.md
[INFO] Exclude: VERSION
[INFO] Exclude: TODO
[INFO] Exclude: **/src/py/**
[INFO] Exclude: **/src/ui/public/js/jquery.dataTables.1.10.4.min.js
[INFO] Exclude: **/src/ui/public/css/jquery.dataTables.1.10.4.min.css
[INFO] Exclude: **/src/ui/public/images/*
[INFO] Exclude: **/src/ui/public/js/bootstrap-3.3.1.min.js
[INFO] Exclude: **/src/ui/public/css/bootstrap-3.3.1.min.css
[INFO] Exclude: **/src/ui/public/js/dataTables.bootstrap.min.js
[INFO] Exclude: **/src/ui/public/css/dataTables.bootstrap.css
[INFO] Exclude: **/src/ui/public/js/jsonFormatter.min.js
[INFO] Exclude: **/src/ui/public/css/jsonFormatter.min.css
[INFO] Exclude: **/src/ui/public/js/jquery-1.11.1.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.cookies.2.2.0.min.js
[INFO] Exclude: **/src/ui/public/js/moment.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.blockUI.min.js
[INFO] Exclude: **/src/ui/public/js/url.min.js
[INFO] Exclude: **/src/ui/public/js/arbor.js
[INFO] Exclude: **/src/ui/public/js/arbor-graphics.js
[INFO] Exclude: **/src/ui/public/js/arbor-tween.js
[INFO] Exclude: **/src/ui/public/js/jquery.mustache.js
[INFO] Exclude: **/src/ui/public/js/typeahead.jquery.min.js
[INFO] Exclude: **/dependency-reduced-pom.xml
[INFO] Exclude: **/docs/**
[INFO] Exclude: **/.git/**
[INFO] Exclude: **/derby.log
[INFO] Exclude: **/src/dev/**
[INFO] Exclude: **/src/codegen/config.fmpp
[INFO] Exclude: **/src/codegen/data/Parser.tdd
[INFO] Exclude: **/src/test/resources/FixedAvroSerializer.config
[INFO] 44 resources included (use -debug for more details)
Warning:  org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
Compiler warnings:
  WARNING:  'org.apache.xerces.jaxp.SAXParserImpl: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.'
Warning:  org.apache.xerces.parsers.SAXParser: Feature 'http://javax.xml.XMLConstants/feature/secure-processing' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
[INFO] Rat check: Summary of files. Unapproved: 0 unknown: 0 generated: 0 approved: 44 licence.
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building storm-eventhubs 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ storm-eventhubs ---
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ storm-eventhubs ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 1 resource
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ storm-eventhubs ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ storm-eventhubs ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/storm-eventhubs/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ storm-eventhubs ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:2.18.1:test (default-test) @ storm-eventhubs ---
[INFO] Surefire report directory: /home/travis/build/apache/storm/external/storm-eventhubs/target/surefire-reports
[INFO] parallel='none', perCoreThreadCount=true, threadCount=0, useUnlimitedThreads=false, threadCountSuites=0, threadCountClasses=0, threadCountMethods=0, parallelOptimized=true

-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running org.apache.storm.eventhubs.trident.TestTransactionalTridentEmitter
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.54 sec - in org.apache.storm.eventhubs.trident.TestTransactionalTridentEmitter
Running org.apache.storm.eventhubs.spout.TestPartitionManager
Tests run: 8, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.038 sec - in org.apache.storm.eventhubs.spout.TestPartitionManager
Running org.apache.storm.eventhubs.spout.TestEventData
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.001 sec - in org.apache.storm.eventhubs.spout.TestEventData
Running org.apache.storm.eventhubs.spout.TestEventHubSpout
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 2.017 sec - in org.apache.storm.eventhubs.spout.TestEventHubSpout

Results :

Tests run: 14, Failures: 0, Errors: 0, Skipped: 0

[INFO] 
[INFO] --- apache-rat-plugin:0.11:check (default) @ storm-eventhubs ---
[INFO] 51 implicit excludes (use -debug for more details).
[INFO] Exclude: **/target/**
[INFO] Exclude: **/*.iml
[INFO] Exclude: **/.idea/**
[INFO] Exclude: **/metastore_db/**
[INFO] Exclude: **/build/**
[INFO] Exclude: **/logs/**
[INFO] Exclude: **/CHANGELOG.md
[INFO] Exclude: **/README.md
[INFO] Exclude: **/README.markdown
[INFO] Exclude: **/DEVELOPER.md
[INFO] Exclude: **/BYLAWS.md
[INFO] Exclude: **/STORM-UI-REST-API.md
[INFO] Exclude: SECURITY.md
[INFO] Exclude: VERSION
[INFO] Exclude: TODO
[INFO] Exclude: **/src/py/**
[INFO] Exclude: **/src/ui/public/js/jquery.dataTables.1.10.4.min.js
[INFO] Exclude: **/src/ui/public/css/jquery.dataTables.1.10.4.min.css
[INFO] Exclude: **/src/ui/public/images/*
[INFO] Exclude: **/src/ui/public/js/bootstrap-3.3.1.min.js
[INFO] Exclude: **/src/ui/public/css/bootstrap-3.3.1.min.css
[INFO] Exclude: **/src/ui/public/js/dataTables.bootstrap.min.js
[INFO] Exclude: **/src/ui/public/css/dataTables.bootstrap.css
[INFO] Exclude: **/src/ui/public/js/jsonFormatter.min.js
[INFO] Exclude: **/src/ui/public/css/jsonFormatter.min.css
[INFO] Exclude: **/src/ui/public/js/jquery-1.11.1.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.cookies.2.2.0.min.js
[INFO] Exclude: **/src/ui/public/js/moment.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.blockUI.min.js
[INFO] Exclude: **/src/ui/public/js/url.min.js
[INFO] Exclude: **/src/ui/public/js/arbor.js
[INFO] Exclude: **/src/ui/public/js/arbor-graphics.js
[INFO] Exclude: **/src/ui/public/js/arbor-tween.js
[INFO] Exclude: **/src/ui/public/js/jquery.mustache.js
[INFO] Exclude: **/src/ui/public/js/typeahead.jquery.min.js
[INFO] Exclude: **/dependency-reduced-pom.xml
[INFO] Exclude: **/docs/**
[INFO] Exclude: **/.git/**
[INFO] Exclude: **/derby.log
[INFO] Exclude: **/src/dev/**
[INFO] Exclude: **/src/codegen/config.fmpp
[INFO] Exclude: **/src/codegen/data/Parser.tdd
[INFO] Exclude: **/src/test/resources/FixedAvroSerializer.config
[INFO] 52 resources included (use -debug for more details)
Warning:  org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
Compiler warnings:
  WARNING:  'org.apache.xerces.jaxp.SAXParserImpl: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.'
Warning:  org.apache.xerces.parsers.SAXParser: Feature 'http://javax.xml.XMLConstants/feature/secure-processing' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
[INFO] Rat check: Summary of files. Unapproved: 0 unknown: 0 generated: 0 approved: 52 licence.
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building flux 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ flux ---
[INFO] 
[INFO] --- apache-rat-plugin:0.11:check (default) @ flux ---
[INFO] 51 implicit excludes (use -debug for more details).
[INFO] Exclude: **/target/**
[INFO] Exclude: **/*.iml
[INFO] Exclude: **/.idea/**
[INFO] Exclude: **/metastore_db/**
[INFO] Exclude: **/build/**
[INFO] Exclude: **/logs/**
[INFO] Exclude: **/CHANGELOG.md
[INFO] Exclude: **/README.md
[INFO] Exclude: **/README.markdown
[INFO] Exclude: **/DEVELOPER.md
[INFO] Exclude: **/BYLAWS.md
[INFO] Exclude: **/STORM-UI-REST-API.md
[INFO] Exclude: SECURITY.md
[INFO] Exclude: VERSION
[INFO] Exclude: TODO
[INFO] Exclude: **/src/py/**
[INFO] Exclude: **/src/ui/public/js/jquery.dataTables.1.10.4.min.js
[INFO] Exclude: **/src/ui/public/css/jquery.dataTables.1.10.4.min.css
[INFO] Exclude: **/src/ui/public/images/*
[INFO] Exclude: **/src/ui/public/js/bootstrap-3.3.1.min.js
[INFO] Exclude: **/src/ui/public/css/bootstrap-3.3.1.min.css
[INFO] Exclude: **/src/ui/public/js/dataTables.bootstrap.min.js
[INFO] Exclude: **/src/ui/public/css/dataTables.bootstrap.css
[INFO] Exclude: **/src/ui/public/js/jsonFormatter.min.js
[INFO] Exclude: **/src/ui/public/css/jsonFormatter.min.css
[INFO] Exclude: **/src/ui/public/js/jquery-1.11.1.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.cookies.2.2.0.min.js
[INFO] Exclude: **/src/ui/public/js/moment.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.blockUI.min.js
[INFO] Exclude: **/src/ui/public/js/url.min.js
[INFO] Exclude: **/src/ui/public/js/arbor.js
[INFO] Exclude: **/src/ui/public/js/arbor-graphics.js
[INFO] Exclude: **/src/ui/public/js/arbor-tween.js
[INFO] Exclude: **/src/ui/public/js/jquery.mustache.js
[INFO] Exclude: **/src/ui/public/js/typeahead.jquery.min.js
[INFO] Exclude: **/dependency-reduced-pom.xml
[INFO] Exclude: **/docs/**
[INFO] Exclude: **/.git/**
[INFO] Exclude: **/derby.log
[INFO] Exclude: **/src/dev/**
[INFO] Exclude: **/src/codegen/config.fmpp
[INFO] Exclude: **/src/codegen/data/Parser.tdd
[INFO] Exclude: **/src/test/resources/FixedAvroSerializer.config
[INFO] 69 resources included (use -debug for more details)
Warning:  org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
Compiler warnings:
  WARNING:  'org.apache.xerces.jaxp.SAXParserImpl: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.'
Warning:  org.apache.xerces.parsers.SAXParser: Feature 'http://javax.xml.XMLConstants/feature/secure-processing' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
[INFO] Rat check: Summary of files. Unapproved: 0 unknown: 0 generated: 0 approved: 68 licence.
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building flux-wrappers 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ flux-wrappers ---
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ flux-wrappers ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 2 resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.3:compile (default-compile) @ flux-wrappers ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ flux-wrappers ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/flux/flux-wrappers/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.3:testCompile (default-testCompile) @ flux-wrappers ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-surefire-plugin:2.18.1:test (default-test) @ flux-wrappers ---
[INFO] 
[INFO] --- apache-rat-plugin:0.11:check (default) @ flux-wrappers ---
[INFO] 51 implicit excludes (use -debug for more details).
[INFO] Exclude: **/target/**
[INFO] Exclude: **/*.iml
[INFO] Exclude: **/.idea/**
[INFO] Exclude: **/metastore_db/**
[INFO] Exclude: **/build/**
[INFO] Exclude: **/logs/**
[INFO] Exclude: **/CHANGELOG.md
[INFO] Exclude: **/README.md
[INFO] Exclude: **/README.markdown
[INFO] Exclude: **/DEVELOPER.md
[INFO] Exclude: **/BYLAWS.md
[INFO] Exclude: **/STORM-UI-REST-API.md
[INFO] Exclude: SECURITY.md
[INFO] Exclude: VERSION
[INFO] Exclude: TODO
[INFO] Exclude: **/src/py/**
[INFO] Exclude: **/src/ui/public/js/jquery.dataTables.1.10.4.min.js
[INFO] Exclude: **/src/ui/public/css/jquery.dataTables.1.10.4.min.css
[INFO] Exclude: **/src/ui/public/images/*
[INFO] Exclude: **/src/ui/public/js/bootstrap-3.3.1.min.js
[INFO] Exclude: **/src/ui/public/css/bootstrap-3.3.1.min.css
[INFO] Exclude: **/src/ui/public/js/dataTables.bootstrap.min.js
[INFO] Exclude: **/src/ui/public/css/dataTables.bootstrap.css
[INFO] Exclude: **/src/ui/public/js/jsonFormatter.min.js
[INFO] Exclude: **/src/ui/public/css/jsonFormatter.min.css
[INFO] Exclude: **/src/ui/public/js/jquery-1.11.1.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.cookies.2.2.0.min.js
[INFO] Exclude: **/src/ui/public/js/moment.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.blockUI.min.js
[INFO] Exclude: **/src/ui/public/js/url.min.js
[INFO] Exclude: **/src/ui/public/js/arbor.js
[INFO] Exclude: **/src/ui/public/js/arbor-graphics.js
[INFO] Exclude: **/src/ui/public/js/arbor-tween.js
[INFO] Exclude: **/src/ui/public/js/jquery.mustache.js
[INFO] Exclude: **/src/ui/public/js/typeahead.jquery.min.js
[INFO] Exclude: **/dependency-reduced-pom.xml
[INFO] Exclude: **/docs/**
[INFO] Exclude: **/.git/**
[INFO] Exclude: **/derby.log
[INFO] Exclude: **/src/dev/**
[INFO] Exclude: **/src/codegen/config.fmpp
[INFO] Exclude: **/src/codegen/data/Parser.tdd
[INFO] Exclude: **/src/test/resources/FixedAvroSerializer.config
[INFO] 6 resources included (use -debug for more details)
Warning:  org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
Compiler warnings:
  WARNING:  'org.apache.xerces.jaxp.SAXParserImpl: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.'
Warning:  org.apache.xerces.parsers.SAXParser: Feature 'http://javax.xml.XMLConstants/feature/secure-processing' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
[INFO] Rat check: Summary of files. Unapproved: 0 unknown: 0 generated: 0 approved: 6 licence.
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Skipping flux-core
[INFO] This project has been banned from the build due to previous failures.
[INFO] ------------------------------------------------------------------------
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Skipping flux-examples
[INFO] This project has been banned from the build due to previous failures.
[INFO] ------------------------------------------------------------------------
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building storm-sql-runtime 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ storm-sql-runtime ---
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ storm-sql-runtime ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/sql/storm-sql-runtime/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ storm-sql-runtime ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ storm-sql-runtime ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/sql/storm-sql-runtime/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ storm-sql-runtime ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:2.18.1:test (default-test) @ storm-sql-runtime ---
[INFO] Surefire report directory: /home/travis/build/apache/storm/external/sql/storm-sql-runtime/target/surefire-reports
[INFO] parallel='none', perCoreThreadCount=true, threadCount=0, useUnlimitedThreads=false, threadCountSuites=0, threadCountClasses=0, threadCountMethods=0, parallelOptimized=true

-------------------------------------------------------
 T E S T S
-------------------------------------------------------

Results :

Tests run: 0, Failures: 0, Errors: 0, Skipped: 0

[INFO] 
[INFO] --- apache-rat-plugin:0.11:check (default) @ storm-sql-runtime ---
[INFO] 51 implicit excludes (use -debug for more details).
[INFO] Exclude: **/target/**
[INFO] Exclude: **/*.iml
[INFO] Exclude: **/.idea/**
[INFO] Exclude: **/metastore_db/**
[INFO] Exclude: **/build/**
[INFO] Exclude: **/logs/**
[INFO] Exclude: **/CHANGELOG.md
[INFO] Exclude: **/README.md
[INFO] Exclude: **/README.markdown
[INFO] Exclude: **/DEVELOPER.md
[INFO] Exclude: **/BYLAWS.md
[INFO] Exclude: **/STORM-UI-REST-API.md
[INFO] Exclude: SECURITY.md
[INFO] Exclude: VERSION
[INFO] Exclude: TODO
[INFO] Exclude: **/src/py/**
[INFO] Exclude: **/src/ui/public/js/jquery.dataTables.1.10.4.min.js
[INFO] Exclude: **/src/ui/public/css/jquery.dataTables.1.10.4.min.css
[INFO] Exclude: **/src/ui/public/images/*
[INFO] Exclude: **/src/ui/public/js/bootstrap-3.3.1.min.js
[INFO] Exclude: **/src/ui/public/css/bootstrap-3.3.1.min.css
[INFO] Exclude: **/src/ui/public/js/dataTables.bootstrap.min.js
[INFO] Exclude: **/src/ui/public/css/dataTables.bootstrap.css
[INFO] Exclude: **/src/ui/public/js/jsonFormatter.min.js
[INFO] Exclude: **/src/ui/public/css/jsonFormatter.min.css
[INFO] Exclude: **/src/ui/public/js/jquery-1.11.1.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.cookies.2.2.0.min.js
[INFO] Exclude: **/src/ui/public/js/moment.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.blockUI.min.js
[INFO] Exclude: **/src/ui/public/js/url.min.js
[INFO] Exclude: **/src/ui/public/js/arbor.js
[INFO] Exclude: **/src/ui/public/js/arbor-graphics.js
[INFO] Exclude: **/src/ui/public/js/arbor-tween.js
[INFO] Exclude: **/src/ui/public/js/jquery.mustache.js
[INFO] Exclude: **/src/ui/public/js/typeahead.jquery.min.js
[INFO] Exclude: **/dependency-reduced-pom.xml
[INFO] Exclude: **/docs/**
[INFO] Exclude: **/.git/**
[INFO] Exclude: **/derby.log
[INFO] Exclude: **/src/dev/**
[INFO] Exclude: **/src/codegen/config.fmpp
[INFO] Exclude: **/src/codegen/data/Parser.tdd
[INFO] Exclude: **/src/test/resources/FixedAvroSerializer.config
[INFO] 15 resources included (use -debug for more details)
Warning:  org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
Compiler warnings:
  WARNING:  'org.apache.xerces.jaxp.SAXParserImpl: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.'
Warning:  org.apache.xerces.parsers.SAXParser: Feature 'http://javax.xml.XMLConstants/feature/secure-processing' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
[INFO] Rat check: Summary of files. Unapproved: 0 unknown: 0 generated: 0 approved: 15 licence.
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building storm-sql-core 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-resources-plugin:2.5:copy-resources (copy-fmpp-resources) @ storm-sql-core ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 4 resources
[INFO] 
[INFO] --- maven-dependency-plugin:2.8:unpack (unpack-parser-template) @ storm-sql-core ---
[INFO] Configured Artifact: org.apache.calcite:calcite-core:?:jar
[INFO] Unpacking /home/travis/.m2/repository/org/apache/calcite/calcite-core/1.4.0-incubating/calcite-core-1.4.0-incubating.jar to /home/travis/build/apache/storm/external/sql/storm-sql-core/target with includes "**/Parser.jj" and excludes ""
[INFO] 
[INFO] --- fmpp-maven-plugin:1.0:generate (generate-fmpp-sources) @ storm-sql-core ---
- Executing: Parser.jj
log4j:WARN No appenders could be found for logger (freemarker.cache).
log4j:WARN Please initialize the log4j system properly.
[INFO] Done
[INFO] 
[INFO] --- javacc-maven-plugin:2.4:javacc (javacc) @ storm-sql-core ---
Java Compiler Compiler Version 4.0 (Parser Generator)
(type "javacc" with no arguments for help)
Reading from file /home/travis/build/apache/storm/external/sql/storm-sql-core/target/generated-sources/javacc/Parser.jj . . .
Note: UNICODE_INPUT option is specified. Please make sure you create the parser/lexer using a Reader with the correct character encoding.
Warning: Lookahead adequacy checking not being performed since option LOOKAHEAD is more than 1.  Set option FORCE_LA_CHECK to true to force checking.
Parser generated with 0 errors and 1 warnings.
[INFO] Processed 1 grammar
[INFO] 
[INFO] --- maven-resources-plugin:2.5:copy-resources (copy-java-sources) @ storm-sql-core ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 19 resources
[INFO] Copying 8 resources
[INFO] Copying 8 resources
[INFO] 
[INFO] --- build-helper-maven-plugin:1.5:add-source (add-generated-sources) @ storm-sql-core ---
[INFO] Source directory: /home/travis/build/apache/storm/external/sql/storm-sql-core/target/generated-sources added.
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ storm-sql-core ---
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ storm-sql-core ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/sql/storm-sql-core/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ storm-sql-core ---
[INFO] Changes detected - recompiling the module!
[INFO] Compiling 26 source files to /home/travis/build/apache/storm/external/sql/storm-sql-core/target/classes
[WARNING] /home/travis/build/apache/storm/external/sql/storm-sql-core/target/generated-sources/org/apache/storm/sql/parser/impl/StormParserImpl.java: Some input files use or override a deprecated API.
[WARNING] /home/travis/build/apache/storm/external/sql/storm-sql-core/target/generated-sources/org/apache/storm/sql/parser/impl/StormParserImpl.java: Recompile with -Xlint:deprecation for details.
[WARNING] /home/travis/build/apache/storm/external/sql/storm-sql-core/target/generated-sources/org/apache/storm/sql/parser/impl/StormParserImpl.java: /home/travis/build/apache/storm/external/sql/storm-sql-core/target/generated-sources/org/apache/storm/sql/parser/impl/StormParserImpl.java uses unchecked or unsafe operations.
[WARNING] /home/travis/build/apache/storm/external/sql/storm-sql-core/target/generated-sources/org/apache/storm/sql/parser/impl/StormParserImpl.java: Recompile with -Xlint:unchecked for details.
[INFO] 
[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ storm-sql-core ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/sql/storm-sql-core/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ storm-sql-core ---
[INFO] Changes detected - recompiling the module!
[INFO] Compiling 15 source files to /home/travis/build/apache/storm/external/sql/storm-sql-core/target/test-classes
[WARNING] /home/travis/build/apache/storm/external/sql/storm-sql-core/target/generated-sources/org/apache/storm/sql/parser/impl/StormParserImpl.java: /home/travis/build/apache/storm/external/sql/storm-sql-core/target/generated-sources/org/apache/storm/sql/parser/impl/StormParserImpl.java uses or overrides a deprecated API.
[WARNING] /home/travis/build/apache/storm/external/sql/storm-sql-core/target/generated-sources/org/apache/storm/sql/parser/impl/StormParserImpl.java: Recompile with -Xlint:deprecation for details.
[WARNING] /home/travis/build/apache/storm/external/sql/storm-sql-core/target/generated-sources/org/apache/storm/sql/parser/impl/StormParserImpl.java: /home/travis/build/apache/storm/external/sql/storm-sql-core/target/generated-sources/org/apache/storm/sql/parser/impl/StormParserImpl.java uses unchecked or unsafe operations.
[WARNING] /home/travis/build/apache/storm/external/sql/storm-sql-core/target/generated-sources/org/apache/storm/sql/parser/impl/StormParserImpl.java: Recompile with -Xlint:unchecked for details.
[INFO] 
[INFO] --- maven-surefire-plugin:2.18.1:test (default-test) @ storm-sql-core ---
[INFO] Surefire report directory: /home/travis/build/apache/storm/external/sql/storm-sql-core/target/surefire-reports
[INFO] parallel='none', perCoreThreadCount=true, threadCount=0, useUnlimitedThreads=false, threadCountSuites=0, threadCountClasses=0, threadCountMethods=0, parallelOptimized=true

-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running org.apache.storm.sql.parser.TestSqlParser
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.347 sec - in org.apache.storm.sql.parser.TestSqlParser
Running org.apache.storm.sql.compiler.backends.trident.TestPlanCompiler
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 22.206 sec - in org.apache.storm.sql.compiler.backends.trident.TestPlanCompiler
Running org.apache.storm.sql.compiler.backends.standalone.TestPlanCompiler
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.61 sec - in org.apache.storm.sql.compiler.backends.standalone.TestPlanCompiler
Running org.apache.storm.sql.compiler.backends.standalone.TestRelNodeCompiler
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.012 sec - in org.apache.storm.sql.compiler.backends.standalone.TestRelNodeCompiler
Running org.apache.storm.sql.compiler.TestExprCompiler
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.021 sec - in org.apache.storm.sql.compiler.TestExprCompiler
Running org.apache.storm.sql.compiler.TestExprSemantic
Tests run: 10, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.828 sec - in org.apache.storm.sql.compiler.TestExprSemantic
Running org.apache.storm.sql.TestStormSql
Tests run: 15, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.314 sec - in org.apache.storm.sql.TestStormSql

Results :

Tests run: 39, Failures: 0, Errors: 0, Skipped: 0

[INFO] 
[INFO] --- apache-rat-plugin:0.11:check (default) @ storm-sql-core ---
[INFO] 51 implicit excludes (use -debug for more details).
[INFO] Exclude: **/target/**
[INFO] Exclude: **/*.iml
[INFO] Exclude: **/.idea/**
[INFO] Exclude: **/metastore_db/**
[INFO] Exclude: **/build/**
[INFO] Exclude: **/logs/**
[INFO] Exclude: **/CHANGELOG.md
[INFO] Exclude: **/README.md
[INFO] Exclude: **/README.markdown
[INFO] Exclude: **/DEVELOPER.md
[INFO] Exclude: **/BYLAWS.md
[INFO] Exclude: **/STORM-UI-REST-API.md
[INFO] Exclude: SECURITY.md
[INFO] Exclude: VERSION
[INFO] Exclude: TODO
[INFO] Exclude: **/src/py/**
[INFO] Exclude: **/src/ui/public/js/jquery.dataTables.1.10.4.min.js
[INFO] Exclude: **/src/ui/public/css/jquery.dataTables.1.10.4.min.css
[INFO] Exclude: **/src/ui/public/images/*
[INFO] Exclude: **/src/ui/public/js/bootstrap-3.3.1.min.js
[INFO] Exclude: **/src/ui/public/css/bootstrap-3.3.1.min.css
[INFO] Exclude: **/src/ui/public/js/dataTables.bootstrap.min.js
[INFO] Exclude: **/src/ui/public/css/dataTables.bootstrap.css
[INFO] Exclude: **/src/ui/public/js/jsonFormatter.min.js
[INFO] Exclude: **/src/ui/public/css/jsonFormatter.min.css
[INFO] Exclude: **/src/ui/public/js/jquery-1.11.1.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.cookies.2.2.0.min.js
[INFO] Exclude: **/src/ui/public/js/moment.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.blockUI.min.js
[INFO] Exclude: **/src/ui/public/js/url.min.js
[INFO] Exclude: **/src/ui/public/js/arbor.js
[INFO] Exclude: **/src/ui/public/js/arbor-graphics.js
[INFO] Exclude: **/src/ui/public/js/arbor-tween.js
[INFO] Exclude: **/src/ui/public/js/jquery.mustache.js
[INFO] Exclude: **/src/ui/public/js/typeahead.jquery.min.js
[INFO] Exclude: **/dependency-reduced-pom.xml
[INFO] Exclude: **/docs/**
[INFO] Exclude: **/.git/**
[INFO] Exclude: **/derby.log
[INFO] Exclude: **/src/dev/**
[INFO] Exclude: **/src/codegen/config.fmpp
[INFO] Exclude: **/src/codegen/data/Parser.tdd
[INFO] Exclude: **/src/test/resources/FixedAvroSerializer.config
[INFO] 30 resources included (use -debug for more details)
Warning:  org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
Compiler warnings:
  WARNING:  'org.apache.xerces.jaxp.SAXParserImpl: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.'
Warning:  org.apache.xerces.parsers.SAXParser: Feature 'http://javax.xml.XMLConstants/feature/secure-processing' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
[INFO] Rat check: Summary of files. Unapproved: 0 unknown: 0 generated: 0 approved: 30 licence.
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Skipping storm-sql-kafka
[INFO] This project has been banned from the build due to previous failures.
[INFO] ------------------------------------------------------------------------
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building sql 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ sql ---
[INFO] 
[INFO] --- apache-rat-plugin:0.11:check (default) @ sql ---
[INFO] 51 implicit excludes (use -debug for more details).
[INFO] Exclude: **/target/**
[INFO] Exclude: **/*.iml
[INFO] Exclude: **/.idea/**
[INFO] Exclude: **/metastore_db/**
[INFO] Exclude: **/build/**
[INFO] Exclude: **/logs/**
[INFO] Exclude: **/CHANGELOG.md
[INFO] Exclude: **/README.md
[INFO] Exclude: **/README.markdown
[INFO] Exclude: **/DEVELOPER.md
[INFO] Exclude: **/BYLAWS.md
[INFO] Exclude: **/STORM-UI-REST-API.md
[INFO] Exclude: SECURITY.md
[INFO] Exclude: VERSION
[INFO] Exclude: TODO
[INFO] Exclude: **/src/py/**
[INFO] Exclude: **/src/ui/public/js/jquery.dataTables.1.10.4.min.js
[INFO] Exclude: **/src/ui/public/css/jquery.dataTables.1.10.4.min.css
[INFO] Exclude: **/src/ui/public/images/*
[INFO] Exclude: **/src/ui/public/js/bootstrap-3.3.1.min.js
[INFO] Exclude: **/src/ui/public/css/bootstrap-3.3.1.min.css
[INFO] Exclude: **/src/ui/public/js/dataTables.bootstrap.min.js
[INFO] Exclude: **/src/ui/public/css/dataTables.bootstrap.css
[INFO] Exclude: **/src/ui/public/js/jsonFormatter.min.js
[INFO] Exclude: **/src/ui/public/css/jsonFormatter.min.css
[INFO] Exclude: **/src/ui/public/js/jquery-1.11.1.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.cookies.2.2.0.min.js
[INFO] Exclude: **/src/ui/public/js/moment.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.blockUI.min.js
[INFO] Exclude: **/src/ui/public/js/url.min.js
[INFO] Exclude: **/src/ui/public/js/arbor.js
[INFO] Exclude: **/src/ui/public/js/arbor-graphics.js
[INFO] Exclude: **/src/ui/public/js/arbor-tween.js
[INFO] Exclude: **/src/ui/public/js/jquery.mustache.js
[INFO] Exclude: **/src/ui/public/js/typeahead.jquery.min.js
[INFO] Exclude: **/dependency-reduced-pom.xml
[INFO] Exclude: **/docs/**
[INFO] Exclude: **/.git/**
[INFO] Exclude: **/derby.log
[INFO] Exclude: **/src/dev/**
[INFO] Exclude: **/src/codegen/config.fmpp
[INFO] Exclude: **/src/codegen/data/Parser.tdd
[INFO] Exclude: **/src/test/resources/FixedAvroSerializer.config
[INFO] 53 resources included (use -debug for more details)
Warning:  org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
Compiler warnings:
  WARNING:  'org.apache.xerces.jaxp.SAXParserImpl: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.'
Warning:  org.apache.xerces.parsers.SAXParser: Feature 'http://javax.xml.XMLConstants/feature/secure-processing' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
[INFO] Rat check: Summary of files. Unapproved: 0 unknown: 0 generated: 0 approved: 53 licence.
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building storm-elasticsearch 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ storm-elasticsearch ---
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ storm-elasticsearch ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/storm-elasticsearch/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ storm-elasticsearch ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ storm-elasticsearch ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/storm-elasticsearch/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ storm-elasticsearch ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (cleanup) @ storm-elasticsearch ---
[INFO] 
[INFO] --- maven-surefire-plugin:2.18.1:test (default-test) @ storm-elasticsearch ---
[INFO] Surefire report directory: /home/travis/build/apache/storm/external/storm-elasticsearch/target/surefire-reports
[INFO] parallel='none', perCoreThreadCount=true, threadCount=0, useUnlimitedThreads=false, threadCountSuites=0, threadCountClasses=0, threadCountMethods=0, parallelOptimized=true

-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running org.apache.storm.elasticsearch.common.TransportAddressesTest
Tests run: 9, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.053 sec - in org.apache.storm.elasticsearch.common.TransportAddressesTest
Running org.apache.storm.elasticsearch.common.EsConfigTest
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.812 sec - in org.apache.storm.elasticsearch.common.EsConfigTest
Running org.apache.storm.elasticsearch.bolt.EsLookupBoltTest
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.071 sec - in org.apache.storm.elasticsearch.bolt.EsLookupBoltTest
Running org.apache.storm.elasticsearch.trident.EsStateFactoryTest
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.007 sec - in org.apache.storm.elasticsearch.trident.EsStateFactoryTest

Results :

Tests run: 18, Failures: 0, Errors: 0, Skipped: 0

[INFO] 
[INFO] --- apache-rat-plugin:0.11:check (default) @ storm-elasticsearch ---
[INFO] 51 implicit excludes (use -debug for more details).
[INFO] Exclude: **/target/**
[INFO] Exclude: **/*.iml
[INFO] Exclude: **/.idea/**
[INFO] Exclude: **/metastore_db/**
[INFO] Exclude: **/build/**
[INFO] Exclude: **/logs/**
[INFO] Exclude: **/CHANGELOG.md
[INFO] Exclude: **/README.md
[INFO] Exclude: **/README.markdown
[INFO] Exclude: **/DEVELOPER.md
[INFO] Exclude: **/BYLAWS.md
[INFO] Exclude: **/STORM-UI-REST-API.md
[INFO] Exclude: SECURITY.md
[INFO] Exclude: VERSION
[INFO] Exclude: TODO
[INFO] Exclude: **/src/py/**
[INFO] Exclude: **/src/ui/public/js/jquery.dataTables.1.10.4.min.js
[INFO] Exclude: **/src/ui/public/css/jquery.dataTables.1.10.4.min.css
[INFO] Exclude: **/src/ui/public/images/*
[INFO] Exclude: **/src/ui/public/js/bootstrap-3.3.1.min.js
[INFO] Exclude: **/src/ui/public/css/bootstrap-3.3.1.min.css
[INFO] Exclude: **/src/ui/public/js/dataTables.bootstrap.min.js
[INFO] Exclude: **/src/ui/public/css/dataTables.bootstrap.css
[INFO] Exclude: **/src/ui/public/js/jsonFormatter.min.js
[INFO] Exclude: **/src/ui/public/css/jsonFormatter.min.css
[INFO] Exclude: **/src/ui/public/js/jquery-1.11.1.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.cookies.2.2.0.min.js
[INFO] Exclude: **/src/ui/public/js/moment.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.blockUI.min.js
[INFO] Exclude: **/src/ui/public/js/url.min.js
[INFO] Exclude: **/src/ui/public/js/arbor.js
[INFO] Exclude: **/src/ui/public/js/arbor-graphics.js
[INFO] Exclude: **/src/ui/public/js/arbor-tween.js
[INFO] Exclude: **/src/ui/public/js/jquery.mustache.js
[INFO] Exclude: **/src/ui/public/js/typeahead.jquery.min.js
[INFO] Exclude: **/dependency-reduced-pom.xml
[INFO] Exclude: **/docs/**
[INFO] Exclude: **/.git/**
[INFO] Exclude: **/derby.log
[INFO] Exclude: **/src/dev/**
[INFO] Exclude: **/src/codegen/config.fmpp
[INFO] Exclude: **/src/codegen/data/Parser.tdd
[INFO] Exclude: **/src/test/resources/FixedAvroSerializer.config
[INFO] 28 resources included (use -debug for more details)
Warning:  org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
Compiler warnings:
  WARNING:  'org.apache.xerces.jaxp.SAXParserImpl: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.'
Warning:  org.apache.xerces.parsers.SAXParser: Feature 'http://javax.xml.XMLConstants/feature/secure-processing' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
[INFO] Rat check: Summary of files. Unapproved: 0 unknown: 0 generated: 0 approved: 28 licence.
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building storm-solr 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ storm-solr ---
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ storm-solr ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/storm-solr/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ storm-solr ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ storm-solr ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/storm-solr/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ storm-solr ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:2.18.1:test (default-test) @ storm-solr ---
[INFO] Surefire report directory: /home/travis/build/apache/storm/external/storm-solr/target/surefire-reports
[INFO] parallel='none', perCoreThreadCount=true, threadCount=0, useUnlimitedThreads=false, threadCountSuites=0, threadCountClasses=0, threadCountMethods=0, parallelOptimized=true

-------------------------------------------------------
 T E S T S
-------------------------------------------------------

Results :

Tests run: 0, Failures: 0, Errors: 0, Skipped: 0

[INFO] 
[INFO] --- apache-rat-plugin:0.11:check (default) @ storm-solr ---
[INFO] 51 implicit excludes (use -debug for more details).
[INFO] Exclude: **/target/**
[INFO] Exclude: **/*.iml
[INFO] Exclude: **/.idea/**
[INFO] Exclude: **/metastore_db/**
[INFO] Exclude: **/build/**
[INFO] Exclude: **/logs/**
[INFO] Exclude: **/CHANGELOG.md
[INFO] Exclude: **/README.md
[INFO] Exclude: **/README.markdown
[INFO] Exclude: **/DEVELOPER.md
[INFO] Exclude: **/BYLAWS.md
[INFO] Exclude: **/STORM-UI-REST-API.md
[INFO] Exclude: SECURITY.md
[INFO] Exclude: VERSION
[INFO] Exclude: TODO
[INFO] Exclude: **/src/py/**
[INFO] Exclude: **/src/ui/public/js/jquery.dataTables.1.10.4.min.js
[INFO] Exclude: **/src/ui/public/css/jquery.dataTables.1.10.4.min.css
[INFO] Exclude: **/src/ui/public/images/*
[INFO] Exclude: **/src/ui/public/js/bootstrap-3.3.1.min.js
[INFO] Exclude: **/src/ui/public/css/bootstrap-3.3.1.min.css
[INFO] Exclude: **/src/ui/public/js/dataTables.bootstrap.min.js
[INFO] Exclude: **/src/ui/public/css/dataTables.bootstrap.css
[INFO] Exclude: **/src/ui/public/js/jsonFormatter.min.js
[INFO] Exclude: **/src/ui/public/css/jsonFormatter.min.css
[INFO] Exclude: **/src/ui/public/js/jquery-1.11.1.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.cookies.2.2.0.min.js
[INFO] Exclude: **/src/ui/public/js/moment.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.blockUI.min.js
[INFO] Exclude: **/src/ui/public/js/url.min.js
[INFO] Exclude: **/src/ui/public/js/arbor.js
[INFO] Exclude: **/src/ui/public/js/arbor-graphics.js
[INFO] Exclude: **/src/ui/public/js/arbor-tween.js
[INFO] Exclude: **/src/ui/public/js/jquery.mustache.js
[INFO] Exclude: **/src/ui/public/js/typeahead.jquery.min.js
[INFO] Exclude: **/dependency-reduced-pom.xml
[INFO] Exclude: **/docs/**
[INFO] Exclude: **/.git/**
[INFO] Exclude: **/derby.log
[INFO] Exclude: **/src/dev/**
[INFO] Exclude: **/src/codegen/config.fmpp
[INFO] Exclude: **/src/codegen/data/Parser.tdd
[INFO] Exclude: **/src/test/resources/FixedAvroSerializer.config
[INFO] 27 resources included (use -debug for more details)
Warning:  org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
Compiler warnings:
  WARNING:  'org.apache.xerces.jaxp.SAXParserImpl: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.'
Warning:  org.apache.xerces.parsers.SAXParser: Feature 'http://javax.xml.XMLConstants/feature/secure-processing' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
[INFO] Rat check: Summary of files. Unapproved: 0 unknown: 0 generated: 0 approved: 27 licence.
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building storm-metrics 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-antrun-plugin:1.6:run (prepare) @ storm-metrics ---
[WARNING] Parameter tasks is deprecated, use target instead
[INFO] Executing tasks

main:
     [echo] Downloading sigar native binaries...
      [get] Destination already exists (skipping): /home/travis/.m2/repository/org/fusesource/sigar/1.6.4/hyperic-sigar-1.6.4.zip
    [unzip] Expanding: /home/travis/.m2/repository/org/fusesource/sigar/1.6.4/hyperic-sigar-1.6.4.zip into /home/travis/build/apache/storm/external/storm-metrics/target/classes/resources
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ storm-metrics ---
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ storm-metrics ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/storm-metrics/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ storm-metrics ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ storm-metrics ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/storm-metrics/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ storm-metrics ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-surefire-plugin:2.18.1:test (default-test) @ storm-metrics ---
[INFO] 
[INFO] --- apache-rat-plugin:0.11:check (default) @ storm-metrics ---
[INFO] 51 implicit excludes (use -debug for more details).
[INFO] Exclude: **/target/**
[INFO] Exclude: **/*.iml
[INFO] Exclude: **/.idea/**
[INFO] Exclude: **/metastore_db/**
[INFO] Exclude: **/build/**
[INFO] Exclude: **/logs/**
[INFO] Exclude: **/CHANGELOG.md
[INFO] Exclude: **/README.md
[INFO] Exclude: **/README.markdown
[INFO] Exclude: **/DEVELOPER.md
[INFO] Exclude: **/BYLAWS.md
[INFO] Exclude: **/STORM-UI-REST-API.md
[INFO] Exclude: SECURITY.md
[INFO] Exclude: VERSION
[INFO] Exclude: TODO
[INFO] Exclude: **/src/py/**
[INFO] Exclude: **/src/ui/public/js/jquery.dataTables.1.10.4.min.js
[INFO] Exclude: **/src/ui/public/css/jquery.dataTables.1.10.4.min.css
[INFO] Exclude: **/src/ui/public/images/*
[INFO] Exclude: **/src/ui/public/js/bootstrap-3.3.1.min.js
[INFO] Exclude: **/src/ui/public/css/bootstrap-3.3.1.min.css
[INFO] Exclude: **/src/ui/public/js/dataTables.bootstrap.min.js
[INFO] Exclude: **/src/ui/public/css/dataTables.bootstrap.css
[INFO] Exclude: **/src/ui/public/js/jsonFormatter.min.js
[INFO] Exclude: **/src/ui/public/css/jsonFormatter.min.css
[INFO] Exclude: **/src/ui/public/js/jquery-1.11.1.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.cookies.2.2.0.min.js
[INFO] Exclude: **/src/ui/public/js/moment.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.blockUI.min.js
[INFO] Exclude: **/src/ui/public/js/url.min.js
[INFO] Exclude: **/src/ui/public/js/arbor.js
[INFO] Exclude: **/src/ui/public/js/arbor-graphics.js
[INFO] Exclude: **/src/ui/public/js/arbor-tween.js
[INFO] Exclude: **/src/ui/public/js/jquery.mustache.js
[INFO] Exclude: **/src/ui/public/js/typeahead.jquery.min.js
[INFO] Exclude: **/dependency-reduced-pom.xml
[INFO] Exclude: **/docs/**
[INFO] Exclude: **/.git/**
[INFO] Exclude: **/derby.log
[INFO] Exclude: **/src/dev/**
[INFO] Exclude: **/src/codegen/config.fmpp
[INFO] Exclude: **/src/codegen/data/Parser.tdd
[INFO] Exclude: **/src/test/resources/FixedAvroSerializer.config
[INFO] 3 resources included (use -debug for more details)
Warning:  org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
Compiler warnings:
  WARNING:  'org.apache.xerces.jaxp.SAXParserImpl: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.'
Warning:  org.apache.xerces.parsers.SAXParser: Feature 'http://javax.xml.XMLConstants/feature/secure-processing' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
[INFO] Rat check: Summary of files. Unapproved: 0 unknown: 0 generated: 0 approved: 3 licence.
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building storm-cassandra 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ storm-cassandra ---
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ storm-cassandra ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/storm-cassandra/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ storm-cassandra ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ storm-cassandra ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 1 resource
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ storm-cassandra ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:2.18.1:test (default-test) @ storm-cassandra ---
[INFO] 
[INFO] --- apache-rat-plugin:0.11:check (default) @ storm-cassandra ---
[INFO] 51 implicit excludes (use -debug for more details).
[INFO] Exclude: **/target/**
[INFO] Exclude: **/*.iml
[INFO] Exclude: **/.idea/**
[INFO] Exclude: **/metastore_db/**
[INFO] Exclude: **/build/**
[INFO] Exclude: **/logs/**
[INFO] Exclude: **/CHANGELOG.md
[INFO] Exclude: **/README.md
[INFO] Exclude: **/README.markdown
[INFO] Exclude: **/DEVELOPER.md
[INFO] Exclude: **/BYLAWS.md
[INFO] Exclude: **/STORM-UI-REST-API.md
[INFO] Exclude: SECURITY.md
[INFO] Exclude: VERSION
[INFO] Exclude: TODO
[INFO] Exclude: **/src/py/**
[INFO] Exclude: **/src/ui/public/js/jquery.dataTables.1.10.4.min.js
[INFO] Exclude: **/src/ui/public/css/jquery.dataTables.1.10.4.min.css
[INFO] Exclude: **/src/ui/public/images/*
[INFO] Exclude: **/src/ui/public/js/bootstrap-3.3.1.min.js
[INFO] Exclude: **/src/ui/public/css/bootstrap-3.3.1.min.css
[INFO] Exclude: **/src/ui/public/js/dataTables.bootstrap.min.js
[INFO] Exclude: **/src/ui/public/css/dataTables.bootstrap.css
[INFO] Exclude: **/src/ui/public/js/jsonFormatter.min.js
[INFO] Exclude: **/src/ui/public/css/jsonFormatter.min.css
[INFO] Exclude: **/src/ui/public/js/jquery-1.11.1.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.cookies.2.2.0.min.js
[INFO] Exclude: **/src/ui/public/js/moment.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.blockUI.min.js
[INFO] Exclude: **/src/ui/public/js/url.min.js
[INFO] Exclude: **/src/ui/public/js/arbor.js
[INFO] Exclude: **/src/ui/public/js/arbor-graphics.js
[INFO] Exclude: **/src/ui/public/js/arbor-tween.js
[INFO] Exclude: **/src/ui/public/js/jquery.mustache.js
[INFO] Exclude: **/src/ui/public/js/typeahead.jquery.min.js
[INFO] Exclude: **/dependency-reduced-pom.xml
[INFO] Exclude: **/docs/**
[INFO] Exclude: **/.git/**
[INFO] Exclude: **/derby.log
[INFO] Exclude: **/src/dev/**
[INFO] Exclude: **/src/codegen/config.fmpp
[INFO] Exclude: **/src/codegen/data/Parser.tdd
[INFO] Exclude: **/src/test/resources/FixedAvroSerializer.config
[INFO] 50 resources included (use -debug for more details)
Warning:  org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
Compiler warnings:
  WARNING:  'org.apache.xerces.jaxp.SAXParserImpl: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.'
Warning:  org.apache.xerces.parsers.SAXParser: Feature 'http://javax.xml.XMLConstants/feature/secure-processing' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
[INFO] Rat check: Summary of files. Unapproved: 0 unknown: 0 generated: 0 approved: 50 licence.
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building storm-mqtt-parent 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ storm-mqtt-parent ---
[INFO] 
[INFO] --- apache-rat-plugin:0.11:check (default) @ storm-mqtt-parent ---
[INFO] 51 implicit excludes (use -debug for more details).
[INFO] Exclude: **/target/**
[INFO] Exclude: **/*.iml
[INFO] Exclude: **/.idea/**
[INFO] Exclude: **/metastore_db/**
[INFO] Exclude: **/build/**
[INFO] Exclude: **/logs/**
[INFO] Exclude: **/CHANGELOG.md
[INFO] Exclude: **/README.md
[INFO] Exclude: **/README.markdown
[INFO] Exclude: **/DEVELOPER.md
[INFO] Exclude: **/BYLAWS.md
[INFO] Exclude: **/STORM-UI-REST-API.md
[INFO] Exclude: SECURITY.md
[INFO] Exclude: VERSION
[INFO] Exclude: TODO
[INFO] Exclude: **/src/py/**
[INFO] Exclude: **/src/ui/public/js/jquery.dataTables.1.10.4.min.js
[INFO] Exclude: **/src/ui/public/css/jquery.dataTables.1.10.4.min.css
[INFO] Exclude: **/src/ui/public/images/*
[INFO] Exclude: **/src/ui/public/js/bootstrap-3.3.1.min.js
[INFO] Exclude: **/src/ui/public/css/bootstrap-3.3.1.min.css
[INFO] Exclude: **/src/ui/public/js/dataTables.bootstrap.min.js
[INFO] Exclude: **/src/ui/public/css/dataTables.bootstrap.css
[INFO] Exclude: **/src/ui/public/js/jsonFormatter.min.js
[INFO] Exclude: **/src/ui/public/css/jsonFormatter.min.css
[INFO] Exclude: **/src/ui/public/js/jquery-1.11.1.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.cookies.2.2.0.min.js
[INFO] Exclude: **/src/ui/public/js/moment.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.blockUI.min.js
[INFO] Exclude: **/src/ui/public/js/url.min.js
[INFO] Exclude: **/src/ui/public/js/arbor.js
[INFO] Exclude: **/src/ui/public/js/arbor-graphics.js
[INFO] Exclude: **/src/ui/public/js/arbor-tween.js
[INFO] Exclude: **/src/ui/public/js/jquery.mustache.js
[INFO] Exclude: **/src/ui/public/js/typeahead.jquery.min.js
[INFO] Exclude: **/dependency-reduced-pom.xml
[INFO] Exclude: **/docs/**
[INFO] Exclude: **/.git/**
[INFO] Exclude: **/derby.log
[INFO] Exclude: **/src/dev/**
[INFO] Exclude: **/src/codegen/config.fmpp
[INFO] Exclude: **/src/codegen/data/Parser.tdd
[INFO] Exclude: **/src/test/resources/FixedAvroSerializer.config
[INFO] 25 resources included (use -debug for more details)
Warning:  org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
Compiler warnings:
  WARNING:  'org.apache.xerces.jaxp.SAXParserImpl: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.'
Warning:  org.apache.xerces.parsers.SAXParser: Feature 'http://javax.xml.XMLConstants/feature/secure-processing' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
[INFO] Rat check: Summary of files. Unapproved: 0 unknown: 0 generated: 0 approved: 25 licence.
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building storm-mqtt 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ storm-mqtt ---
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ storm-mqtt ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/storm-mqtt/core/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ storm-mqtt ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ storm-mqtt ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/storm-mqtt/core/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ storm-mqtt ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:2.18.1:test (default-test) @ storm-mqtt ---
[WARNING] The parameter forkMode is deprecated since version 2.14. Use forkCount and reuseForks instead.
[INFO] Surefire report directory: /home/travis/build/apache/storm/external/storm-mqtt/core/target/surefire-reports
[INFO] parallel='none', perCoreThreadCount=true, threadCount=0, useUnlimitedThreads=false, threadCountSuites=0, threadCountClasses=0, threadCountMethods=0, parallelOptimized=true

-------------------------------------------------------
 T E S T S
-------------------------------------------------------

Results :

Tests run: 0, Failures: 0, Errors: 0, Skipped: 0

[INFO] 
[INFO] --- apache-rat-plugin:0.11:check (default) @ storm-mqtt ---
[INFO] 51 implicit excludes (use -debug for more details).
[INFO] Exclude: **/target/**
[INFO] Exclude: **/*.iml
[INFO] Exclude: **/.idea/**
[INFO] Exclude: **/metastore_db/**
[INFO] Exclude: **/build/**
[INFO] Exclude: **/logs/**
[INFO] Exclude: **/CHANGELOG.md
[INFO] Exclude: **/README.md
[INFO] Exclude: **/README.markdown
[INFO] Exclude: **/DEVELOPER.md
[INFO] Exclude: **/BYLAWS.md
[INFO] Exclude: **/STORM-UI-REST-API.md
[INFO] Exclude: SECURITY.md
[INFO] Exclude: VERSION
[INFO] Exclude: TODO
[INFO] Exclude: **/src/py/**
[INFO] Exclude: **/src/ui/public/js/jquery.dataTables.1.10.4.min.js
[INFO] Exclude: **/src/ui/public/css/jquery.dataTables.1.10.4.min.css
[INFO] Exclude: **/src/ui/public/images/*
[INFO] Exclude: **/src/ui/public/js/bootstrap-3.3.1.min.js
[INFO] Exclude: **/src/ui/public/css/bootstrap-3.3.1.min.css
[INFO] Exclude: **/src/ui/public/js/dataTables.bootstrap.min.js
[INFO] Exclude: **/src/ui/public/css/dataTables.bootstrap.css
[INFO] Exclude: **/src/ui/public/js/jsonFormatter.min.js
[INFO] Exclude: **/src/ui/public/css/jsonFormatter.min.css
[INFO] Exclude: **/src/ui/public/js/jquery-1.11.1.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.cookies.2.2.0.min.js
[INFO] Exclude: **/src/ui/public/js/moment.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.blockUI.min.js
[INFO] Exclude: **/src/ui/public/js/url.min.js
[INFO] Exclude: **/src/ui/public/js/arbor.js
[INFO] Exclude: **/src/ui/public/js/arbor-graphics.js
[INFO] Exclude: **/src/ui/public/js/arbor-tween.js
[INFO] Exclude: **/src/ui/public/js/jquery.mustache.js
[INFO] Exclude: **/src/ui/public/js/typeahead.jquery.min.js
[INFO] Exclude: **/dependency-reduced-pom.xml
[INFO] Exclude: **/docs/**
[INFO] Exclude: **/.git/**
[INFO] Exclude: **/derby.log
[INFO] Exclude: **/src/dev/**
[INFO] Exclude: **/src/codegen/config.fmpp
[INFO] Exclude: **/src/codegen/data/Parser.tdd
[INFO] Exclude: **/src/test/resources/FixedAvroSerializer.config
[INFO] 18 resources included (use -debug for more details)
Warning:  org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
Compiler warnings:
  WARNING:  'org.apache.xerces.jaxp.SAXParserImpl: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.'
Warning:  org.apache.xerces.parsers.SAXParser: Feature 'http://javax.xml.XMLConstants/feature/secure-processing' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
[INFO] Rat check: Summary of files. Unapproved: 0 unknown: 0 generated: 0 approved: 18 licence.
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Skipping storm-mqtt-examples
[INFO] This project has been banned from the build due to previous failures.
[INFO] ------------------------------------------------------------------------
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building storm-mongodb 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ storm-mongodb ---
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ storm-mongodb ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/storm-mongodb/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ storm-mongodb ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ storm-mongodb ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/storm-mongodb/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ storm-mongodb ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:2.18.1:test (default-test) @ storm-mongodb ---
[INFO] 
[INFO] --- apache-rat-plugin:0.11:check (default) @ storm-mongodb ---
[INFO] 51 implicit excludes (use -debug for more details).
[INFO] Exclude: **/target/**
[INFO] Exclude: **/*.iml
[INFO] Exclude: **/.idea/**
[INFO] Exclude: **/metastore_db/**
[INFO] Exclude: **/build/**
[INFO] Exclude: **/logs/**
[INFO] Exclude: **/CHANGELOG.md
[INFO] Exclude: **/README.md
[INFO] Exclude: **/README.markdown
[INFO] Exclude: **/DEVELOPER.md
[INFO] Exclude: **/BYLAWS.md
[INFO] Exclude: **/STORM-UI-REST-API.md
[INFO] Exclude: SECURITY.md
[INFO] Exclude: VERSION
[INFO] Exclude: TODO
[INFO] Exclude: **/src/py/**
[INFO] Exclude: **/src/ui/public/js/jquery.dataTables.1.10.4.min.js
[INFO] Exclude: **/src/ui/public/css/jquery.dataTables.1.10.4.min.css
[INFO] Exclude: **/src/ui/public/images/*
[INFO] Exclude: **/src/ui/public/js/bootstrap-3.3.1.min.js
[INFO] Exclude: **/src/ui/public/css/bootstrap-3.3.1.min.css
[INFO] Exclude: **/src/ui/public/js/dataTables.bootstrap.min.js
[INFO] Exclude: **/src/ui/public/css/dataTables.bootstrap.css
[INFO] Exclude: **/src/ui/public/js/jsonFormatter.min.js
[INFO] Exclude: **/src/ui/public/css/jsonFormatter.min.css
[INFO] Exclude: **/src/ui/public/js/jquery-1.11.1.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.cookies.2.2.0.min.js
[INFO] Exclude: **/src/ui/public/js/moment.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.blockUI.min.js
[INFO] Exclude: **/src/ui/public/js/url.min.js
[INFO] Exclude: **/src/ui/public/js/arbor.js
[INFO] Exclude: **/src/ui/public/js/arbor-graphics.js
[INFO] Exclude: **/src/ui/public/js/arbor-tween.js
[INFO] Exclude: **/src/ui/public/js/jquery.mustache.js
[INFO] Exclude: **/src/ui/public/js/typeahead.jquery.min.js
[INFO] Exclude: **/dependency-reduced-pom.xml
[INFO] Exclude: **/docs/**
[INFO] Exclude: **/.git/**
[INFO] Exclude: **/derby.log
[INFO] Exclude: **/src/dev/**
[INFO] Exclude: **/src/codegen/config.fmpp
[INFO] Exclude: **/src/codegen/data/Parser.tdd
[INFO] Exclude: **/src/test/resources/FixedAvroSerializer.config
[INFO] 18 resources included (use -debug for more details)
Warning:  org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
Compiler warnings:
  WARNING:  'org.apache.xerces.jaxp.SAXParserImpl: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.'
Warning:  org.apache.xerces.parsers.SAXParser: Feature 'http://javax.xml.XMLConstants/feature/secure-processing' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
[INFO] Rat check: Summary of files. Unapproved: 0 unknown: 0 generated: 0 approved: 18 licence.
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building storm-clojure 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ storm-clojure ---
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ storm-clojure ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/storm-clojure/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ storm-clojure ---
[INFO] No sources to compile
[INFO] 
[INFO] --- clojure-maven-plugin:1.7.1:compile (compile) @ storm-clojure ---
[INFO] 
[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ storm-clojure ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/storm-clojure/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ storm-clojure ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-surefire-plugin:2.18.1:test (default-test) @ storm-clojure ---
[INFO] 
[INFO] --- apache-rat-plugin:0.11:check (default) @ storm-clojure ---
[INFO] 51 implicit excludes (use -debug for more details).
[INFO] Exclude: **/target/**
[INFO] Exclude: **/*.iml
[INFO] Exclude: **/.idea/**
[INFO] Exclude: **/metastore_db/**
[INFO] Exclude: **/build/**
[INFO] Exclude: **/logs/**
[INFO] Exclude: **/CHANGELOG.md
[INFO] Exclude: **/README.md
[INFO] Exclude: **/README.markdown
[INFO] Exclude: **/DEVELOPER.md
[INFO] Exclude: **/BYLAWS.md
[INFO] Exclude: **/STORM-UI-REST-API.md
[INFO] Exclude: SECURITY.md
[INFO] Exclude: VERSION
[INFO] Exclude: TODO
[INFO] Exclude: **/src/py/**
[INFO] Exclude: **/src/ui/public/js/jquery.dataTables.1.10.4.min.js
[INFO] Exclude: **/src/ui/public/css/jquery.dataTables.1.10.4.min.css
[INFO] Exclude: **/src/ui/public/images/*
[INFO] Exclude: **/src/ui/public/js/bootstrap-3.3.1.min.js
[INFO] Exclude: **/src/ui/public/css/bootstrap-3.3.1.min.css
[INFO] Exclude: **/src/ui/public/js/dataTables.bootstrap.min.js
[INFO] Exclude: **/src/ui/public/css/dataTables.bootstrap.css
[INFO] Exclude: **/src/ui/public/js/jsonFormatter.min.js
[INFO] Exclude: **/src/ui/public/css/jsonFormatter.min.css
[INFO] Exclude: **/src/ui/public/js/jquery-1.11.1.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.cookies.2.2.0.min.js
[INFO] Exclude: **/src/ui/public/js/moment.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.blockUI.min.js
[INFO] Exclude: **/src/ui/public/js/url.min.js
[INFO] Exclude: **/src/ui/public/js/arbor.js
[INFO] Exclude: **/src/ui/public/js/arbor-graphics.js
[INFO] Exclude: **/src/ui/public/js/arbor-tween.js
[INFO] Exclude: **/src/ui/public/js/jquery.mustache.js
[INFO] Exclude: **/src/ui/public/js/typeahead.jquery.min.js
[INFO] Exclude: **/dependency-reduced-pom.xml
[INFO] Exclude: **/docs/**
[INFO] Exclude: **/.git/**
[INFO] Exclude: **/derby.log
[INFO] Exclude: **/src/dev/**
[INFO] Exclude: **/src/codegen/config.fmpp
[INFO] Exclude: **/src/codegen/data/Parser.tdd
[INFO] Exclude: **/src/test/resources/FixedAvroSerializer.config
[INFO] 4 resources included (use -debug for more details)
Warning:  org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
Compiler warnings:
  WARNING:  'org.apache.xerces.jaxp.SAXParserImpl: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.'
Warning:  org.apache.xerces.parsers.SAXParser: Feature 'http://javax.xml.XMLConstants/feature/secure-processing' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
[INFO] Rat check: Summary of files. Unapproved: 0 unknown: 0 generated: 0 approved: 4 licence.
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Skipping storm-starter
[INFO] This project has been banned from the build due to previous failures.
[INFO] ------------------------------------------------------------------------
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building storm-kafka-client 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ storm-kafka-client ---
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ storm-kafka-client ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/storm-kafka-client/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ storm-kafka-client ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ storm-kafka-client ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/storm-kafka-client/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ storm-kafka-client ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:2.18.1:test (default-test) @ storm-kafka-client ---
[INFO] 
[INFO] --- apache-rat-plugin:0.11:check (default) @ storm-kafka-client ---
[INFO] 51 implicit excludes (use -debug for more details).
[INFO] Exclude: **/target/**
[INFO] Exclude: **/*.iml
[INFO] Exclude: **/.idea/**
[INFO] Exclude: **/metastore_db/**
[INFO] Exclude: **/build/**
[INFO] Exclude: **/logs/**
[INFO] Exclude: **/CHANGELOG.md
[INFO] Exclude: **/README.md
[INFO] Exclude: **/README.markdown
[INFO] Exclude: **/DEVELOPER.md
[INFO] Exclude: **/BYLAWS.md
[INFO] Exclude: **/STORM-UI-REST-API.md
[INFO] Exclude: SECURITY.md
[INFO] Exclude: VERSION
[INFO] Exclude: TODO
[INFO] Exclude: **/src/py/**
[INFO] Exclude: **/src/ui/public/js/jquery.dataTables.1.10.4.min.js
[INFO] Exclude: **/src/ui/public/css/jquery.dataTables.1.10.4.min.css
[INFO] Exclude: **/src/ui/public/images/*
[INFO] Exclude: **/src/ui/public/js/bootstrap-3.3.1.min.js
[INFO] Exclude: **/src/ui/public/css/bootstrap-3.3.1.min.css
[INFO] Exclude: **/src/ui/public/js/dataTables.bootstrap.min.js
[INFO] Exclude: **/src/ui/public/css/dataTables.bootstrap.css
[INFO] Exclude: **/src/ui/public/js/jsonFormatter.min.js
[INFO] Exclude: **/src/ui/public/css/jsonFormatter.min.css
[INFO] Exclude: **/src/ui/public/js/jquery-1.11.1.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.cookies.2.2.0.min.js
[INFO] Exclude: **/src/ui/public/js/moment.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.blockUI.min.js
[INFO] Exclude: **/src/ui/public/js/url.min.js
[INFO] Exclude: **/src/ui/public/js/arbor.js
[INFO] Exclude: **/src/ui/public/js/arbor-graphics.js
[INFO] Exclude: **/src/ui/public/js/arbor-tween.js
[INFO] Exclude: **/src/ui/public/js/jquery.mustache.js
[INFO] Exclude: **/src/ui/public/js/typeahead.jquery.min.js
[INFO] Exclude: **/dependency-reduced-pom.xml
[INFO] Exclude: **/docs/**
[INFO] Exclude: **/.git/**
[INFO] Exclude: **/derby.log
[INFO] Exclude: **/src/dev/**
[INFO] Exclude: **/src/codegen/config.fmpp
[INFO] Exclude: **/src/codegen/data/Parser.tdd
[INFO] Exclude: **/src/test/resources/FixedAvroSerializer.config
[INFO] 14 resources included (use -debug for more details)
Warning:  org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
Compiler warnings:
  WARNING:  'org.apache.xerces.jaxp.SAXParserImpl: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.'
Warning:  org.apache.xerces.parsers.SAXParser: Feature 'http://javax.xml.XMLConstants/feature/secure-processing' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
[INFO] Rat check: Summary of files. Unapproved: 0 unknown: 0 generated: 0 approved: 14 licence.
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building storm-opentsdb 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ storm-opentsdb ---
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ storm-opentsdb ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/storm-opentsdb/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ storm-opentsdb ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ storm-opentsdb ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/storm-opentsdb/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ storm-opentsdb ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:2.18.1:test (default-test) @ storm-opentsdb ---
[INFO] 
[INFO] --- apache-rat-plugin:0.11:check (default) @ storm-opentsdb ---
[INFO] 51 implicit excludes (use -debug for more details).
[INFO] Exclude: **/target/**
[INFO] Exclude: **/*.iml
[INFO] Exclude: **/.idea/**
[INFO] Exclude: **/metastore_db/**
[INFO] Exclude: **/build/**
[INFO] Exclude: **/logs/**
[INFO] Exclude: **/CHANGELOG.md
[INFO] Exclude: **/README.md
[INFO] Exclude: **/README.markdown
[INFO] Exclude: **/DEVELOPER.md
[INFO] Exclude: **/BYLAWS.md
[INFO] Exclude: **/STORM-UI-REST-API.md
[INFO] Exclude: SECURITY.md
[INFO] Exclude: VERSION
[INFO] Exclude: TODO
[INFO] Exclude: **/src/py/**
[INFO] Exclude: **/src/ui/public/js/jquery.dataTables.1.10.4.min.js
[INFO] Exclude: **/src/ui/public/css/jquery.dataTables.1.10.4.min.css
[INFO] Exclude: **/src/ui/public/images/*
[INFO] Exclude: **/src/ui/public/js/bootstrap-3.3.1.min.js
[INFO] Exclude: **/src/ui/public/css/bootstrap-3.3.1.min.css
[INFO] Exclude: **/src/ui/public/js/dataTables.bootstrap.min.js
[INFO] Exclude: **/src/ui/public/css/dataTables.bootstrap.css
[INFO] Exclude: **/src/ui/public/js/jsonFormatter.min.js
[INFO] Exclude: **/src/ui/public/css/jsonFormatter.min.css
[INFO] Exclude: **/src/ui/public/js/jquery-1.11.1.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.cookies.2.2.0.min.js
[INFO] Exclude: **/src/ui/public/js/moment.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.blockUI.min.js
[INFO] Exclude: **/src/ui/public/js/url.min.js
[INFO] Exclude: **/src/ui/public/js/arbor.js
[INFO] Exclude: **/src/ui/public/js/arbor-graphics.js
[INFO] Exclude: **/src/ui/public/js/arbor-tween.js
[INFO] Exclude: **/src/ui/public/js/jquery.mustache.js
[INFO] Exclude: **/src/ui/public/js/typeahead.jquery.min.js
[INFO] Exclude: **/dependency-reduced-pom.xml
[INFO] Exclude: **/docs/**
[INFO] Exclude: **/.git/**
[INFO] Exclude: **/derby.log
[INFO] Exclude: **/src/dev/**
[INFO] Exclude: **/src/codegen/config.fmpp
[INFO] Exclude: **/src/codegen/data/Parser.tdd
[INFO] Exclude: **/src/test/resources/FixedAvroSerializer.config
[INFO] 14 resources included (use -debug for more details)
Warning:  org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
Compiler warnings:
  WARNING:  'org.apache.xerces.jaxp.SAXParserImpl: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.'
Warning:  org.apache.xerces.parsers.SAXParser: Feature 'http://javax.xml.XMLConstants/feature/secure-processing' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
[INFO] Rat check: Summary of files. Unapproved: 0 unknown: 0 generated: 0 approved: 14 licence.
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building storm-kafka-monitor 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ storm-kafka-monitor ---
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ storm-kafka-monitor ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/storm-kafka-monitor/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ storm-kafka-monitor ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ storm-kafka-monitor ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/apache/storm/external/storm-kafka-monitor/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ storm-kafka-monitor ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-surefire-plugin:2.18.1:test (default-test) @ storm-kafka-monitor ---
[INFO] 
[INFO] --- apache-rat-plugin:0.11:check (default) @ storm-kafka-monitor ---
[INFO] 51 implicit excludes (use -debug for more details).
[INFO] Exclude: **/target/**
[INFO] Exclude: **/*.iml
[INFO] Exclude: **/.idea/**
[INFO] Exclude: **/metastore_db/**
[INFO] Exclude: **/build/**
[INFO] Exclude: **/logs/**
[INFO] Exclude: **/CHANGELOG.md
[INFO] Exclude: **/README.md
[INFO] Exclude: **/README.markdown
[INFO] Exclude: **/DEVELOPER.md
[INFO] Exclude: **/BYLAWS.md
[INFO] Exclude: **/STORM-UI-REST-API.md
[INFO] Exclude: SECURITY.md
[INFO] Exclude: VERSION
[INFO] Exclude: TODO
[INFO] Exclude: **/src/py/**
[INFO] Exclude: **/src/ui/public/js/jquery.dataTables.1.10.4.min.js
[INFO] Exclude: **/src/ui/public/css/jquery.dataTables.1.10.4.min.css
[INFO] Exclude: **/src/ui/public/images/*
[INFO] Exclude: **/src/ui/public/js/bootstrap-3.3.1.min.js
[INFO] Exclude: **/src/ui/public/css/bootstrap-3.3.1.min.css
[INFO] Exclude: **/src/ui/public/js/dataTables.bootstrap.min.js
[INFO] Exclude: **/src/ui/public/css/dataTables.bootstrap.css
[INFO] Exclude: **/src/ui/public/js/jsonFormatter.min.js
[INFO] Exclude: **/src/ui/public/css/jsonFormatter.min.css
[INFO] Exclude: **/src/ui/public/js/jquery-1.11.1.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.cookies.2.2.0.min.js
[INFO] Exclude: **/src/ui/public/js/moment.min.js
[INFO] Exclude: **/src/ui/public/js/jquery.blockUI.min.js
[INFO] Exclude: **/src/ui/public/js/url.min.js
[INFO] Exclude: **/src/ui/public/js/arbor.js
[INFO] Exclude: **/src/ui/public/js/arbor-graphics.js
[INFO] Exclude: **/src/ui/public/js/arbor-tween.js
[INFO] Exclude: **/src/ui/public/js/jquery.mustache.js
[INFO] Exclude: **/src/ui/public/js/typeahead.jquery.min.js
[INFO] Exclude: **/dependency-reduced-pom.xml
[INFO] Exclude: **/docs/**
[INFO] Exclude: **/.git/**
[INFO] Exclude: **/derby.log
[INFO] Exclude: **/src/dev/**
[INFO] Exclude: **/src/codegen/config.fmpp
[INFO] Exclude: **/src/codegen/data/Parser.tdd
[INFO] Exclude: **/src/test/resources/FixedAvroSerializer.config
[INFO] 5 resources included (use -debug for more details)
Warning:  org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
Compiler warnings:
  WARNING:  'org.apache.xerces.jaxp.SAXParserImpl: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.'
Warning:  org.apache.xerces.parsers.SAXParser: Feature 'http://javax.xml.XMLConstants/feature/secure-processing' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.
Warning:  org.apache.xerces.parsers.SAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.
[INFO] Rat check: Summary of files. Unapproved: 0 unknown: 0 generated: 0 approved: 5 licence.
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO] 
[INFO] Storm .............................................. SUCCESS [  3.725 s]
[INFO] multilang-javascript ............................... SUCCESS [  0.789 s]
[INFO] multilang-python ................................... SUCCESS [  0.099 s]
[INFO] multilang-ruby ..................................... SUCCESS [  0.071 s]
[INFO] maven-shade-clojure-transformer .................... SUCCESS [  1.015 s]
[INFO] storm-maven-plugins ................................ SUCCESS [  2.625 s]
[INFO] storm-rename-hack .................................. SUCCESS [  1.938 s]
[INFO] storm-kafka ........................................ FAILURE [01:01 min]
[INFO] storm-hdfs ......................................... FAILURE [01:08 min]
[INFO] storm-hbase ........................................ SUCCESS [  3.743 s]
[INFO] storm-hive ......................................... FAILURE [ 31.707 s]
[INFO] storm-jdbc ......................................... SUCCESS [  1.916 s]
[INFO] storm-redis ........................................ SUCCESS [  1.676 s]
[INFO] storm-eventhubs .................................... SUCCESS [  3.226 s]
[INFO] flux ............................................... SUCCESS [  0.381 s]
[INFO] flux-wrappers ...................................... SUCCESS [  0.223 s]
[INFO] flux-core .......................................... SKIPPED
[INFO] flux-examples ...................................... SKIPPED
[INFO] storm-sql-runtime .................................. SUCCESS [  0.394 s]
[INFO] storm-sql-core ..................................... SUCCESS [ 43.126 s]
[INFO] storm-sql-kafka .................................... SKIPPED
[INFO] sql ................................................ SUCCESS [  0.258 s]
[INFO] storm-elasticsearch ................................ SUCCESS [  4.132 s]
[INFO] storm-solr ......................................... SUCCESS [  2.425 s]
[INFO] storm-metrics ...................................... SUCCESS [  0.531 s]
[INFO] storm-cassandra .................................... SUCCESS [  0.350 s]
[INFO] storm-mqtt-parent .................................. SUCCESS [  0.195 s]
[INFO] storm-mqtt ......................................... SUCCESS [  1.219 s]
[INFO] storm-mqtt-examples ................................ SKIPPED
[INFO] storm-mongodb ...................................... SUCCESS [  0.121 s]
[INFO] storm-clojure ...................................... SUCCESS [  1.353 s]
[INFO] storm-starter ...................................... SKIPPED
[INFO] storm-kafka-client ................................. SUCCESS [  0.117 s]
[INFO] storm-opentsdb ..................................... SUCCESS [  1.153 s]
[INFO] storm-kafka-monitor ................................ SUCCESS [  0.225 s]
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 04:00 min
[INFO] Finished at: 2016-06-23T16:27:22+00:00
[INFO] Final Memory: 83M/390M
[INFO] ------------------------------------------------------------------------
[WARNING] The requested profile "native" could not be activated because it does not exist.
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:2.18.1:test (default-test) on project storm-kafka: There are test failures.
[ERROR] 
[ERROR] Please refer to /home/travis/build/apache/storm/external/storm-kafka/target/surefire-reports for the individual test results.
[ERROR] -> [Help 1]
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:2.18.1:test (default-test) on project storm-hdfs: There are test failures.
[ERROR] 
[ERROR] Please refer to /home/travis/build/apache/storm/external/storm-hdfs/target/surefire-reports for the individual test results.
[ERROR] -> [Help 1]
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:2.18.1:test (default-test) on project storm-hive: There are test failures.
[ERROR] 
[ERROR] Please refer to /home/travis/build/apache/storm/external/storm-hive/target/surefire-reports for the individual test results.
[ERROR] -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException
[ERROR] 
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn <goals> -rf :storm-kafka
Looking for errors in ./external/sql/storm-sql-core/target/surefire-reports
Checking ./external/sql/storm-sql-core/target/surefire-reports/TEST-org.apache.storm.sql.parser.TestSqlParser.xml
Checking ./external/sql/storm-sql-core/target/surefire-reports/TEST-org.apache.storm.sql.compiler.backends.trident.TestPlanCompiler.xml
Checking ./external/sql/storm-sql-core/target/surefire-reports/TEST-org.apache.storm.sql.compiler.backends.standalone.TestPlanCompiler.xml
Checking ./external/sql/storm-sql-core/target/surefire-reports/TEST-org.apache.storm.sql.compiler.backends.standalone.TestRelNodeCompiler.xml
Checking ./external/sql/storm-sql-core/target/surefire-reports/TEST-org.apache.storm.sql.compiler.TestExprCompiler.xml
Checking ./external/sql/storm-sql-core/target/surefire-reports/TEST-org.apache.storm.sql.compiler.TestExprSemantic.xml
Checking ./external/sql/storm-sql-core/target/surefire-reports/TEST-org.apache.storm.sql.TestStormSql.xml
Looking for errors in ./external/storm-elasticsearch/target/surefire-reports
Checking ./external/storm-elasticsearch/target/surefire-reports/TEST-org.apache.storm.elasticsearch.common.TransportAddressesTest.xml
Checking ./external/storm-elasticsearch/target/surefire-reports/TEST-org.apache.storm.elasticsearch.common.EsConfigTest.xml
Checking ./external/storm-elasticsearch/target/surefire-reports/TEST-org.apache.storm.elasticsearch.bolt.EsLookupBoltTest.xml
Checking ./external/storm-elasticsearch/target/surefire-reports/TEST-org.apache.storm.elasticsearch.trident.EsStateFactoryTest.xml
Looking for errors in ./external/storm-eventhubs/target/surefire-reports
Checking ./external/storm-eventhubs/target/surefire-reports/TEST-org.apache.storm.eventhubs.trident.TestTransactionalTridentEmitter.xml
Checking ./external/storm-eventhubs/target/surefire-reports/TEST-org.apache.storm.eventhubs.spout.TestPartitionManager.xml
Checking ./external/storm-eventhubs/target/surefire-reports/TEST-org.apache.storm.eventhubs.spout.TestEventData.xml
Checking ./external/storm-eventhubs/target/surefire-reports/TEST-org.apache.storm.eventhubs.spout.TestEventHubSpout.xml
Looking for errors in ./external/storm-hdfs/target/surefire-reports
Checking ./external/storm-hdfs/target/surefire-reports/TEST-org.apache.storm.hdfs.bolt.TestHdfsBolt.xml
--------------------------------------------------
classname: org.apache.storm.hdfs.bolt.TestHdfsBolt / testname: testTwoTuplesTwoFiles
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hdfs.bolt.TestHdfsBolt.generateTestTuple(TestHdfsBolt.java:243)
	at org.apache.storm.hdfs.bolt.TestHdfsBolt.<init>(TestHdfsBolt.java:69)

--------------------------------------------------
--------------------------------------------------
classname: org.apache.storm.hdfs.bolt.TestHdfsBolt / testname: testPartitionedOutput
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hdfs.bolt.TestHdfsBolt.generateTestTuple(TestHdfsBolt.java:243)
	at org.apache.storm.hdfs.bolt.TestHdfsBolt.<init>(TestHdfsBolt.java:69)

--------------------------------------------------
--------------------------------------------------
classname: org.apache.storm.hdfs.bolt.TestHdfsBolt / testname: testTwoTuplesOneFile
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hdfs.bolt.TestHdfsBolt.generateTestTuple(TestHdfsBolt.java:243)
	at org.apache.storm.hdfs.bolt.TestHdfsBolt.<init>(TestHdfsBolt.java:69)

--------------------------------------------------
--------------------------------------------------
classname: org.apache.storm.hdfs.bolt.TestHdfsBolt / testname: testFailedSync
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hdfs.bolt.TestHdfsBolt.generateTestTuple(TestHdfsBolt.java:243)
	at org.apache.storm.hdfs.bolt.TestHdfsBolt.<init>(TestHdfsBolt.java:69)

--------------------------------------------------
--------------------------------------------------
classname: org.apache.storm.hdfs.bolt.TestHdfsBolt / testname: testTickTuples
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hdfs.bolt.TestHdfsBolt.generateTestTuple(TestHdfsBolt.java:243)
	at org.apache.storm.hdfs.bolt.TestHdfsBolt.<init>(TestHdfsBolt.java:69)

--------------------------------------------------
--------------------------------------------------
classname: org.apache.storm.hdfs.bolt.TestHdfsBolt / testname: testFailureFilecount
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hdfs.bolt.TestHdfsBolt.generateTestTuple(TestHdfsBolt.java:243)
	at org.apache.storm.hdfs.bolt.TestHdfsBolt.<init>(TestHdfsBolt.java:69)

--------------------------------------------------
Checking ./external/storm-hdfs/target/surefire-reports/TEST-org.apache.storm.hdfs.bolt.TestSequenceFileBolt.xml
--------------------------------------------------
classname: org.apache.storm.hdfs.bolt.TestSequenceFileBolt / testname: testTwoTuplesTwoFiles
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hdfs.bolt.TestSequenceFileBolt.generateTestTuple(TestSequenceFileBolt.java:161)
	at org.apache.storm.hdfs.bolt.TestSequenceFileBolt.<init>(TestSequenceFileBolt.java:68)

--------------------------------------------------
--------------------------------------------------
classname: org.apache.storm.hdfs.bolt.TestSequenceFileBolt / testname: testTwoTuplesOneFile
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hdfs.bolt.TestSequenceFileBolt.generateTestTuple(TestSequenceFileBolt.java:161)
	at org.apache.storm.hdfs.bolt.TestSequenceFileBolt.<init>(TestSequenceFileBolt.java:68)

--------------------------------------------------
--------------------------------------------------
classname: org.apache.storm.hdfs.bolt.TestSequenceFileBolt / testname: testFailedSync
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hdfs.bolt.TestSequenceFileBolt.generateTestTuple(TestSequenceFileBolt.java:161)
	at org.apache.storm.hdfs.bolt.TestSequenceFileBolt.<init>(TestSequenceFileBolt.java:68)

--------------------------------------------------
Checking ./external/storm-hdfs/target/surefire-reports/TEST-org.apache.storm.hdfs.bolt.AvroGenericRecordBoltTest.xml
--------------------------------------------------
classname: org.apache.storm.hdfs.bolt.AvroGenericRecordBoltTest / testname: schemaThrashing
java.lang.ExceptionInInitializerError: null
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hdfs.bolt.AvroGenericRecordBoltTest.generateTestTuple(AvroGenericRecordBoltTest.java:220)
	at org.apache.storm.hdfs.bolt.AvroGenericRecordBoltTest.<clinit>(AvroGenericRecordBoltTest.java:95)

--------------------------------------------------
--------------------------------------------------
classname: org.apache.storm.hdfs.bolt.AvroGenericRecordBoltTest / testname: forwardSchemaChangeWorks
java.lang.NoClassDefFoundError: Could not initialize class org.apache.storm.hdfs.bolt.AvroGenericRecordBoltTest
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.junit.runners.BlockJUnit4ClassRunner.createTest(BlockJUnit4ClassRunner.java:195)
	at org.junit.runners.BlockJUnit4ClassRunner$1.runReflectiveCall(BlockJUnit4ClassRunner.java:244)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.BlockJUnit4ClassRunner.methodBlock(BlockJUnit4ClassRunner.java:241)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:108)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:78)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:54)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:144)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:203)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:155)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:103)

--------------------------------------------------
--------------------------------------------------
classname: org.apache.storm.hdfs.bolt.AvroGenericRecordBoltTest / testname: multipleTuplesOneFile
java.lang.NoClassDefFoundError: Could not initialize class org.apache.storm.hdfs.bolt.AvroGenericRecordBoltTest
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.junit.runners.BlockJUnit4ClassRunner.createTest(BlockJUnit4ClassRunner.java:195)
	at org.junit.runners.BlockJUnit4ClassRunner$1.runReflectiveCall(BlockJUnit4ClassRunner.java:244)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.BlockJUnit4ClassRunner.methodBlock(BlockJUnit4ClassRunner.java:241)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:108)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:78)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:54)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:144)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:203)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:155)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:103)

--------------------------------------------------
--------------------------------------------------
classname: org.apache.storm.hdfs.bolt.AvroGenericRecordBoltTest / testname: multipleTuplesMutliplesFiles
java.lang.NoClassDefFoundError: Could not initialize class org.apache.storm.hdfs.bolt.AvroGenericRecordBoltTest
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.junit.runners.BlockJUnit4ClassRunner.createTest(BlockJUnit4ClassRunner.java:195)
	at org.junit.runners.BlockJUnit4ClassRunner$1.runReflectiveCall(BlockJUnit4ClassRunner.java:244)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.BlockJUnit4ClassRunner.methodBlock(BlockJUnit4ClassRunner.java:241)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:108)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:78)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:54)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:144)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:203)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:155)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:103)

--------------------------------------------------
--------------------------------------------------
classname: org.apache.storm.hdfs.bolt.AvroGenericRecordBoltTest / testname: backwardSchemaChangeWorks
java.lang.NoClassDefFoundError: Could not initialize class org.apache.storm.hdfs.bolt.AvroGenericRecordBoltTest
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.junit.runners.BlockJUnit4ClassRunner.createTest(BlockJUnit4ClassRunner.java:195)
	at org.junit.runners.BlockJUnit4ClassRunner$1.runReflectiveCall(BlockJUnit4ClassRunner.java:244)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.BlockJUnit4ClassRunner.methodBlock(BlockJUnit4ClassRunner.java:241)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:108)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:78)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:54)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:144)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:203)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:155)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:103)

--------------------------------------------------
Checking ./external/storm-hdfs/target/surefire-reports/TEST-org.apache.storm.hdfs.bolt.TestWritersMap.xml
Checking ./external/storm-hdfs/target/surefire-reports/TEST-org.apache.storm.hdfs.spout.TestDirLock.xml
Checking ./external/storm-hdfs/target/surefire-reports/TEST-org.apache.storm.hdfs.spout.TestHdfsSemantics.xml
Checking ./external/storm-hdfs/target/surefire-reports/TEST-org.apache.storm.hdfs.spout.TestHdfsSpout.xml
Checking ./external/storm-hdfs/target/surefire-reports/TEST-org.apache.storm.hdfs.spout.TestProgressTracker.xml
Checking ./external/storm-hdfs/target/surefire-reports/TEST-org.apache.storm.hdfs.spout.TestFileLock.xml
Checking ./external/storm-hdfs/target/surefire-reports/TEST-org.apache.storm.hdfs.trident.HdfsStateTest.xml
Checking ./external/storm-hdfs/target/surefire-reports/TEST-org.apache.storm.hdfs.blobstore.BlobStoreTest.xml
Checking ./external/storm-hdfs/target/surefire-reports/TEST-org.apache.storm.hdfs.blobstore.HdfsBlobStoreImplTest.xml
Checking ./external/storm-hdfs/target/surefire-reports/TEST-org.apache.storm.hdfs.avro.TestFixedAvroSerializer.xml
Checking ./external/storm-hdfs/target/surefire-reports/TEST-org.apache.storm.hdfs.avro.TestGenericAvroSerializer.xml
Looking for errors in ./external/storm-hive/target/surefire-reports
Checking ./external/storm-hive/target/surefire-reports/TEST-org.apache.storm.hive.bolt.TestHiveBolt.xml
--------------------------------------------------
classname: org.apache.storm.hive.bolt.TestHiveBolt / testname: testMultiPartitionTuples
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hive.bolt.TestHiveBolt.generateTestTuple(TestHiveBolt.java:447)
	at org.apache.storm.hive.bolt.TestHiveBolt.testMultiPartitionTuples(TestHiveBolt.java:411)

-------------------- system-out --------------------
3928 [main] WARN  o.a.h.u.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
4032 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
4055 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
4267 [main] INFO  D.Persistence - Property datanucleus.cache.level2 unknown - will be ignored
4268 [main] INFO  D.Persistence - Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
5085 [main] INFO  o.a.h.h.m.ObjectStore - Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
5122 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
5908 [main] INFO  D.Datastore - The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
5909 [main] INFO  D.Datastore - The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
7169 [main] INFO  D.Datastore - The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
7169 [main] INFO  D.Datastore - The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
7483 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
7609 [main] WARN  o.a.h.h.m.ObjectStore - Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 0.14.0
7737 [main] WARN  o.a.h.h.m.ObjectStore - Failed to get database default, returning NoSuchObjectException
8123 [main] INFO  o.a.h.h.m.HiveMetaStore - Added admin role in metastore
8125 [main] INFO  o.a.h.h.m.HiveMetaStore - Added public role in metastore
8231 [main] INFO  o.a.h.h.m.HiveMetaStore - No user is added in admin role, since config is empty
8372 [main] INFO  o.a.h.h.q.s.SessionState - Created HDFS directory: /tmp/hive/travis
8374 [main] INFO  o.a.h.h.q.s.SessionState - Created local directory: /tmp/travis
8377 [main] INFO  o.a.h.h.q.s.SessionState - Created local directory: /tmp/362e6466-df73-4b6a-98e4-7d34ba413204_resources
8379 [main] INFO  o.a.h.h.q.s.SessionState - Created HDFS directory: /tmp/hive/travis/362e6466-df73-4b6a-98e4-7d34ba413204
8382 [main] INFO  o.a.h.h.q.s.SessionState - Created local directory: /tmp/travis/362e6466-df73-4b6a-98e4-7d34ba413204
8392 [main] INFO  o.a.h.h.q.s.SessionState - Created HDFS directory: /tmp/hive/travis/362e6466-df73-4b6a-98e4-7d34ba413204/_tmp_space.db
8393 [main] INFO  o.a.h.h.q.s.SessionState - No Tez session required at this point. hive.execution.engine=mr.
8518 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
8519 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
8530 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
8530 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
8531 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_table_names_by_filter: db = testdb, filter = 
8531 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table_names_by_filter: db = testdb, filter = 	
8554 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_database: testdb
8555 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_database: testdb	
8556 [main] WARN  o.a.h.h.m.ObjectStore - Failed to get database testdb, returning NoSuchObjectException
8558 [main] ERROR o.a.h.h.m.RetryingHMSHandler - NoSuchObjectException(message:testdb)
	at org.apache.hadoop.hive.metastore.ObjectStore.getDatabase(ObjectStore.java:539)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:98)
	at com.sun.proxy.$Proxy23.getDatabase(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_database_core(HiveMetaStore.java:914)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_database(HiveMetaStore.java:888)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:102)
	at com.sun.proxy.$Proxy25.get_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getDatabase(HiveMetaStoreClient.java:1043)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:657)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:644)
	at org.apache.storm.hive.bolt.HiveSetupUtil.dropDB(HiveSetupUtil.java:166)
	at org.apache.storm.hive.bolt.TestHiveBolt.setup(TestHiveBolt.java:119)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:108)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:78)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:54)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:144)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:203)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:155)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:103)

8558 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Shutting down the object store...
8558 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=Shutting down the object store...	
8558 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Metastore shutdown complete.
8559 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=Metastore shutdown complete.	
8559 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: create_database: Database(name:testdb, description:null, locationUri:raw:///tmp/junit1530735515135437124/testdb.db, parameters:null)
8560 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=create_database: Database(name:testdb, description:null, locationUri:raw:///tmp/junit1530735515135437124/testdb.db, parameters:null)	
8560 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
8561 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
8562 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
8564 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
8565 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
8567 [main] WARN  o.a.h.h.m.ObjectStore - Failed to get database testdb, returning NoSuchObjectException
8584 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: create_table: Table(tableName:test_table, dbName:testdb, owner:null, createTime:0, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:), FieldSchema(name:msg, type:string, comment:)], location:raw:///tmp/junit1530735515135437124/testdb.db/test_table, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:1, serdeInfo:SerDeInfo(name:test_table, serializationLib:org.apache.hadoop.hive.ql.io.orc.OrcSerde, parameters:{serialization.format=1}), bucketCols:[], sortCols:null, parameters:null), partitionKeys:[FieldSchema(name:city, type:string, comment:), FieldSchema(name:state, type:string, comment:)], parameters:{}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)
8585 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=create_table: Table(tableName:test_table, dbName:testdb, owner:null, createTime:0, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:), FieldSchema(name:msg, type:string, comment:)], location:raw:///tmp/junit1530735515135437124/testdb.db/test_table, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:1, serdeInfo:SerDeInfo(name:test_table, serializationLib:org.apache.hadoop.hive.ql.io.orc.OrcSerde, parameters:{serialization.format=1}), bucketCols:[], sortCols:null, parameters:null), partitionKeys:[FieldSchema(name:city, type:string, comment:), FieldSchema(name:state, type:string, comment:)], parameters:{}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
8618 [main] WARN  o.a.h.h.m.HiveMetaStore - Location: raw:///tmp/junit1530735515135437124/testdb.db/test_table specified for non-external table:test_table
8621 [main] INFO  o.a.h.h.c.FileUtils - Creating directory if it doesn't exist: raw:/tmp/junit1530735515135437124/testdb.db/test_table
8815 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: add_partition : db=testdb tbl=test_table
8815 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=add_partition : db=testdb tbl=test_table	
9008 [main] INFO  o.a.h.h.c.FileUtils - Creating directory if it doesn't exist: raw:/tmp/junit1530735515135437124/testdb.db/test_table/city=sunnyvale/state=ca
9060 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Shutting down the object store...
9061 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=Shutting down the object store...	
9061 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Metastore shutdown complete.
9061 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=Metastore shutdown complete.	
done
9069 [main] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=compile from=org.apache.hadoop.hive.ql.Driver>
9103 [main] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=parse from=org.apache.hadoop.hive.ql.Driver>
9107 [main] INFO  h.q.p.ParseDriver - Parsing command: select * from testdb.test_table
9384 [main] INFO  h.q.p.ParseDriver - Parse Completed
9385 [main] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=parse start=1466699161674 end=1466699161956 duration=282 from=org.apache.hadoop.hive.ql.Driver>
9428 [main] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=semanticAnalyze from=org.apache.hadoop.hive.ql.Driver>
9493 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Starting Semantic Analysis
9494 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Completed phase 1 of Semantic Analysis
9494 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Get metadata for source tables
9495 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_table : db=testdb tbl=test_table
9495 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table : db=testdb tbl=test_table	
9495 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
9497 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
9498 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
9501 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
9502 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
9537 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Get metadata for subqueries
9547 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Get metadata for destination tables
9555 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Completed getting MetaData in Semantic Analysis
9714 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Set stats collection dir : file:/tmp/travis/362e6466-df73-4b6a-98e4-7d34ba413204/hive_2016-06-23_16-26-01_672_9040402906493578368-1/-ext-10002
9809 [main] INFO  o.a.h.h.q.p.OpProcFactory - Processing for FS(2)
9809 [main] INFO  o.a.h.h.q.p.OpProcFactory - Processing for SEL(1)
9810 [main] INFO  o.a.h.h.q.p.OpProcFactory - Processing for TS(0)
9844 [main] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=partition-retrieving from=org.apache.hadoop.hive.ql.optimizer.ppr.PartitionPruner>
9844 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_partitions : db=testdb tbl=test_table
9844 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_partitions : db=testdb tbl=test_table	
9953 [main] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=partition-retrieving start=1466699162415 end=1466699162524 duration=109 from=org.apache.hadoop.hive.ql.optimizer.ppr.PartitionPruner>
9995 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Completed plan generation
9995 [main] INFO  o.a.h.h.q.Driver - Semantic Analysis Completed
9995 [main] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=semanticAnalyze start=1466699161999 end=1466699162566 duration=567 from=org.apache.hadoop.hive.ql.Driver>
10018 [main] INFO  o.a.h.h.q.e.TableScanOperator - Initializing Self TS[0]
10018 [main] INFO  o.a.h.h.q.e.TableScanOperator - Operator 0 TS initialized
10018 [main] INFO  o.a.h.h.q.e.TableScanOperator - Initializing children of 0 TS
10019 [main] INFO  o.a.h.h.q.e.SelectOperator - Initializing child 1 SEL
10019 [main] INFO  o.a.h.h.q.e.SelectOperator - Initializing Self SEL[1]
10022 [main] INFO  o.a.h.h.q.e.SelectOperator - SELECT struct<id:int,msg:string,city:string,state:string>
10022 [main] INFO  o.a.h.h.q.e.SelectOperator - Operator 1 SEL initialized
10022 [main] INFO  o.a.h.h.q.e.SelectOperator - Initializing children of 1 SEL
10022 [main] INFO  o.a.h.h.q.e.ListSinkOperator - Initializing child 3 OP
10022 [main] INFO  o.a.h.h.q.e.ListSinkOperator - Initializing Self OP[3]
10024 [main] INFO  o.a.h.h.q.e.ListSinkOperator - Operator 3 OP initialized
10024 [main] INFO  o.a.h.h.q.e.ListSinkOperator - Initialization Done 3 OP
10024 [main] INFO  o.a.h.h.q.e.SelectOperator - Initialization Done 1 SEL
10024 [main] INFO  o.a.h.h.q.e.TableScanOperator - Initialization Done 0 TS
10027 [main] INFO  o.a.h.h.q.Driver - Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:test_table.id, type:int, comment:null), FieldSchema(name:test_table.msg, type:string, comment:null), FieldSchema(name:test_table.city, type:string, comment:null), FieldSchema(name:test_table.state, type:string, comment:null)], properties:null)
10027 [main] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=compile start=1466699161640 end=1466699162598 duration=958 from=org.apache.hadoop.hive.ql.Driver>

-------------------- system-err --------------------
Unable to drop index HL_TXNID_INDEX Index 'HL_TXNID_INDEX' does not exist.
Unable to drop table TXN_COMPONENTS: 'DROP TABLE' cannot be performed on 'TXN_COMPONENTS' because it does not exist.
Unable to drop table COMPLETED_TXN_COMPONENTS: 'DROP TABLE' cannot be performed on 'COMPLETED_TXN_COMPONENTS' because it does not exist.
Unable to drop table TXNS: 'DROP TABLE' cannot be performed on 'TXNS' because it does not exist.
Unable to drop table NEXT_TXN_ID: 'DROP TABLE' cannot be performed on 'NEXT_TXN_ID' because it does not exist.
Unable to drop table HIVE_LOCKS: 'DROP TABLE' cannot be performed on 'HIVE_LOCKS' because it does not exist.
Unable to drop table NEXT_LOCK_ID: 'DROP TABLE' cannot be performed on 'NEXT_LOCK_ID' because it does not exist.
Unable to drop table COMPACTION_QUEUE: 'DROP TABLE' cannot be performed on 'COMPACTION_QUEUE' because it does not exist.
Unable to drop table NEXT_COMPACTION_QUEUE_ID: 'DROP TABLE' cannot be performed on 'NEXT_COMPACTION_QUEUE_ID' because it does not exist.

--------------------------------------------------
--------------------------------------------------
classname: org.apache.storm.hive.bolt.TestHiveBolt / testname: testNoAcksUntilFlushed
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hive.bolt.TestHiveBolt.generateTestTuple(TestHiveBolt.java:447)
	at org.apache.storm.hive.bolt.TestHiveBolt.testNoAcksUntilFlushed(TestHiveBolt.java:297)

-------------------- system-out --------------------
10857 [main] INFO  o.a.h.h.q.s.SessionState - Created local directory: /tmp/958cb37b-8432-4caa-b167-e3bafadd4a6a_resources
10859 [main] INFO  o.a.h.h.q.s.SessionState - Created HDFS directory: /tmp/hive/travis/958cb37b-8432-4caa-b167-e3bafadd4a6a
10862 [main] INFO  o.a.h.h.q.s.SessionState - Created local directory: /tmp/travis/958cb37b-8432-4caa-b167-e3bafadd4a6a
10865 [main] INFO  o.a.h.h.q.s.SessionState - Created HDFS directory: /tmp/hive/travis/958cb37b-8432-4caa-b167-e3bafadd4a6a/_tmp_space.db
10865 [main] INFO  o.a.h.h.q.s.SessionState - No Tez session required at this point. hive.execution.engine=mr.
10868 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
10869 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
10871 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
10872 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
10872 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_table_names_by_filter: db = testdb, filter = 
10872 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table_names_by_filter: db = testdb, filter = 	
10876 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_table : db=testdb tbl=test_table
10876 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table : db=testdb tbl=test_table	
10894 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: drop_table : db=testdb tbl=test_table
10894 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=drop_table : db=testdb tbl=test_table	
10908 [main] INFO  D.Datastore - The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
10908 [main] INFO  D.Datastore - The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
11240 [main] INFO  D.Datastore - The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
11240 [main] INFO  D.Datastore - The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
11369 [main] INFO  D.Datastore - The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
11369 [main] INFO  D.Datastore - The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
11848 [main] INFO  D.Datastore - The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
11848 [main] INFO  D.Datastore - The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
11943 [main] INFO  D.Datastore - The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
11943 [main] INFO  D.Datastore - The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
12213 [main] INFO  h.m.hivemetastoressimpl - deleting  raw:/tmp/junit1530735515135437124/testdb.db/test_table
12214 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_database: testdb
12214 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_database: testdb	
12226 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: drop_database: testdb
12226 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=drop_database: testdb	
12228 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_all_tables: db=testdb
12228 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_all_tables: db=testdb	
12252 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_functions: db=testdb pat=*
12253 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_functions: db=testdb pat=*	
12267 [main] INFO  D.Datastore - The class "org.apache.hadoop.hive.metastore.model.MResourceUri" is tagged as "embedded-only" so does not have its own datastore table.
12385 [main] INFO  o.a.h.h.m.ObjectStore - Dropping database testdb along with all tables
12482 [main] INFO  h.m.hivemetastoressimpl - deleting  raw:/tmp/junit1530735515135437124/testdb.db
12485 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
12486 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
12495 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
12495 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
12496 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: create_database: Database(name:testdb, description:null, locationUri:raw:///tmp/junit4168830248763489466/testdb.db, parameters:null)
12496 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=create_database: Database(name:testdb, description:null, locationUri:raw:///tmp/junit4168830248763489466/testdb.db, parameters:null)	
12501 [main] WARN  o.a.h.h.m.ObjectStore - Failed to get database testdb, returning NoSuchObjectException
12507 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: create_table: Table(tableName:test_table, dbName:testdb, owner:null, createTime:0, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:), FieldSchema(name:msg, type:string, comment:)], location:raw:///tmp/junit4168830248763489466/testdb.db/test_table, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:1, serdeInfo:SerDeInfo(name:test_table, serializationLib:org.apache.hadoop.hive.ql.io.orc.OrcSerde, parameters:{serialization.format=1}), bucketCols:[], sortCols:null, parameters:null), partitionKeys:[FieldSchema(name:city, type:string, comment:), FieldSchema(name:state, type:string, comment:)], parameters:{}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)
12508 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=create_table: Table(tableName:test_table, dbName:testdb, owner:null, createTime:0, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:), FieldSchema(name:msg, type:string, comment:)], location:raw:///tmp/junit4168830248763489466/testdb.db/test_table, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:1, serdeInfo:SerDeInfo(name:test_table, serializationLib:org.apache.hadoop.hive.ql.io.orc.OrcSerde, parameters:{serialization.format=1}), bucketCols:[], sortCols:null, parameters:null), partitionKeys:[FieldSchema(name:city, type:string, comment:), FieldSchema(name:state, type:string, comment:)], parameters:{}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
12520 [main] WARN  o.a.h.h.m.HiveMetaStore - Location: raw:///tmp/junit4168830248763489466/testdb.db/test_table specified for non-external table:test_table
12521 [main] INFO  o.a.h.h.c.FileUtils - Creating directory if it doesn't exist: raw:/tmp/junit4168830248763489466/testdb.db/test_table
12617 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: add_partition : db=testdb tbl=test_table
12618 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=add_partition : db=testdb tbl=test_table	
12736 [main] INFO  o.a.h.h.c.FileUtils - Creating directory if it doesn't exist: raw:/tmp/junit4168830248763489466/testdb.db/test_table/city=sunnyvale/state=ca
12772 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Shutting down the object store...
12773 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=Shutting down the object store...	
12773 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Metastore shutdown complete.
12773 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=Metastore shutdown complete.	
done

--------------------------------------------------
--------------------------------------------------
classname: org.apache.storm.hive.bolt.TestHiveBolt / testname: testData
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hive.bolt.TestHiveBolt.generateTestTuple(TestHiveBolt.java:447)
	at org.apache.storm.hive.bolt.TestHiveBolt.testData(TestHiveBolt.java:251)

-------------------- system-out --------------------
13235 [main] INFO  o.a.h.h.q.s.SessionState - Created local directory: /tmp/824d2521-ba14-408b-a11f-cdeb67c18c01_resources
13241 [main] INFO  o.a.h.h.q.s.SessionState - Created HDFS directory: /tmp/hive/travis/824d2521-ba14-408b-a11f-cdeb67c18c01
13244 [main] INFO  o.a.h.h.q.s.SessionState - Created local directory: /tmp/travis/824d2521-ba14-408b-a11f-cdeb67c18c01
13247 [main] INFO  o.a.h.h.q.s.SessionState - Created HDFS directory: /tmp/hive/travis/824d2521-ba14-408b-a11f-cdeb67c18c01/_tmp_space.db
13248 [main] INFO  o.a.h.h.q.s.SessionState - No Tez session required at this point. hive.execution.engine=mr.
13250 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_table_names_by_filter: db = testdb, filter = 
13250 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table_names_by_filter: db = testdb, filter = 	
13250 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
13252 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
13253 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
13303 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
13304 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
13316 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_table : db=testdb tbl=test_table
13316 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table : db=testdb tbl=test_table	
13336 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: drop_table : db=testdb tbl=test_table
13336 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=drop_table : db=testdb tbl=test_table	
13926 [main] INFO  h.m.hivemetastoressimpl - deleting  raw:/tmp/junit4168830248763489466/testdb.db/test_table
13926 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_database: testdb
13926 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_database: testdb	
13937 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: drop_database: testdb
13938 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=drop_database: testdb	
13939 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_all_tables: db=testdb
13939 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_all_tables: db=testdb	
13947 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_functions: db=testdb pat=*
13947 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_functions: db=testdb pat=*	
13957 [main] INFO  o.a.h.h.m.ObjectStore - Dropping database testdb along with all tables
13981 [main] INFO  h.m.hivemetastoressimpl - deleting  raw:/tmp/junit4168830248763489466/testdb.db
13983 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
13984 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
13986 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
13987 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
13988 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: create_database: Database(name:testdb, description:null, locationUri:raw:///tmp/junit2746520610378279151/testdb.db, parameters:null)
13988 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=create_database: Database(name:testdb, description:null, locationUri:raw:///tmp/junit2746520610378279151/testdb.db, parameters:null)	
13989 [main] WARN  o.a.h.h.m.ObjectStore - Failed to get database testdb, returning NoSuchObjectException
13995 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: create_table: Table(tableName:test_table, dbName:testdb, owner:null, createTime:0, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:), FieldSchema(name:msg, type:string, comment:)], location:raw:///tmp/junit2746520610378279151/testdb.db/test_table, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:1, serdeInfo:SerDeInfo(name:test_table, serializationLib:org.apache.hadoop.hive.ql.io.orc.OrcSerde, parameters:{serialization.format=1}), bucketCols:[], sortCols:null, parameters:null), partitionKeys:[FieldSchema(name:city, type:string, comment:), FieldSchema(name:state, type:string, comment:)], parameters:{}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)
13995 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=create_table: Table(tableName:test_table, dbName:testdb, owner:null, createTime:0, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:), FieldSchema(name:msg, type:string, comment:)], location:raw:///tmp/junit2746520610378279151/testdb.db/test_table, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:1, serdeInfo:SerDeInfo(name:test_table, serializationLib:org.apache.hadoop.hive.ql.io.orc.OrcSerde, parameters:{serialization.format=1}), bucketCols:[], sortCols:null, parameters:null), partitionKeys:[FieldSchema(name:city, type:string, comment:), FieldSchema(name:state, type:string, comment:)], parameters:{}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
13998 [main] WARN  o.a.h.h.m.HiveMetaStore - Location: raw:///tmp/junit2746520610378279151/testdb.db/test_table specified for non-external table:test_table
13998 [main] INFO  o.a.h.h.c.FileUtils - Creating directory if it doesn't exist: raw:/tmp/junit2746520610378279151/testdb.db/test_table
14073 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: add_partition : db=testdb tbl=test_table
14073 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=add_partition : db=testdb tbl=test_table	
14155 [main] INFO  o.a.h.h.c.FileUtils - Creating directory if it doesn't exist: raw:/tmp/junit2746520610378279151/testdb.db/test_table/city=sunnyvale/state=ca
14591 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Shutting down the object store...
14591 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=Shutting down the object store...	
14592 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Metastore shutdown complete.
14592 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=Metastore shutdown complete.	
done

--------------------------------------------------
--------------------------------------------------
classname: org.apache.storm.hive.bolt.TestHiveBolt / testname: testWithoutPartitions
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hive.bolt.TestHiveBolt.generateTestTuple(TestHiveBolt.java:447)
	at org.apache.storm.hive.bolt.TestHiveBolt.testWithoutPartitions(TestHiveBolt.java:194)

-------------------- system-out --------------------
14916 [main] INFO  o.a.h.h.q.s.SessionState - Created local directory: /tmp/225fac7a-5c4a-48d6-84a5-273f14c05d65_resources
14919 [main] INFO  o.a.h.h.q.s.SessionState - Created HDFS directory: /tmp/hive/travis/225fac7a-5c4a-48d6-84a5-273f14c05d65
14922 [main] INFO  o.a.h.h.q.s.SessionState - Created local directory: /tmp/travis/225fac7a-5c4a-48d6-84a5-273f14c05d65
14932 [main] INFO  o.a.h.h.q.s.SessionState - Created HDFS directory: /tmp/hive/travis/225fac7a-5c4a-48d6-84a5-273f14c05d65/_tmp_space.db
14933 [main] INFO  o.a.h.h.q.s.SessionState - No Tez session required at this point. hive.execution.engine=mr.
14936 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_table_names_by_filter: db = testdb, filter = 
14936 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table_names_by_filter: db = testdb, filter = 	
14936 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
14937 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
14939 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
14942 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
14942 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
14956 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_table : db=testdb tbl=test_table
14957 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table : db=testdb tbl=test_table	
14978 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: drop_table : db=testdb tbl=test_table
14978 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=drop_table : db=testdb tbl=test_table	
15610 [main] INFO  h.m.hivemetastoressimpl - deleting  raw:/tmp/junit2746520610378279151/testdb.db/test_table
15610 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_database: testdb
15611 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_database: testdb	
15619 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: drop_database: testdb
15619 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=drop_database: testdb	
15620 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_all_tables: db=testdb
15620 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_all_tables: db=testdb	
15625 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_functions: db=testdb pat=*
15639 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_functions: db=testdb pat=*	
15646 [main] INFO  o.a.h.h.m.ObjectStore - Dropping database testdb along with all tables
15696 [main] INFO  h.m.hivemetastoressimpl - deleting  raw:/tmp/junit2746520610378279151/testdb.db
15697 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
15698 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
15700 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
15700 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
15701 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: create_database: Database(name:testdb, description:null, locationUri:raw:///tmp/junit6064902820246943844/testdb.db, parameters:null)
15701 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=create_database: Database(name:testdb, description:null, locationUri:raw:///tmp/junit6064902820246943844/testdb.db, parameters:null)	
15702 [main] WARN  o.a.h.h.m.ObjectStore - Failed to get database testdb, returning NoSuchObjectException
15719 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: create_table: Table(tableName:test_table, dbName:testdb, owner:null, createTime:0, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:), FieldSchema(name:msg, type:string, comment:)], location:raw:///tmp/junit6064902820246943844/testdb.db/test_table, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:1, serdeInfo:SerDeInfo(name:test_table, serializationLib:org.apache.hadoop.hive.ql.io.orc.OrcSerde, parameters:{serialization.format=1}), bucketCols:[], sortCols:null, parameters:null), partitionKeys:[FieldSchema(name:city, type:string, comment:), FieldSchema(name:state, type:string, comment:)], parameters:{}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)
15720 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=create_table: Table(tableName:test_table, dbName:testdb, owner:null, createTime:0, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:), FieldSchema(name:msg, type:string, comment:)], location:raw:///tmp/junit6064902820246943844/testdb.db/test_table, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:1, serdeInfo:SerDeInfo(name:test_table, serializationLib:org.apache.hadoop.hive.ql.io.orc.OrcSerde, parameters:{serialization.format=1}), bucketCols:[], sortCols:null, parameters:null), partitionKeys:[FieldSchema(name:city, type:string, comment:), FieldSchema(name:state, type:string, comment:)], parameters:{}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
15722 [main] WARN  o.a.h.h.m.HiveMetaStore - Location: raw:///tmp/junit6064902820246943844/testdb.db/test_table specified for non-external table:test_table
15723 [main] INFO  o.a.h.h.c.FileUtils - Creating directory if it doesn't exist: raw:/tmp/junit6064902820246943844/testdb.db/test_table
15826 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: add_partition : db=testdb tbl=test_table
15827 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=add_partition : db=testdb tbl=test_table	
15876 [main] INFO  o.a.h.h.c.FileUtils - Creating directory if it doesn't exist: raw:/tmp/junit6064902820246943844/testdb.db/test_table/city=sunnyvale/state=ca
15896 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Shutting down the object store...
15897 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=Shutting down the object store...	
15897 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Metastore shutdown complete.
15897 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=Metastore shutdown complete.	
done
15897 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_table_names_by_filter: db = testdb1, filter = 
15898 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table_names_by_filter: db = testdb1, filter = 	
15898 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
15899 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
15900 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
15901 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
15902 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
15904 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_database: testdb1
15904 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_database: testdb1	
15905 [main] WARN  o.a.h.h.m.ObjectStore - Failed to get database testdb1, returning NoSuchObjectException
15906 [main] ERROR o.a.h.h.m.RetryingHMSHandler - NoSuchObjectException(message:testdb1)
	at org.apache.hadoop.hive.metastore.ObjectStore.getDatabase(ObjectStore.java:539)
	at sun.reflect.GeneratedMethodAccessor10.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:98)
	at com.sun.proxy.$Proxy23.getDatabase(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_database_core(HiveMetaStore.java:914)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_database(HiveMetaStore.java:888)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:102)
	at com.sun.proxy.$Proxy25.get_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getDatabase(HiveMetaStoreClient.java:1043)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:657)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:644)
	at org.apache.storm.hive.bolt.HiveSetupUtil.dropDB(HiveSetupUtil.java:166)
	at org.apache.storm.hive.bolt.TestHiveBolt.testWithoutPartitions(TestHiveBolt.java:175)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:108)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:78)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:54)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:144)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:203)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:155)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:103)

15906 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Shutting down the object store...
15906 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=Shutting down the object store...	
15906 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Metastore shutdown complete.
15907 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=Metastore shutdown complete.	
15907 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: create_database: Database(name:testdb1, description:null, locationUri:raw:///tmp/junit6064902820246943844/testdb.db, parameters:null)
15907 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=create_database: Database(name:testdb1, description:null, locationUri:raw:///tmp/junit6064902820246943844/testdb.db, parameters:null)	
15907 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
15908 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
15909 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
15911 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
15911 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
15913 [main] WARN  o.a.h.h.m.ObjectStore - Failed to get database testdb1, returning NoSuchObjectException
15923 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: create_table: Table(tableName:test_table1, dbName:testdb1, owner:null, createTime:0, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:), FieldSchema(name:msg, type:string, comment:)], location:raw:///tmp/junit6064902820246943844/testdb.db/test_table1, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:1, serdeInfo:SerDeInfo(name:test_table1, serializationLib:org.apache.hadoop.hive.ql.io.orc.OrcSerde, parameters:{serialization.format=1}), bucketCols:[], sortCols:null, parameters:null), partitionKeys:null, parameters:{}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)
15924 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=create_table: Table(tableName:test_table1, dbName:testdb1, owner:null, createTime:0, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:), FieldSchema(name:msg, type:string, comment:)], location:raw:///tmp/junit6064902820246943844/testdb.db/test_table1, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:1, serdeInfo:SerDeInfo(name:test_table1, serializationLib:org.apache.hadoop.hive.ql.io.orc.OrcSerde, parameters:{serialization.format=1}), bucketCols:[], sortCols:null, parameters:null), partitionKeys:null, parameters:{}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
15925 [main] WARN  o.a.h.h.m.HiveMetaStore - Location: raw:///tmp/junit6064902820246943844/testdb.db/test_table1 specified for non-external table:test_table1
15926 [main] INFO  o.a.h.h.c.FileUtils - Creating directory if it doesn't exist: raw:/tmp/junit6064902820246943844/testdb.db/test_table1
15941 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Shutting down the object store...
15941 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=Shutting down the object store...	
15941 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Metastore shutdown complete.
15941 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=Metastore shutdown complete.	
15942 [main] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=compile from=org.apache.hadoop.hive.ql.Driver>
15942 [main] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=parse from=org.apache.hadoop.hive.ql.Driver>
15942 [main] INFO  h.q.p.ParseDriver - Parsing command: select * from testdb1.test_table1
15943 [main] INFO  h.q.p.ParseDriver - Parse Completed
15943 [main] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=parse start=1466699168513 end=1466699168514 duration=1 from=org.apache.hadoop.hive.ql.Driver>
15959 [main] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=semanticAnalyze from=org.apache.hadoop.hive.ql.Driver>
15960 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Starting Semantic Analysis
15960 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Completed phase 1 of Semantic Analysis
15960 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Get metadata for source tables
15960 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_table : db=testdb1 tbl=test_table1
15960 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table : db=testdb1 tbl=test_table1	
15961 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
15961 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
15962 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
15964 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
15964 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
15977 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Get metadata for subqueries
15977 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Get metadata for destination tables
15980 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Completed getting MetaData in Semantic Analysis
15982 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Set stats collection dir : file:/tmp/travis/225fac7a-5c4a-48d6-84a5-273f14c05d65/hive_2016-06-23_16-26-08_513_8179054262483810619-1/-ext-10002
15985 [main] INFO  o.a.h.h.q.p.OpProcFactory - Processing for FS(6)
15985 [main] INFO  o.a.h.h.q.p.OpProcFactory - Processing for SEL(5)
15985 [main] INFO  o.a.h.h.q.p.OpProcFactory - Processing for TS(4)
15990 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Completed plan generation
15990 [main] INFO  o.a.h.h.q.Driver - Semantic Analysis Completed
15990 [main] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=semanticAnalyze start=1466699168530 end=1466699168561 duration=31 from=org.apache.hadoop.hive.ql.Driver>
15991 [main] INFO  o.a.h.h.q.e.TableScanOperator - Initializing Self TS[4]
15991 [main] INFO  o.a.h.h.q.e.TableScanOperator - Operator 4 TS initialized
15991 [main] INFO  o.a.h.h.q.e.TableScanOperator - Initializing children of 4 TS
15991 [main] INFO  o.a.h.h.q.e.SelectOperator - Initializing child 5 SEL
15991 [main] INFO  o.a.h.h.q.e.SelectOperator - Initializing Self SEL[5]
15991 [main] INFO  o.a.h.h.q.e.SelectOperator - SELECT struct<id:int,msg:string>
15991 [main] INFO  o.a.h.h.q.e.SelectOperator - Operator 5 SEL initialized
15991 [main] INFO  o.a.h.h.q.e.SelectOperator - Initializing children of 5 SEL
15991 [main] INFO  o.a.h.h.q.e.ListSinkOperator - Initializing child 7 OP
15992 [main] INFO  o.a.h.h.q.e.ListSinkOperator - Initializing Self OP[7]
15992 [main] INFO  o.a.h.h.q.e.ListSinkOperator - Operator 7 OP initialized
15992 [main] INFO  o.a.h.h.q.e.ListSinkOperator - Initialization Done 7 OP
15992 [main] INFO  o.a.h.h.q.e.SelectOperator - Initialization Done 5 SEL
15992 [main] INFO  o.a.h.h.q.e.TableScanOperator - Initialization Done 4 TS
15992 [main] INFO  o.a.h.h.q.Driver - Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:test_table1.id, type:int, comment:null), FieldSchema(name:test_table1.msg, type:string, comment:null)], properties:null)
15992 [main] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=compile start=1466699168513 end=1466699168563 duration=50 from=org.apache.hadoop.hive.ql.Driver>

--------------------------------------------------
--------------------------------------------------
classname: org.apache.storm.hive.bolt.TestHiveBolt / testname: testJsonWriter
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hive.bolt.TestHiveBolt.generateTestTuple(TestHiveBolt.java:447)
	at org.apache.storm.hive.bolt.TestHiveBolt.testJsonWriter(TestHiveBolt.java:274)

-------------------- system-out --------------------
17356 [main] INFO  o.a.h.h.q.s.SessionState - Created local directory: /tmp/703da6f0-fd9b-450e-80e3-baf664ea36ee_resources
17358 [main] INFO  o.a.h.h.q.s.SessionState - Created HDFS directory: /tmp/hive/travis/703da6f0-fd9b-450e-80e3-baf664ea36ee
17360 [main] INFO  o.a.h.h.q.s.SessionState - Created local directory: /tmp/travis/703da6f0-fd9b-450e-80e3-baf664ea36ee
17362 [main] INFO  o.a.h.h.q.s.SessionState - Created HDFS directory: /tmp/hive/travis/703da6f0-fd9b-450e-80e3-baf664ea36ee/_tmp_space.db
17362 [main] INFO  o.a.h.h.q.s.SessionState - No Tez session required at this point. hive.execution.engine=mr.
17364 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_table_names_by_filter: db = testdb, filter = 
17364 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table_names_by_filter: db = testdb, filter = 	
17364 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
17364 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
17365 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
17368 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
17368 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
17375 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_table : db=testdb tbl=test_table
17375 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table : db=testdb tbl=test_table	
17388 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: drop_table : db=testdb tbl=test_table
17388 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=drop_table : db=testdb tbl=test_table	
17684 [main] INFO  h.m.hivemetastoressimpl - deleting  raw:/tmp/junit1447263901563066141/testdb.db/test_table
17684 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_database: testdb
17685 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_database: testdb	
17694 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: drop_database: testdb
17694 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=drop_database: testdb	
17696 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_all_tables: db=testdb
17696 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_all_tables: db=testdb	
17702 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_functions: db=testdb pat=*
17702 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_functions: db=testdb pat=*	
17709 [main] INFO  o.a.h.h.m.ObjectStore - Dropping database testdb along with all tables
17731 [main] INFO  h.m.hivemetastoressimpl - deleting  raw:/tmp/junit1447263901563066141/testdb.db
17732 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
17733 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
17735 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
17736 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
17736 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: create_database: Database(name:testdb, description:null, locationUri:raw:///tmp/junit3716598454706007428/testdb.db, parameters:null)
17736 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=create_database: Database(name:testdb, description:null, locationUri:raw:///tmp/junit3716598454706007428/testdb.db, parameters:null)	
17737 [main] WARN  o.a.h.h.m.ObjectStore - Failed to get database testdb, returning NoSuchObjectException
17743 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: create_table: Table(tableName:test_table, dbName:testdb, owner:null, createTime:0, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:), FieldSchema(name:msg, type:string, comment:)], location:raw:///tmp/junit3716598454706007428/testdb.db/test_table, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:1, serdeInfo:SerDeInfo(name:test_table, serializationLib:org.apache.hadoop.hive.ql.io.orc.OrcSerde, parameters:{serialization.format=1}), bucketCols:[], sortCols:null, parameters:null), partitionKeys:[FieldSchema(name:city, type:string, comment:), FieldSchema(name:state, type:string, comment:)], parameters:{}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)
17743 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=create_table: Table(tableName:test_table, dbName:testdb, owner:null, createTime:0, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:), FieldSchema(name:msg, type:string, comment:)], location:raw:///tmp/junit3716598454706007428/testdb.db/test_table, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:1, serdeInfo:SerDeInfo(name:test_table, serializationLib:org.apache.hadoop.hive.ql.io.orc.OrcSerde, parameters:{serialization.format=1}), bucketCols:[], sortCols:null, parameters:null), partitionKeys:[FieldSchema(name:city, type:string, comment:), FieldSchema(name:state, type:string, comment:)], parameters:{}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
17745 [main] WARN  o.a.h.h.m.HiveMetaStore - Location: raw:///tmp/junit3716598454706007428/testdb.db/test_table specified for non-external table:test_table
17746 [main] INFO  o.a.h.h.c.FileUtils - Creating directory if it doesn't exist: raw:/tmp/junit3716598454706007428/testdb.db/test_table
17806 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: add_partition : db=testdb tbl=test_table
17806 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=add_partition : db=testdb tbl=test_table	
17871 [main] INFO  o.a.h.h.c.FileUtils - Creating directory if it doesn't exist: raw:/tmp/junit3716598454706007428/testdb.db/test_table/city=sunnyvale/state=ca
17907 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Shutting down the object store...
17907 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=Shutting down the object store...	
17908 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Metastore shutdown complete.
17908 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=Metastore shutdown complete.	
done

--------------------------------------------------
--------------------------------------------------
classname: org.apache.storm.hive.bolt.TestHiveBolt / testname: testTickTuple
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hive.bolt.TestHiveBolt.generateTestTuple(TestHiveBolt.java:447)
	at org.apache.storm.hive.bolt.TestHiveBolt.testTickTuple(TestHiveBolt.java:352)

-------------------- system-out --------------------
18941 [main] INFO  o.a.h.h.q.s.SessionState - Created local directory: /tmp/eae57fdc-c623-40fd-9f86-a189ef9760ed_resources
18943 [main] INFO  o.a.h.h.q.s.SessionState - Created HDFS directory: /tmp/hive/travis/eae57fdc-c623-40fd-9f86-a189ef9760ed
18945 [main] INFO  o.a.h.h.q.s.SessionState - Created local directory: /tmp/travis/eae57fdc-c623-40fd-9f86-a189ef9760ed
18947 [main] INFO  o.a.h.h.q.s.SessionState - Created HDFS directory: /tmp/hive/travis/eae57fdc-c623-40fd-9f86-a189ef9760ed/_tmp_space.db
18947 [main] INFO  o.a.h.h.q.s.SessionState - No Tez session required at this point. hive.execution.engine=mr.
18949 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_table_names_by_filter: db = testdb, filter = 
18949 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table_names_by_filter: db = testdb, filter = 	
18949 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
18950 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
18951 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
18953 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
18953 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
18960 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_table : db=testdb tbl=test_table
18960 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table : db=testdb tbl=test_table	
18973 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: drop_table : db=testdb tbl=test_table
18973 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=drop_table : db=testdb tbl=test_table	
19249 [main] INFO  h.m.hivemetastoressimpl - deleting  raw:/tmp/junit4969291405143039347/testdb.db/test_table
19249 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_database: testdb
19250 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_database: testdb	
19258 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: drop_database: testdb
19258 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=drop_database: testdb	
19259 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_all_tables: db=testdb
19259 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_all_tables: db=testdb	
19265 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_functions: db=testdb pat=*
19265 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_functions: db=testdb pat=*	
19271 [main] INFO  o.a.h.h.m.ObjectStore - Dropping database testdb along with all tables
19293 [main] INFO  h.m.hivemetastoressimpl - deleting  raw:/tmp/junit4969291405143039347/testdb.db
19294 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
19295 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
19297 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
19297 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
19298 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: create_database: Database(name:testdb, description:null, locationUri:raw:///tmp/junit7338232992245714288/testdb.db, parameters:null)
19298 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=create_database: Database(name:testdb, description:null, locationUri:raw:///tmp/junit7338232992245714288/testdb.db, parameters:null)	
19299 [main] WARN  o.a.h.h.m.ObjectStore - Failed to get database testdb, returning NoSuchObjectException
19304 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: create_table: Table(tableName:test_table, dbName:testdb, owner:null, createTime:0, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:), FieldSchema(name:msg, type:string, comment:)], location:raw:///tmp/junit7338232992245714288/testdb.db/test_table, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:1, serdeInfo:SerDeInfo(name:test_table, serializationLib:org.apache.hadoop.hive.ql.io.orc.OrcSerde, parameters:{serialization.format=1}), bucketCols:[], sortCols:null, parameters:null), partitionKeys:[FieldSchema(name:city, type:string, comment:), FieldSchema(name:state, type:string, comment:)], parameters:{}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)
19305 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=create_table: Table(tableName:test_table, dbName:testdb, owner:null, createTime:0, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:), FieldSchema(name:msg, type:string, comment:)], location:raw:///tmp/junit7338232992245714288/testdb.db/test_table, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:1, serdeInfo:SerDeInfo(name:test_table, serializationLib:org.apache.hadoop.hive.ql.io.orc.OrcSerde, parameters:{serialization.format=1}), bucketCols:[], sortCols:null, parameters:null), partitionKeys:[FieldSchema(name:city, type:string, comment:), FieldSchema(name:state, type:string, comment:)], parameters:{}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
19307 [main] WARN  o.a.h.h.m.HiveMetaStore - Location: raw:///tmp/junit7338232992245714288/testdb.db/test_table specified for non-external table:test_table
19307 [main] INFO  o.a.h.h.c.FileUtils - Creating directory if it doesn't exist: raw:/tmp/junit7338232992245714288/testdb.db/test_table
19382 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: add_partition : db=testdb tbl=test_table
19382 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=add_partition : db=testdb tbl=test_table	
19431 [main] INFO  o.a.h.h.c.FileUtils - Creating directory if it doesn't exist: raw:/tmp/junit7338232992245714288/testdb.db/test_table/city=sunnyvale/state=ca
19455 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Shutting down the object store...
19455 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=Shutting down the object store...	
19456 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Metastore shutdown complete.
19456 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=Metastore shutdown complete.	
done

--------------------------------------------------
--------------------------------------------------
classname: org.apache.storm.hive.bolt.TestHiveBolt / testname: testWithTimeformat
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hive.bolt.TestHiveBolt.generateTestTuple(TestHiveBolt.java:447)
	at org.apache.storm.hive.bolt.TestHiveBolt.testWithTimeformat(TestHiveBolt.java:230)

-------------------- system-out --------------------
20430 [main] INFO  o.a.h.h.q.s.SessionState - Created local directory: /tmp/2c86a0af-0a0d-402f-b5a0-cb05961b3d25_resources
20435 [main] INFO  o.a.h.h.q.s.SessionState - Created HDFS directory: /tmp/hive/travis/2c86a0af-0a0d-402f-b5a0-cb05961b3d25
20437 [main] INFO  o.a.h.h.q.s.SessionState - Created local directory: /tmp/travis/2c86a0af-0a0d-402f-b5a0-cb05961b3d25
20439 [main] INFO  o.a.h.h.q.s.SessionState - Created HDFS directory: /tmp/hive/travis/2c86a0af-0a0d-402f-b5a0-cb05961b3d25/_tmp_space.db
20440 [main] INFO  o.a.h.h.q.s.SessionState - No Tez session required at this point. hive.execution.engine=mr.
20441 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_table_names_by_filter: db = testdb, filter = 
20441 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table_names_by_filter: db = testdb, filter = 	
20441 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
20442 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
20443 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
20445 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
20445 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
20452 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_table : db=testdb tbl=test_table
20452 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table : db=testdb tbl=test_table	
20469 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: drop_table : db=testdb tbl=test_table
20469 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=drop_table : db=testdb tbl=test_table	
20739 [main] INFO  h.m.hivemetastoressimpl - deleting  raw:/tmp/junit7338232992245714288/testdb.db/test_table
20739 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_database: testdb
20739 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_database: testdb	
20747 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: drop_database: testdb
20747 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=drop_database: testdb	
20748 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_all_tables: db=testdb
20749 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_all_tables: db=testdb	
20754 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_functions: db=testdb pat=*
20754 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_functions: db=testdb pat=*	
20760 [main] INFO  o.a.h.h.m.ObjectStore - Dropping database testdb along with all tables
20780 [main] INFO  h.m.hivemetastoressimpl - deleting  raw:/tmp/junit7338232992245714288/testdb.db
20781 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
20782 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
20796 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
20797 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
20797 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: create_database: Database(name:testdb, description:null, locationUri:raw:///tmp/junit2666015677573978721/testdb.db, parameters:null)
20797 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=create_database: Database(name:testdb, description:null, locationUri:raw:///tmp/junit2666015677573978721/testdb.db, parameters:null)	
20798 [main] WARN  o.a.h.h.m.ObjectStore - Failed to get database testdb, returning NoSuchObjectException
20814 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: create_table: Table(tableName:test_table, dbName:testdb, owner:null, createTime:0, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:), FieldSchema(name:msg, type:string, comment:)], location:raw:///tmp/junit2666015677573978721/testdb.db/test_table, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:1, serdeInfo:SerDeInfo(name:test_table, serializationLib:org.apache.hadoop.hive.ql.io.orc.OrcSerde, parameters:{serialization.format=1}), bucketCols:[], sortCols:null, parameters:null), partitionKeys:[FieldSchema(name:city, type:string, comment:), FieldSchema(name:state, type:string, comment:)], parameters:{}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)
20815 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=create_table: Table(tableName:test_table, dbName:testdb, owner:null, createTime:0, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:), FieldSchema(name:msg, type:string, comment:)], location:raw:///tmp/junit2666015677573978721/testdb.db/test_table, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:1, serdeInfo:SerDeInfo(name:test_table, serializationLib:org.apache.hadoop.hive.ql.io.orc.OrcSerde, parameters:{serialization.format=1}), bucketCols:[], sortCols:null, parameters:null), partitionKeys:[FieldSchema(name:city, type:string, comment:), FieldSchema(name:state, type:string, comment:)], parameters:{}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
20816 [main] WARN  o.a.h.h.m.HiveMetaStore - Location: raw:///tmp/junit2666015677573978721/testdb.db/test_table specified for non-external table:test_table
20817 [main] INFO  o.a.h.h.c.FileUtils - Creating directory if it doesn't exist: raw:/tmp/junit2666015677573978721/testdb.db/test_table
20874 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: add_partition : db=testdb tbl=test_table
20875 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=add_partition : db=testdb tbl=test_table	
20932 [main] INFO  o.a.h.h.c.FileUtils - Creating directory if it doesn't exist: raw:/tmp/junit2666015677573978721/testdb.db/test_table/city=sunnyvale/state=ca
20957 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Shutting down the object store...
20958 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=Shutting down the object store...	
20958 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Metastore shutdown complete.
20958 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=Metastore shutdown complete.	
done
20959 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_table_names_by_filter: db = testdb1, filter = 
20959 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table_names_by_filter: db = testdb1, filter = 	
20959 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
20960 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
20960 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
20963 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
20963 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
20972 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_table : db=testdb1 tbl=test_table1
20972 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table : db=testdb1 tbl=test_table1	
20986 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: drop_table : db=testdb1 tbl=test_table1
20986 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=drop_table : db=testdb1 tbl=test_table1	
21031 [main] INFO  h.m.hivemetastoressimpl - deleting  raw:/tmp/junit6064902820246943844/testdb.db/test_table1
21031 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_database: testdb1
21032 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_database: testdb1	
21033 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: drop_database: testdb1
21033 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=drop_database: testdb1	
21035 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_all_tables: db=testdb1
21035 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_all_tables: db=testdb1	
21036 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_functions: db=testdb1 pat=*
21036 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_functions: db=testdb1 pat=*	
21037 [main] INFO  o.a.h.h.m.ObjectStore - Dropping database testdb1 along with all tables
21041 [main] INFO  h.m.hivemetastoressimpl - deleting  raw:/tmp/junit6064902820246943844/testdb.db
21042 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
21043 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
21045 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
21045 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
21046 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: create_database: Database(name:testdb1, description:null, locationUri:raw:///tmp/junit2666015677573978721/testdb.db, parameters:null)
21046 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=create_database: Database(name:testdb1, description:null, locationUri:raw:///tmp/junit2666015677573978721/testdb.db, parameters:null)	
21047 [main] WARN  o.a.h.h.m.ObjectStore - Failed to get database testdb1, returning NoSuchObjectException
21050 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: create_table: Table(tableName:test_table1, dbName:testdb1, owner:null, createTime:0, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:), FieldSchema(name:msg, type:string, comment:)], location:raw:///tmp/junit2666015677573978721/testdb.db/test_table1, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:1, serdeInfo:SerDeInfo(name:test_table1, serializationLib:org.apache.hadoop.hive.ql.io.orc.OrcSerde, parameters:{serialization.format=1}), bucketCols:[], sortCols:null, parameters:null), partitionKeys:[FieldSchema(name:dt, type:string, comment:)], parameters:{}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)
21050 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=create_table: Table(tableName:test_table1, dbName:testdb1, owner:null, createTime:0, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:), FieldSchema(name:msg, type:string, comment:)], location:raw:///tmp/junit2666015677573978721/testdb.db/test_table1, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:1, serdeInfo:SerDeInfo(name:test_table1, serializationLib:org.apache.hadoop.hive.ql.io.orc.OrcSerde, parameters:{serialization.format=1}), bucketCols:[], sortCols:null, parameters:null), partitionKeys:[FieldSchema(name:dt, type:string, comment:)], parameters:{}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
21052 [main] WARN  o.a.h.h.m.HiveMetaStore - Location: raw:///tmp/junit2666015677573978721/testdb.db/test_table1 specified for non-external table:test_table1
21053 [main] INFO  o.a.h.h.c.FileUtils - Creating directory if it doesn't exist: raw:/tmp/junit2666015677573978721/testdb.db/test_table1
21070 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Shutting down the object store...
21070 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=Shutting down the object store...	
21070 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Metastore shutdown complete.
21070 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=Metastore shutdown complete.	
21071 [main] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=compile from=org.apache.hadoop.hive.ql.Driver>
21072 [main] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=parse from=org.apache.hadoop.hive.ql.Driver>
21072 [main] INFO  h.q.p.ParseDriver - Parsing command: select * from testdb1.test_table1
21073 [main] INFO  h.q.p.ParseDriver - Parse Completed
21073 [main] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=parse start=1466699173643 end=1466699173644 duration=1 from=org.apache.hadoop.hive.ql.Driver>
21084 [main] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=semanticAnalyze from=org.apache.hadoop.hive.ql.Driver>
21084 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Starting Semantic Analysis
21085 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Completed phase 1 of Semantic Analysis
21085 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Get metadata for source tables
21085 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_table : db=testdb1 tbl=test_table1
21085 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table : db=testdb1 tbl=test_table1	
21085 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
21086 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
21087 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
21089 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
21089 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
21104 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Get metadata for subqueries
21104 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Get metadata for destination tables
21107 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Completed getting MetaData in Semantic Analysis
21109 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Set stats collection dir : file:/tmp/travis/2c86a0af-0a0d-402f-b5a0-cb05961b3d25/hive_2016-06-23_16-26-13_643_71467574703281403-1/-ext-10002
21112 [main] INFO  o.a.h.h.q.p.OpProcFactory - Processing for FS(10)
21112 [main] INFO  o.a.h.h.q.p.OpProcFactory - Processing for SEL(9)
21112 [main] INFO  o.a.h.h.q.p.OpProcFactory - Processing for TS(8)
21114 [main] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=partition-retrieving from=org.apache.hadoop.hive.ql.optimizer.ppr.PartitionPruner>
21114 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_partitions : db=testdb1 tbl=test_table1
21115 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_partitions : db=testdb1 tbl=test_table1	
21123 [main] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=partition-retrieving start=1466699173685 end=1466699173694 duration=9 from=org.apache.hadoop.hive.ql.optimizer.ppr.PartitionPruner>
21124 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Completed plan generation
21124 [main] INFO  o.a.h.h.q.Driver - Semantic Analysis Completed
21124 [main] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=semanticAnalyze start=1466699173655 end=1466699173695 duration=40 from=org.apache.hadoop.hive.ql.Driver>
21125 [main] INFO  o.a.h.h.q.e.TableScanOperator - Initializing Self TS[8]
21125 [main] INFO  o.a.h.h.q.e.TableScanOperator - Operator 8 TS initialized
21125 [main] INFO  o.a.h.h.q.e.TableScanOperator - Initializing children of 8 TS
21125 [main] INFO  o.a.h.h.q.e.SelectOperator - Initializing child 9 SEL
21125 [main] INFO  o.a.h.h.q.e.SelectOperator - Initializing Self SEL[9]
21125 [main] INFO  o.a.h.h.q.e.SelectOperator - SELECT struct<id:int,msg:string,dt:string>
21125 [main] INFO  o.a.h.h.q.e.SelectOperator - Operator 9 SEL initialized
21125 [main] INFO  o.a.h.h.q.e.SelectOperator - Initializing children of 9 SEL
21125 [main] INFO  o.a.h.h.q.e.ListSinkOperator - Initializing child 11 OP
21126 [main] INFO  o.a.h.h.q.e.ListSinkOperator - Initializing Self OP[11]
21126 [main] INFO  o.a.h.h.q.e.ListSinkOperator - Operator 11 OP initialized
21126 [main] INFO  o.a.h.h.q.e.ListSinkOperator - Initialization Done 11 OP
21126 [main] INFO  o.a.h.h.q.e.SelectOperator - Initialization Done 9 SEL
21126 [main] INFO  o.a.h.h.q.e.TableScanOperator - Initialization Done 8 TS
21126 [main] INFO  o.a.h.h.q.Driver - Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:test_table1.id, type:int, comment:null), FieldSchema(name:test_table1.msg, type:string, comment:null), FieldSchema(name:test_table1.dt, type:string, comment:null)], properties:null)
21126 [main] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=compile start=1466699173642 end=1466699173697 duration=55 from=org.apache.hadoop.hive.ql.Driver>

--------------------------------------------------
--------------------------------------------------
classname: org.apache.storm.hive.bolt.TestHiveBolt / testname: testWithByteArrayIdandMessage
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hive.bolt.TestHiveBolt.generateTestTuple(TestHiveBolt.java:447)
	at org.apache.storm.hive.bolt.TestHiveBolt.testWithByteArrayIdandMessage(TestHiveBolt.java:161)

-------------------- system-out --------------------
21427 [main] INFO  o.a.h.h.q.s.SessionState - Created local directory: /tmp/2758418b-9902-4222-b935-9d6748138867_resources
21429 [main] INFO  o.a.h.h.q.s.SessionState - Created HDFS directory: /tmp/hive/travis/2758418b-9902-4222-b935-9d6748138867
21431 [main] INFO  o.a.h.h.q.s.SessionState - Created local directory: /tmp/travis/2758418b-9902-4222-b935-9d6748138867
21432 [main] INFO  o.a.h.h.q.s.SessionState - Created HDFS directory: /tmp/hive/travis/2758418b-9902-4222-b935-9d6748138867/_tmp_space.db
21433 [main] INFO  o.a.h.h.q.s.SessionState - No Tez session required at this point. hive.execution.engine=mr.
21435 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
21436 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
21439 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
21439 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
21440 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_table_names_by_filter: db = testdb, filter = 
21440 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table_names_by_filter: db = testdb, filter = 	
21441 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_table : db=testdb tbl=test_table
21441 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table : db=testdb tbl=test_table	
21457 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: drop_table : db=testdb tbl=test_table
21458 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=drop_table : db=testdb tbl=test_table	
21663 [main] INFO  h.m.hivemetastoressimpl - deleting  raw:/tmp/junit2666015677573978721/testdb.db/test_table
21664 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_database: testdb
21664 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_database: testdb	
21672 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: drop_database: testdb
21672 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=drop_database: testdb	
21674 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_all_tables: db=testdb
21674 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_all_tables: db=testdb	
21679 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_functions: db=testdb pat=*
21679 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_functions: db=testdb pat=*	
21686 [main] INFO  o.a.h.h.m.ObjectStore - Dropping database testdb along with all tables
21711 [main] INFO  h.m.hivemetastoressimpl - deleting  raw:/tmp/junit2666015677573978721/testdb.db
21712 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
21713 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
21715 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
21715 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
21716 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: create_database: Database(name:testdb, description:null, locationUri:raw:///tmp/junit4887865730003758399/testdb.db, parameters:null)
21716 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=create_database: Database(name:testdb, description:null, locationUri:raw:///tmp/junit4887865730003758399/testdb.db, parameters:null)	
21717 [main] WARN  o.a.h.h.m.ObjectStore - Failed to get database testdb, returning NoSuchObjectException
21723 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: create_table: Table(tableName:test_table, dbName:testdb, owner:null, createTime:0, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:), FieldSchema(name:msg, type:string, comment:)], location:raw:///tmp/junit4887865730003758399/testdb.db/test_table, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:1, serdeInfo:SerDeInfo(name:test_table, serializationLib:org.apache.hadoop.hive.ql.io.orc.OrcSerde, parameters:{serialization.format=1}), bucketCols:[], sortCols:null, parameters:null), partitionKeys:[FieldSchema(name:city, type:string, comment:), FieldSchema(name:state, type:string, comment:)], parameters:{}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)
21723 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=create_table: Table(tableName:test_table, dbName:testdb, owner:null, createTime:0, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:), FieldSchema(name:msg, type:string, comment:)], location:raw:///tmp/junit4887865730003758399/testdb.db/test_table, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:1, serdeInfo:SerDeInfo(name:test_table, serializationLib:org.apache.hadoop.hive.ql.io.orc.OrcSerde, parameters:{serialization.format=1}), bucketCols:[], sortCols:null, parameters:null), partitionKeys:[FieldSchema(name:city, type:string, comment:), FieldSchema(name:state, type:string, comment:)], parameters:{}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
21725 [main] WARN  o.a.h.h.m.HiveMetaStore - Location: raw:///tmp/junit4887865730003758399/testdb.db/test_table specified for non-external table:test_table
21725 [main] INFO  o.a.h.h.c.FileUtils - Creating directory if it doesn't exist: raw:/tmp/junit4887865730003758399/testdb.db/test_table
21797 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: add_partition : db=testdb tbl=test_table
21797 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=add_partition : db=testdb tbl=test_table	
21847 [main] INFO  o.a.h.h.c.FileUtils - Creating directory if it doesn't exist: raw:/tmp/junit4887865730003758399/testdb.db/test_table/city=sunnyvale/state=ca
21875 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Shutting down the object store...
21875 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=Shutting down the object store...	
21876 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Metastore shutdown complete.
21876 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=Metastore shutdown complete.	
done
21876 [main] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=compile from=org.apache.hadoop.hive.ql.Driver>
21877 [main] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=parse from=org.apache.hadoop.hive.ql.Driver>
21877 [main] INFO  h.q.p.ParseDriver - Parsing command: select * from testdb.test_table
21878 [main] INFO  h.q.p.ParseDriver - Parse Completed
21878 [main] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=parse start=1466699174448 end=1466699174449 duration=1 from=org.apache.hadoop.hive.ql.Driver>
21889 [main] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=semanticAnalyze from=org.apache.hadoop.hive.ql.Driver>
21889 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Starting Semantic Analysis
21889 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Completed phase 1 of Semantic Analysis
21890 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Get metadata for source tables
21890 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_table : db=testdb tbl=test_table
21890 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table : db=testdb tbl=test_table	
21890 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
21891 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
21892 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
21894 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
21895 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
21914 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Get metadata for subqueries
21914 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Get metadata for destination tables
21917 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Completed getting MetaData in Semantic Analysis
21919 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Set stats collection dir : file:/tmp/travis/2758418b-9902-4222-b935-9d6748138867/hive_2016-06-23_16-26-14_448_298757015279871324-1/-ext-10002
21922 [main] INFO  o.a.h.h.q.p.OpProcFactory - Processing for FS(14)
21923 [main] INFO  o.a.h.h.q.p.OpProcFactory - Processing for SEL(13)
21923 [main] INFO  o.a.h.h.q.p.OpProcFactory - Processing for TS(12)
21925 [main] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=partition-retrieving from=org.apache.hadoop.hive.ql.optimizer.ppr.PartitionPruner>
21926 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_partitions : db=testdb tbl=test_table
21926 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_partitions : db=testdb tbl=test_table	
21986 [main] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=partition-retrieving start=1466699174496 end=1466699174557 duration=61 from=org.apache.hadoop.hive.ql.optimizer.ppr.PartitionPruner>
21989 [main] INFO  o.a.h.h.q.p.SemanticAnalyzer - Completed plan generation
21989 [main] INFO  o.a.h.h.q.Driver - Semantic Analysis Completed
21989 [main] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=semanticAnalyze start=1466699174460 end=1466699174560 duration=100 from=org.apache.hadoop.hive.ql.Driver>
21990 [main] INFO  o.a.h.h.q.e.TableScanOperator - Initializing Self TS[12]
21990 [main] INFO  o.a.h.h.q.e.TableScanOperator - Operator 12 TS initialized
21990 [main] INFO  o.a.h.h.q.e.TableScanOperator - Initializing children of 12 TS
21990 [main] INFO  o.a.h.h.q.e.SelectOperator - Initializing child 13 SEL
21990 [main] INFO  o.a.h.h.q.e.SelectOperator - Initializing Self SEL[13]
21991 [main] INFO  o.a.h.h.q.e.SelectOperator - SELECT struct<id:int,msg:string,city:string,state:string>
21991 [main] INFO  o.a.h.h.q.e.SelectOperator - Operator 13 SEL initialized
21991 [main] INFO  o.a.h.h.q.e.SelectOperator - Initializing children of 13 SEL
21991 [main] INFO  o.a.h.h.q.e.ListSinkOperator - Initializing child 15 OP
21991 [main] INFO  o.a.h.h.q.e.ListSinkOperator - Initializing Self OP[15]
21991 [main] INFO  o.a.h.h.q.e.ListSinkOperator - Operator 15 OP initialized
21991 [main] INFO  o.a.h.h.q.e.ListSinkOperator - Initialization Done 15 OP
21992 [main] INFO  o.a.h.h.q.e.SelectOperator - Initialization Done 13 SEL
21992 [main] INFO  o.a.h.h.q.e.TableScanOperator - Initialization Done 12 TS
21992 [main] INFO  o.a.h.h.q.Driver - Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:test_table.id, type:int, comment:null), FieldSchema(name:test_table.msg, type:string, comment:null), FieldSchema(name:test_table.city, type:string, comment:null), FieldSchema(name:test_table.state, type:string, comment:null)], properties:null)
21992 [main] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=compile start=1466699174447 end=1466699174563 duration=116 from=org.apache.hadoop.hive.ql.Driver>

--------------------------------------------------
--------------------------------------------------
classname: org.apache.storm.hive.bolt.TestHiveBolt / testname: testNoAcksIfFlushFails
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hive.bolt.TestHiveBolt.generateTestTuple(TestHiveBolt.java:447)
	at org.apache.storm.hive.bolt.TestHiveBolt.testNoAcksIfFlushFails(TestHiveBolt.java:327)

-------------------- system-out --------------------
22527 [main] INFO  o.a.h.h.q.s.SessionState - Created local directory: /tmp/c33fcf9d-280e-4f8a-85ef-19ac1856b3bc_resources
22529 [main] INFO  o.a.h.h.q.s.SessionState - Created HDFS directory: /tmp/hive/travis/c33fcf9d-280e-4f8a-85ef-19ac1856b3bc
22531 [main] INFO  o.a.h.h.q.s.SessionState - Created local directory: /tmp/travis/c33fcf9d-280e-4f8a-85ef-19ac1856b3bc
22532 [main] INFO  o.a.h.h.q.s.SessionState - Created HDFS directory: /tmp/hive/travis/c33fcf9d-280e-4f8a-85ef-19ac1856b3bc/_tmp_space.db
22533 [main] INFO  o.a.h.h.q.s.SessionState - No Tez session required at this point. hive.execution.engine=mr.
22534 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
22535 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
22538 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
22538 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
22538 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_table_names_by_filter: db = testdb, filter = 
22539 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table_names_by_filter: db = testdb, filter = 	
22545 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_table : db=testdb tbl=test_table
22545 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table : db=testdb tbl=test_table	
22559 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: drop_table : db=testdb tbl=test_table
22559 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=drop_table : db=testdb tbl=test_table	
22811 [main] INFO  h.m.hivemetastoressimpl - deleting  raw:/tmp/junit4887865730003758399/testdb.db/test_table
22811 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_database: testdb
22811 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_database: testdb	
22818 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: drop_database: testdb
22818 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=drop_database: testdb	
22819 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_all_tables: db=testdb
22819 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_all_tables: db=testdb	
22833 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_functions: db=testdb pat=*
22834 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_functions: db=testdb pat=*	
22846 [main] INFO  o.a.h.h.m.ObjectStore - Dropping database testdb along with all tables
22890 [main] INFO  h.m.hivemetastoressimpl - deleting  raw:/tmp/junit4887865730003758399/testdb.db
22891 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
22892 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
22894 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
22894 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
22895 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: create_database: Database(name:testdb, description:null, locationUri:raw:///tmp/junit714844961444882051/testdb.db, parameters:null)
22895 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=create_database: Database(name:testdb, description:null, locationUri:raw:///tmp/junit714844961444882051/testdb.db, parameters:null)	
22897 [main] WARN  o.a.h.h.m.ObjectStore - Failed to get database testdb, returning NoSuchObjectException
22902 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: create_table: Table(tableName:test_table, dbName:testdb, owner:null, createTime:0, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:), FieldSchema(name:msg, type:string, comment:)], location:raw:///tmp/junit714844961444882051/testdb.db/test_table, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:1, serdeInfo:SerDeInfo(name:test_table, serializationLib:org.apache.hadoop.hive.ql.io.orc.OrcSerde, parameters:{serialization.format=1}), bucketCols:[], sortCols:null, parameters:null), partitionKeys:[FieldSchema(name:city, type:string, comment:), FieldSchema(name:state, type:string, comment:)], parameters:{}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)
22902 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=create_table: Table(tableName:test_table, dbName:testdb, owner:null, createTime:0, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:), FieldSchema(name:msg, type:string, comment:)], location:raw:///tmp/junit714844961444882051/testdb.db/test_table, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:1, serdeInfo:SerDeInfo(name:test_table, serializationLib:org.apache.hadoop.hive.ql.io.orc.OrcSerde, parameters:{serialization.format=1}), bucketCols:[], sortCols:null, parameters:null), partitionKeys:[FieldSchema(name:city, type:string, comment:), FieldSchema(name:state, type:string, comment:)], parameters:{}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
22904 [main] WARN  o.a.h.h.m.HiveMetaStore - Location: raw:///tmp/junit714844961444882051/testdb.db/test_table specified for non-external table:test_table
22904 [main] INFO  o.a.h.h.c.FileUtils - Creating directory if it doesn't exist: raw:/tmp/junit714844961444882051/testdb.db/test_table
22953 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: add_partition : db=testdb tbl=test_table
22953 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=add_partition : db=testdb tbl=test_table	
22996 [main] INFO  o.a.h.h.c.FileUtils - Creating directory if it doesn't exist: raw:/tmp/junit714844961444882051/testdb.db/test_table/city=sunnyvale/state=ca
23019 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Shutting down the object store...
23019 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=Shutting down the object store...	
23019 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Metastore shutdown complete.
23023 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=Metastore shutdown complete.	
done

--------------------------------------------------
Checking ./external/storm-hive/target/surefire-reports/TEST-org.apache.storm.hive.common.TestHiveWriter.xml
--------------------------------------------------
classname: org.apache.storm.hive.common.TestHiveWriter / testname: testWriteBasic
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hive.common.TestHiveWriter.generateTestTuple(TestHiveWriter.java:164)
	at org.apache.storm.hive.common.TestHiveWriter.writeTuples(TestHiveWriter.java:179)
	at org.apache.storm.hive.common.TestHiveWriter.testWriteBasic(TestHiveWriter.java:127)

-------------------- system-out --------------------
23310 [main] INFO  o.a.h.h.q.s.SessionState - Created local directory: /tmp/2f6d985a-6af2-482f-bf96-a06a319a7e98_resources
23311 [main] INFO  o.a.h.h.q.s.SessionState - Created HDFS directory: /tmp/hive/travis/2f6d985a-6af2-482f-bf96-a06a319a7e98
23313 [main] INFO  o.a.h.h.q.s.SessionState - Created local directory: /tmp/travis/2f6d985a-6af2-482f-bf96-a06a319a7e98
23315 [main] INFO  o.a.h.h.q.s.SessionState - Created HDFS directory: /tmp/hive/travis/2f6d985a-6af2-482f-bf96-a06a319a7e98/_tmp_space.db
23315 [main] INFO  o.a.h.h.q.s.SessionState - No Tez session required at this point. hive.execution.engine=mr.
23315 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_table_names_by_filter: db = testdb, filter = 
23316 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table_names_by_filter: db = testdb, filter = 	
23316 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
23316 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
23317 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
23319 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
23320 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
23341 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_table : db=testdb tbl=test_table
23341 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table : db=testdb tbl=test_table	
23353 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: drop_table : db=testdb tbl=test_table
23353 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=drop_table : db=testdb tbl=test_table	
23589 [main] INFO  h.m.hivemetastoressimpl - deleting  raw:/tmp/junit714844961444882051/testdb.db/test_table
23589 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_database: testdb
23589 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_database: testdb	
23596 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: drop_database: testdb
23596 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=drop_database: testdb	
23597 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_all_tables: db=testdb
23597 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_all_tables: db=testdb	
23601 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_functions: db=testdb pat=*
23601 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_functions: db=testdb pat=*	
23609 [main] INFO  o.a.h.h.m.ObjectStore - Dropping database testdb along with all tables
23635 [main] INFO  h.m.hivemetastoressimpl - deleting  raw:/tmp/junit714844961444882051/testdb.db
23636 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
23637 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
23638 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
23638 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
23639 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: create_database: Database(name:testdb, description:null, locationUri:/tmp/junit7672546308236490048/testdb.db, parameters:null)
23639 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=create_database: Database(name:testdb, description:null, locationUri:/tmp/junit7672546308236490048/testdb.db, parameters:null)	
23640 [main] WARN  o.a.h.h.m.ObjectStore - Failed to get database testdb, returning NoSuchObjectException
23640 [main] INFO  o.a.h.h.c.FileUtils - Creating directory if it doesn't exist: file:/tmp/junit7672546308236490048/testdb.db
23644 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: create_table: Table(tableName:test_table2, dbName:testdb, owner:null, createTime:0, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:), FieldSchema(name:msg, type:string, comment:)], location:/tmp/junit7672546308236490048/testdb.db/test_table2, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:1, serdeInfo:SerDeInfo(name:test_table2, serializationLib:org.apache.hadoop.hive.ql.io.orc.OrcSerde, parameters:{serialization.format=1}), bucketCols:[], sortCols:null, parameters:null), partitionKeys:[FieldSchema(name:city, type:string, comment:), FieldSchema(name:state, type:string, comment:)], parameters:{}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)
23644 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=create_table: Table(tableName:test_table2, dbName:testdb, owner:null, createTime:0, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:), FieldSchema(name:msg, type:string, comment:)], location:/tmp/junit7672546308236490048/testdb.db/test_table2, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:1, serdeInfo:SerDeInfo(name:test_table2, serializationLib:org.apache.hadoop.hive.ql.io.orc.OrcSerde, parameters:{serialization.format=1}), bucketCols:[], sortCols:null, parameters:null), partitionKeys:[FieldSchema(name:city, type:string, comment:), FieldSchema(name:state, type:string, comment:)], parameters:{}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
23646 [main] WARN  o.a.h.h.m.HiveMetaStore - Location: /tmp/junit7672546308236490048/testdb.db/test_table2 specified for non-external table:test_table2
23646 [main] INFO  o.a.h.h.c.FileUtils - Creating directory if it doesn't exist: file:/tmp/junit7672546308236490048/testdb.db/test_table2
23696 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: add_partition : db=testdb tbl=test_table2
23696 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=add_partition : db=testdb tbl=test_table2	
23735 [main] INFO  o.a.h.h.c.FileUtils - Creating directory if it doesn't exist: file:/tmp/junit7672546308236490048/testdb.db/test_table2/city=sunnyvale/state=ca
23779 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Shutting down the object store...
23779 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=Shutting down the object store...	
23780 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Metastore shutdown complete.
23780 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=Metastore shutdown complete.	
23854 [hiveWriterTest] INFO  o.a.h.h.q.s.SessionState - Created local directory: /tmp/dc4fd29c-0d2f-4b69-a6f2-a71cf2133275_resources
23856 [hiveWriterTest] INFO  o.a.h.h.q.s.SessionState - Created HDFS directory: /tmp/hive/travis/dc4fd29c-0d2f-4b69-a6f2-a71cf2133275
23858 [hiveWriterTest] INFO  o.a.h.h.q.s.SessionState - Created local directory: /tmp/travis/dc4fd29c-0d2f-4b69-a6f2-a71cf2133275
23859 [hiveWriterTest] INFO  o.a.h.h.q.s.SessionState - Created HDFS directory: /tmp/hive/travis/dc4fd29c-0d2f-4b69-a6f2-a71cf2133275/_tmp_space.db
23860 [hiveWriterTest] INFO  o.a.h.h.q.s.SessionState - No Tez session required at this point. hive.execution.engine=mr.
23860 [hiveWriterTest] INFO  o.a.h.h.m.HiveMetaStore - 1: get_table : db=testdb tbl=test_table2
23860 [hiveWriterTest] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table : db=testdb tbl=test_table2	
23860 [hiveWriterTest] INFO  o.a.h.h.m.HiveMetaStore - 1: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
23860 [hiveWriterTest] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
23862 [hiveWriterTest] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
23864 [hiveWriterTest] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
23864 [hiveWriterTest] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
23880 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=Driver.run from=org.apache.hadoop.hive.ql.Driver>
23880 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=TimeToSubmit from=org.apache.hadoop.hive.ql.Driver>
23881 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=compile from=org.apache.hadoop.hive.ql.Driver>
23881 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=parse from=org.apache.hadoop.hive.ql.Driver>
23881 [hiveWriterTest] INFO  h.q.p.ParseDriver - Parsing command: use testdb
23882 [hiveWriterTest] INFO  h.q.p.ParseDriver - Parse Completed
23895 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=parse start=1466699176452 end=1466699176466 duration=14 from=org.apache.hadoop.hive.ql.Driver>
23902 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=semanticAnalyze from=org.apache.hadoop.hive.ql.Driver>
23909 [hiveWriterTest] INFO  o.a.h.h.m.HiveMetaStore - 1: get_database: testdb
23909 [hiveWriterTest] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_database: testdb	
23920 [hiveWriterTest] INFO  o.a.h.h.q.Driver - Semantic Analysis Completed
23920 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=semanticAnalyze start=1466699176473 end=1466699176491 duration=18 from=org.apache.hadoop.hive.ql.Driver>
23921 [hiveWriterTest] INFO  o.a.h.h.q.Driver - Returning Hive schema: Schema(fieldSchemas:null, properties:null)
23921 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=compile start=1466699176452 end=1466699176492 duration=40 from=org.apache.hadoop.hive.ql.Driver>
23921 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=acquireReadWriteLocks from=org.apache.hadoop.hive.ql.Driver>
23923 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=acquireReadWriteLocks start=1466699176492 end=1466699176494 duration=2 from=org.apache.hadoop.hive.ql.Driver>
23923 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=Driver.execute from=org.apache.hadoop.hive.ql.Driver>
23923 [hiveWriterTest] INFO  o.a.h.h.q.Driver - Starting command: use testdb
23926 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=TimeToSubmit start=1466699176451 end=1466699176497 duration=46 from=org.apache.hadoop.hive.ql.Driver>
23926 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=runTasks from=org.apache.hadoop.hive.ql.Driver>
23926 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=task.DDL.Stage-0 from=org.apache.hadoop.hive.ql.Driver>
23930 [hiveWriterTest] INFO  o.a.h.h.q.Driver - Starting task [Stage-0:DDL] in serial mode
23930 [hiveWriterTest] INFO  o.a.h.h.m.HiveMetaStore - 1: get_database: testdb
23931 [hiveWriterTest] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_database: testdb	
23932 [hiveWriterTest] INFO  o.a.h.h.m.HiveMetaStore - 1: get_database: testdb
23932 [hiveWriterTest] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_database: testdb	
23934 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=runTasks start=1466699176497 end=1466699176504 duration=7 from=org.apache.hadoop.hive.ql.Driver>
23934 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=Driver.execute start=1466699176494 end=1466699176505 duration=11 from=org.apache.hadoop.hive.ql.Driver>
23934 [hiveWriterTest] INFO  o.a.h.h.q.Driver - OK
23934 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=releaseLocks from=org.apache.hadoop.hive.ql.Driver>
23934 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=releaseLocks start=1466699176505 end=1466699176505 duration=0 from=org.apache.hadoop.hive.ql.Driver>
23934 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=Driver.run start=1466699176451 end=1466699176505 duration=54 from=org.apache.hadoop.hive.ql.Driver>
23935 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=Driver.run from=org.apache.hadoop.hive.ql.Driver>
23935 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=TimeToSubmit from=org.apache.hadoop.hive.ql.Driver>
23935 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=compile from=org.apache.hadoop.hive.ql.Driver>
23936 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=parse from=org.apache.hadoop.hive.ql.Driver>
23936 [hiveWriterTest] INFO  h.q.p.ParseDriver - Parsing command: alter table test_table2 add if not exists partition  ( city='sunnyvale',state='ca' )
23939 [hiveWriterTest] INFO  h.q.p.ParseDriver - Parse Completed
23939 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=parse start=1466699176507 end=1466699176510 duration=3 from=org.apache.hadoop.hive.ql.Driver>
23942 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=semanticAnalyze from=org.apache.hadoop.hive.ql.Driver>
23942 [hiveWriterTest] INFO  o.a.h.h.m.HiveMetaStore - 1: get_table : db=testdb tbl=test_table2
23943 [hiveWriterTest] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table : db=testdb tbl=test_table2	
23963 [hiveWriterTest] INFO  o.a.h.h.q.Driver - Semantic Analysis Completed
23964 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=semanticAnalyze start=1466699176513 end=1466699176535 duration=22 from=org.apache.hadoop.hive.ql.Driver>
23964 [hiveWriterTest] INFO  o.a.h.h.q.Driver - Returning Hive schema: Schema(fieldSchemas:null, properties:null)
23964 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=compile start=1466699176506 end=1466699176535 duration=29 from=org.apache.hadoop.hive.ql.Driver>
23964 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=acquireReadWriteLocks from=org.apache.hadoop.hive.ql.Driver>
23989 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=acquireReadWriteLocks start=1466699176535 end=1466699176560 duration=25 from=org.apache.hadoop.hive.ql.Driver>
23990 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=Driver.execute from=org.apache.hadoop.hive.ql.Driver>
23990 [hiveWriterTest] INFO  o.a.h.h.q.Driver - Starting command: alter table test_table2 add if not exists partition  ( city='sunnyvale',state='ca' )
23991 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=TimeToSubmit start=1466699176506 end=1466699176562 duration=56 from=org.apache.hadoop.hive.ql.Driver>
23991 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=runTasks from=org.apache.hadoop.hive.ql.Driver>
23991 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=task.DDL.Stage-0 from=org.apache.hadoop.hive.ql.Driver>
23991 [hiveWriterTest] INFO  o.a.h.h.q.Driver - Starting task [Stage-0:DDL] in serial mode
23991 [hiveWriterTest] INFO  o.a.h.h.m.HiveMetaStore - 1: get_table : db=testdb tbl=test_table2
23992 [hiveWriterTest] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table : db=testdb tbl=test_table2	
24014 [hiveWriterTest] INFO  o.a.h.h.m.HiveMetaStore - 1: add_partitions
24014 [hiveWriterTest] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=add_partitions	
24040 [hiveWriterTest] INFO  o.a.h.h.m.HiveMetaStore - Not adding partition Partition(values:[sunnyvale, ca], dbName:testdb, tableName:test_table2, createTime:0, lastAccessTime:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:), FieldSchema(name:msg, type:string, comment:)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:1, serdeInfo:SerDeInfo(name:test_table2, serializationLib:org.apache.hadoop.hive.ql.io.orc.OrcSerde, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), parameters:null) as it already exists
24041 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=runTasks start=1466699176562 end=1466699176612 duration=50 from=org.apache.hadoop.hive.ql.Driver>
24041 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=Driver.execute start=1466699176561 end=1466699176612 duration=51 from=org.apache.hadoop.hive.ql.Driver>
24042 [hiveWriterTest] INFO  o.a.h.h.q.Driver - OK
24042 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=releaseLocks from=org.apache.hadoop.hive.ql.Driver>
24050 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=releaseLocks start=1466699176613 end=1466699176621 duration=8 from=org.apache.hadoop.hive.ql.Driver>
24050 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=Driver.run start=1466699176506 end=1466699176621 duration=115 from=org.apache.hadoop.hive.ql.Driver>
24098 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_table : db=testdb tbl=test_table2
24099 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table : db=testdb tbl=test_table2	
24099 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
24100 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
24101 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
24102 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
24103 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
24114 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_partition : db=testdb tbl=test_table2[sunnyvale,ca]
24114 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_partition : db=testdb tbl=test_table2[sunnyvale,ca]	
24144 [hiveWriterTest] INFO  o.a.h.h.m.HiveMetaStore - 1: get_table : db=testdb tbl=test_table2
24144 [hiveWriterTest] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table : db=testdb tbl=test_table2	

-------------------- system-err --------------------
OK
OK

--------------------------------------------------
--------------------------------------------------
classname: org.apache.storm.hive.common.TestHiveWriter / testname: testWriteMultiFlush
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.hive.common.TestHiveWriter.generateTestTuple(TestHiveWriter.java:164)
	at org.apache.storm.hive.common.TestHiveWriter.testWriteMultiFlush(TestHiveWriter.java:142)

-------------------- system-out --------------------
24519 [main] INFO  o.a.h.h.q.s.SessionState - Created local directory: /tmp/172d7089-5707-4edb-a83a-2efb53625c2c_resources
24521 [main] INFO  o.a.h.h.q.s.SessionState - Created HDFS directory: /tmp/hive/travis/172d7089-5707-4edb-a83a-2efb53625c2c
24523 [main] INFO  o.a.h.h.q.s.SessionState - Created local directory: /tmp/travis/172d7089-5707-4edb-a83a-2efb53625c2c
24524 [main] INFO  o.a.h.h.q.s.SessionState - Created HDFS directory: /tmp/hive/travis/172d7089-5707-4edb-a83a-2efb53625c2c/_tmp_space.db
24525 [main] INFO  o.a.h.h.q.s.SessionState - No Tez session required at this point. hive.execution.engine=mr.
24526 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
24526 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
24528 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
24528 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
24529 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_table_names_by_filter: db = testdb, filter = 
24529 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table_names_by_filter: db = testdb, filter = 	
24533 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_table : db=testdb tbl=test_table2
24533 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table : db=testdb tbl=test_table2	
24555 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: drop_table : db=testdb tbl=test_table2
24555 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=drop_table : db=testdb tbl=test_table2	
24758 [main] INFO  h.m.hivemetastoressimpl - deleting  file:/tmp/junit7672546308236490048/testdb.db/test_table2
24758 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_database: testdb
24758 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_database: testdb	
24765 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: drop_database: testdb
24765 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=drop_database: testdb	
24766 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_all_tables: db=testdb
24766 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_all_tables: db=testdb	
24770 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_functions: db=testdb pat=*
24770 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_functions: db=testdb pat=*	
24774 [main] INFO  o.a.h.h.m.ObjectStore - Dropping database testdb along with all tables
24788 [main] INFO  h.m.hivemetastoressimpl - deleting  file:/tmp/junit7672546308236490048/testdb.db
24789 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
24790 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
24799 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
24799 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
24799 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: create_database: Database(name:testdb, description:null, locationUri:/tmp/junit2021555152232828772/testdb.db, parameters:null)
24799 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=create_database: Database(name:testdb, description:null, locationUri:/tmp/junit2021555152232828772/testdb.db, parameters:null)	
24800 [main] WARN  o.a.h.h.m.ObjectStore - Failed to get database testdb, returning NoSuchObjectException
24800 [main] INFO  o.a.h.h.c.FileUtils - Creating directory if it doesn't exist: file:/tmp/junit2021555152232828772/testdb.db
24810 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: create_table: Table(tableName:test_table2, dbName:testdb, owner:null, createTime:0, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:), FieldSchema(name:msg, type:string, comment:)], location:/tmp/junit2021555152232828772/testdb.db/test_table2, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:1, serdeInfo:SerDeInfo(name:test_table2, serializationLib:org.apache.hadoop.hive.ql.io.orc.OrcSerde, parameters:{serialization.format=1}), bucketCols:[], sortCols:null, parameters:null), partitionKeys:[FieldSchema(name:city, type:string, comment:), FieldSchema(name:state, type:string, comment:)], parameters:{}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)
24810 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=create_table: Table(tableName:test_table2, dbName:testdb, owner:null, createTime:0, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:), FieldSchema(name:msg, type:string, comment:)], location:/tmp/junit2021555152232828772/testdb.db/test_table2, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:1, serdeInfo:SerDeInfo(name:test_table2, serializationLib:org.apache.hadoop.hive.ql.io.orc.OrcSerde, parameters:{serialization.format=1}), bucketCols:[], sortCols:null, parameters:null), partitionKeys:[FieldSchema(name:city, type:string, comment:), FieldSchema(name:state, type:string, comment:)], parameters:{}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
24812 [main] WARN  o.a.h.h.m.HiveMetaStore - Location: /tmp/junit2021555152232828772/testdb.db/test_table2 specified for non-external table:test_table2
24812 [main] INFO  o.a.h.h.c.FileUtils - Creating directory if it doesn't exist: file:/tmp/junit2021555152232828772/testdb.db/test_table2
24852 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: add_partition : db=testdb tbl=test_table2
24853 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=add_partition : db=testdb tbl=test_table2	
24895 [main] INFO  o.a.h.h.c.FileUtils - Creating directory if it doesn't exist: file:/tmp/junit2021555152232828772/testdb.db/test_table2/city=sunnyvale/state=ca
24912 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Shutting down the object store...
24912 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=Shutting down the object store...	
24912 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Metastore shutdown complete.
24912 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=Metastore shutdown complete.	
24949 [hiveWriterTest] INFO  o.a.h.h.q.s.SessionState - Created local directory: /tmp/5fae9fe6-c6d3-4268-a80a-e562e4fc8e3d_resources
24951 [hiveWriterTest] INFO  o.a.h.h.q.s.SessionState - Created HDFS directory: /tmp/hive/travis/5fae9fe6-c6d3-4268-a80a-e562e4fc8e3d
24952 [hiveWriterTest] INFO  o.a.h.h.q.s.SessionState - Created local directory: /tmp/travis/5fae9fe6-c6d3-4268-a80a-e562e4fc8e3d
24954 [hiveWriterTest] INFO  o.a.h.h.q.s.SessionState - Created HDFS directory: /tmp/hive/travis/5fae9fe6-c6d3-4268-a80a-e562e4fc8e3d/_tmp_space.db
24954 [hiveWriterTest] INFO  o.a.h.h.q.s.SessionState - No Tez session required at this point. hive.execution.engine=mr.
24954 [hiveWriterTest] INFO  o.a.h.h.m.HiveMetaStore - 2: get_table : db=testdb tbl=test_table2
24954 [hiveWriterTest] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table : db=testdb tbl=test_table2	
24954 [hiveWriterTest] INFO  o.a.h.h.m.HiveMetaStore - 2: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
24955 [hiveWriterTest] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
24956 [hiveWriterTest] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
24958 [hiveWriterTest] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
24958 [hiveWriterTest] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
24990 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=Driver.run from=org.apache.hadoop.hive.ql.Driver>
24990 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=TimeToSubmit from=org.apache.hadoop.hive.ql.Driver>
24990 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=compile from=org.apache.hadoop.hive.ql.Driver>
24990 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=parse from=org.apache.hadoop.hive.ql.Driver>
24991 [hiveWriterTest] INFO  h.q.p.ParseDriver - Parsing command: use testdb
24991 [hiveWriterTest] INFO  h.q.p.ParseDriver - Parse Completed
24991 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=parse start=1466699177561 end=1466699177562 duration=1 from=org.apache.hadoop.hive.ql.Driver>
24999 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=semanticAnalyze from=org.apache.hadoop.hive.ql.Driver>
25000 [hiveWriterTest] INFO  o.a.h.h.m.HiveMetaStore - 2: get_database: testdb
25000 [hiveWriterTest] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_database: testdb	
25001 [hiveWriterTest] INFO  o.a.h.h.q.Driver - Semantic Analysis Completed
25001 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=semanticAnalyze start=1466699177570 end=1466699177572 duration=2 from=org.apache.hadoop.hive.ql.Driver>
25002 [hiveWriterTest] INFO  o.a.h.h.q.Driver - Returning Hive schema: Schema(fieldSchemas:null, properties:null)
25002 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=compile start=1466699177561 end=1466699177573 duration=12 from=org.apache.hadoop.hive.ql.Driver>
25002 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=acquireReadWriteLocks from=org.apache.hadoop.hive.ql.Driver>
25002 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=acquireReadWriteLocks start=1466699177573 end=1466699177573 duration=0 from=org.apache.hadoop.hive.ql.Driver>
25002 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=Driver.execute from=org.apache.hadoop.hive.ql.Driver>
25002 [hiveWriterTest] INFO  o.a.h.h.q.Driver - Starting command: use testdb
25002 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=TimeToSubmit start=1466699177561 end=1466699177573 duration=12 from=org.apache.hadoop.hive.ql.Driver>
25002 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=runTasks from=org.apache.hadoop.hive.ql.Driver>
25002 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=task.DDL.Stage-0 from=org.apache.hadoop.hive.ql.Driver>
25003 [hiveWriterTest] INFO  o.a.h.h.q.Driver - Starting task [Stage-0:DDL] in serial mode
25003 [hiveWriterTest] INFO  o.a.h.h.m.HiveMetaStore - 2: get_database: testdb
25003 [hiveWriterTest] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_database: testdb	
25004 [hiveWriterTest] INFO  o.a.h.h.m.HiveMetaStore - 2: get_database: testdb
25004 [hiveWriterTest] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_database: testdb	
25005 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=runTasks start=1466699177573 end=1466699177576 duration=3 from=org.apache.hadoop.hive.ql.Driver>
25006 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=Driver.execute start=1466699177573 end=1466699177577 duration=4 from=org.apache.hadoop.hive.ql.Driver>
25006 [hiveWriterTest] INFO  o.a.h.h.q.Driver - OK
25006 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=releaseLocks from=org.apache.hadoop.hive.ql.Driver>
25006 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=releaseLocks start=1466699177577 end=1466699177577 duration=0 from=org.apache.hadoop.hive.ql.Driver>
25006 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=Driver.run start=1466699177561 end=1466699177577 duration=16 from=org.apache.hadoop.hive.ql.Driver>
25006 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=Driver.run from=org.apache.hadoop.hive.ql.Driver>
25006 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=TimeToSubmit from=org.apache.hadoop.hive.ql.Driver>
25006 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=compile from=org.apache.hadoop.hive.ql.Driver>
25007 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=parse from=org.apache.hadoop.hive.ql.Driver>
25007 [hiveWriterTest] INFO  h.q.p.ParseDriver - Parsing command: alter table test_table2 add if not exists partition  ( city='sunnyvale',state='ca' )
25008 [hiveWriterTest] INFO  h.q.p.ParseDriver - Parse Completed
25008 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=parse start=1466699177578 end=1466699177579 duration=1 from=org.apache.hadoop.hive.ql.Driver>
25010 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=semanticAnalyze from=org.apache.hadoop.hive.ql.Driver>
25011 [hiveWriterTest] INFO  o.a.h.h.m.HiveMetaStore - 2: get_table : db=testdb tbl=test_table2
25011 [hiveWriterTest] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table : db=testdb tbl=test_table2	
25027 [hiveWriterTest] INFO  o.a.h.h.q.Driver - Semantic Analysis Completed
25028 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=semanticAnalyze start=1466699177581 end=1466699177599 duration=18 from=org.apache.hadoop.hive.ql.Driver>
25028 [hiveWriterTest] INFO  o.a.h.h.q.Driver - Returning Hive schema: Schema(fieldSchemas:null, properties:null)
25028 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=compile start=1466699177577 end=1466699177599 duration=22 from=org.apache.hadoop.hive.ql.Driver>
25028 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=acquireReadWriteLocks from=org.apache.hadoop.hive.ql.Driver>
25046 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=acquireReadWriteLocks start=1466699177599 end=1466699177617 duration=18 from=org.apache.hadoop.hive.ql.Driver>
25047 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=Driver.execute from=org.apache.hadoop.hive.ql.Driver>
25047 [hiveWriterTest] INFO  o.a.h.h.q.Driver - Starting command: alter table test_table2 add if not exists partition  ( city='sunnyvale',state='ca' )
25047 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=TimeToSubmit start=1466699177577 end=1466699177618 duration=41 from=org.apache.hadoop.hive.ql.Driver>
25047 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=runTasks from=org.apache.hadoop.hive.ql.Driver>
25047 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=task.DDL.Stage-0 from=org.apache.hadoop.hive.ql.Driver>
25047 [hiveWriterTest] INFO  o.a.h.h.q.Driver - Starting task [Stage-0:DDL] in serial mode
25047 [hiveWriterTest] INFO  o.a.h.h.m.HiveMetaStore - 2: get_table : db=testdb tbl=test_table2
25048 [hiveWriterTest] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table : db=testdb tbl=test_table2	
25057 [hiveWriterTest] INFO  o.a.h.h.m.HiveMetaStore - 2: add_partitions
25058 [hiveWriterTest] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=add_partitions	
25067 [hiveWriterTest] INFO  o.a.h.h.m.HiveMetaStore - Not adding partition Partition(values:[sunnyvale, ca], dbName:testdb, tableName:test_table2, createTime:0, lastAccessTime:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:), FieldSchema(name:msg, type:string, comment:)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:1, serdeInfo:SerDeInfo(name:test_table2, serializationLib:org.apache.hadoop.hive.ql.io.orc.OrcSerde, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), parameters:null) as it already exists
25068 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=runTasks start=1466699177618 end=1466699177639 duration=21 from=org.apache.hadoop.hive.ql.Driver>
25068 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=Driver.execute start=1466699177618 end=1466699177639 duration=21 from=org.apache.hadoop.hive.ql.Driver>
25068 [hiveWriterTest] INFO  o.a.h.h.q.Driver - OK
25069 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - <PERFLOG method=releaseLocks from=org.apache.hadoop.hive.ql.Driver>
25077 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=releaseLocks start=1466699177640 end=1466699177648 duration=8 from=org.apache.hadoop.hive.ql.Driver>
25077 [hiveWriterTest] INFO  o.a.h.h.q.l.PerfLogger - </PERFLOG method=Driver.run start=1466699177577 end=1466699177648 duration=71 from=org.apache.hadoop.hive.ql.Driver>
25112 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_table : db=testdb tbl=test_table2
25112 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table : db=testdb tbl=test_table2	
25112 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
25112 [main] INFO  o.a.h.h.m.ObjectStore - ObjectStore, initialize called
25113 [main] INFO  o.a.h.h.m.MetaStoreDirectSql - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
25115 [main] INFO  D.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
25115 [main] INFO  o.a.h.h.m.ObjectStore - Initialized ObjectStore
25127 [main] INFO  o.a.h.h.m.HiveMetaStore - 0: get_partition : db=testdb tbl=test_table2[sunnyvale,ca]
25127 [main] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_partition : db=testdb tbl=test_table2[sunnyvale,ca]	
25148 [hiveWriterTest] INFO  o.a.h.h.m.HiveMetaStore - 2: get_table : db=testdb tbl=test_table2
25148 [hiveWriterTest] INFO  o.a.h.h.m.H.audit - ugi=travis	ip=unknown-ip-addr	cmd=get_table : db=testdb tbl=test_table2	

-------------------- system-err --------------------
OK
OK

--------------------------------------------------
Looking for errors in ./external/storm-jdbc/target/surefire-reports
Checking ./external/storm-jdbc/target/surefire-reports/TEST-org.apache.storm.jdbc.bolt.JdbcInsertBoltTest.xml
Checking ./external/storm-jdbc/target/surefire-reports/TEST-org.apache.storm.jdbc.bolt.JdbcLookupBoltTest.xml
Checking ./external/storm-jdbc/target/surefire-reports/TEST-org.apache.storm.jdbc.common.UtilTest.xml
Checking ./external/storm-jdbc/target/surefire-reports/TEST-org.apache.storm.jdbc.common.JdbcClientTest.xml
Looking for errors in ./external/storm-kafka/target/surefire-reports
Checking ./external/storm-kafka/target/surefire-reports/TEST-org.apache.storm.kafka.KafkaErrorTest.xml
Checking ./external/storm-kafka/target/surefire-reports/TEST-org.apache.storm.kafka.KafkaUtilsTest.xml
Checking ./external/storm-kafka/target/surefire-reports/TEST-org.apache.storm.kafka.StringKeyValueSchemeTest.xml
Checking ./external/storm-kafka/target/surefire-reports/TEST-org.apache.storm.kafka.ZkCoordinatorTest.xml
Checking ./external/storm-kafka/target/surefire-reports/TEST-org.apache.storm.kafka.TridentKafkaTest.xml
Checking ./external/storm-kafka/target/surefire-reports/TEST-org.apache.storm.kafka.ExponentialBackoffMsgRetryManagerTest.xml
Checking ./external/storm-kafka/target/surefire-reports/TEST-org.apache.storm.kafka.TestStringScheme.xml
Checking ./external/storm-kafka/target/surefire-reports/TEST-org.apache.storm.kafka.DynamicBrokersReaderTest.xml
Checking ./external/storm-kafka/target/surefire-reports/TEST-org.apache.storm.kafka.bolt.KafkaBoltTest.xml
--------------------------------------------------
classname: org.apache.storm.kafka.bolt.KafkaBoltTest / testname: executeWithBrokerDown
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.kafka.bolt.KafkaBoltTest.generateTestTuple(KafkaBoltTest.java:301)
	at org.apache.storm.kafka.bolt.KafkaBoltTest.executeWithBrokerDown(KafkaBoltTest.java:266)

-------------------- system-out --------------------
16:24:23.395 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-e9cd6d11-3391-linux-10.prod.travis-ci.org/172.17.5.158
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
16:24:23.424 [Curator-Framework-0-SendThread(127.0.0.1:33245)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:24:23.443 [Curator-Framework-0] ERROR o.a.c.f.i.CuratorFrameworkImpl - Background operation retry gave up
org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:99) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.curator.framework.imps.CuratorFrameworkImpl.checkBackgroundRetry(CuratorFrameworkImpl.java:728) [curator-framework-2.10.0.jar:?]
	at org.apache.curator.framework.imps.CuratorFrameworkImpl.performBackgroundOperation(CuratorFrameworkImpl.java:857) [curator-framework-2.10.0.jar:?]
	at org.apache.curator.framework.imps.CuratorFrameworkImpl.backgroundOperationsLoop(CuratorFrameworkImpl.java:809) [curator-framework-2.10.0.jar:?]
	at org.apache.curator.framework.imps.CuratorFrameworkImpl.access$300(CuratorFrameworkImpl.java:64) [curator-framework-2.10.0.jar:?]
	at org.apache.curator.framework.imps.CuratorFrameworkImpl$4.call(CuratorFrameworkImpl.java:267) [curator-framework-2.10.0.jar:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:262) [?:1.7.0_76]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:178) [?:1.7.0_76]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:292) [?:1.7.0_76]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [?:1.7.0_76]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [?:1.7.0_76]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
16:24:23.444 [Curator-Framework-0] ERROR o.a.c.f.i.CuratorFrameworkImpl - Background retry gave up
org.apache.curator.CuratorConnectionLossException: KeeperErrorCode = ConnectionLoss
	at org.apache.curator.framework.imps.CuratorFrameworkImpl.performBackgroundOperation(CuratorFrameworkImpl.java:838) [curator-framework-2.10.0.jar:?]
	at org.apache.curator.framework.imps.CuratorFrameworkImpl.backgroundOperationsLoop(CuratorFrameworkImpl.java:809) [curator-framework-2.10.0.jar:?]
	at org.apache.curator.framework.imps.CuratorFrameworkImpl.access$300(CuratorFrameworkImpl.java:64) [curator-framework-2.10.0.jar:?]
	at org.apache.curator.framework.imps.CuratorFrameworkImpl$4.call(CuratorFrameworkImpl.java:267) [curator-framework-2.10.0.jar:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:262) [?:1.7.0_76]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:178) [?:1.7.0_76]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:292) [?:1.7.0_76]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [?:1.7.0_76]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [?:1.7.0_76]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
16:24:23.465 [main-EventThread] INFO  o.a.c.f.s.ConnectionStateManager - State change: SUSPENDED
16:24:23.496 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-e9cd6d11-3391-linux-10.prod.travis-ci.org/172.17.5.158
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
16:24:23.511 [Curator-Framework-0-SendThread(127.0.0.1:33449)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:24:23.540 [main-SendThread(localhost:51237)] WARN  o.a.z.ClientCnxn - Session 0x1557e1372d10000 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:24:23.597 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-e9cd6d11-3391-linux-10.prod.travis-ci.org/172.17.5.158
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
16:24:23.663 [Curator-Framework-0-SendThread(127.0.0.1:50763)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:24:23.684 [Curator-Framework-0] ERROR o.a.c.f.i.CuratorFrameworkImpl - Background operation retry gave up
org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:99) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.curator.framework.imps.CuratorFrameworkImpl.checkBackgroundRetry(CuratorFrameworkImpl.java:728) [curator-framework-2.10.0.jar:?]
	at org.apache.curator.framework.imps.CuratorFrameworkImpl.performBackgroundOperation(CuratorFrameworkImpl.java:857) [curator-framework-2.10.0.jar:?]
	at org.apache.curator.framework.imps.CuratorFrameworkImpl.backgroundOperationsLoop(CuratorFrameworkImpl.java:809) [curator-framework-2.10.0.jar:?]
	at org.apache.curator.framework.imps.CuratorFrameworkImpl.access$300(CuratorFrameworkImpl.java:64) [curator-framework-2.10.0.jar:?]
	at org.apache.curator.framework.imps.CuratorFrameworkImpl$4.call(CuratorFrameworkImpl.java:267) [curator-framework-2.10.0.jar:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:262) [?:1.7.0_76]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:178) [?:1.7.0_76]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:292) [?:1.7.0_76]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [?:1.7.0_76]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [?:1.7.0_76]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
16:24:23.685 [Curator-Framework-0] ERROR o.a.c.f.i.CuratorFrameworkImpl - Background retry gave up
org.apache.curator.CuratorConnectionLossException: KeeperErrorCode = ConnectionLoss
	at org.apache.curator.framework.imps.CuratorFrameworkImpl.performBackgroundOperation(CuratorFrameworkImpl.java:838) [curator-framework-2.10.0.jar:?]
	at org.apache.curator.framework.imps.CuratorFrameworkImpl.backgroundOperationsLoop(CuratorFrameworkImpl.java:809) [curator-framework-2.10.0.jar:?]
	at org.apache.curator.framework.imps.CuratorFrameworkImpl.access$300(CuratorFrameworkImpl.java:64) [curator-framework-2.10.0.jar:?]
	at org.apache.curator.framework.imps.CuratorFrameworkImpl$4.call(CuratorFrameworkImpl.java:267) [curator-framework-2.10.0.jar:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:262) [?:1.7.0_76]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:178) [?:1.7.0_76]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:292) [?:1.7.0_76]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [?:1.7.0_76]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [?:1.7.0_76]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
16:24:23.698 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-e9cd6d11-3391-linux-10.prod.travis-ci.org/172.17.5.158
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
16:24:23.799 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-e9cd6d11-3391-linux-10.prod.travis-ci.org/172.17.5.158
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
16:24:23.900 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-e9cd6d11-3391-linux-10.prod.travis-ci.org/172.17.5.158
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
16:24:23.980 [Curator-Framework-0-SendThread(127.0.0.1:33163)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:24:24.001 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-e9cd6d11-3391-linux-10.prod.travis-ci.org/172.17.5.158
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
16:24:24.031 [Curator-Framework-0-SendThread(127.0.0.1:46779)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:24:24.102 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-e9cd6d11-3391-linux-10.prod.travis-ci.org/172.17.5.158
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
16:24:24.203 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-e9cd6d11-3391-linux-10.prod.travis-ci.org/172.17.5.158
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
16:24:24.304 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-e9cd6d11-3391-linux-10.prod.travis-ci.org/172.17.5.158
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
16:24:24.406 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-e9cd6d11-3391-linux-10.prod.travis-ci.org/172.17.5.158
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
16:24:24.434 [main] INFO  o.a.c.f.i.CuratorFrameworkImpl - Starting
16:24:24.436 [main] INFO  k.u.VerifiableProperties - Verifying properties
16:24:24.436 [main] INFO  k.u.VerifiableProperties - Property broker.id is overridden to 0
16:24:24.436 [main] INFO  k.u.VerifiableProperties - Property log.dirs is overridden to /tmp/kafka/logs/kafka-test-40013
16:24:24.436 [main] INFO  k.u.VerifiableProperties - Property port is overridden to 40013
16:24:24.436 [main] INFO  k.u.VerifiableProperties - Property zookeeper.connect is overridden to 127.0.0.1:43444
16:24:24.437 [main] INFO  k.s.KafkaServer - [Kafka Server 0], starting
16:24:24.437 [main] INFO  k.s.KafkaServer - [Kafka Server 0], Connecting to zookeeper on 127.0.0.1:43444
16:24:24.437 [ZkClient-EventThread-941-127.0.0.1:43444] INFO  o.I.z.ZkEventThread - Starting ZkClient event thread.
16:24:24.440 [main-EventThread] INFO  o.a.c.f.s.ConnectionStateManager - State change: CONNECTED
16:24:24.440 [main-EventThread] INFO  o.I.z.ZkClient - zookeeper state changed (SyncConnected)
16:24:24.443 [Curator-Framework-0] WARN  o.a.c.ConnectionState - Connection attempt unsuccessful after 1125 (greater than max timeout of 1000). Resetting connection and trying again with a new connection.
16:24:24.453 [main] INFO  k.l.LogManager - Log directory '/tmp/kafka/logs/kafka-test-40013' not found, creating it.
16:24:24.453 [main] INFO  k.l.LogManager - Loading logs.
16:24:24.453 [main] INFO  k.l.LogManager - Logs loading complete.
16:24:24.453 [main] INFO  k.l.LogManager - Starting log cleanup with a period of 300000 ms.
16:24:24.454 [main] INFO  k.l.LogManager - Starting log flusher with a default period of 9223372036854775807 ms.
16:24:24.457 [main] INFO  k.n.Acceptor - Awaiting socket connections on 0.0.0.0:40013.
16:24:24.457 [main] INFO  k.n.SocketServer - [Socket Server on Broker 0], Started
16:24:24.470 [Curator-Framework-0] WARN  o.a.c.ConnectionState - Connection attempt unsuccessful after 1005 (greater than max timeout of 1000). Resetting connection and trying again with a new connection.
16:24:24.471 [main] INFO  k.u.Mx4jLoader$ - Will not load MX4J, mx4j-tools.jar is not in the classpath
16:24:24.471 [main] INFO  k.c.KafkaController - [Controller 0]: Controller starting up
16:24:24.476 [main] INFO  k.s.ZookeeperLeaderElector - 0 successfully elected as leader
16:24:24.476 [main] INFO  k.c.KafkaController - [Controller 0]: Broker 0 starting become controller state transition
16:24:24.479 [main] INFO  k.c.KafkaController - [Controller 0]: Controller 0 incremented epoch to 1
16:24:24.483 [main] INFO  k.c.KafkaController - [Controller 0]: Partitions undergoing preferred replica election: 
16:24:24.483 [main] INFO  k.c.KafkaController - [Controller 0]: Partitions that completed preferred replica election: 
16:24:24.483 [main] INFO  k.c.KafkaController - [Controller 0]: Resuming preferred replica election for partitions: 
16:24:24.483 [main] INFO  k.c.KafkaController - [Controller 0]: Partitions being reassigned: Map()
16:24:24.483 [main] INFO  k.c.KafkaController - [Controller 0]: Partitions already reassigned: List()
16:24:24.483 [main] INFO  k.c.KafkaController - [Controller 0]: Resuming reassignment of partitions: Map()
16:24:24.484 [main] INFO  k.c.KafkaController - [Controller 0]: List of topics to be deleted: 
16:24:24.484 [main] INFO  k.c.KafkaController - [Controller 0]: List of topics ineligible for deletion: 
16:24:24.484 [main] INFO  k.c.KafkaController - [Controller 0]: Currently active brokers in the cluster: Set()
16:24:24.484 [main] INFO  k.c.KafkaController - [Controller 0]: Currently shutting brokers in the cluster: Set()
16:24:24.484 [main] INFO  k.c.KafkaController - [Controller 0]: Current list of topics in the cluster: Set()
16:24:24.484 [main] INFO  k.c.ReplicaStateMachine - [Replica state machine on controller 0]: Started replica state machine with initial state -> Map()
16:24:24.484 [main] INFO  k.c.PartitionStateMachine - [Partition state machine on Controller 0]: Started partition state machine with initial state -> Map()
16:24:24.484 [main] INFO  k.c.KafkaController - [Controller 0]: Broker 0 is ready to serve as the new controller with epoch 1
16:24:24.484 [main] INFO  k.c.KafkaController - [Controller 0]: Starting preferred replica leader election for partitions 
16:24:24.485 [main] INFO  k.c.PartitionStateMachine - [Partition state machine on Controller 0]: Invoking state change to OnlinePartition for partitions 
16:24:24.486 [main] INFO  k.c.KafkaController - [Controller 0]: starting the partition rebalance scheduler
16:24:24.486 [main] INFO  k.c.KafkaController - [Controller 0]: Controller startup complete
16:24:24.488 [ZkClient-EventThread-941-127.0.0.1:43444] INFO  k.s.ZookeeperLeaderElector$LeaderChangeListener - New leader is 0
16:24:24.492 [main] INFO  k.u.ZkUtils$ - Registered broker 0 at path /brokers/ids/0 with address testing-worker-linux-docker-e9cd6d11-3391-linux-10.prod.travis-ci.org:40013.
16:24:24.492 [main] INFO  k.s.KafkaServer - [Kafka Server 0], started
16:24:24.496 [main] INFO  o.a.k.c.p.ProducerConfig - ProducerConfig values: 
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	block.on.buffer.full = true
	retry.backoff.ms = 100
	buffer.memory = 33554432
	batch.size = 16384
	metrics.sample.window.ms = 30000
	metadata.max.age.ms = 300000
	receive.buffer.bytes = 32768
	timeout.ms = 30000
	max.in.flight.requests.per.connection = 5
	bootstrap.servers = [localhost:40013]
	metric.reporters = []
	client.id = 
	compression.type = none
	retries = 0
	max.request.size = 1048576
	send.buffer.bytes = 131072
	acks = 1
	reconnect.backoff.ms = 10
	linger.ms = 0
	metrics.num.samples = 2
	metadata.fetch.timeout.ms = 1000

16:24:24.499 [ZkClient-EventThread-941-127.0.0.1:43444] INFO  k.c.ReplicaStateMachine$BrokerChangeListener - [BrokerChangeListener on Controller 0]: Broker change listener fired for path /brokers/ids with children 0
16:24:24.506 [main] INFO  k.s.KafkaServer - [Kafka Server 0], shutting down
16:24:24.507 [main] INFO  k.s.KafkaServer - [Kafka Server 0], Starting controlled shutdown
16:24:24.507 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-e9cd6d11-3391-linux-10.prod.travis-ci.org/172.17.5.158
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
16:24:24.516 [ZkClient-EventThread-941-127.0.0.1:43444] INFO  k.c.ReplicaStateMachine$BrokerChangeListener - [BrokerChangeListener on Controller 0]: Newly added brokers: 0, deleted brokers: , all live brokers: 0
16:24:24.517 [ZkClient-EventThread-941-127.0.0.1:43444] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Controller 0 connected to id:0,host:testing-worker-linux-docker-e9cd6d11-3391-linux-10.prod.travis-ci.org,port:40013 for sending state change requests
16:24:24.517 [ZkClient-EventThread-941-127.0.0.1:43444] INFO  k.c.KafkaController - [Controller 0]: New broker startup callback for 0
16:24:24.518 [Controller-0-to-broker-0-send-thread] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Starting 
16:24:24.777 [kafka-request-handler-0] INFO  k.c.KafkaController - [Controller 0]: Shutting down broker 0
16:24:24.794 [Curator-Framework-0-SendThread(127.0.0.1:50763)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:24:24.784 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-e9cd6d11-3391-linux-10.prod.travis-ci.org/172.17.5.158
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
16:24:24.798 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-e9cd6d11-3391-linux-10.prod.travis-ci.org/172.17.5.158
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
16:24:24.783 [Curator-Framework-0-SendThread(127.0.0.1:33449)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:24:24.783 [Curator-Framework-0-SendThread(127.0.0.1:33245)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:24:24.782 [main-SendThread(localhost:48389)] WARN  o.a.z.ClientCnxn - Session 0x1557e13639f0000 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:24:24.781 [Curator-Framework-0] WARN  o.a.c.ConnectionState - Connection attempt unsuccessful after 1445 (greater than max timeout of 1000). Resetting connection and trying again with a new connection.
16:24:24.799 [main] INFO  k.s.KafkaServer - [Kafka Server 0], Controlled shutdown succeeded
16:24:24.799 [main] INFO  k.n.SocketServer - [Socket Server on Broker 0], Shutting down
16:24:24.799 [kafka-network-thread-40013-1] INFO  k.n.Processor - Closing socket connection to /172.17.5.158.
16:24:24.800 [main] INFO  k.n.SocketServer - [Socket Server on Broker 0], Shutdown completed
16:24:24.801 [main] INFO  k.s.KafkaRequestHandlerPool - [Kafka Request Handler on Broker 0], shutting down
16:24:24.801 [main] INFO  k.s.KafkaRequestHandlerPool - [Kafka Request Handler on Broker 0], shut down completely
16:24:24.811 [Curator-Framework-0] INFO  o.a.c.f.s.ConnectionStateManager - State change: LOST
16:24:24.811 [Curator-Framework-0] ERROR o.a.c.f.i.CuratorFrameworkImpl - Background operation retry gave up
org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:99) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.curator.framework.imps.CuratorFrameworkImpl.checkBackgroundRetry(CuratorFrameworkImpl.java:728) [curator-framework-2.10.0.jar:?]
	at org.apache.curator.framework.imps.CuratorFrameworkImpl.performBackgroundOperation(CuratorFrameworkImpl.java:857) [curator-framework-2.10.0.jar:?]
	at org.apache.curator.framework.imps.CuratorFrameworkImpl.backgroundOperationsLoop(CuratorFrameworkImpl.java:809) [curator-framework-2.10.0.jar:?]
	at org.apache.curator.framework.imps.CuratorFrameworkImpl.access$300(CuratorFrameworkImpl.java:64) [curator-framework-2.10.0.jar:?]
	at org.apache.curator.framework.imps.CuratorFrameworkImpl$4.call(CuratorFrameworkImpl.java:267) [curator-framework-2.10.0.jar:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:262) [?:1.7.0_76]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:178) [?:1.7.0_76]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:292) [?:1.7.0_76]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [?:1.7.0_76]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [?:1.7.0_76]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
16:24:24.812 [Curator-Framework-0] ERROR o.a.c.f.i.CuratorFrameworkImpl - Background retry gave up
org.apache.curator.CuratorConnectionLossException: KeeperErrorCode = ConnectionLoss
	at org.apache.curator.framework.imps.CuratorFrameworkImpl.performBackgroundOperation(CuratorFrameworkImpl.java:838) [curator-framework-2.10.0.jar:?]
	at org.apache.curator.framework.imps.CuratorFrameworkImpl.backgroundOperationsLoop(CuratorFrameworkImpl.java:809) [curator-framework-2.10.0.jar:?]
	at org.apache.curator.framework.imps.CuratorFrameworkImpl.access$300(CuratorFrameworkImpl.java:64) [curator-framework-2.10.0.jar:?]
	at org.apache.curator.framework.imps.CuratorFrameworkImpl$4.call(CuratorFrameworkImpl.java:267) [curator-framework-2.10.0.jar:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:262) [?:1.7.0_76]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:178) [?:1.7.0_76]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:292) [?:1.7.0_76]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [?:1.7.0_76]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [?:1.7.0_76]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
16:24:24.897 [Curator-Framework-0-SendThread(127.0.0.1:50639)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:24:24.898 [Curator-Framework-0-SendThread(127.0.0.1:50763)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:24:24.899 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-e9cd6d11-3391-linux-10.prod.travis-ci.org/172.17.5.158
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
16:24:24.918 [Curator-Framework-0] ERROR o.a.c.f.i.CuratorFrameworkImpl - Background operation retry gave up
org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:99) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.curator.framework.imps.CuratorFrameworkImpl.checkBackgroundRetry(CuratorFrameworkImpl.java:728) [curator-framework-2.10.0.jar:?]
	at org.apache.curator.framework.imps.CuratorFrameworkImpl.performBackgroundOperation(CuratorFrameworkImpl.java:857) [curator-framework-2.10.0.jar:?]
	at org.apache.curator.framework.imps.CuratorFrameworkImpl.backgroundOperationsLoop(CuratorFrameworkImpl.java:809) [curator-framework-2.10.0.jar:?]
	at org.apache.curator.framework.imps.CuratorFrameworkImpl.access$300(CuratorFrameworkImpl.java:64) [curator-framework-2.10.0.jar:?]
	at org.apache.curator.framework.imps.CuratorFrameworkImpl$4.call(CuratorFrameworkImpl.java:267) [curator-framework-2.10.0.jar:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:262) [?:1.7.0_76]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:178) [?:1.7.0_76]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:292) [?:1.7.0_76]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [?:1.7.0_76]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [?:1.7.0_76]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
16:24:24.919 [Curator-Framework-0] ERROR o.a.c.f.i.CuratorFrameworkImpl - Background retry gave up
org.apache.curator.CuratorConnectionLossException: KeeperErrorCode = ConnectionLoss
	at org.apache.curator.framework.imps.CuratorFrameworkImpl.performBackgroundOperation(CuratorFrameworkImpl.java:838) [curator-framework-2.10.0.jar:?]
	at org.apache.curator.framework.imps.CuratorFrameworkImpl.backgroundOperationsLoop(CuratorFrameworkImpl.java:809) [curator-framework-2.10.0.jar:?]
	at org.apache.curator.framework.imps.CuratorFrameworkImpl.access$300(CuratorFrameworkImpl.java:64) [curator-framework-2.10.0.jar:?]
	at org.apache.curator.framework.imps.CuratorFrameworkImpl$4.call(CuratorFrameworkImpl.java:267) [curator-framework-2.10.0.jar:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:262) [?:1.7.0_76]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:178) [?:1.7.0_76]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:292) [?:1.7.0_76]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [?:1.7.0_76]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [?:1.7.0_76]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
16:24:24.919 [Curator-Framework-0] INFO  o.a.c.f.s.ConnectionStateManager - State change: LOST
16:24:24.919 [Curator-Framework-0] ERROR o.a.c.f.i.CuratorFrameworkImpl - Background operation retry gave up
org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:99) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.curator.framework.imps.CuratorFrameworkImpl.checkBackgroundRetry(CuratorFrameworkImpl.java:728) [curator-framework-2.10.0.jar:?]
	at org.apache.curator.framework.imps.CuratorFrameworkImpl.performBackgroundOperation(CuratorFrameworkImpl.java:857) [curator-framework-2.10.0.jar:?]
	at org.apache.curator.framework.imps.CuratorFrameworkImpl.backgroundOperationsLoop(CuratorFrameworkImpl.java:809) [curator-framework-2.10.0.jar:?]
	at org.apache.curator.framework.imps.CuratorFrameworkImpl.access$300(CuratorFrameworkImpl.java:64) [curator-framework-2.10.0.jar:?]
	at org.apache.curator.framework.imps.CuratorFrameworkImpl$4.call(CuratorFrameworkImpl.java:267) [curator-framework-2.10.0.jar:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:262) [?:1.7.0_76]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:178) [?:1.7.0_76]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:292) [?:1.7.0_76]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [?:1.7.0_76]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [?:1.7.0_76]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
16:24:24.919 [Curator-Framework-0] ERROR o.a.c.f.i.CuratorFrameworkImpl - Background retry gave up
org.apache.curator.CuratorConnectionLossException: KeeperErrorCode = ConnectionLoss
	at org.apache.curator.framework.imps.CuratorFrameworkImpl.performBackgroundOperation(CuratorFrameworkImpl.java:838) [curator-framework-2.10.0.jar:?]
	at org.apache.curator.framework.imps.CuratorFrameworkImpl.backgroundOperationsLoop(CuratorFrameworkImpl.java:809) [curator-framework-2.10.0.jar:?]
	at org.apache.curator.framework.imps.CuratorFrameworkImpl.access$300(CuratorFrameworkImpl.java:64) [curator-framework-2.10.0.jar:?]
	at org.apache.curator.framework.imps.CuratorFrameworkImpl$4.call(CuratorFrameworkImpl.java:267) [curator-framework-2.10.0.jar:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:262) [?:1.7.0_76]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:178) [?:1.7.0_76]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:292) [?:1.7.0_76]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [?:1.7.0_76]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [?:1.7.0_76]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
16:24:25.000 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-e9cd6d11-3391-linux-10.prod.travis-ci.org/172.17.5.158
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
16:24:25.081 [Curator-Framework-0-SendThread(127.0.0.1:33163)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:24:25.101 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-e9cd6d11-3391-linux-10.prod.travis-ci.org/172.17.5.158
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
16:24:25.133 [Curator-Framework-0-SendThread(127.0.0.1:46779)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:24:25.182 [main] INFO  k.s.ReplicaManager - [Replica Manager on Broker 0]: Shut down
16:24:25.183 [main] INFO  k.s.ReplicaFetcherManager - [ReplicaFetcherManager on broker 0] shutting down
16:24:25.183 [main] INFO  k.s.ReplicaFetcherManager - [ReplicaFetcherManager on broker 0] shutdown completed
16:24:25.183 [main] INFO  k.s.ReplicaManager - [Replica Manager on Broker 0]: Shut down completely
16:24:25.183 [main] INFO  k.l.LogManager - Shutting down.
16:24:25.183 [main] INFO  k.l.LogManager - Shutdown complete.
16:24:25.184 [main] INFO  k.c.PartitionStateMachine - [Partition state machine on Controller 0]: Stopped partition state machine
16:24:25.184 [main] INFO  k.c.ReplicaStateMachine - [Replica state machine on controller 0]: Stopped replica state machine
16:24:25.184 [main] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Shutting down
16:24:25.184 [Controller-0-to-broker-0-send-thread] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Stopped 
16:24:25.186 [main] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Shutdown completed
16:24:25.187 [main-SendThread(localhost:51237)] WARN  o.a.z.ClientCnxn - Session 0x1557e1372d10000 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:24:25.187 [ZkClient-EventThread-941-127.0.0.1:43444] INFO  o.I.z.ZkEventThread - Terminate ZkClient event thread.
16:24:25.189 [main] INFO  k.s.KafkaServer - [Kafka Server 0], shut down completed
16:24:25.189 [Curator-Framework-0] INFO  o.a.c.f.i.CuratorFrameworkImpl - backgroundOperationsLoop exiting

--------------------------------------------------
--------------------------------------------------
classname: org.apache.storm.kafka.bolt.KafkaBoltTest / testname: executeWithoutKey
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.kafka.bolt.KafkaBoltTest.generateTestTuple(KafkaBoltTest.java:301)
	at org.apache.storm.kafka.bolt.KafkaBoltTest.executeWithoutKey(KafkaBoltTest.java:255)

-------------------- system-out --------------------
16:24:26.722 [main-SendThread(localhost:48389)] WARN  o.a.z.ClientCnxn - Session 0x1557e13639f0000 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:24:26.727 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-e9cd6d11-3391-linux-10.prod.travis-ci.org/172.17.5.158
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
16:24:26.828 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-e9cd6d11-3391-linux-10.prod.travis-ci.org/172.17.5.158
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
16:24:26.929 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-e9cd6d11-3391-linux-10.prod.travis-ci.org/172.17.5.158
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
16:24:27.001 [Curator-Framework-0-SendThread(127.0.0.1:33245)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:24:27.001 [Curator-Framework-0-SendThread(127.0.0.1:33449)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:24:27.031 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-e9cd6d11-3391-linux-10.prod.travis-ci.org/172.17.5.158
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
16:24:27.101 [Curator-Framework-0-SendThread(127.0.0.1:50763)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:24:27.132 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-e9cd6d11-3391-linux-10.prod.travis-ci.org/172.17.5.158
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
16:24:27.201 [Curator-Framework-0-SendThread(127.0.0.1:50639)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:24:27.233 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-e9cd6d11-3391-linux-10.prod.travis-ci.org/172.17.5.158
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
16:24:27.283 [Curator-Framework-0-SendThread(127.0.0.1:33163)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:24:27.334 [Curator-Framework-0-SendThread(127.0.0.1:46779)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:24:27.334 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-e9cd6d11-3391-linux-10.prod.travis-ci.org/172.17.5.158
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
16:24:27.436 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-e9cd6d11-3391-linux-10.prod.travis-ci.org/172.17.5.158
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
16:24:27.537 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-e9cd6d11-3391-linux-10.prod.travis-ci.org/172.17.5.158
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
16:24:27.638 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-e9cd6d11-3391-linux-10.prod.travis-ci.org/172.17.5.158
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
16:24:27.660 [main-SendThread(localhost:48389)] WARN  o.a.z.ClientCnxn - Session 0x1557e13639f0000 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:24:27.695 [main-SendThread(localhost:51237)] WARN  o.a.z.ClientCnxn - Session 0x1557e1372d10000 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:24:27.696 [main] INFO  o.a.c.f.i.CuratorFrameworkImpl - Starting
16:24:27.699 [main] INFO  k.u.VerifiableProperties - Verifying properties
16:24:27.699 [main] INFO  k.u.VerifiableProperties - Property broker.id is overridden to 0
16:24:27.699 [main-EventThread] INFO  o.a.c.f.s.ConnectionStateManager - State change: CONNECTED
16:24:27.700 [main] INFO  k.u.VerifiableProperties - Property log.dirs is overridden to /tmp/kafka/logs/kafka-test-59756
16:24:27.700 [main] INFO  k.u.VerifiableProperties - Property port is overridden to 59756
16:24:27.700 [main] INFO  k.u.VerifiableProperties - Property zookeeper.connect is overridden to 127.0.0.1:36960
16:24:27.700 [main] INFO  k.s.KafkaServer - [Kafka Server 0], starting
16:24:27.700 [main] INFO  k.s.KafkaServer - [Kafka Server 0], Connecting to zookeeper on 127.0.0.1:36960
16:24:27.701 [ZkClient-EventThread-1021-127.0.0.1:36960] INFO  o.I.z.ZkEventThread - Starting ZkClient event thread.
16:24:27.702 [main-EventThread] INFO  o.I.z.ZkClient - zookeeper state changed (SyncConnected)
16:24:27.715 [main] INFO  k.l.LogManager - Log directory '/tmp/kafka/logs/kafka-test-59756' not found, creating it.
16:24:27.715 [main] INFO  k.l.LogManager - Loading logs.
16:24:27.716 [main] INFO  k.l.LogManager - Logs loading complete.
16:24:27.716 [main] INFO  k.l.LogManager - Starting log cleanup with a period of 300000 ms.
16:24:27.716 [main] INFO  k.l.LogManager - Starting log flusher with a default period of 9223372036854775807 ms.
16:24:27.719 [main] INFO  k.n.Acceptor - Awaiting socket connections on 0.0.0.0:59756.
16:24:27.719 [main] INFO  k.n.SocketServer - [Socket Server on Broker 0], Started
16:24:27.723 [main] INFO  k.u.Mx4jLoader$ - Will not load MX4J, mx4j-tools.jar is not in the classpath
16:24:27.724 [main] INFO  k.c.KafkaController - [Controller 0]: Controller starting up
16:24:27.726 [main] INFO  k.s.ZookeeperLeaderElector - 0 successfully elected as leader
16:24:27.726 [main] INFO  k.c.KafkaController - [Controller 0]: Broker 0 starting become controller state transition
16:24:27.728 [main] INFO  k.c.KafkaController - [Controller 0]: Controller 0 incremented epoch to 1
16:24:27.732 [main] INFO  k.c.KafkaController - [Controller 0]: Partitions undergoing preferred replica election: 
16:24:27.732 [main] INFO  k.c.KafkaController - [Controller 0]: Partitions that completed preferred replica election: 
16:24:27.732 [main] INFO  k.c.KafkaController - [Controller 0]: Resuming preferred replica election for partitions: 
16:24:27.733 [main] INFO  k.c.KafkaController - [Controller 0]: Partitions being reassigned: Map()
16:24:27.733 [main] INFO  k.c.KafkaController - [Controller 0]: Partitions already reassigned: List()
16:24:27.733 [main] INFO  k.c.KafkaController - [Controller 0]: Resuming reassignment of partitions: Map()
16:24:27.733 [main] INFO  k.c.KafkaController - [Controller 0]: List of topics to be deleted: 
16:24:27.734 [main] INFO  k.c.KafkaController - [Controller 0]: List of topics ineligible for deletion: 
16:24:27.734 [main] INFO  k.c.KafkaController - [Controller 0]: Currently active brokers in the cluster: Set()
16:24:27.734 [main] INFO  k.c.KafkaController - [Controller 0]: Currently shutting brokers in the cluster: Set()
16:24:27.734 [main] INFO  k.c.KafkaController - [Controller 0]: Current list of topics in the cluster: Set()
16:24:27.734 [main] INFO  k.c.ReplicaStateMachine - [Replica state machine on controller 0]: Started replica state machine with initial state -> Map()
16:24:27.734 [main] INFO  k.c.PartitionStateMachine - [Partition state machine on Controller 0]: Started partition state machine with initial state -> Map()
16:24:27.735 [main] INFO  k.c.KafkaController - [Controller 0]: Broker 0 is ready to serve as the new controller with epoch 1
16:24:27.735 [main] INFO  k.c.KafkaController - [Controller 0]: Starting preferred replica leader election for partitions 
16:24:27.735 [main] INFO  k.c.PartitionStateMachine - [Partition state machine on Controller 0]: Invoking state change to OnlinePartition for partitions 
16:24:27.736 [main] INFO  k.c.KafkaController - [Controller 0]: starting the partition rebalance scheduler
16:24:27.736 [main] INFO  k.c.KafkaController - [Controller 0]: Controller startup complete
16:24:27.738 [ZkClient-EventThread-1021-127.0.0.1:36960] INFO  k.s.ZookeeperLeaderElector$LeaderChangeListener - New leader is 0
16:24:27.739 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-e9cd6d11-3391-linux-10.prod.travis-ci.org/172.17.5.158
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
16:24:27.742 [main] INFO  k.u.ZkUtils$ - Registered broker 0 at path /brokers/ids/0 with address testing-worker-linux-docker-e9cd6d11-3391-linux-10.prod.travis-ci.org:59756.
16:24:27.742 [main] INFO  k.s.KafkaServer - [Kafka Server 0], started
16:24:27.743 [ZkClient-EventThread-1021-127.0.0.1:36960] INFO  k.c.ReplicaStateMachine$BrokerChangeListener - [BrokerChangeListener on Controller 0]: Broker change listener fired for path /brokers/ids with children 0
16:24:27.746 [main] INFO  o.a.k.c.p.ProducerConfig - ProducerConfig values: 
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	block.on.buffer.full = true
	retry.backoff.ms = 100
	buffer.memory = 33554432
	batch.size = 16384
	metrics.sample.window.ms = 30000
	metadata.max.age.ms = 300000
	receive.buffer.bytes = 32768
	timeout.ms = 30000
	max.in.flight.requests.per.connection = 5
	bootstrap.servers = [localhost:59756]
	metric.reporters = []
	client.id = 
	compression.type = none
	retries = 0
	max.request.size = 1048576
	send.buffer.bytes = 131072
	acks = 1
	reconnect.backoff.ms = 10
	linger.ms = 0
	metrics.num.samples = 2
	metadata.fetch.timeout.ms = 1000

16:24:27.749 [ZkClient-EventThread-1021-127.0.0.1:36960] INFO  k.c.ReplicaStateMachine$BrokerChangeListener - [BrokerChangeListener on Controller 0]: Newly added brokers: 0, deleted brokers: , all live brokers: 0
16:24:27.750 [ZkClient-EventThread-1021-127.0.0.1:36960] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Controller 0 connected to id:0,host:testing-worker-linux-docker-e9cd6d11-3391-linux-10.prod.travis-ci.org,port:59756 for sending state change requests
16:24:27.750 [ZkClient-EventThread-1021-127.0.0.1:36960] INFO  k.c.KafkaController - [Controller 0]: New broker startup callback for 0
16:24:27.750 [Controller-0-to-broker-0-send-thread] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Starting 
16:24:27.754 [main] INFO  k.s.KafkaServer - [Kafka Server 0], shutting down
16:24:27.754 [main] INFO  k.s.KafkaServer - [Kafka Server 0], Starting controlled shutdown
16:24:27.760 [kafka-request-handler-0] INFO  k.c.KafkaController - [Controller 0]: Shutting down broker 0
16:24:27.760 [main] INFO  k.s.KafkaServer - [Kafka Server 0], Controlled shutdown succeeded
16:24:27.761 [kafka-network-thread-59756-1] INFO  k.n.Processor - Closing socket connection to /172.17.5.158.
16:24:27.761 [main] INFO  k.n.SocketServer - [Socket Server on Broker 0], Shutting down
16:24:27.762 [main] INFO  k.n.SocketServer - [Socket Server on Broker 0], Shutdown completed
16:24:27.762 [main] INFO  k.s.KafkaRequestHandlerPool - [Kafka Request Handler on Broker 0], shutting down
16:24:27.763 [main] INFO  k.s.KafkaRequestHandlerPool - [Kafka Request Handler on Broker 0], shut down completely
16:24:27.840 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-e9cd6d11-3391-linux-10.prod.travis-ci.org/172.17.5.158
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
16:24:27.941 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-e9cd6d11-3391-linux-10.prod.travis-ci.org/172.17.5.158
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
16:24:28.043 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-e9cd6d11-3391-linux-10.prod.travis-ci.org/172.17.5.158
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
16:24:28.102 [Curator-Framework-0-SendThread(127.0.0.1:33449)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:24:28.102 [Curator-Framework-0-SendThread(127.0.0.1:33245)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:24:28.122 [main] INFO  k.s.ReplicaManager - [Replica Manager on Broker 0]: Shut down
16:24:28.123 [main] INFO  k.s.ReplicaFetcherManager - [ReplicaFetcherManager on broker 0] shutting down
16:24:28.123 [main] INFO  k.s.ReplicaFetcherManager - [ReplicaFetcherManager on broker 0] shutdown completed
16:24:28.123 [main] INFO  k.s.ReplicaManager - [Replica Manager on Broker 0]: Shut down completely
16:24:28.123 [main] INFO  k.l.LogManager - Shutting down.
16:24:28.124 [main] INFO  k.l.LogManager - Shutdown complete.
16:24:28.124 [main] INFO  k.c.PartitionStateMachine - [Partition state machine on Controller 0]: Stopped partition state machine
16:24:28.124 [main] INFO  k.c.ReplicaStateMachine - [Replica state machine on controller 0]: Stopped replica state machine
16:24:28.124 [main] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Shutting down
16:24:28.124 [Controller-0-to-broker-0-send-thread] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Stopped 
16:24:28.124 [main] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Shutdown completed
16:24:28.125 [ZkClient-EventThread-1021-127.0.0.1:36960] INFO  o.I.z.ZkEventThread - Terminate ZkClient event thread.
16:24:28.127 [main] INFO  k.s.KafkaServer - [Kafka Server 0], shut down completed
16:24:28.127 [Curator-Framework-0] INFO  o.a.c.f.i.CuratorFrameworkImpl - backgroundOperationsLoop exiting

--------------------------------------------------
--------------------------------------------------
classname: org.apache.storm.kafka.bolt.KafkaBoltTest / testname: executeWithByteArrayKeyAndMessageFire
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.kafka.bolt.KafkaBoltTest.generateTestTuple(KafkaBoltTest.java:290)
	at org.apache.storm.kafka.bolt.KafkaBoltTest.executeWithByteArrayKeyAndMessageFire(KafkaBoltTest.java:176)

-------------------- system-out --------------------
16:24:28.147 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-e9cd6d11-3391-linux-10.prod.travis-ci.org/172.17.5.158
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
16:24:28.202 [Curator-Framework-0-SendThread(127.0.0.1:50763)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:24:28.247 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-e9cd6d11-3391-linux-10.prod.travis-ci.org/172.17.5.158
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
16:24:28.302 [Curator-Framework-0-SendThread(127.0.0.1:50639)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:24:28.349 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-e9cd6d11-3391-linux-10.prod.travis-ci.org/172.17.5.158
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
16:24:28.383 [main-SendThread(localhost:51237)] WARN  o.a.z.ClientCnxn - Session 0x1557e1372d10000 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:24:28.384 [Curator-Framework-0-SendThread(127.0.0.1:33163)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:24:28.435 [Curator-Framework-0-SendThread(127.0.0.1:46779)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:24:28.450 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-e9cd6d11-3391-linux-10.prod.travis-ci.org/172.17.5.158
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
16:24:28.551 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-e9cd6d11-3391-linux-10.prod.travis-ci.org/172.17.5.158
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
16:24:28.652 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-e9cd6d11-3391-linux-10.prod.travis-ci.org/172.17.5.158
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
16:24:28.753 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-e9cd6d11-3391-linux-10.prod.travis-ci.org/172.17.5.158
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
16:24:28.854 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-e9cd6d11-3391-linux-10.prod.travis-ci.org/172.17.5.158
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
16:24:28.956 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-e9cd6d11-3391-linux-10.prod.travis-ci.org/172.17.5.158
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
16:24:29.057 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-e9cd6d11-3391-linux-10.prod.travis-ci.org/172.17.5.158
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
16:24:29.104 [main-SendThread(localhost:48389)] WARN  o.a.z.ClientCnxn - Session 0x1557e13639f0000 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:24:29.159 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-e9cd6d11-3391-linux-10.prod.travis-ci.org/172.17.5.158
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
16:24:29.204 [main] INFO  o.a.c.f.i.CuratorFrameworkImpl - Starting
16:24:29.204 [Curator-Framework-0-SendThread(127.0.0.1:33245)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:24:29.203 [Curator-Framework-0-SendThread(127.0.0.1:33449)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:24:29.208 [main-EventThread] INFO  o.a.c.f.s.ConnectionStateManager - State change: CONNECTED
16:24:29.208 [main] INFO  k.u.VerifiableProperties - Verifying properties
16:24:29.208 [main] INFO  k.u.VerifiableProperties - Property broker.id is overridden to 0
16:24:29.209 [main] INFO  k.u.VerifiableProperties - Property log.dirs is overridden to /tmp/kafka/logs/kafka-test-36830
16:24:29.209 [main] INFO  k.u.VerifiableProperties - Property port is overridden to 36830
16:24:29.209 [main] INFO  k.u.VerifiableProperties - Property zookeeper.connect is overridden to 127.0.0.1:43833
16:24:29.209 [main] INFO  k.s.KafkaServer - [Kafka Server 0], starting
16:24:29.209 [main] INFO  k.s.KafkaServer - [Kafka Server 0], Connecting to zookeeper on 127.0.0.1:43833
16:24:29.210 [ZkClient-EventThread-1057-127.0.0.1:43833] INFO  o.I.z.ZkEventThread - Starting ZkClient event thread.
16:24:29.211 [main-EventThread] INFO  o.I.z.ZkClient - zookeeper state changed (SyncConnected)
16:24:29.224 [main] INFO  k.l.LogManager - Log directory '/tmp/kafka/logs/kafka-test-36830' not found, creating it.
16:24:29.225 [main] INFO  k.l.LogManager - Loading logs.
16:24:29.225 [main] INFO  k.l.LogManager - Logs loading complete.
16:24:29.225 [main] INFO  k.l.LogManager - Starting log cleanup with a period of 300000 ms.
16:24:29.226 [main] INFO  k.l.LogManager - Starting log flusher with a default period of 9223372036854775807 ms.
16:24:29.229 [main] INFO  k.n.Acceptor - Awaiting socket connections on 0.0.0.0:36830.
16:24:29.229 [main] INFO  k.n.SocketServer - [Socket Server on Broker 0], Started
16:24:29.233 [main] INFO  k.u.Mx4jLoader$ - Will not load MX4J, mx4j-tools.jar is not in the classpath
16:24:29.233 [main] INFO  k.c.KafkaController - [Controller 0]: Controller starting up
16:24:29.236 [main] INFO  k.s.ZookeeperLeaderElector - 0 successfully elected as leader
16:24:29.236 [main] INFO  k.c.KafkaController - [Controller 0]: Broker 0 starting become controller state transition
16:24:29.238 [main] INFO  k.c.KafkaController - [Controller 0]: Controller 0 incremented epoch to 1
16:24:29.242 [main] INFO  k.c.KafkaController - [Controller 0]: Partitions undergoing preferred replica election: 
16:24:29.242 [main] INFO  k.c.KafkaController - [Controller 0]: Partitions that completed preferred replica election: 
16:24:29.242 [main] INFO  k.c.KafkaController - [Controller 0]: Resuming preferred replica election for partitions: 
16:24:29.243 [main] INFO  k.c.KafkaController - [Controller 0]: Partitions being reassigned: Map()
16:24:29.243 [main] INFO  k.c.KafkaController - [Controller 0]: Partitions already reassigned: List()
16:24:29.243 [main] INFO  k.c.KafkaController - [Controller 0]: Resuming reassignment of partitions: Map()
16:24:29.244 [main] INFO  k.c.KafkaController - [Controller 0]: List of topics to be deleted: 
16:24:29.244 [main] INFO  k.c.KafkaController - [Controller 0]: List of topics ineligible for deletion: 
16:24:29.244 [main] INFO  k.c.KafkaController - [Controller 0]: Currently active brokers in the cluster: Set()
16:24:29.244 [main] INFO  k.c.KafkaController - [Controller 0]: Currently shutting brokers in the cluster: Set()
16:24:29.244 [main] INFO  k.c.KafkaController - [Controller 0]: Current list of topics in the cluster: Set()
16:24:29.244 [main] INFO  k.c.ReplicaStateMachine - [Replica state machine on controller 0]: Started replica state machine with initial state -> Map()
16:24:29.244 [main] INFO  k.c.PartitionStateMachine - [Partition state machine on Controller 0]: Started partition state machine with initial state -> Map()
16:24:29.245 [main] INFO  k.c.KafkaController - [Controller 0]: Broker 0 is ready to serve as the new controller with epoch 1
16:24:29.245 [main] INFO  k.c.KafkaController - [Controller 0]: Starting preferred replica leader election for partitions 
16:24:29.245 [main] INFO  k.c.PartitionStateMachine - [Partition state machine on Controller 0]: Invoking state change to OnlinePartition for partitions 
16:24:29.246 [main] INFO  k.c.KafkaController - [Controller 0]: starting the partition rebalance scheduler
16:24:29.246 [main] INFO  k.c.KafkaController - [Controller 0]: Controller startup complete
16:24:29.248 [ZkClient-EventThread-1057-127.0.0.1:43833] INFO  k.s.ZookeeperLeaderElector$LeaderChangeListener - New leader is 0
16:24:29.251 [main] INFO  k.u.ZkUtils$ - Registered broker 0 at path /brokers/ids/0 with address testing-worker-linux-docker-e9cd6d11-3391-linux-10.prod.travis-ci.org:36830.
16:24:29.251 [main] INFO  k.s.KafkaServer - [Kafka Server 0], started
16:24:29.252 [main] INFO  o.a.k.c.p.ProducerConfig - ProducerConfig values: 
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	block.on.buffer.full = true
	retry.backoff.ms = 100
	buffer.memory = 33554432
	batch.size = 16384
	metrics.sample.window.ms = 30000
	metadata.max.age.ms = 300000
	receive.buffer.bytes = 32768
	timeout.ms = 30000
	max.in.flight.requests.per.connection = 5
	bootstrap.servers = [localhost:36830]
	metric.reporters = []
	client.id = 
	compression.type = none
	retries = 0
	max.request.size = 1048576
	send.buffer.bytes = 131072
	acks = 1
	reconnect.backoff.ms = 10
	linger.ms = 0
	metrics.num.samples = 2
	metadata.fetch.timeout.ms = 1000

16:24:29.252 [ZkClient-EventThread-1057-127.0.0.1:43833] INFO  k.c.ReplicaStateMachine$BrokerChangeListener - [BrokerChangeListener on Controller 0]: Broker change listener fired for path /brokers/ids with children 0
16:24:29.257 [main] INFO  o.a.k.c.p.ProducerConfig - ProducerConfig values: 
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	block.on.buffer.full = true
	retry.backoff.ms = 100
	buffer.memory = 33554432
	batch.size = 16384
	metrics.sample.window.ms = 30000
	metadata.max.age.ms = 300000
	receive.buffer.bytes = 32768
	timeout.ms = 30000
	max.in.flight.requests.per.connection = 5
	bootstrap.servers = [localhost:36830]
	metric.reporters = []
	client.id = 
	compression.type = none
	retries = 0
	max.request.size = 1048576
	send.buffer.bytes = 131072
	acks = 1
	reconnect.backoff.ms = 10
	linger.ms = 0
	metrics.num.samples = 2
	metadata.fetch.timeout.ms = 1000

16:24:29.258 [ZkClient-EventThread-1057-127.0.0.1:43833] INFO  k.c.ReplicaStateMachine$BrokerChangeListener - [BrokerChangeListener on Controller 0]: Newly added brokers: 0, deleted brokers: , all live brokers: 0
16:24:29.259 [ZkClient-EventThread-1057-127.0.0.1:43833] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Controller 0 connected to id:0,host:testing-worker-linux-docker-e9cd6d11-3391-linux-10.prod.travis-ci.org,port:36830 for sending state change requests
16:24:29.259 [ZkClient-EventThread-1057-127.0.0.1:43833] INFO  k.c.KafkaController - [Controller 0]: New broker startup callback for 0
16:24:29.259 [Controller-0-to-broker-0-send-thread] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Starting 
16:24:29.260 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-e9cd6d11-3391-linux-10.prod.travis-ci.org/172.17.5.158
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
16:24:29.277 [main] INFO  k.s.KafkaServer - [Kafka Server 0], shutting down
16:24:29.277 [main] INFO  k.s.KafkaServer - [Kafka Server 0], Starting controlled shutdown
16:24:29.283 [kafka-request-handler-0] INFO  k.c.KafkaController - [Controller 0]: Shutting down broker 0
16:24:29.283 [main] INFO  k.s.KafkaServer - [Kafka Server 0], Controlled shutdown succeeded
16:24:29.284 [kafka-network-thread-36830-1] INFO  k.n.Processor - Closing socket connection to /172.17.5.158.
16:24:29.284 [main] INFO  k.n.SocketServer - [Socket Server on Broker 0], Shutting down
16:24:29.285 [main] INFO  k.n.SocketServer - [Socket Server on Broker 0], Shutdown completed
16:24:29.285 [main] INFO  k.s.KafkaRequestHandlerPool - [Kafka Request Handler on Broker 0], shutting down
16:24:29.286 [main] INFO  k.s.KafkaRequestHandlerPool - [Kafka Request Handler on Broker 0], shut down completely
16:24:29.304 [Curator-Framework-0-SendThread(127.0.0.1:50763)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:24:29.361 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-e9cd6d11-3391-linux-10.prod.travis-ci.org/172.17.5.158
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
16:24:29.403 [Curator-Framework-0-SendThread(127.0.0.1:50639)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:24:29.463 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-e9cd6d11-3391-linux-10.prod.travis-ci.org/172.17.5.158
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
16:24:29.485 [Curator-Framework-0-SendThread(127.0.0.1:33163)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:24:29.509 [main-SendThread(localhost:48389)] WARN  o.a.z.ClientCnxn - Session 0x1557e13639f0000 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:24:29.537 [Curator-Framework-0-SendThread(127.0.0.1:46779)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:24:29.564 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-e9cd6d11-3391-linux-10.prod.travis-ci.org/172.17.5.158
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
16:24:29.632 [main] INFO  k.s.ReplicaManager - [Replica Manager on Broker 0]: Shut down
16:24:29.632 [main] INFO  k.s.ReplicaFetcherManager - [ReplicaFetcherManager on broker 0] shutting down
16:24:29.632 [main] INFO  k.s.ReplicaFetcherManager - [ReplicaFetcherManager on broker 0] shutdown completed
16:24:29.632 [main] INFO  k.s.ReplicaManager - [Replica Manager on Broker 0]: Shut down completely
16:24:29.632 [main] INFO  k.l.LogManager - Shutting down.
16:24:29.633 [main] INFO  k.l.LogManager - Shutdown complete.
16:24:29.633 [main] INFO  k.c.PartitionStateMachine - [Partition state machine on Controller 0]: Stopped partition state machine
16:24:29.633 [main] INFO  k.c.ReplicaStateMachine - [Replica state machine on controller 0]: Stopped replica state machine
16:24:29.634 [main] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Shutting down
16:24:29.634 [Controller-0-to-broker-0-send-thread] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Stopped 
16:24:29.634 [main] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Shutdown completed
16:24:29.634 [ZkClient-EventThread-1057-127.0.0.1:43833] INFO  o.I.z.ZkEventThread - Terminate ZkClient event thread.
16:24:29.637 [main] INFO  k.s.KafkaServer - [Kafka Server 0], shut down completed
16:24:29.637 [Curator-Framework-0] INFO  o.a.c.f.i.CuratorFrameworkImpl - backgroundOperationsLoop exiting

--------------------------------------------------
--------------------------------------------------
classname: org.apache.storm.kafka.bolt.KafkaBoltTest / testname: executeWithByteArrayKeyAndMessageSync
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.kafka.bolt.KafkaBoltTest.generateTestTuple(KafkaBoltTest.java:290)
	at org.apache.storm.kafka.bolt.KafkaBoltTest.executeWithByteArrayKeyAndMessageSync(KafkaBoltTest.java:131)

-------------------- system-out --------------------
16:24:29.665 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-e9cd6d11-3391-linux-10.prod.travis-ci.org/172.17.5.158
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
16:24:29.766 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-e9cd6d11-3391-linux-10.prod.travis-ci.org/172.17.5.158
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
16:24:29.866 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-e9cd6d11-3391-linux-10.prod.travis-ci.org/172.17.5.158
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
16:24:29.906 [main-SendThread(localhost:51237)] WARN  o.a.z.ClientCnxn - Session 0x1557e1372d10000 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:24:29.967 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-e9cd6d11-3391-linux-10.prod.travis-ci.org/172.17.5.158
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
16:24:30.069 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-e9cd6d11-3391-linux-10.prod.travis-ci.org/172.17.5.158
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
16:24:30.170 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-e9cd6d11-3391-linux-10.prod.travis-ci.org/172.17.5.158
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
16:24:30.271 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-e9cd6d11-3391-linux-10.prod.travis-ci.org/172.17.5.158
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
16:24:30.305 [Curator-Framework-0-SendThread(127.0.0.1:33449)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:24:30.305 [Curator-Framework-0-SendThread(127.0.0.1:33245)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:24:30.372 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-e9cd6d11-3391-linux-10.prod.travis-ci.org/172.17.5.158
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
16:24:30.405 [Curator-Framework-0-SendThread(127.0.0.1:50763)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:24:30.473 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-e9cd6d11-3391-linux-10.prod.travis-ci.org/172.17.5.158
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
16:24:30.504 [Curator-Framework-0-SendThread(127.0.0.1:50639)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:24:30.574 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-e9cd6d11-3391-linux-10.prod.travis-ci.org/172.17.5.158
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
16:24:30.586 [Curator-Framework-0-SendThread(127.0.0.1:33163)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:24:30.638 [Curator-Framework-0-SendThread(127.0.0.1:46779)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:24:30.644 [main] INFO  o.a.c.f.i.CuratorFrameworkImpl - Starting
16:24:30.645 [main] INFO  k.u.VerifiableProperties - Verifying properties
16:24:30.646 [main] INFO  k.u.VerifiableProperties - Property broker.id is overridden to 0
16:24:30.646 [main] INFO  k.u.VerifiableProperties - Property log.dirs is overridden to /tmp/kafka/logs/kafka-test-34048
16:24:30.646 [main] INFO  k.u.VerifiableProperties - Property port is overridden to 34048
16:24:30.646 [main] INFO  k.u.VerifiableProperties - Property zookeeper.connect is overridden to 127.0.0.1:59321
16:24:30.646 [main] INFO  k.s.KafkaServer - [Kafka Server 0], starting
16:24:30.646 [main] INFO  k.s.KafkaServer - [Kafka Server 0], Connecting to zookeeper on 127.0.0.1:59321
16:24:30.647 [ZkClient-EventThread-1094-127.0.0.1:59321] INFO  o.I.z.ZkEventThread - Starting ZkClient event thread.
16:24:30.651 [main-EventThread] INFO  o.a.c.f.s.ConnectionStateManager - State change: CONNECTED
16:24:30.651 [main-EventThread] INFO  o.I.z.ZkClient - zookeeper state changed (SyncConnected)
16:24:30.663 [main] INFO  k.l.LogManager - Log directory '/tmp/kafka/logs/kafka-test-34048' not found, creating it.
16:24:30.664 [main] INFO  k.l.LogManager - Loading logs.
16:24:30.664 [main] INFO  k.l.LogManager - Logs loading complete.
16:24:30.664 [main] INFO  k.l.LogManager - Starting log cleanup with a period of 300000 ms.
16:24:30.665 [main] INFO  k.l.LogManager - Starting log flusher with a default period of 9223372036854775807 ms.
16:24:30.666 [main] INFO  k.n.Acceptor - Awaiting socket connections on 0.0.0.0:34048.
16:24:30.667 [main] INFO  k.n.SocketServer - [Socket Server on Broker 0], Started
16:24:30.669 [main] INFO  k.u.Mx4jLoader$ - Will not load MX4J, mx4j-tools.jar is not in the classpath
16:24:30.670 [main] INFO  k.c.KafkaController - [Controller 0]: Controller starting up
16:24:30.672 [main] INFO  k.s.ZookeeperLeaderElector - 0 successfully elected as leader
16:24:30.673 [main] INFO  k.c.KafkaController - [Controller 0]: Broker 0 starting become controller state transition
16:24:30.675 [main] INFO  k.c.KafkaController - [Controller 0]: Controller 0 incremented epoch to 1
16:24:30.675 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-e9cd6d11-3391-linux-10.prod.travis-ci.org/172.17.5.158
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
16:24:30.678 [main] INFO  k.c.KafkaController - [Controller 0]: Partitions undergoing preferred replica election: 
16:24:30.678 [main] INFO  k.c.KafkaController - [Controller 0]: Partitions that completed preferred replica election: 
16:24:30.678 [main] INFO  k.c.KafkaController - [Controller 0]: Resuming preferred replica election for partitions: 
16:24:30.679 [main] INFO  k.c.KafkaController - [Controller 0]: Partitions being reassigned: Map()
16:24:30.679 [main] INFO  k.c.KafkaController - [Controller 0]: Partitions already reassigned: List()
16:24:30.679 [main] INFO  k.c.KafkaController - [Controller 0]: Resuming reassignment of partitions: Map()
16:24:30.679 [main] INFO  k.c.KafkaController - [Controller 0]: List of topics to be deleted: 
16:24:30.679 [main] INFO  k.c.KafkaController - [Controller 0]: List of topics ineligible for deletion: 
16:24:30.679 [main] INFO  k.c.KafkaController - [Controller 0]: Currently active brokers in the cluster: Set()
16:24:30.679 [main] INFO  k.c.KafkaController - [Controller 0]: Currently shutting brokers in the cluster: Set()
16:24:30.679 [main] INFO  k.c.KafkaController - [Controller 0]: Current list of topics in the cluster: Set()
16:24:30.680 [main] INFO  k.c.ReplicaStateMachine - [Replica state machine on controller 0]: Started replica state machine with initial state -> Map()
16:24:30.680 [main] INFO  k.c.PartitionStateMachine - [Partition state machine on Controller 0]: Started partition state machine with initial state -> Map()
16:24:30.680 [main] INFO  k.c.KafkaController - [Controller 0]: Broker 0 is ready to serve as the new controller with epoch 1
16:24:30.680 [main] INFO  k.c.KafkaController - [Controller 0]: Starting preferred replica leader election for partitions 
16:24:30.680 [main] INFO  k.c.PartitionStateMachine - [Partition state machine on Controller 0]: Invoking state change to OnlinePartition for partitions 
16:24:30.681 [main] INFO  k.c.KafkaController - [Controller 0]: starting the partition rebalance scheduler
16:24:30.681 [main] INFO  k.c.KafkaController - [Controller 0]: Controller startup complete
16:24:30.682 [ZkClient-EventThread-1094-127.0.0.1:59321] INFO  k.s.ZookeeperLeaderElector$LeaderChangeListener - New leader is 0
16:24:30.685 [main] INFO  k.u.ZkUtils$ - Registered broker 0 at path /brokers/ids/0 with address testing-worker-linux-docker-e9cd6d11-3391-linux-10.prod.travis-ci.org:34048.
16:24:30.685 [main] INFO  k.s.KafkaServer - [Kafka Server 0], started
16:24:30.686 [main] INFO  o.a.k.c.p.ProducerConfig - ProducerConfig values: 
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	block.on.buffer.full = true
	retry.backoff.ms = 100
	buffer.memory = 33554432
	batch.size = 16384
	metrics.sample.window.ms = 30000
	metadata.max.age.ms = 300000
	receive.buffer.bytes = 32768
	timeout.ms = 30000
	max.in.flight.requests.per.connection = 5
	bootstrap.servers = [localhost:34048]
	metric.reporters = []
	client.id = 
	compression.type = none
	retries = 0
	max.request.size = 1048576
	send.buffer.bytes = 131072
	acks = 1
	reconnect.backoff.ms = 10
	linger.ms = 0
	metrics.num.samples = 2
	metadata.fetch.timeout.ms = 1000

16:24:30.686 [ZkClient-EventThread-1094-127.0.0.1:59321] INFO  k.c.ReplicaStateMachine$BrokerChangeListener - [BrokerChangeListener on Controller 0]: Broker change listener fired for path /brokers/ids with children 0
16:24:30.689 [main] INFO  o.a.k.c.p.ProducerConfig - ProducerConfig values: 
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	block.on.buffer.full = true
	retry.backoff.ms = 100
	buffer.memory = 33554432
	batch.size = 16384
	metrics.sample.window.ms = 30000
	metadata.max.age.ms = 300000
	receive.buffer.bytes = 32768
	timeout.ms = 30000
	max.in.flight.requests.per.connection = 5
	bootstrap.servers = [localhost:34048]
	metric.reporters = []
	client.id = 
	compression.type = none
	retries = 0
	max.request.size = 1048576
	send.buffer.bytes = 131072
	acks = 1
	reconnect.backoff.ms = 10
	linger.ms = 0
	metrics.num.samples = 2
	metadata.fetch.timeout.ms = 1000

16:24:30.691 [main] INFO  k.s.KafkaServer - [Kafka Server 0], shutting down
16:24:30.691 [main] INFO  k.s.KafkaServer - [Kafka Server 0], Starting controlled shutdown
16:24:30.693 [ZkClient-EventThread-1094-127.0.0.1:59321] INFO  k.c.ReplicaStateMachine$BrokerChangeListener - [BrokerChangeListener on Controller 0]: Newly added brokers: 0, deleted brokers: , all live brokers: 0
16:24:30.693 [ZkClient-EventThread-1094-127.0.0.1:59321] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Controller 0 connected to id:0,host:testing-worker-linux-docker-e9cd6d11-3391-linux-10.prod.travis-ci.org,port:34048 for sending state change requests
16:24:30.696 [ZkClient-EventThread-1094-127.0.0.1:59321] INFO  k.c.KafkaController - [Controller 0]: New broker startup callback for 0
16:24:30.696 [kafka-request-handler-0] INFO  k.c.KafkaController - [Controller 0]: Shutting down broker 0
16:24:30.696 [Controller-0-to-broker-0-send-thread] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Starting 
16:24:30.697 [main] INFO  k.s.KafkaServer - [Kafka Server 0], Controlled shutdown succeeded
16:24:30.697 [main] INFO  k.n.SocketServer - [Socket Server on Broker 0], Shutting down
16:24:30.697 [kafka-network-thread-34048-1] INFO  k.n.Processor - Closing socket connection to /172.17.5.158.
16:24:30.698 [main] INFO  k.n.SocketServer - [Socket Server on Broker 0], Shutdown completed
16:24:30.698 [main] INFO  k.s.KafkaRequestHandlerPool - [Kafka Request Handler on Broker 0], shutting down
16:24:30.698 [main] INFO  k.s.KafkaRequestHandlerPool - [Kafka Request Handler on Broker 0], shut down completely
16:24:30.776 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-e9cd6d11-3391-linux-10.prod.travis-ci.org/172.17.5.158
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
16:24:30.783 [Curator-Framework-0] WARN  o.a.c.ConnectionState - Connection attempt unsuccessful after 20097 (greater than max timeout of 20000). Resetting connection and trying again with a new connection.
16:24:30.878 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-e9cd6d11-3391-linux-10.prod.travis-ci.org/172.17.5.158
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
16:24:30.979 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-e9cd6d11-3391-linux-10.prod.travis-ci.org/172.17.5.158
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
16:24:31.000 [main-SendThread(localhost:51237)] WARN  o.a.z.ClientCnxn - Session 0x1557e1372d10000 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:24:31.069 [main] INFO  k.s.ReplicaManager - [Replica Manager on Broker 0]: Shut down
16:24:31.069 [main] INFO  k.s.ReplicaFetcherManager - [ReplicaFetcherManager on broker 0] shutting down
16:24:31.069 [main] INFO  k.s.ReplicaFetcherManager - [ReplicaFetcherManager on broker 0] shutdown completed
16:24:31.069 [main] INFO  k.s.ReplicaManager - [Replica Manager on Broker 0]: Shut down completely
16:24:31.069 [main] INFO  k.l.LogManager - Shutting down.
16:24:31.070 [main] INFO  k.l.LogManager - Shutdown complete.
16:24:31.070 [main] INFO  k.c.PartitionStateMachine - [Partition state machine on Controller 0]: Stopped partition state machine
16:24:31.070 [main] INFO  k.c.ReplicaStateMachine - [Replica state machine on controller 0]: Stopped replica state machine
16:24:31.070 [main] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Shutting down
16:24:31.070 [main] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Shutdown completed
16:24:31.070 [ZkClient-EventThread-1094-127.0.0.1:59321] INFO  o.I.z.ZkEventThread - Terminate ZkClient event thread.
16:24:31.070 [Controller-0-to-broker-0-send-thread] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Stopped 
16:24:31.072 [main] INFO  k.s.KafkaServer - [Kafka Server 0], shut down completed
16:24:31.072 [Curator-Framework-0] INFO  o.a.c.f.i.CuratorFrameworkImpl - backgroundOperationsLoop exiting

--------------------------------------------------
--------------------------------------------------
classname: org.apache.storm.kafka.bolt.KafkaBoltTest / testname: executeWithByteArrayKeyAndMessageAsync
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.kafka.bolt.KafkaBoltTest.generateTestTuple(KafkaBoltTest.java:290)
	at org.apache.storm.kafka.bolt.KafkaBoltTest.executeWithByteArrayKeyAndMessageAsync(KafkaBoltTest.java:146)

-------------------- system-out --------------------
16:24:31.085 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-e9cd6d11-3391-linux-10.prod.travis-ci.org/172.17.5.158
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
16:24:31.186 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-e9cd6d11-3391-linux-10.prod.travis-ci.org/172.17.5.158
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
16:24:31.288 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-e9cd6d11-3391-linux-10.prod.travis-ci.org/172.17.5.158
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
16:24:31.389 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-e9cd6d11-3391-linux-10.prod.travis-ci.org/172.17.5.158
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
16:24:31.406 [Curator-Framework-0-SendThread(127.0.0.1:33245)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:24:31.406 [Curator-Framework-0-SendThread(127.0.0.1:33449)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:24:31.490 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-e9cd6d11-3391-linux-10.prod.travis-ci.org/172.17.5.158
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
16:24:31.506 [Curator-Framework-0-SendThread(127.0.0.1:50763)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:24:31.592 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-e9cd6d11-3391-linux-10.prod.travis-ci.org/172.17.5.158
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
16:24:31.605 [Curator-Framework-0-SendThread(127.0.0.1:50639)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:24:31.633 [Curator-Framework-0-SendThread(localhost:48389)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:24:31.689 [Curator-Framework-0-SendThread(127.0.0.1:33163)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:24:31.693 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-e9cd6d11-3391-linux-10.prod.travis-ci.org/172.17.5.158
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
16:24:31.725 [Curator-Framework-0] ERROR o.a.c.f.i.CuratorFrameworkImpl - Background operation retry gave up
org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:99) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.curator.framework.imps.CuratorFrameworkImpl.checkBackgroundRetry(CuratorFrameworkImpl.java:728) [curator-framework-2.10.0.jar:?]
	at org.apache.curator.framework.imps.CuratorFrameworkImpl.performBackgroundOperation(CuratorFrameworkImpl.java:857) [curator-framework-2.10.0.jar:?]
	at org.apache.curator.framework.imps.CuratorFrameworkImpl.backgroundOperationsLoop(CuratorFrameworkImpl.java:809) [curator-framework-2.10.0.jar:?]
	at org.apache.curator.framework.imps.CuratorFrameworkImpl.access$300(CuratorFrameworkImpl.java:64) [curator-framework-2.10.0.jar:?]
	at org.apache.curator.framework.imps.CuratorFrameworkImpl$4.call(CuratorFrameworkImpl.java:267) [curator-framework-2.10.0.jar:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:262) [?:1.7.0_76]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:178) [?:1.7.0_76]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:292) [?:1.7.0_76]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [?:1.7.0_76]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [?:1.7.0_76]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
16:24:31.725 [Curator-Framework-0] ERROR o.a.c.f.i.CuratorFrameworkImpl - Background retry gave up
org.apache.curator.CuratorConnectionLossException: KeeperErrorCode = ConnectionLoss
	at org.apache.curator.framework.imps.CuratorFrameworkImpl.performBackgroundOperation(CuratorFrameworkImpl.java:838) [curator-framework-2.10.0.jar:?]
	at org.apache.curator.framework.imps.CuratorFrameworkImpl.backgroundOperationsLoop(CuratorFrameworkImpl.java:809) [curator-framework-2.10.0.jar:?]
	at org.apache.curator.framework.imps.CuratorFrameworkImpl.access$300(CuratorFrameworkImpl.java:64) [curator-framework-2.10.0.jar:?]
	at org.apache.curator.framework.imps.CuratorFrameworkImpl$4.call(CuratorFrameworkImpl.java:267) [curator-framework-2.10.0.jar:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:262) [?:1.7.0_76]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:178) [?:1.7.0_76]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:292) [?:1.7.0_76]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [?:1.7.0_76]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [?:1.7.0_76]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
16:24:31.735 [Curator-Framework-0-SendThread(localhost:48389)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:24:31.739 [Curator-Framework-0-SendThread(127.0.0.1:46779)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:24:31.794 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-e9cd6d11-3391-linux-10.prod.travis-ci.org/172.17.5.158
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
16:24:31.895 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-e9cd6d11-3391-linux-10.prod.travis-ci.org/172.17.5.158
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
16:24:31.997 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-e9cd6d11-3391-linux-10.prod.travis-ci.org/172.17.5.158
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
16:24:32.078 [main] INFO  o.a.c.f.i.CuratorFrameworkImpl - Starting
16:24:32.080 [main] INFO  k.u.VerifiableProperties - Verifying properties
16:24:32.080 [main] INFO  k.u.VerifiableProperties - Property broker.id is overridden to 0
16:24:32.080 [main] INFO  k.u.VerifiableProperties - Property log.dirs is overridden to /tmp/kafka/logs/kafka-test-43285
16:24:32.080 [main] INFO  k.u.VerifiableProperties - Property port is overridden to 43285
16:24:32.080 [main] INFO  k.u.VerifiableProperties - Property zookeeper.connect is overridden to 127.0.0.1:47579
16:24:32.080 [main] INFO  k.s.KafkaServer - [Kafka Server 0], starting
16:24:32.080 [main] INFO  k.s.KafkaServer - [Kafka Server 0], Connecting to zookeeper on 127.0.0.1:47579
16:24:32.081 [ZkClient-EventThread-1133-127.0.0.1:47579] INFO  o.I.z.ZkEventThread - Starting ZkClient event thread.
16:24:32.082 [main-EventThread] INFO  o.a.c.f.s.ConnectionStateManager - State change: CONNECTED
16:24:32.082 [main-EventThread] INFO  o.I.z.ZkClient - zookeeper state changed (SyncConnected)
16:24:32.094 [main] INFO  k.l.LogManager - Log directory '/tmp/kafka/logs/kafka-test-43285' not found, creating it.
16:24:32.095 [main] INFO  k.l.LogManager - Loading logs.
16:24:32.095 [main] INFO  k.l.LogManager - Logs loading complete.
16:24:32.095 [main] INFO  k.l.LogManager - Starting log cleanup with a period of 300000 ms.
16:24:32.095 [main] INFO  k.l.LogManager - Starting log flusher with a default period of 9223372036854775807 ms.
16:24:32.097 [main] INFO  k.n.Acceptor - Awaiting socket connections on 0.0.0.0:43285.
16:24:32.098 [main] INFO  k.n.SocketServer - [Socket Server on Broker 0], Started
16:24:32.098 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-e9cd6d11-3391-linux-10.prod.travis-ci.org/172.17.5.158
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
16:24:32.100 [main] INFO  k.u.Mx4jLoader$ - Will not load MX4J, mx4j-tools.jar is not in the classpath
16:24:32.101 [main] INFO  k.c.KafkaController - [Controller 0]: Controller starting up
16:24:32.103 [main] INFO  k.s.ZookeeperLeaderElector - 0 successfully elected as leader
16:24:32.103 [main] INFO  k.c.KafkaController - [Controller 0]: Broker 0 starting become controller state transition
16:24:32.105 [main] INFO  k.c.KafkaController - [Controller 0]: Controller 0 incremented epoch to 1
16:24:32.108 [main] INFO  k.c.KafkaController - [Controller 0]: Partitions undergoing preferred replica election: 
16:24:32.108 [main] INFO  k.c.KafkaController - [Controller 0]: Partitions that completed preferred replica election: 
16:24:32.108 [main] INFO  k.c.KafkaController - [Controller 0]: Resuming preferred replica election for partitions: 
16:24:32.108 [main] INFO  k.c.KafkaController - [Controller 0]: Partitions being reassigned: Map()
16:24:32.109 [main] INFO  k.c.KafkaController - [Controller 0]: Partitions already reassigned: List()
16:24:32.109 [main] INFO  k.c.KafkaController - [Controller 0]: Resuming reassignment of partitions: Map()
16:24:32.109 [main] INFO  k.c.KafkaController - [Controller 0]: List of topics to be deleted: 
16:24:32.109 [main] INFO  k.c.KafkaController - [Controller 0]: List of topics ineligible for deletion: 
16:24:32.109 [main] INFO  k.c.KafkaController - [Controller 0]: Currently active brokers in the cluster: Set()
16:24:32.109 [main] INFO  k.c.KafkaController - [Controller 0]: Currently shutting brokers in the cluster: Set()
16:24:32.109 [main] INFO  k.c.KafkaController - [Controller 0]: Current list of topics in the cluster: Set()
16:24:32.110 [main] INFO  k.c.ReplicaStateMachine - [Replica state machine on controller 0]: Started replica state machine with initial state -> Map()
16:24:32.110 [main] INFO  k.c.PartitionStateMachine - [Partition state machine on Controller 0]: Started partition state machine with initial state -> Map()
16:24:32.110 [main] INFO  k.c.KafkaController - [Controller 0]: Broker 0 is ready to serve as the new controller with epoch 1
16:24:32.110 [main] INFO  k.c.KafkaController - [Controller 0]: Starting preferred replica leader election for partitions 
16:24:32.110 [main] INFO  k.c.PartitionStateMachine - [Partition state machine on Controller 0]: Invoking state change to OnlinePartition for partitions 
16:24:32.111 [main] INFO  k.c.KafkaController - [Controller 0]: starting the partition rebalance scheduler
16:24:32.111 [main] INFO  k.c.KafkaController - [Controller 0]: Controller startup complete
16:24:32.113 [ZkClient-EventThread-1133-127.0.0.1:47579] INFO  k.s.ZookeeperLeaderElector$LeaderChangeListener - New leader is 0
16:24:32.114 [ZkClient-EventThread-1133-127.0.0.1:47579] INFO  k.c.ReplicaStateMachine$BrokerChangeListener - [BrokerChangeListener on Controller 0]: Broker change listener fired for path /brokers/ids with children 0
16:24:32.115 [main] INFO  k.u.ZkUtils$ - Registered broker 0 at path /brokers/ids/0 with address testing-worker-linux-docker-e9cd6d11-3391-linux-10.prod.travis-ci.org:43285.
16:24:32.115 [main] INFO  k.s.KafkaServer - [Kafka Server 0], started
16:24:32.117 [main] INFO  o.a.k.c.p.ProducerConfig - ProducerConfig values: 
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	block.on.buffer.full = true
	retry.backoff.ms = 100
	buffer.memory = 33554432
	batch.size = 16384
	metrics.sample.window.ms = 30000
	metadata.max.age.ms = 300000
	receive.buffer.bytes = 32768
	timeout.ms = 30000
	max.in.flight.requests.per.connection = 5
	bootstrap.servers = [localhost:43285]
	metric.reporters = []
	client.id = 
	compression.type = none
	retries = 0
	max.request.size = 1048576
	send.buffer.bytes = 131072
	acks = 1
	reconnect.backoff.ms = 10
	linger.ms = 0
	metrics.num.samples = 2
	metadata.fetch.timeout.ms = 1000

16:24:32.120 [main] INFO  k.s.KafkaServer - [Kafka Server 0], shutting down
16:24:32.122 [ZkClient-EventThread-1133-127.0.0.1:47579] INFO  k.c.ReplicaStateMachine$BrokerChangeListener - [BrokerChangeListener on Controller 0]: Newly added brokers: 0, deleted brokers: , all live brokers: 0
16:24:32.122 [ZkClient-EventThread-1133-127.0.0.1:47579] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Controller 0 connected to id:0,host:testing-worker-linux-docker-e9cd6d11-3391-linux-10.prod.travis-ci.org,port:43285 for sending state change requests
16:24:32.120 [main] INFO  k.s.KafkaServer - [Kafka Server 0], Starting controlled shutdown
16:24:32.123 [ZkClient-EventThread-1133-127.0.0.1:47579] INFO  k.c.KafkaController - [Controller 0]: New broker startup callback for 0
16:24:32.123 [Controller-0-to-broker-0-send-thread] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Starting 
16:24:32.127 [kafka-request-handler-0] INFO  k.c.KafkaController - [Controller 0]: Shutting down broker 0
16:24:32.127 [main] INFO  k.s.KafkaServer - [Kafka Server 0], Controlled shutdown succeeded
16:24:32.127 [main] INFO  k.n.SocketServer - [Socket Server on Broker 0], Shutting down
16:24:32.128 [kafka-network-thread-43285-1] INFO  k.n.Processor - Closing socket connection to /172.17.5.158.
16:24:32.129 [main] INFO  k.n.SocketServer - [Socket Server on Broker 0], Shutdown completed
16:24:32.129 [main] INFO  k.s.KafkaRequestHandlerPool - [Kafka Request Handler on Broker 0], shutting down
16:24:32.129 [main] INFO  k.s.KafkaRequestHandlerPool - [Kafka Request Handler on Broker 0], shut down completely
16:24:32.199 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-e9cd6d11-3391-linux-10.prod.travis-ci.org/172.17.5.158
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
16:24:32.300 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-e9cd6d11-3391-linux-10.prod.travis-ci.org/172.17.5.158
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
16:24:32.349 [main-SendThread(localhost:51237)] WARN  o.a.z.ClientCnxn - Session 0x1557e1372d10000 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:24:32.401 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-e9cd6d11-3391-linux-10.prod.travis-ci.org/172.17.5.158
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
16:24:32.500 [main] INFO  k.s.ReplicaManager - [Replica Manager on Broker 0]: Shut down
16:24:32.500 [main] INFO  k.s.ReplicaFetcherManager - [ReplicaFetcherManager on broker 0] shutting down
16:24:32.500 [main] INFO  k.s.ReplicaFetcherManager - [ReplicaFetcherManager on broker 0] shutdown completed
16:24:32.501 [main] INFO  k.s.ReplicaManager - [Replica Manager on Broker 0]: Shut down completely
16:24:32.501 [main] INFO  k.l.LogManager - Shutting down.
16:24:32.501 [main] INFO  k.l.LogManager - Shutdown complete.
16:24:32.501 [main] INFO  k.c.PartitionStateMachine - [Partition state machine on Controller 0]: Stopped partition state machine
16:24:32.502 [main] INFO  k.c.ReplicaStateMachine - [Replica state machine on controller 0]: Stopped replica state machine
16:24:32.502 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-e9cd6d11-3391-linux-10.prod.travis-ci.org/172.17.5.158
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
16:24:32.502 [main] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Shutting down
16:24:32.502 [Controller-0-to-broker-0-send-thread] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Stopped 
16:24:32.503 [main] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Shutdown completed
16:24:32.503 [ZkClient-EventThread-1133-127.0.0.1:47579] INFO  o.I.z.ZkEventThread - Terminate ZkClient event thread.
16:24:32.505 [main] INFO  k.s.KafkaServer - [Kafka Server 0], shut down completed
16:24:32.505 [Curator-Framework-0] INFO  o.a.c.f.i.CuratorFrameworkImpl - backgroundOperationsLoop exiting
16:24:32.507 [Curator-Framework-0-SendThread(127.0.0.1:33449)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:24:32.507 [Curator-Framework-0-SendThread(127.0.0.1:33245)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]

--------------------------------------------------
--------------------------------------------------
classname: org.apache.storm.kafka.bolt.KafkaBoltTest / testname: executeWithKey
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.kafka.bolt.KafkaBoltTest.generateTestTuple(KafkaBoltTest.java:290)
	at org.apache.storm.kafka.bolt.KafkaBoltTest.executeWithKey(KafkaBoltTest.java:115)

-------------------- system-out --------------------
16:24:32.602 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-e9cd6d11-3391-linux-10.prod.travis-ci.org/172.17.5.158
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
16:24:32.607 [Curator-Framework-0-SendThread(127.0.0.1:50763)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:24:32.703 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-e9cd6d11-3391-linux-10.prod.travis-ci.org/172.17.5.158
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
16:24:32.706 [Curator-Framework-0-SendThread(127.0.0.1:50639)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:24:32.790 [Curator-Framework-0-SendThread(127.0.0.1:33163)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:24:32.805 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-e9cd6d11-3391-linux-10.prod.travis-ci.org/172.17.5.158
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
16:24:32.836 [Curator-Framework-0-SendThread(localhost:48389)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:24:32.840 [Curator-Framework-0-SendThread(127.0.0.1:46779)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:24:32.906 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-e9cd6d11-3391-linux-10.prod.travis-ci.org/172.17.5.158
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
16:24:32.937 [Curator-Framework-0-SendThread(localhost:48389)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:24:33.007 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-e9cd6d11-3391-linux-10.prod.travis-ci.org/172.17.5.158
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
16:24:33.108 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-e9cd6d11-3391-linux-10.prod.travis-ci.org/172.17.5.158
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
16:24:33.189 [main-SendThread(localhost:51237)] WARN  o.a.z.ClientCnxn - Session 0x1557e1372d10000 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:24:33.209 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-e9cd6d11-3391-linux-10.prod.travis-ci.org/172.17.5.158
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
16:24:33.310 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-e9cd6d11-3391-linux-10.prod.travis-ci.org/172.17.5.158
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
16:24:33.388 [Curator-Framework-0] WARN  o.a.c.ConnectionState - Connection attempt unsuccessful after 20010 (greater than max timeout of 20000). Resetting connection and trying again with a new connection.
16:24:33.411 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-e9cd6d11-3391-linux-10.prod.travis-ci.org/172.17.5.158
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
16:24:33.512 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-e9cd6d11-3391-linux-10.prod.travis-ci.org/172.17.5.158
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
16:24:33.532 [main] INFO  o.a.c.f.i.CuratorFrameworkImpl - Starting
16:24:33.533 [main] INFO  k.u.VerifiableProperties - Verifying properties
16:24:33.534 [main] INFO  k.u.VerifiableProperties - Property broker.id is overridden to 0
16:24:33.534 [main] INFO  k.u.VerifiableProperties - Property log.dirs is overridden to /tmp/kafka/logs/kafka-test-52799
16:24:33.534 [main] INFO  k.u.VerifiableProperties - Property port is overridden to 52799
16:24:33.534 [main] INFO  k.u.VerifiableProperties - Property zookeeper.connect is overridden to 127.0.0.1:60589
16:24:33.534 [main] INFO  k.s.KafkaServer - [Kafka Server 0], starting
16:24:33.534 [main] INFO  k.s.KafkaServer - [Kafka Server 0], Connecting to zookeeper on 127.0.0.1:60589
16:24:33.535 [ZkClient-EventThread-1169-127.0.0.1:60589] INFO  o.I.z.ZkEventThread - Starting ZkClient event thread.
16:24:33.536 [main-EventThread] INFO  o.a.c.f.s.ConnectionStateManager - State change: CONNECTED
16:24:33.536 [main-EventThread] INFO  o.I.z.ZkClient - zookeeper state changed (SyncConnected)
16:24:33.556 [main] INFO  k.l.LogManager - Log directory '/tmp/kafka/logs/kafka-test-52799' not found, creating it.
16:24:33.556 [main] INFO  k.l.LogManager - Loading logs.
16:24:33.557 [main] INFO  k.l.LogManager - Logs loading complete.
16:24:33.557 [main] INFO  k.l.LogManager - Starting log cleanup with a period of 300000 ms.
16:24:33.557 [main] INFO  k.l.LogManager - Starting log flusher with a default period of 9223372036854775807 ms.
16:24:33.569 [main] INFO  k.n.Acceptor - Awaiting socket connections on 0.0.0.0:52799.
16:24:33.569 [main] INFO  k.n.SocketServer - [Socket Server on Broker 0], Started
16:24:33.572 [main] INFO  k.u.Mx4jLoader$ - Will not load MX4J, mx4j-tools.jar is not in the classpath
16:24:33.572 [main] INFO  k.c.KafkaController - [Controller 0]: Controller starting up
16:24:33.574 [main] INFO  k.s.ZookeeperLeaderElector - 0 successfully elected as leader
16:24:33.574 [main] INFO  k.c.KafkaController - [Controller 0]: Broker 0 starting become controller state transition
16:24:33.577 [main] INFO  k.c.KafkaController - [Controller 0]: Controller 0 incremented epoch to 1
16:24:33.579 [main] INFO  k.c.KafkaController - [Controller 0]: Partitions undergoing preferred replica election: 
16:24:33.580 [main] INFO  k.c.KafkaController - [Controller 0]: Partitions that completed preferred replica election: 
16:24:33.580 [main] INFO  k.c.KafkaController - [Controller 0]: Resuming preferred replica election for partitions: 
16:24:33.580 [main] INFO  k.c.KafkaController - [Controller 0]: Partitions being reassigned: Map()
16:24:33.580 [main] INFO  k.c.KafkaController - [Controller 0]: Partitions already reassigned: List()
16:24:33.580 [main] INFO  k.c.KafkaController - [Controller 0]: Resuming reassignment of partitions: Map()
16:24:33.581 [main] INFO  k.c.KafkaController - [Controller 0]: List of topics to be deleted: 
16:24:33.581 [main] INFO  k.c.KafkaController - [Controller 0]: List of topics ineligible for deletion: 
16:24:33.581 [main] INFO  k.c.KafkaController - [Controller 0]: Currently active brokers in the cluster: Set()
16:24:33.581 [main] INFO  k.c.KafkaController - [Controller 0]: Currently shutting brokers in the cluster: Set()
16:24:33.581 [main] INFO  k.c.KafkaController - [Controller 0]: Current list of topics in the cluster: Set()
16:24:33.581 [main] INFO  k.c.ReplicaStateMachine - [Replica state machine on controller 0]: Started replica state machine with initial state -> Map()
16:24:33.581 [main] INFO  k.c.PartitionStateMachine - [Partition state machine on Controller 0]: Started partition state machine with initial state -> Map()
16:24:33.581 [main] INFO  k.c.KafkaController - [Controller 0]: Broker 0 is ready to serve as the new controller with epoch 1
16:24:33.581 [main] INFO  k.c.KafkaController - [Controller 0]: Starting preferred replica leader election for partitions 
16:24:33.581 [main] INFO  k.c.PartitionStateMachine - [Partition state machine on Controller 0]: Invoking state change to OnlinePartition for partitions 
16:24:33.582 [main] INFO  k.c.KafkaController - [Controller 0]: starting the partition rebalance scheduler
16:24:33.583 [main] INFO  k.c.KafkaController - [Controller 0]: Controller startup complete
16:24:33.584 [ZkClient-EventThread-1169-127.0.0.1:60589] INFO  k.s.ZookeeperLeaderElector$LeaderChangeListener - New leader is 0
16:24:33.586 [main] INFO  k.u.ZkUtils$ - Registered broker 0 at path /brokers/ids/0 with address testing-worker-linux-docker-e9cd6d11-3391-linux-10.prod.travis-ci.org:52799.
16:24:33.586 [main] INFO  k.s.KafkaServer - [Kafka Server 0], started
16:24:33.587 [main] INFO  o.a.k.c.p.ProducerConfig - ProducerConfig values: 
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	block.on.buffer.full = true
	retry.backoff.ms = 100
	buffer.memory = 33554432
	batch.size = 16384
	metrics.sample.window.ms = 30000
	metadata.max.age.ms = 300000
	receive.buffer.bytes = 32768
	timeout.ms = 30000
	max.in.flight.requests.per.connection = 5
	bootstrap.servers = [localhost:52799]
	metric.reporters = []
	client.id = 
	compression.type = none
	retries = 0
	max.request.size = 1048576
	send.buffer.bytes = 131072
	acks = 1
	reconnect.backoff.ms = 10
	linger.ms = 0
	metrics.num.samples = 2
	metadata.fetch.timeout.ms = 1000

16:24:33.587 [ZkClient-EventThread-1169-127.0.0.1:60589] INFO  k.c.ReplicaStateMachine$BrokerChangeListener - [BrokerChangeListener on Controller 0]: Broker change listener fired for path /brokers/ids with children 0
16:24:33.594 [ZkClient-EventThread-1169-127.0.0.1:60589] INFO  k.c.ReplicaStateMachine$BrokerChangeListener - [BrokerChangeListener on Controller 0]: Newly added brokers: 0, deleted brokers: , all live brokers: 0
16:24:33.594 [ZkClient-EventThread-1169-127.0.0.1:60589] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Controller 0 connected to id:0,host:testing-worker-linux-docker-e9cd6d11-3391-linux-10.prod.travis-ci.org,port:52799 for sending state change requests
16:24:33.595 [main] INFO  k.s.KafkaServer - [Kafka Server 0], shutting down
16:24:33.595 [ZkClient-EventThread-1169-127.0.0.1:60589] INFO  k.c.KafkaController - [Controller 0]: New broker startup callback for 0
16:24:33.595 [Controller-0-to-broker-0-send-thread] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Starting 
16:24:33.595 [main] INFO  k.s.KafkaServer - [Kafka Server 0], Starting controlled shutdown
16:24:33.605 [kafka-request-handler-0] INFO  k.c.KafkaController - [Controller 0]: Shutting down broker 0
16:24:33.606 [main] INFO  k.s.KafkaServer - [Kafka Server 0], Controlled shutdown succeeded
16:24:33.606 [kafka-network-thread-52799-1] INFO  k.n.Processor - Closing socket connection to /172.17.5.158.
16:24:33.606 [main] INFO  k.n.SocketServer - [Socket Server on Broker 0], Shutting down
16:24:33.607 [main] INFO  k.n.SocketServer - [Socket Server on Broker 0], Shutdown completed
16:24:33.607 [main] INFO  k.s.KafkaRequestHandlerPool - [Kafka Request Handler on Broker 0], shutting down
16:24:33.608 [Curator-Framework-0-SendThread(127.0.0.1:33449)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:24:33.608 [main] INFO  k.s.KafkaRequestHandlerPool - [Kafka Request Handler on Broker 0], shut down completely
16:24:33.610 [Curator-Framework-0-SendThread(127.0.0.1:33245)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:24:33.613 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-e9cd6d11-3391-linux-10.prod.travis-ci.org/172.17.5.158
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
16:24:33.708 [Curator-Framework-0-SendThread(127.0.0.1:50763)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:24:33.714 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-e9cd6d11-3391-linux-10.prod.travis-ci.org/172.17.5.158
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
16:24:33.807 [Curator-Framework-0-SendThread(127.0.0.1:50639)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:24:33.815 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-e9cd6d11-3391-linux-10.prod.travis-ci.org/172.17.5.158
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
16:24:33.891 [Curator-Framework-0-SendThread(127.0.0.1:33163)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:24:33.917 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-e9cd6d11-3391-linux-10.prod.travis-ci.org/172.17.5.158
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
16:24:33.941 [Curator-Framework-0-SendThread(127.0.0.1:46779)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:24:33.971 [main] INFO  k.s.ReplicaManager - [Replica Manager on Broker 0]: Shut down
16:24:33.971 [main] INFO  k.s.ReplicaFetcherManager - [ReplicaFetcherManager on broker 0] shutting down
16:24:33.972 [main] INFO  k.s.ReplicaFetcherManager - [ReplicaFetcherManager on broker 0] shutdown completed
16:24:33.972 [main] INFO  k.s.ReplicaManager - [Replica Manager on Broker 0]: Shut down completely
16:24:33.972 [main] INFO  k.l.LogManager - Shutting down.
16:24:33.972 [main] INFO  k.l.LogManager - Shutdown complete.
16:24:33.972 [main] INFO  k.c.PartitionStateMachine - [Partition state machine on Controller 0]: Stopped partition state machine
16:24:33.972 [main] INFO  k.c.ReplicaStateMachine - [Replica state machine on controller 0]: Stopped replica state machine
16:24:33.973 [main] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Shutting down
16:24:33.973 [Controller-0-to-broker-0-send-thread] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Stopped 
16:24:33.973 [main] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Shutdown completed
16:24:33.973 [ZkClient-EventThread-1169-127.0.0.1:60589] INFO  o.I.z.ZkEventThread - Terminate ZkClient event thread.
16:24:33.975 [main] INFO  k.s.KafkaServer - [Kafka Server 0], shut down completed
16:24:33.975 [Curator-Framework-0] INFO  o.a.c.f.i.CuratorFrameworkImpl - backgroundOperationsLoop exiting

--------------------------------------------------
--------------------------------------------------
classname: org.apache.storm.kafka.bolt.KafkaBoltTest / testname: executeWithBoltSpecifiedProperties
java.lang.IllegalArgumentException: Spouts is not set
	at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:118)
	at org.apache.storm.kafka.bolt.KafkaBoltTest.generateTestTuple(KafkaBoltTest.java:290)
	at org.apache.storm.kafka.bolt.KafkaBoltTest.executeWithBoltSpecifiedProperties(KafkaBoltTest.java:199)

-------------------- system-out --------------------
16:24:34.018 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-e9cd6d11-3391-linux-10.prod.travis-ci.org/172.17.5.158
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
16:24:34.038 [Curator-Framework-0-SendThread(localhost:48389)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:24:34.046 [Curator-Framework-0-SendThread(localhost:51237)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:24:34.119 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-e9cd6d11-3391-linux-10.prod.travis-ci.org/172.17.5.158
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
16:24:34.137 [Curator-Framework-0] ERROR o.a.c.f.i.CuratorFrameworkImpl - Background operation retry gave up
org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:99) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.curator.framework.imps.CuratorFrameworkImpl.checkBackgroundRetry(CuratorFrameworkImpl.java:728) [curator-framework-2.10.0.jar:?]
	at org.apache.curator.framework.imps.CuratorFrameworkImpl.performBackgroundOperation(CuratorFrameworkImpl.java:857) [curator-framework-2.10.0.jar:?]
	at org.apache.curator.framework.imps.CuratorFrameworkImpl.backgroundOperationsLoop(CuratorFrameworkImpl.java:809) [curator-framework-2.10.0.jar:?]
	at org.apache.curator.framework.imps.CuratorFrameworkImpl.access$300(CuratorFrameworkImpl.java:64) [curator-framework-2.10.0.jar:?]
	at org.apache.curator.framework.imps.CuratorFrameworkImpl$4.call(CuratorFrameworkImpl.java:267) [curator-framework-2.10.0.jar:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:262) [?:1.7.0_76]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:178) [?:1.7.0_76]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:292) [?:1.7.0_76]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [?:1.7.0_76]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [?:1.7.0_76]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
16:24:34.137 [Curator-Framework-0] ERROR o.a.c.f.i.CuratorFrameworkImpl - Background retry gave up
org.apache.curator.CuratorConnectionLossException: KeeperErrorCode = ConnectionLoss
	at org.apache.curator.framework.imps.CuratorFrameworkImpl.performBackgroundOperation(CuratorFrameworkImpl.java:838) [curator-framework-2.10.0.jar:?]
	at org.apache.curator.framework.imps.CuratorFrameworkImpl.backgroundOperationsLoop(CuratorFrameworkImpl.java:809) [curator-framework-2.10.0.jar:?]
	at org.apache.curator.framework.imps.CuratorFrameworkImpl.access$300(CuratorFrameworkImpl.java:64) [curator-framework-2.10.0.jar:?]
	at org.apache.curator.framework.imps.CuratorFrameworkImpl$4.call(CuratorFrameworkImpl.java:267) [curator-framework-2.10.0.jar:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:262) [?:1.7.0_76]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:178) [?:1.7.0_76]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:292) [?:1.7.0_76]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [?:1.7.0_76]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [?:1.7.0_76]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
16:24:34.138 [Curator-Framework-0-SendThread(localhost:48389)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:24:34.147 [Curator-Framework-0-SendThread(localhost:51237)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:24:34.220 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-e9cd6d11-3391-linux-10.prod.travis-ci.org/172.17.5.158
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
16:24:34.322 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-e9cd6d11-3391-linux-10.prod.travis-ci.org/172.17.5.158
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
16:24:34.423 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-e9cd6d11-3391-linux-10.prod.travis-ci.org/172.17.5.158
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
16:24:34.524 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-e9cd6d11-3391-linux-10.prod.travis-ci.org/172.17.5.158
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
16:24:34.625 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-e9cd6d11-3391-linux-10.prod.travis-ci.org/172.17.5.158
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
16:24:34.710 [Curator-Framework-0-SendThread(127.0.0.1:33449)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:24:34.710 [Curator-Framework-0-SendThread(127.0.0.1:33245)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:24:34.727 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-e9cd6d11-3391-linux-10.prod.travis-ci.org/172.17.5.158
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
16:24:34.810 [Curator-Framework-0-SendThread(127.0.0.1:50763)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:24:34.828 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-e9cd6d11-3391-linux-10.prod.travis-ci.org/172.17.5.158
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
16:24:34.909 [Curator-Framework-0-SendThread(127.0.0.1:50639)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:24:34.929 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-e9cd6d11-3391-linux-10.prod.travis-ci.org/172.17.5.158
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
16:24:34.992 [Curator-Framework-0-SendThread(127.0.0.1:33163)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:24:34.996 [main] INFO  o.a.c.f.i.CuratorFrameworkImpl - Starting
16:24:34.997 [main] INFO  k.u.VerifiableProperties - Verifying properties
16:24:34.997 [main] INFO  k.u.VerifiableProperties - Property broker.id is overridden to 0
16:24:34.998 [main] INFO  k.u.VerifiableProperties - Property log.dirs is overridden to /tmp/kafka/logs/kafka-test-44065
16:24:34.998 [main] INFO  k.u.VerifiableProperties - Property port is overridden to 44065
16:24:34.998 [main] INFO  k.u.VerifiableProperties - Property zookeeper.connect is overridden to 127.0.0.1:38696
16:24:34.998 [main] INFO  k.s.KafkaServer - [Kafka Server 0], starting
16:24:34.998 [main] INFO  k.s.KafkaServer - [Kafka Server 0], Connecting to zookeeper on 127.0.0.1:38696
16:24:34.999 [ZkClient-EventThread-1207-127.0.0.1:38696] INFO  o.I.z.ZkEventThread - Starting ZkClient event thread.
16:24:35.000 [main-EventThread] INFO  o.a.c.f.s.ConnectionStateManager - State change: CONNECTED
16:24:35.000 [main-EventThread] INFO  o.I.z.ZkClient - zookeeper state changed (SyncConnected)
16:24:35.024 [main] INFO  k.l.LogManager - Log directory '/tmp/kafka/logs/kafka-test-44065' not found, creating it.
16:24:35.024 [main] INFO  k.l.LogManager - Loading logs.
16:24:35.025 [main] INFO  k.l.LogManager - Logs loading complete.
16:24:35.025 [main] INFO  k.l.LogManager - Starting log cleanup with a period of 300000 ms.
16:24:35.025 [main] INFO  k.l.LogManager - Starting log flusher with a default period of 9223372036854775807 ms.
16:24:35.027 [main] INFO  k.n.Acceptor - Awaiting socket connections on 0.0.0.0:44065.
16:24:35.030 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-e9cd6d11-3391-linux-10.prod.travis-ci.org/172.17.5.158
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
16:24:35.034 [main] INFO  k.n.SocketServer - [Socket Server on Broker 0], Started
16:24:35.037 [main] INFO  k.u.Mx4jLoader$ - Will not load MX4J, mx4j-tools.jar is not in the classpath
16:24:35.037 [main] INFO  k.c.KafkaController - [Controller 0]: Controller starting up
16:24:35.040 [main] INFO  k.s.ZookeeperLeaderElector - 0 successfully elected as leader
16:24:35.040 [main] INFO  k.c.KafkaController - [Controller 0]: Broker 0 starting become controller state transition
16:24:35.042 [main] INFO  k.c.KafkaController - [Controller 0]: Controller 0 incremented epoch to 1
16:24:35.042 [Curator-Framework-0-SendThread(127.0.0.1:46779)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:24:35.044 [main] INFO  k.c.KafkaController - [Controller 0]: Partitions undergoing preferred replica election: 
16:24:35.045 [main] INFO  k.c.KafkaController - [Controller 0]: Partitions that completed preferred replica election: 
16:24:35.045 [main] INFO  k.c.KafkaController - [Controller 0]: Resuming preferred replica election for partitions: 
16:24:35.045 [main] INFO  k.c.KafkaController - [Controller 0]: Partitions being reassigned: Map()
16:24:35.045 [main] INFO  k.c.KafkaController - [Controller 0]: Partitions already reassigned: List()
16:24:35.045 [main] INFO  k.c.KafkaController - [Controller 0]: Resuming reassignment of partitions: Map()
16:24:35.046 [main] INFO  k.c.KafkaController - [Controller 0]: List of topics to be deleted: 
16:24:35.046 [main] INFO  k.c.KafkaController - [Controller 0]: List of topics ineligible for deletion: 
16:24:35.046 [main] INFO  k.c.KafkaController - [Controller 0]: Currently active brokers in the cluster: Set()
16:24:35.046 [main] INFO  k.c.KafkaController - [Controller 0]: Currently shutting brokers in the cluster: Set()
16:24:35.046 [main] INFO  k.c.KafkaController - [Controller 0]: Current list of topics in the cluster: Set()
16:24:35.046 [main] INFO  k.c.ReplicaStateMachine - [Replica state machine on controller 0]: Started replica state machine with initial state -> Map()
16:24:35.046 [main] INFO  k.c.PartitionStateMachine - [Partition state machine on Controller 0]: Started partition state machine with initial state -> Map()
16:24:35.046 [main] INFO  k.c.KafkaController - [Controller 0]: Broker 0 is ready to serve as the new controller with epoch 1
16:24:35.046 [main] INFO  k.c.KafkaController - [Controller 0]: Starting preferred replica leader election for partitions 
16:24:35.046 [main] INFO  k.c.PartitionStateMachine - [Partition state machine on Controller 0]: Invoking state change to OnlinePartition for partitions 
16:24:35.048 [main] INFO  k.c.KafkaController - [Controller 0]: starting the partition rebalance scheduler
16:24:35.048 [main] INFO  k.c.KafkaController - [Controller 0]: Controller startup complete
16:24:35.049 [ZkClient-EventThread-1207-127.0.0.1:38696] INFO  k.s.ZookeeperLeaderElector$LeaderChangeListener - New leader is 0
16:24:35.052 [main] INFO  k.u.ZkUtils$ - Registered broker 0 at path /brokers/ids/0 with address testing-worker-linux-docker-e9cd6d11-3391-linux-10.prod.travis-ci.org:44065.
16:24:35.052 [main] INFO  k.s.KafkaServer - [Kafka Server 0], started
16:24:35.052 [main] INFO  o.a.k.c.p.ProducerConfig - ProducerConfig values: 
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	block.on.buffer.full = true
	retry.backoff.ms = 100
	buffer.memory = 33554432
	batch.size = 16384
	metrics.sample.window.ms = 30000
	metadata.max.age.ms = 300000
	receive.buffer.bytes = 32768
	timeout.ms = 30000
	max.in.flight.requests.per.connection = 5
	bootstrap.servers = [localhost:44065]
	metric.reporters = []
	client.id = 
	compression.type = none
	retries = 0
	max.request.size = 1048576
	send.buffer.bytes = 131072
	acks = 1
	reconnect.backoff.ms = 10
	linger.ms = 0
	metrics.num.samples = 2
	metadata.fetch.timeout.ms = 1000

16:24:35.055 [main] INFO  o.a.k.c.p.ProducerConfig - ProducerConfig values: 
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	block.on.buffer.full = true
	retry.backoff.ms = 100
	buffer.memory = 33554432
	batch.size = 16384
	metrics.sample.window.ms = 30000
	metadata.max.age.ms = 300000
	receive.buffer.bytes = 32768
	timeout.ms = 30000
	max.in.flight.requests.per.connection = 5
	bootstrap.servers = [localhost:44065]
	metric.reporters = []
	client.id = 
	compression.type = none
	retries = 0
	max.request.size = 1048576
	send.buffer.bytes = 131072
	acks = 1
	reconnect.backoff.ms = 10
	linger.ms = 0
	metrics.num.samples = 2
	metadata.fetch.timeout.ms = 1000

16:24:35.057 [main] INFO  k.s.KafkaServer - [Kafka Server 0], shutting down
16:24:35.058 [ZkClient-EventThread-1207-127.0.0.1:38696] INFO  k.c.ReplicaStateMachine$BrokerChangeListener - [BrokerChangeListener on Controller 0]: Broker change listener fired for path /brokers/ids with children 0
16:24:35.058 [main] INFO  k.s.KafkaServer - [Kafka Server 0], Starting controlled shutdown
16:24:35.063 [ZkClient-EventThread-1207-127.0.0.1:38696] INFO  k.c.ReplicaStateMachine$BrokerChangeListener - [BrokerChangeListener on Controller 0]: Newly added brokers: 0, deleted brokers: , all live brokers: 0
16:24:35.063 [ZkClient-EventThread-1207-127.0.0.1:38696] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Controller 0 connected to id:0,host:testing-worker-linux-docker-e9cd6d11-3391-linux-10.prod.travis-ci.org,port:44065 for sending state change requests
16:24:35.065 [ZkClient-EventThread-1207-127.0.0.1:38696] INFO  k.c.KafkaController - [Controller 0]: New broker startup callback for 0
16:24:35.065 [kafka-request-handler-0] INFO  k.c.KafkaController - [Controller 0]: Shutting down broker 0
16:24:35.065 [Controller-0-to-broker-0-send-thread] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Starting 
16:24:35.066 [main] INFO  k.s.KafkaServer - [Kafka Server 0], Controlled shutdown succeeded
16:24:35.066 [main] INFO  k.n.SocketServer - [Socket Server on Broker 0], Shutting down
16:24:35.066 [kafka-network-thread-44065-1] INFO  k.n.Processor - Closing socket connection to /172.17.5.158.
16:24:35.066 [main] INFO  k.n.SocketServer - [Socket Server on Broker 0], Shutdown completed
16:24:35.066 [main] INFO  k.s.KafkaRequestHandlerPool - [Kafka Request Handler on Broker 0], shutting down
16:24:35.067 [main] INFO  k.s.KafkaRequestHandlerPool - [Kafka Request Handler on Broker 0], shut down completely
16:24:35.131 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-e9cd6d11-3391-linux-10.prod.travis-ci.org/172.17.5.158
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
16:24:35.232 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-e9cd6d11-3391-linux-10.prod.travis-ci.org/172.17.5.158
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
16:24:35.239 [Curator-Framework-0-SendThread(localhost:48389)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:24:35.248 [Curator-Framework-0-SendThread(localhost:51237)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:24:35.333 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-e9cd6d11-3391-linux-10.prod.travis-ci.org/172.17.5.158
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
16:24:35.340 [Curator-Framework-0-SendThread(localhost:48389)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:24:35.349 [Curator-Framework-0-SendThread(localhost:51237)] WARN  o.a.z.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
16:24:35.434 [kafka-producer-network-thread | producer-11] WARN  o.a.k.c.n.Selector - Error in I/O with testing-worker-linux-docker-e9cd6d11-3391-linux-10.prod.travis-ci.org/172.17.5.158
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.7.0_76]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) ~[?:1.7.0_76]
	at org.apache.kafka.common.network.Selector.poll(Selector.java:238) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:191) [kafka-clients-0.8.2.1.jar:?]
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:122) [kafka-clients-0.8.2.1.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
16:24:35.436 [main] INFO  k.s.ReplicaManager - [Replica Manager on Broker 0]: Shut down
16:24:35.436 [main] INFO  k.s.ReplicaFetcherManager - [ReplicaFetcherManager on broker 0] shutting down
16:24:35.436 [main] INFO  k.s.ReplicaFetcherManager - [ReplicaFetcherManager on broker 0] shutdown completed
16:24:35.436 [main] INFO  k.s.ReplicaManager - [Replica Manager on Broker 0]: Shut down completely
16:24:35.436 [main] INFO  k.l.LogManager - Shutting down.
16:24:35.437 [main] INFO  k.l.LogManager - Shutdown complete.
16:24:35.437 [main] INFO  k.c.PartitionStateMachine - [Partition state machine on Controller 0]: Stopped partition state machine
16:24:35.437 [main] INFO  k.c.ReplicaStateMachine - [Replica state machine on controller 0]: Stopped replica state machine
16:24:35.437 [main] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Shutting down
16:24:35.437 [main] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Shutdown completed
16:24:35.438 [ZkClient-EventThread-1207-127.0.0.1:38696] INFO  o.I.z.ZkEventThread - Terminate ZkClient event thread.
16:24:35.438 [Controller-0-to-broker-0-send-thread] INFO  k.c.RequestSendThread - [Controller-0-to-broker-0-send-thread], Stopped 
16:24:35.439 [main] INFO  k.s.KafkaServer - [Kafka Server 0], shut down completed
16:24:35.440 [Curator-Framework-0] INFO  o.a.c.f.i.CuratorFrameworkImpl - backgroundOperationsLoop exiting

--------------------------------------------------
Looking for errors in ./external/storm-redis/target/surefire-reports
Checking ./external/storm-redis/target/surefire-reports/TEST-org.apache.storm.redis.state.RedisKeyValueStateProviderTest.xml
Checking ./external/storm-redis/target/surefire-reports/TEST-org.apache.storm.redis.state.DefaultStateSerializerTest.xml
Checking ./external/storm-redis/target/surefire-reports/TEST-org.apache.storm.redis.state.RedisKeyValueStateTest.xml

travis_time:end:160d6f40:start=1466699000213072883,finish=1466699245192451527,duration=244979378644[0K
[31;1mThe command "/bin/bash ./dev-tools/travis/travis-script.sh `pwd` $MODULES" exited with 1.[0m
travis_fold:start:cache.2[0Kstore build cache
travis_time:start:232be5a8[0K
travis_time:end:232be5a8:start=1466699245198196997,finish=1466699245202735396,duration=4538399[0Ktravis_time:start:223755e9[0K[32;1mchange detected (content changed, file is created, or file is deleted):
/home/travis/.m2/repository/eigenbase/eigenbase-properties/1.1.4/eigenbase-properties-1.1.4.pom.lastUpdated
/home/travis/.m2/repository/io/confluent/kafka-avro-serializer/1.0/kafka-avro-serializer-1.0.pom.lastUpdated
/home/travis/.m2/repository/io/confluent/kafka-schema-registry-client/1.0/kafka-schema-registry-client-1.0.pom.lastUpdated
/home/travis/.m2/repository/net/hydromatic/linq4j/0.4/linq4j-0.4.pom.lastUpdated
/home/travis/.m2/repository/net/hydromatic/quidem/0.1.1/quidem-0.1.1.pom.lastUpdated
/home/travis/.m2/repository/org/apache/storm/flux/2.0.0-SNAPSHOT/maven-metadata-local.xml
/home/travis/.m2/repository/org/apache/storm/flux/2.0.0-SNAPSHOT/_remote.repositories
/home/travis/.m2/repository/org/apache/storm/flux-core/2.0.0-SNAPSHOT/flux-core-2.0.0-SNAPSHOT.jar
/home/travis/.m2/repository/org/apache/storm/flux-core/2.0.0-SNAPSHOT/maven-metadata-local.xml
/home/travis/.m2/repository/org/apache/storm/flux-core/2.0.0-SNAPSHOT/_remote.repositories
/home/travis/.m2/repository/org/ap
[0m
[32;1m...
[0m
[32;1mchanges detected, packing new archive[0m
.
.
.
.
.
.
[32;1muploading archive[0m

travis_time:end:223755e9:start=1466699245207968141,finish=1466699290381963972,duration=45173995831[0Ktravis_fold:end:cache.2[0K
Done. Your build exited with 1.
